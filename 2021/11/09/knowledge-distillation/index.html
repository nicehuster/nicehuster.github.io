<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="google-site-verification" content="E3FpB-yqMjVKzHwdisKMpUn104x78HxTRcD4_v_URgc">







  <meta name="baidu-site-verification" content="wgGFRLcCc9">







  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Knowledge Distillation,">





  <link rel="alternate" href="/atom.xml" title="一起打怪升级呀" type="application/atom+xml">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2">






<meta name="description" content="知识蒸馏是一种模型压缩方法，是一种基于“教师-学生网络思想”的训练方法，由于其简单，有效，在工业界被广泛应用。这一技术的理论来自于2015年Hinton发表的一篇神作: Distilling the Knowledge in a Neural Network。Knowledge Distillation，简称KD，顾名思义，就是将已经训练好的模型包含的知识(”Knowledge”)，蒸馏(“Dis">
<meta name="keywords" content="Knowledge Distillation">
<meta property="og:type" content="article">
<meta property="og:title" content="知识蒸馏[转载]">
<meta property="og:url" content="https://blog.nicehuster.cn/2021/11/09/knowledge-distillation/index.html">
<meta property="og:site_name" content="一起打怪升级呀">
<meta property="og:description" content="知识蒸馏是一种模型压缩方法，是一种基于“教师-学生网络思想”的训练方法，由于其简单，有效，在工业界被广泛应用。这一技术的理论来自于2015年Hinton发表的一篇神作: Distilling the Knowledge in a Neural Network。Knowledge Distillation，简称KD，顾名思义，就是将已经训练好的模型包含的知识(”Knowledge”)，蒸馏(“Dis">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-f2fc2f02b87a38a9ff34a50664800045_720w.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-29a851c6fa9cc809e51ce738abbec2ce_720w.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-3d77281f38df62990c47d606dd581ee2_720w.jpg">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-a9e90626c5ac6f64a7e04c89f6ce3013_720w.jpg">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-d01f5142d06aa27bc5e207831b5131d9_720w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=v_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=z_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p%5ET_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=q%5ET_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=c_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=c_i%5Cin%5C%7B0%2C1%5C%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=N">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D%3D-%5Csum_j%5EN+p%5ET_j%5Clog%28q%5ET_j%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p%5ET_i%3D%5Cfrac%7B%5Cexp%28v_i%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28v_k%2FT%29%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=q%5ET_i%3D%5Cfrac%7B%5Cexp%28z_i%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%2FT%29%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L_%7Bhard%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L_%7Bhard%7D%3D-%5Csum_j%5EN+c_j%5Clog%28q%5E1_j%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=q%5E1_i%3D%5Cfrac%7B%5Cexp%28z_i%29%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%29%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L_%7Bhard%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=T">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D%3D-%5Csum_j%5EN+p%5ET_j%5Clog%28q%5ET_j%29%3D-%5Csum_j%5EN+%5Cfrac%7Bz_j%2FT%5Ctimes%5Cexp%28v_j%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28v_k%2FT%29%7D%5Cleft%28%5Cfrac%7B1%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%2FT%29%7D-%5Cfrac%7B%5Cexp+%28z_j+%2F+T%29+%7D%7B%5Cleft%28++%5Csum_k%5EN+%5Cexp%28z_k%2F+T%29%5Cright%29+%5E+2%7D%5Cright%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Capprox+-%5Cfrac%7B1%7D%7BT%5Csum_k%5EN+%5Cexp%28v_k%2FT%29%7D%5Cleft%28%5Cfrac%7B%5Csum_j%5ENz_j%5Cexp%28v_j%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%2FT%29%7D-%5Cfrac%7B%5Csum_j%5EN+z_j%5Cexp+%28z_j%2F+T%29%5Cexp%28v_j%2FT%29+%7D%7B%5Cleft%28++%5Csum_k%5EN+%5Cexp%28z_k+%2F+T%29%5Cright%29+%5E+2%7D+%5Cright%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L_%7Bhard%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L_%7Bhard%7D%3D-%5Csum_j%5EN+c_j%5Clog%28q%5E1_j%29%3D-%5Cleft%28%5Cfrac%7B%5Csum_j%5EN+c_jz_j+%7D%7B+%5Csum_k%5EN+%5Cexp%28z_k+%29%7D-%5Cfrac%7B%5Csum_j%5EN+c_jz_j%5Cexp+%28z_j%29+%7D%7B%5Cleft%28++%5Csum_k%5EN+%5Cexp%28z_k%29%5Cright%29+%5E+2%7D+%5Cright%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_i%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bhard%7D%7D%7B%5Cpartial+z_i%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BT%5E2%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=T%5E%7B2%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=T+%5Crightarrow+%5Cinfty">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=z_i">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_%7Bi%7D%7D%3D%5Cfrac%7B1%7D%7BT%7D%5Cleft%28q_%7Bi%7D-p_%7Bi%7D%5Cright%29%3D%5Cfrac%7B1%7D%7BT%7D%5Cleft%28%5Cfrac%7Be%5E%7Bz_%7Bi%7D+%2F+T%7D%7D%7B%5Csum_%7Bj%7D+e%5E%7Bz_%7Bj%7D+%2F+T%7D%7D-%5Cfrac%7Be%5E%7Bv_%7Bi%7D+%2F+T%7D%7D%7B%5Csum_%7Bj%7D+e%5E%7Bv_%7Bj%7D+%2F+T%7D%7D%5Cright%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=T+%5Crightarrow+%5Cinfty">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=1%2Bx%2FT">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=e%5E%7Bx%2FT%7D">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_%7Bi%7D%7D+%5Capprox+%5Cfrac%7B1%7D%7BT%7D%5Cleft%28%5Cfrac%7B1%2Bz_%7Bi%7D+%2F+T%7D%7BN%2B%5Csum_%7Bj%7D+z_%7Bj%7D+%2F+T%7D-%5Cfrac%7B1%2Bv_%7Bi%7D+%2F+T%7D%7BN%2B%5Csum_%7Bj%7D+v_%7Bj%7D+%2F+T%7D%5Cright%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csum_%7Bj%7D+z_%7Bj%7D%3D%5Csum_%7Bj%7D+v_%7Bj%7D%3D0">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_%7Bi%7D%7D+%5Capprox+%5Cfrac%7B1%7D%7BN+T%5E%7B2%7D%7D%5Cleft%28z_%7Bi%7D-v_%7Bi%7D%5Cright%29">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D%27%3D1+%2F+2%5Cleft%28z_%7Bi%7D-v_%7Bi%7D%5Cright%29%5E%7B2%7D">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-a120cc4bbb70b96968210b995b2e39d1_720w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=T%3D1+">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=T%3C1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=T%3E1">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=T%3D%5Cinfty">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=T%5Crightarrow0">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=argmax">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p_i">
<meta property="og:updated_time" content="2022-04-24T08:30:24.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="知识蒸馏[转载]">
<meta name="twitter:description" content="知识蒸馏是一种模型压缩方法，是一种基于“教师-学生网络思想”的训练方法，由于其简单，有效，在工业界被广泛应用。这一技术的理论来自于2015年Hinton发表的一篇神作: Distilling the Knowledge in a Neural Network。Knowledge Distillation，简称KD，顾名思义，就是将已经训练好的模型包含的知识(”Knowledge”)，蒸馏(“Dis">
<meta name="twitter:image" content="https://pic2.zhimg.com/80/v2-f2fc2f02b87a38a9ff34a50664800045_720w.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"always","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://blog.nicehuster.cn/2021/11/09/knowledge-distillation/">





  <title>知识蒸馏[转载] | 一起打怪升级呀</title>
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">一起打怪升级呀</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">别整太大鸭力,多鸡立自己qaq</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://blog.nicehuster.cn/2021/11/09/knowledge-distillation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="nicehuster">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/avatar/weichat.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="一起打怪升级呀">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">知识蒸馏[转载]</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2021-11-09T19:13:39+08:00">
                2021-11-09
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/paper-reading/" itemprop="url" rel="index">
                    <span itemprop="name">paper reading</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>知识蒸馏是一种模型压缩方法，是一种基于“教师-学生网络思想”的训练方法，由于其简单，有效，在工业界被广泛应用。这一技术的理论来自于2015年Hinton发表的一篇神作: <a href="https://arxiv.org/pdf/1503.02531.pdf" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a>。Knowledge Distillation，简称KD，顾名思义，就是将已经训练好的模型包含的知识(”Knowledge”)，蒸馏(“Distill”)提取到另一个模型里面去。</p>
<a id="more"></a>
<p>以下内容均转载于：<a href="https://zhuanlan.zhihu.com/p/102038521" target="_blank" rel="noopener">【经典简读】知识蒸馏(Knowledge Distillation) 经典之作</a> ，侵权删。</p>
<h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><h3 id="1-1-论文提出的背景"><a href="#1-1-论文提出的背景" class="headerlink" title="1.1. 论文提出的背景"></a>1.1. 论文提出的背景</h3><p>虽然在一般情况下，我们不会去区分训练和部署使用的模型，但是训练和部署之间存在着一定的不一致性:</p>
<ul>
<li>在训练过程中，我们需要使用复杂的模型，大量的计算资源，以便从非常大、高度冗余的数据集中提取出信息。在实验中，效果最好的模型往往规模很大，甚至由多个模型集成得到。而大模型不方便部署到服务中去，常见的瓶颈如下:</li>
</ul>
<ol>
<li>推断速度慢</li>
<li>对部署资源要求高(内存，显存等)</li>
</ol>
<ul>
<li>在部署时，我们对延迟以及计算资源都有着严格的限制。</li>
</ul>
<p>因此，模型压缩（在保证性能的前提下减少模型的参数量）成为了一个重要的问题。而”模型蒸馏“属于模型压缩的一种方法。</p>
<p><strong>插句题外话</strong>，我们可以从模型参数量和训练数据量之间的相对关系来理解underfitting和overfitting。AI领域的从业者可能对此已经习以为常，但是为了力求让小白也能读懂本文，还是引用我同事的解释（我印象很深）形象地说明一下:</p>
<blockquote>
<p>模型就像一个容器，训练数据中蕴含的知识就像是要装进容器里的水。当数据知识量(水量)超过模型所能建模的范围时(容器的容积)，加再多的数据也不能提升效果(水再多也装不进容器)，因为模型的表达空间有限(容器容积有限)，就会造成<strong>underfitting</strong>；而当模型的参数量大于已有知识所需要的表达空间时(容积大于水量，水装不满容器)，就会造成<strong>overfitting</strong>，即模型的variance会增大(想象一下摇晃半满的容器，里面水的形状是不稳定的)。</p>
</blockquote>
<h3 id="1-2-“思想歧路”"><a href="#1-2-“思想歧路”" class="headerlink" title="1.2. “思想歧路”"></a>1.2. “思想歧路”</h3><p>上面容器和水的比喻非常经典和贴切，但是会引起一个误解: 人们在直觉上会觉得，要保留相近的知识量，必须保留相近规模的模型。也就是说，一个模型的参数量基本决定了其所能捕获到的数据内蕴含的“知识”的量。</p>
<p>这样的想法是基本正确的，但是需要注意的是:</p>
<ol>
<li>模型的参数量和其所能捕获的“知识“量之间并非稳定的线性关系(下图中的1)，而是接近边际收益逐渐减少的一种增长曲线(下图中的2和3)</li>
<li>完全相同的模型架构和模型参数量，使用完全相同的训练数据，能捕获的“知识”量并不一定完全相同，另一个关键因素是训练的方法。合适的训练方法可以使得在模型参数总量比较小时，尽可能地获取到更多的“知识”(下图中的3与2曲线的对比).</li>
</ol>
<p><img src="https://pic2.zhimg.com/80/v2-f2fc2f02b87a38a9ff34a50664800045_720w.jpg" alt="img"></p>
<h2 id="2-知识蒸馏的理论依据"><a href="#2-知识蒸馏的理论依据" class="headerlink" title="2. 知识蒸馏的理论依据"></a>2. 知识蒸馏的理论依据</h2><h3 id="2-1-Teacher-Model和Student-Model"><a href="#2-1-Teacher-Model和Student-Model" class="headerlink" title="2.1. Teacher Model和Student Model"></a>2.1. Teacher Model和Student Model</h3><p>知识蒸馏使用的是Teacher—Student模型，其中teacher是“知识”的输出者，student是“知识”的接受者。知识蒸馏的过程分为2个阶段:</p>
<ol>
<li>原始模型训练: 训练”Teacher模型”, 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对”Teacher模型”不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值。</li>
<li>精简模型训练: 训练”Student模型”, 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值。</li>
</ol>
<p>在本论文中，作者将问题限定在<strong>分类问题</strong>下，或者其他本质上属于分类问题的问题，该类问题的共同点是模型最后会有一个softmax层，其输出值对应了相应类别的概率值。</p>
<h3 id="2-2-知识蒸馏的关键点"><a href="#2-2-知识蒸馏的关键点" class="headerlink" title="2.2. 知识蒸馏的关键点"></a>2.2. 知识蒸馏的关键点</h3><p>如果回归机器学习最最基础的理论，我们可以很清楚地意识到一点(而这一点往往在我们深入研究机器学习之后被忽略): <strong>机器学习最根本的目的</strong>在于训练出在某个问题上泛化能力强的模型。</p>
<ul>
<li><strong>泛化能力强</strong>: 在某问题的所有数据上都能很好地反应输入和输出之间的关系，无论是训练数据，还是测试数据，还是任何属于该问题的未知数据。</li>
</ul>
<p>而现实中，由于我们不可能收集到某问题的所有数据来作为训练数据，并且新数据总是在源源不断的产生，因此我们只能退而求其次，训练目标变成在已有的训练数据集上建模输入和输出之间的关系。由于训练数据集是对真实数据分布情况的采样，训练数据集上的最优解往往会多少偏离真正的最优解(这里的讨论不考虑模型容量)。</p>
<p>而在知识蒸馏时，由于我们已经有了一个泛化能力较强的Net-T，我们在利用Net-T来蒸馏训练Net-S时，可以直接让Net-S去学习Net-T的泛化能力。</p>
<p>一个很直白且高效的迁移泛化能力的方法就是：使用softmax层输出的类别的概率来作为“soft target”。</p>
<p><strong>【KD的训练过程和传统的训练过程的对比】</strong></p>
<ol>
<li>传统training过程(<strong>hard targets</strong>): 对ground truth求极大似然</li>
<li>KD的training过程(<strong>soft targets</strong>): 用large model的class probabilities作为soft targets</li>
</ol>
<p><img src="https://pic3.zhimg.com/80/v2-29a851c6fa9cc809e51ce738abbec2ce_720w.jpg" alt="img"></p>
<p>上图: Hard Target 下图: Soft Target</p>
<p><strong>KD的训练过程为什么更有效?</strong></p>
<p>softmax层的输出，除了正例之外，<strong>负标签也带有大量的信息</strong>，比如某些负标签对应的概率远远大于其他负标签。而在传统的训练过程(hard target)中，所有负标签都被统一对待。也就是说，<strong>KD的训练方式使得每个样本给Net-S带来的信息量大于传统的训练方式</strong>。</p>
<p>【<strong>举个例子】</strong></p>
<p>在手写体数字识别任务MNIST中，输出类别有10个。</p>
<p><img src="https://pic3.zhimg.com/80/v2-3d77281f38df62990c47d606dd581ee2_720w.jpg" alt="img"></p>
<p>假设某个输入的“2”更加形似”3”，softmax的输出值中”3”对应的概率为0.1，而其他负标签对应的值都很小，而另一个”2”更加形似”7”，”7”对应的概率为0.1。这两个”2”对应的hard target的值是相同的，但是它们的soft target却是不同的，由此我们可见soft target蕴含着比hard target多的信息。并且soft target分布的熵相对高时，其soft target蕴含的知识就更丰富。</p>
<p><img src="https://pic4.zhimg.com/80/v2-a9e90626c5ac6f64a7e04c89f6ce3013_720w.jpg" alt="img"></p>
<p>两个”2“的hard target相同而soft target不同</p>
<p>这就解释了为什么通过蒸馏的方法训练出的Net-S相比使用完全相同的模型结构和训练数据只使用hard target的训练方法得到的模型，拥有更好的泛化能力。</p>
<h3 id="2-3-softmax函数"><a href="#2-3-softmax函数" class="headerlink" title="2.3. softmax函数"></a>2.3. softmax函数</h3><p>先回顾一下原始的softmax函数:</p>
<script type="math/tex; mode=display">
q_{i}=\frac{\exp \left(z_{i}\right)}{\sum_{j} \exp \left(z_{j}\right)}</script><p>但要是直接使用softmax层的输出值作为soft target, 这又会带来一个问题: 当softmax输出的概率分布熵相对较小时，负标签的值都很接近0，对损失函数的贡献非常小，小到可以忽略不计。因此<strong>“温度”</strong>这个变量就派上了用场。</p>
<p>下面的公式时加了温度这个变量之后的softmax函数:</p>
<script type="math/tex; mode=display">
q_{i}=\frac{\exp \left(z_{i} / T\right)}{\sum_{j} \exp \left(z_{j} / T\right)}</script><ul>
<li>这里的T就是<strong>温度</strong>。</li>
<li>原来的softmax函数是T = 1的特例。 T越高，softmax的output probability distribution越趋于平滑，其分布的熵越大，负标签携带的信息会被相对地放大，模型训练将更加关注负标签。</li>
</ul>
<h2 id="3-知识蒸馏的具体方法"><a href="#3-知识蒸馏的具体方法" class="headerlink" title="3. 知识蒸馏的具体方法"></a>3. 知识蒸馏的具体方法</h2><h3 id="3-1-通用的知识蒸馏方法"><a href="#3-1-通用的知识蒸馏方法" class="headerlink" title="3.1. 通用的知识蒸馏方法"></a>3.1. 通用的知识蒸馏方法</h3><ul>
<li><strong>第一步</strong>是训练Net-T；<strong>第二步</strong>是在高温T下，蒸馏Net-T的知识到Net-S</li>
</ul>
<p><img src="https://pic2.zhimg.com/80/v2-d01f5142d06aa27bc5e207831b5131d9_720w.jpg" alt="img"></p>
<p>知识蒸馏示意图(来自<a href="https://nervanasystems.github.io/distiller/knowledge_distillation.html" target="_blank" rel="noopener">https://nervanasystems.github.io/distiller/knowledge_distillation.html</a>)</p>
<p>训练Net-T的过程很简单，下面详细讲讲第二步:高温蒸馏的过程。高温蒸馏过程的目标函数由distill loss(对应soft target)和student loss(对应hard target)加权得到。示意图如上。</p>
<script type="math/tex; mode=display">
L=\alpha L_{s o f t}+\beta L_{h a r d}</script><ul>
<li><img src="https://www.zhihu.com/equation?tex=v_i" alt="[公式]">: Net-T的logits</li>
<li><img src="https://www.zhihu.com/equation?tex=z_i" alt="[公式]">: Net-S的logits</li>
<li><img src="https://www.zhihu.com/equation?tex=p%5ET_i" alt="[公式]">: Net-T的在温度=T下的softmax输出在第i类上的值</li>
<li><img src="https://www.zhihu.com/equation?tex=q%5ET_i" alt="[公式]">: Net-S的在温度=T下的softmax输出在第i类上的值</li>
<li><img src="https://www.zhihu.com/equation?tex=c_i" alt="[公式]">: 在第i类上的ground truth值, <img src="https://www.zhihu.com/equation?tex=c_i%5Cin%5C%7B0%2C1%5C%7D" alt="[公式]">, 正标签取1，负标签取0.</li>
<li><img src="https://www.zhihu.com/equation?tex=N" alt="[公式]">: 总标签数量</li>
<li>Net-T 和 Net-S同时输入 transfer set (这里可以直接复用训练Net-T用到的training set), 用Net-T产生的softmax distribution (with high temperature) 来作为soft target，Net-S在相同温度T条件下的softmax输出和soft target的cross entropy就是<strong>Loss函数的第一部分</strong> <img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D" alt="[公式]"></li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D%3D-%5Csum_j%5EN+p%5ET_j%5Clog%28q%5ET_j%29" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=p%5ET_i%3D%5Cfrac%7B%5Cexp%28v_i%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28v_k%2FT%29%7D" alt="[公式]"> , <img src="https://www.zhihu.com/equation?tex=q%5ET_i%3D%5Cfrac%7B%5Cexp%28z_i%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%2FT%29%7D" alt="[公式]"></p>
<ul>
<li>Net-S在T=1的条件下的softmax输出和ground truth的cross entropy就是<strong>Loss函数的第二部分</strong> <img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D" alt="[公式]"> 。</li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D%3D-%5Csum_j%5EN+c_j%5Clog%28q%5E1_j%29" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=q%5E1_i%3D%5Cfrac%7B%5Cexp%28z_i%29%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%29%7D" alt="[公式]"></p>
<ul>
<li>第二部分Loss <img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D" alt="[公式]"> 的必要性其实很好理解: Net-T也有一定的错误率，使用ground truth可以有效降低错误被传播给Net-S的可能。打个比方，老师虽然学识远远超过学生，但是他仍然有出错的可能，而这时候如果学生在老师的教授之外，可以同时参考到标准答案，就可以有效地降低被老师偶尔的错误“带偏”的可能性。</li>
</ul>
<p><strong>【讨论】</strong></p>
<ul>
<li>实验发现第二部分所占比重比较小的时候，能产生最好的结果，这是一个经验的结论。一个可能的原因是，由于soft target产生的gradient与hard target产生的gradient之间有与 <img src="https://www.zhihu.com/equation?tex=T" alt="[公式]"> 相关的比值。原论文中只是一笔带过，我在下面补充了一些简单的推导。(ps. 下面推导可能有些错误，如果有读者能够正确推出来请私信我～)</li>
<li><strong>Soft Target:</strong><img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D" alt="[公式]"></li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D%3D-%5Csum_j%5EN+p%5ET_j%5Clog%28q%5ET_j%29%3D-%5Csum_j%5EN+%5Cfrac%7Bz_j%2FT%5Ctimes%5Cexp%28v_j%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28v_k%2FT%29%7D%5Cleft%28%5Cfrac%7B1%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%2FT%29%7D-%5Cfrac%7B%5Cexp+%28z_j+%2F+T%29+%7D%7B%5Cleft%28++%5Csum_k%5EN+%5Cexp%28z_k%2F+T%29%5Cright%29+%5E+2%7D%5Cright%29" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Capprox+-%5Cfrac%7B1%7D%7BT%5Csum_k%5EN+%5Cexp%28v_k%2FT%29%7D%5Cleft%28%5Cfrac%7B%5Csum_j%5ENz_j%5Cexp%28v_j%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%2FT%29%7D-%5Cfrac%7B%5Csum_j%5EN+z_j%5Cexp+%28z_j%2F+T%29%5Cexp%28v_j%2FT%29+%7D%7B%5Cleft%28++%5Csum_k%5EN+%5Cexp%28z_k+%2F+T%29%5Cright%29+%5E+2%7D+%5Cright%29" alt="[公式]"></p>
<ul>
<li><strong>Hard Target:</strong> <img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D" alt="[公式]"></li>
</ul>
<p><img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D%3D-%5Csum_j%5EN+c_j%5Clog%28q%5E1_j%29%3D-%5Cleft%28%5Cfrac%7B%5Csum_j%5EN+c_jz_j+%7D%7B+%5Csum_k%5EN+%5Cexp%28z_k+%29%7D-%5Cfrac%7B%5Csum_j%5EN+c_jz_j%5Cexp+%28z_j%29+%7D%7B%5Cleft%28++%5Csum_k%5EN+%5Cexp%28z_k%29%5Cright%29+%5E+2%7D+%5Cright%29" alt="[公式]"></p>
<ul>
<li>由于 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_i%7D" alt="[公式]">的magnitude大约是 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bhard%7D%7D%7B%5Cpartial+z_i%7D" alt="[公式]"> 的 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BT%5E2%7D" alt="[公式]"> ，因此在同时使用soft target和hard target的时候，需要在soft target之前乘上<img src="https://www.zhihu.com/equation?tex=T%5E%7B2%7D" alt="[公式]">的系数，这样才能保证soft target和hard target贡献的梯度量基本一致。</li>
</ul>
<p><strong>【注意】</strong> 在Net-S训练完毕后，做inference时其softmax的温度T要恢复到1.</p>
<h3 id="3-2-一种特殊情形-直接match-logits-不经过softmax"><a href="#3-2-一种特殊情形-直接match-logits-不经过softmax" class="headerlink" title="3.2. 一种特殊情形: 直接match logits(不经过softmax)"></a>3.2. 一种特殊情形: 直接match logits(不经过softmax)</h3><p>直接match logits指的是，直接使用softmax层的输入logits（而不是输出）作为soft targets，需要最小化的目标函数是Net-T和Net-S的logits之间的平方差。</p>
<p><strong>直接上结论: 直接match logits的做法是</strong> <img src="https://www.zhihu.com/equation?tex=T+%5Crightarrow+%5Cinfty" alt="[公式]"> <strong>的情况下的特殊情形。</strong></p>
<p>由单个case贡献的loss，推算出对应在Net-S每个logit <img src="https://www.zhihu.com/equation?tex=z_i" alt="[公式]"> 上的gradient:</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_%7Bi%7D%7D%3D%5Cfrac%7B1%7D%7BT%7D%5Cleft%28q_%7Bi%7D-p_%7Bi%7D%5Cright%29%3D%5Cfrac%7B1%7D%7BT%7D%5Cleft%28%5Cfrac%7Be%5E%7Bz_%7Bi%7D+%2F+T%7D%7D%7B%5Csum_%7Bj%7D+e%5E%7Bz_%7Bj%7D+%2F+T%7D%7D-%5Cfrac%7Be%5E%7Bv_%7Bi%7D+%2F+T%7D%7D%7B%5Csum_%7Bj%7D+e%5E%7Bv_%7Bj%7D+%2F+T%7D%7D%5Cright%29" alt="[公式]"></p>
<p>当 <img src="https://www.zhihu.com/equation?tex=T+%5Crightarrow+%5Cinfty" alt="[公式]"> 时，我们使用 <img src="https://www.zhihu.com/equation?tex=1%2Bx%2FT" alt="[公式]"> 来近似 <img src="https://www.zhihu.com/equation?tex=e%5E%7Bx%2FT%7D" alt="[公式]"> ，于是得到</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_%7Bi%7D%7D+%5Capprox+%5Cfrac%7B1%7D%7BT%7D%5Cleft%28%5Cfrac%7B1%2Bz_%7Bi%7D+%2F+T%7D%7BN%2B%5Csum_%7Bj%7D+z_%7Bj%7D+%2F+T%7D-%5Cfrac%7B1%2Bv_%7Bi%7D+%2F+T%7D%7BN%2B%5Csum_%7Bj%7D+v_%7Bj%7D+%2F+T%7D%5Cright%29" alt="[公式]"></p>
<p>如果再加上logits是零均值的假设</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bj%7D+z_%7Bj%7D%3D%5Csum_%7Bj%7D+v_%7Bj%7D%3D0" alt="[公式]"></p>
<p>那么上面的公式可以简化成</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_%7Bi%7D%7D+%5Capprox+%5Cfrac%7B1%7D%7BN+T%5E%7B2%7D%7D%5Cleft%28z_%7Bi%7D-v_%7Bi%7D%5Cright%29" alt="[公式]"></p>
<p>也就是等价于minimise下面的损失函数</p>
<p><img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D%27%3D1+%2F+2%5Cleft%28z_%7Bi%7D-v_%7Bi%7D%5Cright%29%5E%7B2%7D" alt="[公式]"></p>
<h2 id="4-关于”温度”的讨论"><a href="#4-关于”温度”的讨论" class="headerlink" title="4. 关于”温度”的讨论"></a>4. 关于”温度”的讨论</h2><p>【问题】 我们都知道“蒸馏”需要在高温下进行，那么这个“蒸馏”的温度代表了什么，又是如何选取合适的温度？</p>
<p><img src="https://pic2.zhimg.com/80/v2-a120cc4bbb70b96968210b995b2e39d1_720w.jpg" alt="img">随着温度T的增大，概率分布的熵逐渐增大</p>
<h3 id="4-1-温度的特点"><a href="#4-1-温度的特点" class="headerlink" title="4.1. 温度的特点"></a>4.1. 温度的特点</h3><p>在回答这个问题之前，先讨论一下<strong>温度T的特点</strong></p>
<ol>
<li>原始的softmax函数是 <img src="https://www.zhihu.com/equation?tex=T%3D1+" alt="[公式]"> 时的特例， <img src="https://www.zhihu.com/equation?tex=T%3C1" alt="[公式]"> 时，概率分布比原始更“陡峭”， <img src="https://www.zhihu.com/equation?tex=T%3E1" alt="[公式]"> 时，概率分布比原始更“平缓”。</li>
<li>温度越高，softmax上各个值的分布就越平均（思考极端情况: (i) <img src="https://www.zhihu.com/equation?tex=T%3D%5Cinfty" alt="[公式]"> , 此时softmax的值是平均分布的；(ii) <img src="https://www.zhihu.com/equation?tex=T%5Crightarrow0" alt="[公式]">，此时softmax的值就相当于 <img src="https://www.zhihu.com/equation?tex=argmax" alt="[公式]"> , 即最大的概率处的值趋近于1，而其他值趋近于0）</li>
<li>不管温度T怎么取值，Soft target都有忽略相对较小的 <img src="https://www.zhihu.com/equation?tex=p_i" alt="[公式]"> 携带的信息的倾向</li>
</ol>
<h3 id="4-2-温度代表了什么，如何选取合适的温度？"><a href="#4-2-温度代表了什么，如何选取合适的温度？" class="headerlink" title="4.2. 温度代表了什么，如何选取合适的温度？"></a><strong>4.2. 温度代表了什么，如何选取合适的温度？</strong></h3><p><strong>温度的高低改变的是Net-S训练过程中对负标签的关注程度</strong>: 温度较低时，对负标签的关注，尤其是那些显著低于平均值的负标签的关注较少；而温度较高时，负标签相关的值会相对增大，Net-S会相对多地关注到负标签。</p>
<p>实际上，负标签中包含一定的信息，尤其是那些值显著<strong>高于</strong>平均值的负标签。但由于Net-T的训练过程决定了负标签部分比较noisy，并且负标签的值越低，其信息就越不可靠。因此温度的选取比较empirical，本质上就是在下面两件事之中取舍:</p>
<ol>
<li>从有部分信息量的负标签中学习 —&gt; 温度要高一些</li>
<li>防止受负标签中噪声的影响 —&gt;温度要低一些</li>
</ol>
<p>总的来说，T的选择和Net-S的大小有关，Net-S参数量比较小的时候，相对比较低的温度就可以了（因为参数量小的模型不能capture all knowledge，所以可以适当忽略掉一些负标签的信息）</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Knowledge-Distillation/" rel="tag"># Knowledge Distillation</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2021/08/12/swin/" rel="next" title="Swin-transformer">
                <i class="fa fa-chevron-left"></i> Swin-transformer
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2021/12/05/transformer-soup/" rel="prev" title="Transformer 杂记">
                Transformer 杂记 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/avatar/weichat.jpg" alt="nicehuster">
          <p class="site-author-name" itemprop="name">nicehuster</p>
           
              <p class="site-description motion-element" itemprop="description">欢迎来到小白(@nicehuster)的博客。</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">112</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">55</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/nicehuster" target="_blank" title="Github">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      Github
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://blog.csdn.net/auto1993" target="_blank" title="CSDN">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                    
                      CSDN
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://weibo.com/u/3335530510" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                    
                      Weibo
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-介绍"><span class="nav-number">1.</span> <span class="nav-text">1. 介绍</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-论文提出的背景"><span class="nav-number">1.1.</span> <span class="nav-text">1.1. 论文提出的背景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-“思想歧路”"><span class="nav-number">1.2.</span> <span class="nav-text">1.2. “思想歧路”</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-知识蒸馏的理论依据"><span class="nav-number">2.</span> <span class="nav-text">2. 知识蒸馏的理论依据</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Teacher-Model和Student-Model"><span class="nav-number">2.1.</span> <span class="nav-text">2.1. Teacher Model和Student Model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-知识蒸馏的关键点"><span class="nav-number">2.2.</span> <span class="nav-text">2.2. 知识蒸馏的关键点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-softmax函数"><span class="nav-number">2.3.</span> <span class="nav-text">2.3. softmax函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-知识蒸馏的具体方法"><span class="nav-number">3.</span> <span class="nav-text">3. 知识蒸馏的具体方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-通用的知识蒸馏方法"><span class="nav-number">3.1.</span> <span class="nav-text">3.1. 通用的知识蒸馏方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-一种特殊情形-直接match-logits-不经过softmax"><span class="nav-number">3.2.</span> <span class="nav-text">3.2. 一种特殊情形: 直接match logits(不经过softmax)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-关于”温度”的讨论"><span class="nav-number">4.</span> <span class="nav-text">4. 关于”温度”的讨论</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-温度的特点"><span class="nav-number">4.1.</span> <span class="nav-text">4.1. 温度的特点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-温度代表了什么，如何选取合适的温度？"><span class="nav-number">4.2.</span> <span class="nav-text">4.2. 温度代表了什么，如何选取合适的温度？</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">nicehuster</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  





  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
