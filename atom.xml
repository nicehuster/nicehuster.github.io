<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一起打怪升级呀</title>
  <icon>https://www.gravatar.com/avatar/2555127dc0de830d31ceeb98d8565ac8</icon>
  <subtitle>别整太大鸭力,多鸡立自己qaq</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://blog.nicehuster.cn/"/>
  <updated>2022-04-13T04:30:07.119Z</updated>
  <id>https://blog.nicehuster.cn/</id>
  
  <author>
    <name>nicehuster</name>
    <email>nicehuster@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>增量目标检测方法 Faster ILOD</title>
    <link href="https://blog.nicehuster.cn/2022/03/24/Faster_ILOD/"/>
    <id>https://blog.nicehuster.cn/2022/03/24/Faster_ILOD/</id>
    <published>2022-03-24T11:13:39.000Z</published>
    <updated>2022-04-13T04:30:07.119Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/abs/2003.03901" target="_blank" rel="noopener">Faster ILOD: Incremental Learning for Object Detectors based on Faster RCNN</a><br><strong>代码链接：</strong><a href="https://github.com/CanPeng123/Faster-ILOD" target="_blank" rel="noopener">https://github.com/CanPeng123/Faster-ILOD</a><br><strong>整体信息：</strong>这是发表在PRL2020上的一篇文章关于增量目标检测的文章，作者是来自The University of Queensland，这篇文章基于Faster RCNN，使用multi-network 自适应蒸馏，设计了一种end2end的增量目标检测方法。</p><a id="more"></a><h2 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h2><p>增量目标检测包含S个增量steps，在每个增量step，训练数据只包含新类别Cn ，给定一个在旧类别 C0 上已训练好的目标检测模型，增量目标检测的任务是在新类别 Cn 数据上重新训练模型(一般是fine-tune)，同时维持模型在旧类别 C0 上的性能，在增量训练过程中，旧类别 C0 不可见。</p><p><img src="/img/faster_ilod_1.png" alt="img"></p><p>上图展示的是一个在VOC数据集上的增量目标检测的示例，模型首先在前15类训练，然后逐步增加每个类别。增量训练过程中，只提供当前新类别的标注，其他类别不可见。Normal Training是使用所有数据（旧数据和新数据）从头开始重新训练模型，该模型在测试集上的指标即为增量目标检测的指标上界。 Catastrophic forgetting是使用已训练好的旧类模型直接在新类数据上fine-tune的结果，可以看到在不断进行增量训练时，总体指标在逐渐下降，即出现了灾难性遗忘的问题( Catastrophic forgetting)。</p><h2 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h2><p><img src="/img/faster_ilod_method.png" alt="img"></p><p>作者使用Multi-network自适应蒸馏的方法来解决增量目标检测中出现的Catastrophic forgetting问题。具体做法如上图所示，上面是旧模型即teacher模型T，下面是新模型即student模型S，中间为蒸馏过程。</p><p>（1）<strong>特征蒸馏</strong>，所有feature map进行减均值归一化，然后使用L1 loss进行蒸馏。具体地，针对特征图上每个激活值大小进行比较，确定是否蒸馏，如下公式所示，</p><script type="math/tex; mode=display">\mathcal{L}_{F_{-} D i s t}=\frac{1}{\mathcal{M}} \sum \begin{cases}\left\|\tilde{f}_{t e}-\tilde{f}_{s t}\right\|_{1}, & \text { if } \tilde{f}_{t e}>\tilde{f}_{s t} \\ 0, & \text { otherwise }\end{cases}</script><p>（2）<strong>RPN蒸馏</strong>，同样地，使用T模型RPN的输出作为下界进行自适应蒸馏，使用的是L2 loss。</p><script type="math/tex; mode=display">\begin{aligned} &\mathcal{L}_{R P N_{-} D i s t}=\frac{1}{\mathcal{N}} \sum \begin{cases}\left\|q_{t e}-q_{s t}\right\|_{2}^{2}+\beta\left\|r_{t e}-r_{s t}\right\|_{2}^{2}, & \text { if } q_{t e}>q_{s t} \\ 0, & \text { otherwise }\end{cases} \\ &\text { where } \\ &\beta= \begin{cases}1, & \text { if } q_{t e}>\left(q_{s t}+\mathcal{T}\right) \\ 0, & \text { otherwise. }\end{cases} \end{aligned}</script><p>（3）<strong>RCNN蒸馏</strong>，具体做法和ILOD做法一致，在T模型中背景分数最小的128个ROI中随机选择64个proposal进行蒸馏，即只蒸馏背景信息，新类别不参与RCNN蒸馏，具体地，如下公式所示，</p><script type="math/tex; mode=display">\mathcal{L}_{R C N_{-} D i s t}=\frac{1}{\mathcal{K} \times C_{o}} \sum\left[\left\|\tilde{p}_{t e}-\tilde{p}_{s t}\right\|_{2}^{2}+\left\|t_{t e}-t_{s t}\right\|_{2}^{2}\right]</script><p>最后，总的loss为三者相加，</p><script type="math/tex; mode=display">\mathcal{L}_{\text {total }}=\mathcal{L}_{R C N N}+\lambda_{1} \mathcal{L}_{F_{-} \text {Dist }}+\lambda_{2} \mathcal{L}_{R P N_{-} \text {Dist }}+\lambda_{3} \mathcal{L}_{R C N_{-} \text {Dist }}</script><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ul><li>数据集(指标)：PASCAL VOC(AP@0.5)，COCO(mAP)</li><li>settings：one-step 和 multi-step</li><li><p>对比方法，ILOD</p></li><li><p>对比方法，ILOD</p></li></ul><p><img src="/img/faster_ilod_exp1.png" alt="img"></p><p><img src="/img/faster_ilod_exp2.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2003.03901&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Faster ILOD: Incremental Learning for Object Detectors based on Faster RCNN&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/CanPeng123/Faster-ILOD&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/CanPeng123/Faster-ILOD&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这是发表在PRL2020上的一篇文章关于增量目标检测的文章，作者是来自The University of Queensland，这篇文章基于Faster RCNN，使用multi-network 自适应蒸馏，设计了一种end2end的增量目标检测方法。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="Continual learning" scheme="https://blog.nicehuster.cn/tags/Continual-learning/"/>
    
  </entry>
  
  <entry>
    <title>增量目标检测方法调研</title>
    <link href="https://blog.nicehuster.cn/2022/03/21/IOD/"/>
    <id>https://blog.nicehuster.cn/2022/03/21/IOD/</id>
    <published>2022-03-21T11:13:39.000Z</published>
    <updated>2022-04-12T11:36:14.547Z</updated>
    
    <content type="html"><![CDATA[<p>最近参加了CVPR2022 有关增量学习的一个Workshop：<a href="https://sites.google.com/view/clvision2022/overview" target="_blank" rel="noopener">Workshop on Continual Learning in Computer Vision</a>，这个workshop有三个赛道：(1)Instance Classification Track，(2)Category Detection Track；(3)Instance Detection Track，三个赛道的任务介绍可以看官网。个人是做检测方向出身，因此关注了一下track2，并基于增量目标检测方向做了部分调研，并简单介绍其中部分算法。</p><a id="more"></a><h3 id="增量目标检测方法调研"><a href="#增量目标检测方法调研" class="headerlink" title="增量目标检测方法调研"></a>增量目标检测方法调研</h3><p><img src="/img/iod_survey.png" alt="img"></p><h3 id="部分方法介绍"><a href="#部分方法介绍" class="headerlink" title="部分方法介绍"></a>部分方法介绍</h3><p><img src="/img/IOD_1.PNG" alt="IOD (1)"></p><p><img src="/img/IOD_2.PNG" alt="IOD (2)"></p><p><img src="/img/IOD_3.PNG" alt="IOD (3)"></p><p><img src="/img/IOD_4.PNG" alt="IOD (4)"></p><p><img src="/img/IOD_5.PNG" alt="IOD (5)"></p><p><img src="/img/IOD_6.PNG" alt="IOD (6)"></p><p><img src="/img/IOD_7.PNG" alt="IOD (7)"></p><p><img src="/img/IOD_8.PNG" alt="IOD (8)"></p><p><img src="/img/IOD_9.PNG" alt="IOD (9)"></p><p><img src="/img/IOD_10.PNG" alt="IOD (10)"></p><p><img src="/img/IOD_11.PNG" alt="IOD (11)"></p><p><img src="/img/IOD_12.PNG" alt="IOD (12)"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近参加了CVPR2022 有关增量学习的一个Workshop：&lt;a href=&quot;https://sites.google.com/view/clvision2022/overview&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Workshop on Continual Learning in Computer Vision&lt;/a&gt;，这个workshop有三个赛道：(1)Instance Classification Track，(2)Category Detection Track；(3)Instance Detection Track，三个赛道的任务介绍可以看官网。个人是做检测方向出身，因此关注了一下track2，并基于增量目标检测方向做了部分调研，并简单介绍其中部分算法。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="Continual learning" scheme="https://blog.nicehuster.cn/tags/Continual-learning/"/>
    
  </entry>
  
  <entry>
    <title>Panoptic Segmentation详解</title>
    <link href="https://blog.nicehuster.cn/2022/01/07/PSeg/"/>
    <id>https://blog.nicehuster.cn/2022/01/07/PSeg/</id>
    <published>2022-01-07T11:13:39.000Z</published>
    <updated>2022-04-12T04:35:47.174Z</updated>
    
    <content type="html"><![CDATA[<p>最近在知乎上频繁地刷到有关Mask2Former地帖子，这么多人吹捧地必是精品，就跟一波风看了一下Mask2Former，顺带地也了解一下Panoptic Segmentation这个任务。众所周知，图像分割主要有两个方向：</p><ul><li><strong>语义分割（semantic segmentation）</strong>，常用来识别天空、草地、道路等没有固定形状的不可数<strong>事物（stuff）</strong>。语义分割的标记方法通常是给每个像素加上标签。</li><li><strong>实例分割（instance segmentation）</strong>，人、动物或工具等可数的、独立的明显物体<strong>（things）</strong>。实例分割通常用包围盒或分割掩码标记目标。</li></ul><p><strong>全景分割（Panoptic Segmentation）</strong>其实就是把这两个方向结合起来，生成统一的、全局的分割图像，既识别事物，也识别物体。</p><p><img src="/img/ps.png" alt="img"></p><a id="more"></a><p><strong>标记方法</strong></p><p>全景分割的标记方法结合了语义分割和实例分割，给每个像素加上标签$\left(l_{i}, z_{i}\right)$,其中i表示第i个像素，l表示语义类别，z表示实例ID。语义类别由两部分组成，事物类别$L^{ST}$和$L^{TH}$分别为stuff和thing的简写）。当$l_{i} \in L^{S T}$，忽略$z_i$（事物类别）；</p><p><strong>评估标准</strong></p><p>首先是常规的IoU &gt; 0.5，然后结合TF、FN、FP搞出了一个PQ标准。（PQ是Panoptic Quality，即全景质量的简称。)</p><p><img src="/img/metric.png" alt="img"></p><p>PQ的具体公式为：</p><script type="math/tex; mode=display">\mathrm{PQ}=\frac{\sum_{(p, g) \in T P} \operatorname{IoU}(p, g)}{|T P|+\frac{1}{2}|F P|+\frac{1}{2}|F N|}</script><p>另外，PQ可以分解为<strong>分割质量（segmentation quality，SQ）</strong>和<strong>识别质量（recognition quality，RQ）</strong>的乘积，便于进一步评估分割和识别环节的表现。</p><script type="math/tex; mode=display">\mathrm{PQ}=\underbrace{\frac{\sum_{(p, g) \in T P} \operatorname{IoU}(p, g)}{|T P|}}_{\text {segmentation quality }(\mathrm{SQ})} \times \underbrace{\frac{|T P|}{|T P|+\frac{1}{2}|F P|+\frac{1}{2}|F N|}}_{\text {recognition quality }(\mathrm{RQ})}</script><p><strong>数据集</strong></p><p>全景分割数据集需要既有语义分割标注，也有实例分割标注。</p><ul><li>Cityscapes(19classes)：5000张街景图片，97%的图片有像素标注，共有19个类别，其中8个类别符合语义分割的特征；</li><li>ADE20k(150classes)：图像总量超过25000张，并经过公开标注。其中包括100种物体和59种事物。</li><li>Mapillary Vistas(65classes)：25000张分辨率不同的街景照片。其中98%的图片都经过了像素标注，涵盖28种事物与37种物体。</li><li>COCO：知名数据集COCO最近加入了全景分割标注。</li></ul><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h4 id="1-MaskFormer"><a href="#1-MaskFormer" class="headerlink" title="1.MaskFormer"></a>1.<strong>MaskFormer</strong></h4><p>Per-Pixel Classification is Not All You Need for Semantic Segmentation</p><p>篇文章提出了一个新的分割输出端范式，传统方法会将每个像素点预测成一种类别来完成分割任务，而本文则是会输出很多个二分类分割图，如下图所示：</p><p><img src="/img/mask_cls.png" alt="img"></p><p>上图左侧为传统方法，右侧为本文方法，不是只输出 K 个类别二值分类图，而是提前设定一个较大值，同时与分类图一同预测的还有一个预测类别，这个类别可以为空，即该二值分类图没有用。因此损失函数由两个组成，一个是预测分类图与真值图的损失，另一个是预测类别的交叉熵损失.</p><script type="math/tex; mode=display">\mathcal{L}_{\text {mask-cls }}\left(z, z^{\mathrm{gt}}\right)=\sum_{j=1}^{N}\left[-\log p_{\sigma(j)}\left(c_{j}^{\mathrm{gt}}\right)+\mathbb{1}_{c_{j}^{\mathrm{gt}} \neq \varnothing} \mathcal{L}_{\text {mask }}\left(m_{\sigma(j)}, m_{j}^{\mathrm{gt}}\right)\right]</script><p>匹配策略上依旧采用匈牙利匹配，匹配成本：</p><script type="math/tex; mode=display">-p_{i}\left(c_{j}^{\mathrm{gt}}\right)+\mathcal{L}_{\text {mask }}\left(m_{i}, m_{j}^{\mathrm{gt}}\right)</script><p>其中，$L_{mask}$为二类交叉熵损失。MaskFormer具体结构如下：</p><p><img src="/img/maskformer.png" alt="img"></p><h4 id="2-Mask2Former"><a href="#2-Mask2Former" class="headerlink" title="2.Mask2Former"></a>2.Mask2Former</h4><p>Masked-attention Mask Transformer for Universal Image Segmentation</p><p>一个model去建模所有的分割任务：语义分割，实例分割和以及全景分割。一个模型取得三个不同分割任务STOA.。具体而言本文方法沿用了上一篇的分割图分别产生方式，另外有两个主要创新点，包括每个transformer decoder层都会使用pixel-decoder的金字塔对应结构，以及 Mask Attention 形式。可参考下图：</p><p><img src="/img/mask2former.png" alt="img"></p><p>因此，作者基于上述的问题和最初的motivation(一个模型取得三个不同分割任务STOA)，提出了几个改进，节省训练的时间(MaskFormer训练需要300epoch)和cost的同时能够提升性能。</p><p><strong>第一个改进</strong>：Mask Attention加速收敛,相比于之前的cross attention，这个里面的attention affinity是一种稀疏的attention，其实就是将上一层预测的分割图使用阈值0.5转换成[0,1]mask图，将转换后的mask进一步转换成[-inf,0]，然后和原始att相加，过softmax得到att_mask，相当于不计算原始分割图中为0的区域att.</p><p>standard cross-attention:  $\mathbf{X}_{l}=\operatorname{softmax}\left(\mathbf{Q}_{l} \mathbf{K}_{l}^{\mathrm{T}}\right) \mathbf{V}_{l}+\mathbf{X}_{l-1}$</p><p> masked cross-attention: </p><script type="math/tex; mode=display">\mathbf{X}_{l}=\operatorname{softmax}\left(\mathcal{M}_{l-1}+\mathbf{Q}_{l} \mathbf{K}_{l}^{\mathrm{T}}\right) \mathbf{V}_{l}+\mathbf{X}_{l-1}</script><script type="math/tex; mode=display">\mathcal{M}_{l-1}(x, y)=\left\{\begin{array}{ll}0 & \text { if } \mathbf{M}_{l-1}(x, y)=1 \\-\infty & \text { otherwise }\end{array} .\right.</script><p>具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiScaleMaskedTransformerDecoder</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">       ...</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask_features, mask = None)</span>:</span></span><br><span class="line">       ...</span><br><span class="line">       outputs_class, outputs_mask, attn_mask = self.forward_prediction_heads(output, mask_features, attn_mask_target_size=size_list[<span class="number">0</span>])</span><br><span class="line">       <span class="comment">#print(outputs_class.shape,outputs_mask.shape,attn_mask.shape)</span></span><br><span class="line">       predictions_class.append(outputs_class)</span><br><span class="line">       predictions_mask.append(outputs_mask)</span><br><span class="line"></span><br><span class="line">       <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_layers):</span><br><span class="line">           level_index = i % self.num_feature_levels</span><br><span class="line">           attn_mask[torch.where(attn_mask.sum(<span class="number">-1</span>) == attn_mask.shape[<span class="number">-1</span>])] = <span class="keyword">False</span></span><br><span class="line">           <span class="comment"># attention: cross-attention first</span></span><br><span class="line">           output = self.transformer_cross_attention_layers[i](</span><br><span class="line">               output, src[level_index],</span><br><span class="line">               memory_mask=attn_mask,</span><br><span class="line">               memory_key_padding_mask=<span class="keyword">None</span>,  <span class="comment"># here we do not apply masking on padded region</span></span><br><span class="line">               pos=pos[level_index], query_pos=query_embed</span><br><span class="line">           )</span><br><span class="line"></span><br><span class="line">           output = self.transformer_self_attention_layers[i](</span><br><span class="line">               output, tgt_mask=<span class="keyword">None</span>,</span><br><span class="line">               tgt_key_padding_mask=<span class="keyword">None</span>,</span><br><span class="line">               query_pos=query_embed</span><br><span class="line">           )</span><br><span class="line"></span><br><span class="line">           <span class="comment"># FFN</span></span><br><span class="line">           output = self.transformer_ffn_layers[i](</span><br><span class="line">               output</span><br><span class="line">           )</span><br><span class="line"></span><br><span class="line">           outputs_class, outputs_mask, attn_mask = self.forward_prediction_heads(output, mask_features, attn_mask_target_size=size_list[(i + <span class="number">1</span>) % self.num_feature_levels])</span><br><span class="line">           predictions_class.append(outputs_class)</span><br><span class="line">           predictions_mask.append(outputs_mask)</span><br><span class="line">           ...</span><br><span class="line">           </span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward_prediction_heads</span><span class="params">(self, output, mask_features, attn_mask_target_size)</span>:</span></span><br><span class="line">       decoder_output = self.decoder_norm(output)</span><br><span class="line">       decoder_output = decoder_output.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">       outputs_class = self.class_embed(decoder_output)</span><br><span class="line">       mask_embed = self.mask_embed(decoder_output)</span><br><span class="line">       outputs_mask = torch.einsum(<span class="string">"bqc,bchw-&gt;bqhw"</span>, mask_embed, mask_features)</span><br><span class="line"></span><br><span class="line">       <span class="comment"># <span class="doctag">NOTE:</span> prediction is of higher-resolution</span></span><br><span class="line">       <span class="comment"># [B, Q, H, W] -&gt; [B, Q, H*W] -&gt; [B, h, Q, H*W] -&gt; [B*h, Q, HW]</span></span><br><span class="line">       attn_mask = F.interpolate(outputs_mask, size=attn_mask_target_size, mode=<span class="string">"bilinear"</span>, align_corners=<span class="keyword">False</span>)</span><br><span class="line">       <span class="comment">#print(outputs_mask.shape,attn_mask.shape)</span></span><br><span class="line">       <span class="comment"># must use bool type</span></span><br><span class="line">       <span class="comment"># If a BoolTensor is provided, positions with ``True`` are not allowed to attend while ``False`` values will be unchanged.</span></span><br><span class="line">       attn_mask = (attn_mask.sigmoid().flatten(<span class="number">2</span>).unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, self.num_heads, <span class="number">1</span>, <span class="number">1</span>).flatten(<span class="number">0</span>, <span class="number">1</span>) &lt; <span class="number">0.5</span>).bool()</span><br><span class="line">       attn_mask = attn_mask.detach()</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> outputs_class, outputs_mask, attn_mask</span><br></pre></td></tr></table></figure><p><strong>第二个改进是多尺度特征改善小目标分割</strong>，对应于pixel-decoder，作者使用了类似于Deformable DETR decoder端的设置，在decoder端采用了multi scale的特征输入做attention。这个步骤对于提升small object的segmentation帮助很大。具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MSDeformAttnPixelDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_features</span><span class="params">(self, features)</span>:</span> <span class="comment">#[res3,res4,res5]</span></span><br><span class="line">        srcs = []</span><br><span class="line">        pos = []</span><br><span class="line">        <span class="comment"># Reverse feature maps into top-down order (from low to high resolution)</span></span><br><span class="line">        <span class="keyword">for</span> idx, f <span class="keyword">in</span> enumerate(self.transformer_in_features[::<span class="number">-1</span>]):</span><br><span class="line">            x = features[f].float()  <span class="comment"># deformable detr does not support half precision</span></span><br><span class="line">            srcs.append(self.input_proj[idx](x))</span><br><span class="line">            pos.append(self.pe_layer(x))</span><br><span class="line"></span><br><span class="line">        y, spatial_shapes, level_start_index = self.transformer(srcs, pos) <span class="comment">#MSDeformAttnTransformerEncoderOnly</span></span><br><span class="line"></span><br><span class="line">        bs = y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        split_size_or_sections = [<span class="keyword">None</span>] * self.transformer_num_feature_levels</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.transformer_num_feature_levels):</span><br><span class="line">            <span class="keyword">if</span> i &lt; self.transformer_num_feature_levels - <span class="number">1</span>:</span><br><span class="line">                split_size_or_sections[i] = level_start_index[i + <span class="number">1</span>] - level_start_index[i]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                split_size_or_sections[i] = y.shape[<span class="number">1</span>] - level_start_index[i]</span><br><span class="line">        y = torch.split(y, split_size_or_sections, dim=<span class="number">1</span>) [x3,x4,x5]</span><br><span class="line">        </span><br><span class="line">        out = []</span><br><span class="line">        multi_scale_features = []</span><br><span class="line">        num_cur_levels = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, z <span class="keyword">in</span> enumerate(y):</span><br><span class="line">            zz = z.transpose(<span class="number">1</span>, <span class="number">2</span>).view(bs, <span class="number">-1</span>, spatial_shapes[i][<span class="number">0</span>], spatial_shapes[i][<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            out.append(zz)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># append `out` with extra FPN levels</span></span><br><span class="line">        <span class="comment"># Reverse feature maps into top-down order (from low to high resolution)</span></span><br><span class="line">        <span class="keyword">for</span> idx, f <span class="keyword">in</span> enumerate(self.in_features[:self.num_fpn_levels][::<span class="number">-1</span>]):</span><br><span class="line"></span><br><span class="line">            x = features[f].float()</span><br><span class="line"></span><br><span class="line">            lateral_conv = self.lateral_convs[idx]</span><br><span class="line">            output_conv = self.output_convs[idx]</span><br><span class="line">            cur_fpn = lateral_conv(x)</span><br><span class="line">            <span class="comment"># Following FPN implementation, we use nearest upsampling here</span></span><br><span class="line">            y = cur_fpn + F.interpolate(out[<span class="number">-1</span>], size=cur_fpn.shape[<span class="number">-2</span>:], mode=<span class="string">"bilinear"</span>, align_corners=<span class="keyword">False</span>)</span><br><span class="line">            y = output_conv(y)</span><br><span class="line">            out.append(y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> o <span class="keyword">in</span> out:</span><br><span class="line">            <span class="keyword">if</span> num_cur_levels &lt; self.maskformer_num_feature_levels:</span><br><span class="line">                multi_scale_features.append(o)</span><br><span class="line">                num_cur_levels += <span class="number">1</span>      </span><br><span class="line">        <span class="keyword">return</span> self.mask_features(out[<span class="number">-1</span>]), out[<span class="number">0</span>], multi_scale_features</span><br></pre></td></tr></table></figure><p>此外，文章还额外提出了三个小改进：</p><ul><li>将 Self 和 Cross Attention 的顺序做一个变换，让 Cross 在前面，因为在图像特征没加入计算时自身做 Self 效率会较低；</li><li>文章将 Transformer 解码器的初始序列设置为可学版本；</li><li>整个模型将不使用 dropout 操作。</li><li>使用point-rend head 改善分割边界质量，同时降低显存；</li></ul><p>mask2former/modeling/meta_arch/mask_former_head.py </p><p><strong>backbone:res50</strong></p><p>800x800(short size=800) —&gt;res2[1, 256, 200, 200],res3[1, 512, 100, 100],res4[1, 1024, 50, 50],res5[1, 2048, 25, 25],</p><p><strong>pixel decoder:MSDeformAttnPixelDecoder</strong>(6 layers)</p><p> res3,res4,res5—&gt;MSDeformAttnTransformerEncoderOnly—&gt;x1[1, 256, 25, 25],x2[1, 256, 50, 50],x3[1, 256, 100, 100],</p><p>x1,x2,x3+FPN(res2) —-&gt;x1[1, 256, 25, 25],x2[1, 256, 50, 50],x3[1, 256, 100, 100],x4[1, 256, 200, 200]</p><p> res3,res4,res5—&gt;  <strong>mask_features</strong>:conv[x4],<strong>transformer_encoder_features</strong>:x4,<strong>multi_scale_features</strong>:[x1,x2,x3]</p><p><strong>Transformer decoder:MultiScaleMaskedTransformerDecoder</strong></p><p>input：multi_scale_features,mask_features </p><p>query_feat(100x256)，mask_features[1, 256, 200, 200]  —&gt;forward_prediction_heads—&gt;outputs_class[1, 100, 134],outputs_mask[1, 100, 200, 200],attn_mask[8, 100, 625]，attn_mask是有outputs_mask插值降采样得到</p><h4 id="3-PointRend"><a href="#3-PointRend" class="headerlink" title="3.PointRend"></a>3.PointRend</h4><p>文中使用了各种术语，比如rend、subdivision、ray-tracing，是想说明一个问题: 图像是对真实目标的一个离散化表达，真实目标的一些属性，如区域联通性、边缘连续性，在图像中同样存在。那么分割问题就可以看作预测一个真实目标在离散化后的图像中所占的区域，即：point-wise label prediction（点对点分类）。连续性体现在：图像中的像素可以通过插值得到与真实目标一一对应的坐标点。分割是在离散化的网格区域点对点的分类，但是有些点很难分类的准确，这些点大部分处在目标的边缘。pointrend方法的提出是对这些模糊分割的点，做更进一步的预测，即：精细分割。主要分成3步，如下图所示：1）候选点（或模糊分割点）选取；2）点特征提取；3）点分割预测。</p><p><img src="/img/pointRend.png" alt="img"></p><h5 id="1-候选点选取"><a href="#1-候选点选取" class="headerlink" title="1.候选点选取"></a><strong>1.候选点选取</strong></h5><p>候选点选取在训练和测试过程是不一样的，其中推断过程是通过迭代的方式从一个低分辨率的分割图像得到一个高分辨率的图像，这种方式不适合训练过程中的梯度反传，故采用另一种方式。</p><p><strong>Point select for training</strong></p><ul><li>随机生成kN个点，其中k&gt;1。</li><li>估计这kN个点的不确定程度，并根据不确定程度，筛选前βN个点，β取值范围为[0, 1]。其中不确定程度的估计方式与推断过程估计方式相同，使用低分辨率的分割置信度。</li><li>在剩余的点中，均匀采用得到（1-β）N个点。</li></ul><p><img src="/img/point_select_train.png" alt="img"></p><p>在PointRend中，采用了k=3和$\beta=0.75$的采样策略参数，采样得到$14^{2}$个点；从 coarse prediction 中插值的 GT 类别的概率和 0.5 之间的距离作为 point-wise 不确定性度量.</p><p><strong>Point select for inference</strong></p><p>推断过程是迭代进行的，具体过程如下图所示，首先通过上采样，将模型直接输出的低分辨率的分割图的长宽各扩大2倍得到高分辨的分割图，如从4x4的特征图上采样得到8x8的特征图；然后在高分辨分割图中，筛选N个分割模糊的点，即分割置信度（若置信度区间为[0,1]）在0.5左右的点，如在下图8x8的高分辨率分割图中，点集中在边缘附近。这N个点为最终筛选出来进行再次确认的点。以此类推，逐步迭代，得到最终目标分辨率的分割图。</p><p><img src="/img/subdivision.png" alt="img"></p><p>在PointRend中，对于预测类别 c 的矩形框，除非特别说明，均采用 adaptive subdivision 来通过 5 steps 将 7x7 coarse 预测精细化到 224x224. 每一次迭代，选择并更新 $N=28^2$个基于预测值和 0.5 之间的差异性距离得到最不确定的点.</p><h5 id="2-点特征提取"><a href="#2-点特征提取" class="headerlink" title="2.点特征提取"></a><strong>2.点特征提取</strong></h5><p>点特征提取包括fine-grained特征和coarse特征，其中coarse为K维，来自低分辨的分割mask，fine-grained特征来自原cnn backbone某个stage或者多个stage的组合特征。文中使用P2层的卷积特征作为fine-grained特征提取的特征图，具体流程如下图。</p><p><img src="/img/pointRend-ppl.png" alt="img"></p><h5 id="3-点分割预测"><a href="#3-点分割预测" class="headerlink" title="3. 点分割预测"></a>3. 点分割预测</h5><p>如上图所示紫色箭头，得到候选点的特征表达后，经过一组MLP，来得到最后的N个点的分割预测结果。其中，在实验中使用了4个全连接层，其中<strong>每个全连接层的输出，都联合coarse feature</strong>，作为下一个全连接层的输入。</p><h5 id="5-Results"><a href="#5-Results" class="headerlink" title="5. Results"></a>5. Results</h5><p><img src="/img/pointRend-res.png" alt="img"></p><p>其中左边一列图为mask rcnn的结果，右边一列图为PointRend的结果，从视觉效果看，PointRend对物体边缘描述更加精细化。</p><h4 id="4-Panoptic-SegFormer"><a href="#4-Panoptic-SegFormer" class="headerlink" title="4.Panoptic SegFormer"></a>4.Panoptic SegFormer</h4><p> Delving Deeper into Panoptic Segmentation with Transformers</p><p><img src="/img/PSFormer-ppl.png" alt="img"></p><p>该结构与DETR类似，不同之处在于：</p><p>（1）backbone使用多尺度特征(C3,C4,C5);</p><p>（2）针对decoder中query做了进一步精细划分,解耦location和Mask。针对Thing Query使用location Decoder捕获thing类别位置信息对Thing Query进行refine；此后使用refine后的Thing Query和Stuff Query作为Mask Decoder 输出mask结果。其中location Decoders使用bbox信息辅助监督，可以加速网络收敛；</p><p>（3）后处理部分，使用Mask-wise merge策略融合things和stuff获取最终的mask结果.</p><p>下面详细讲一下location decoder、mask decoder和mask-wise merge部分。</p><p><strong>Location Decoder</strong></p><p>给定N个初始化queries，训练阶段，在location decoder后面添加一个辅助MLP来预测位置和尺寸，location decoder的输出称为location-aware queries；推理阶段，去除辅助MLP。这一个辅助loss，可以帮助网络快速收敛，每个query关注区域指向性更明确。</p><p><strong>Mask Decoder</strong></p><p>mask decoder将location decoder的输出location-wise queries当作query，和MaskFormer预测mask和类别不同的是，Panoptic SegFormer预测mask需要先将attention map拆分成A3，A4，A5，然后都上采样到H/8xW/8的分辨率，concat在一起得到A_fuse，最后通过1x1卷积得到mask预测结果。</p><p><strong>Mask-wise merge</strong></p><p><img src="/img/mask-wise-merge.png" alt="img"></p><p>之前的分割去重，一般都是使用pixel-wise argmax策略，也就是重叠部分保留预测分数最大的类别。本文提出的mask-wise merge策略，对于重叠部分进行舍弃，上图是伪代码。</p><p>很喜欢作者在conclusion里面提到的一句话， Given the similarities and differences among the various segmentation tasks, “seek common ground while reserving differences” is a more reasonable guiding ideology.  完全的统一框架不见得是最好的选择，“求同存异”才是一个更合理的指导思想。</p><p>代码链接：<a href="https://github.com/zhiqi-li/Panoptic-SegFormer" target="_blank" rel="noopener">https://github.com/zhiqi-li/Panoptic-SegFormer</a></p><p>结果复现：</p><div class="table-container"><table><thead><tr><th>Method</th><th>PQ</th><th>SQ</th><th>RQ</th><th>N</th></tr></thead><tbody><tr><td>All(paper)</td><td>49.600</td><td>81.600</td><td>59.900</td><td>133</td></tr><tr><td>All(rep)</td><td>49.900</td><td>81.500</td><td>60.200</td><td>133</td></tr></tbody></table></div><h4 id="5-K-Net"><a href="#5-K-Net" class="headerlink" title="5. K-Net"></a>5. K-Net</h4><p>K-Net: Towards Unified Image Segmentation</p><p><img src="/img/knet-simple.png" alt="img"></p><p>K-Net的目标也是在于统一实例分割和语义分割。如上图所示，语义分割核心结构就是由一组kernel来负责语义mask的生成，让kernel数量与数据集类别数据保持一致，每个kernel负责一个固定类别masker的生成。受此启发，在实例分割中，可以通过同样方式引入一组卷积核来负责 mask 的生成，限定一个 kernel 只分割一个物体，每个kernel负责分割不同的物体，实例分割任务统一到一个框架内。</p><p><strong>Group-Aware Kernels</strong></p><p>理论上一组instance kernel就可以得到实例分割结果，但实验结果却相差甚远，与sem seg 相比，ins seg需要的kernel要求更高：</p><p>(1)在sem seg中，每个单独的sem kernel与类别(sem class)是绑定的，在每张图上都可以学习分割同一个类别，而ins seg不具备，而是通过Bipartite matching 来做的 target assignment,这导致每个kernel在每张图上学习的目标是根据当前的预测情况动态分配的。</p><p>(2)ins kernel需要区分appearence和scale变化的物体，需要具备更强的判别特性；</p><p>基于此，作者设计了<strong>Kernel Update Head</strong> 基于 mask 和特征图来将 kernel 动态化；如下图所示，Kernel Update Head 首先获得每个 kernel 对应 pixel group 的 feature，然后以某种方式动态地更新当前kernel。</p><p><img src="/img/kernel-update-head.png" alt="img"></p><p>此外，为了使得kernel可以modeling全局信息，作者还新增kernel interaction模块，最终得到的特征可用于class prediction, dynamic kernels 和mask predictions. </p><p><img src="/img/knet-ppl.png" alt="img"></p><p>为了得到更精细化的mask，可以通过叠加多个Kernel Update Head对mask和kernel进行迭代式refine.最终K-Net pipeline如上图所示，在论文中使用了3个Kernel Update Head和100个ins kernel.</p><p>注：在COCO-Panoptic上多尺度训练36epoch，训练一个K-Net，使用16张V1100需要两天半，在两台机器的情况下，训练时间有点长。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在知乎上频繁地刷到有关Mask2Former地帖子，这么多人吹捧地必是精品，就跟一波风看了一下Mask2Former，顺带地也了解一下Panoptic Segmentation这个任务。众所周知，图像分割主要有两个方向：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;语义分割（semantic segmentation）&lt;/strong&gt;，常用来识别天空、草地、道路等没有固定形状的不可数&lt;strong&gt;事物（stuff）&lt;/strong&gt;。语义分割的标记方法通常是给每个像素加上标签。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实例分割（instance segmentation）&lt;/strong&gt;，人、动物或工具等可数的、独立的明显物体&lt;strong&gt;（things）&lt;/strong&gt;。实例分割通常用包围盒或分割掩码标记目标。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;全景分割（Panoptic Segmentation）&lt;/strong&gt;其实就是把这两个方向结合起来，生成统一的、全局的分割图像，既识别事物，也识别物体。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/ps.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="transformer" scheme="https://blog.nicehuster.cn/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>自监督学习--基于contrastive learning方法</title>
    <link href="https://blog.nicehuster.cn/2021/12/13/self-sup-contrastive-learning/"/>
    <id>https://blog.nicehuster.cn/2021/12/13/self-sup-contrastive-learning/</id>
    <published>2021-12-13T11:13:39.000Z</published>
    <updated>2022-04-23T13:38:33.828Z</updated>
    
    <content type="html"><![CDATA[<p>基于contrastive learning的自监督学习方法从2020-2021年涌现了许多工作，这里不一一列举，但是会简单介绍一些高引用的方法：MoCo，SimCLR，MoCov2，BYOL，SwAV，SimSiam，MoCov3。</p><p><img src="/img/contrastive learning.svg" alt="contrastive learning"></p><a id="more"></a><h3 id="1-MoCo-CVPR2020"><a href="#1-MoCo-CVPR2020" class="headerlink" title="1. MoCo(CVPR2020)"></a><a href="https://arxiv.org/abs/1911.05722" target="_blank" rel="noopener">1. MoCo(CVPR2020)</a></h3><p><img src="/img/MoCo.svg" alt="MoCo"></p><h3 id="2-SimCLR-ICML2020"><a href="#2-SimCLR-ICML2020" class="headerlink" title="2. SimCLR(ICML2020)"></a><a href="https://arxiv.org/abs/2002.05709" target="_blank" rel="noopener">2. SimCLR(ICML2020)</a></h3><p><img src="/img/SimCLR.svg" alt="SimCLR"></p><h3 id="3-MoCov2"><a href="#3-MoCov2" class="headerlink" title="3. MoCov2"></a><a href="https://arxiv.org/abs/2003.04297" target="_blank" rel="noopener">3. MoCov2</a></h3><p><img src="/img/MoCov2.svg" alt="MoCov2"></p><h3 id="4-BYOL-NIPS2020"><a href="#4-BYOL-NIPS2020" class="headerlink" title="4. BYOL(NIPS2020)"></a><a href="https://arxiv.org/abs/2006.07733" target="_blank" rel="noopener">4. BYOL(NIPS2020)</a></h3><p>待更新。。。</p><h3 id="5-SwAV-NIPS2020"><a href="#5-SwAV-NIPS2020" class="headerlink" title="5. SwAV(NIPS2020)"></a><a href="https://arxiv.org/abs/2006.07733" target="_blank" rel="noopener">5. SwAV(NIPS2020)</a></h3><p>待更新。。。</p><h3 id="6-SimSiam-CVPR2021"><a href="#6-SimSiam-CVPR2021" class="headerlink" title="6. SimSiam(CVPR2021)"></a><a href="https://arxiv.org/abs/2011.10566" target="_blank" rel="noopener">6. SimSiam(CVPR2021)</a></h3><p>待更新。。。</p><h3 id="7-MoCov3-ICCV2021"><a href="#7-MoCov3-ICCV2021" class="headerlink" title="7.MoCov3(ICCV2021)"></a><a href="https://arxiv.org/abs/2104.02057" target="_blank" rel="noopener">7.MoCov3(ICCV2021)</a></h3><p>待更新。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;基于contrastive learning的自监督学习方法从2020-2021年涌现了许多工作，这里不一一列举，但是会简单介绍一些高引用的方法：MoCo，SimCLR，MoCov2，BYOL，SwAV，SimSiam，MoCov3。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/contrastive learning.svg&quot; alt=&quot;contrastive learning&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="self-supervised" scheme="https://blog.nicehuster.cn/tags/self-supervised/"/>
    
  </entry>
  
  <entry>
    <title>自监督学习--基于pretext-task方法</title>
    <link href="https://blog.nicehuster.cn/2021/12/10/self-sup-pretext-task/"/>
    <id>https://blog.nicehuster.cn/2021/12/10/self-sup-pretext-task/</id>
    <published>2021-12-10T11:13:39.000Z</published>
    <updated>2022-04-23T13:11:11.530Z</updated>
    
    <content type="html"><![CDATA[<p>自从2019年MoCo横空出世，掀起了一股自监督学习的浪潮，随后SimCLR,MoCo,BYOL,SwAV等一系列优秀的工作被提出，2021年底，何凯明的MAE更是将自监督学习带到另外一个高度。自监督学习的背后一个强大的动机就是，打破目前神经网络训练对于标注数据的依赖，即使在没有标注数据的情况下，也可以高效的训练网络。自监督学习的核心在于合理构建有利于模型学习的任务，其大致可分为三类：</p><p><img src="/img/self-sup-categories.png" alt="img"></p><a id="more"></a><p>基于pretext task的自监督学习方法大概有四篇经典的工作：</p><h3 id="1-Relative-Location-CVPR2015"><a href="#1-Relative-Location-CVPR2015" class="headerlink" title="1. Relative Location(CVPR2015)"></a>1. <a href="https://arxiv.org/abs/1505.05192" target="_blank" rel="noopener">Relative Location(CVPR2015)</a></h3><p><img src="/img/self-sup-rl.png" alt="img"></p><h3 id="2-Colorization-ECCV2016"><a href="#2-Colorization-ECCV2016" class="headerlink" title="2. Colorization(ECCV2016)"></a>2. <a href="https://arxiv.org/abs/1603.08511" target="_blank" rel="noopener">Colorization(ECCV2016)</a></h3><p><img src="/img/self-sup-color.png" alt="img"></p><h3 id="3-Context-Encoders（CVPR2016）"><a href="#3-Context-Encoders（CVPR2016）" class="headerlink" title="3. Context Encoders（CVPR2016）"></a>3. <a href="https://arxiv.org/abs/1604.07379" target="_blank" rel="noopener">Context Encoders（CVPR2016）</a></h3><p><img src="/img/self-sup-ce.png" alt="img"></p><h3 id="4-Rotation-Prediction-ICLR2018"><a href="#4-Rotation-Prediction-ICLR2018" class="headerlink" title="4. Rotation Prediction(ICLR2018)"></a>4. <a href="https://arxiv.org/abs/1803.07728" target="_blank" rel="noopener">Rotation Prediction(ICLR2018)</a></h3><p><img src="/img/self-sup-rp.png" alt="img"></p><p>从上面4种方法可以看出，基于pretext task的自监督学习方法都具备俩个特点：</p><blockquote><ol><li>良好的任务定义，比如预测旋转角度，或者相对位置</li><li>合理的限制条件，避免模型出现无效解</li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;自从2019年MoCo横空出世，掀起了一股自监督学习的浪潮，随后SimCLR,MoCo,BYOL,SwAV等一系列优秀的工作被提出，2021年底，何凯明的MAE更是将自监督学习带到另外一个高度。自监督学习的背后一个强大的动机就是，打破目前神经网络训练对于标注数据的依赖，即使在没有标注数据的情况下，也可以高效的训练网络。自监督学习的核心在于合理构建有利于模型学习的任务，其大致可分为三类：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/self-sup-categories.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="self-supervised learning" scheme="https://blog.nicehuster.cn/tags/self-supervised-learning/"/>
    
  </entry>
  
  <entry>
    <title>Transformer 杂记</title>
    <link href="https://blog.nicehuster.cn/2021/12/05/transformer-soup/"/>
    <id>https://blog.nicehuster.cn/2021/12/05/transformer-soup/</id>
    <published>2021-12-05T11:13:39.000Z</published>
    <updated>2022-04-12T11:39:50.984Z</updated>
    
    <content type="html"><![CDATA[<p>近期断断续续的看了一些transformer相关的paper，看的比较杂，有些是对应领域比较有代表性地工作。偷个懒就不详细介绍每篇Paper，简单地记录一下这些paper大致要解决地问题。</p><a id="more"></a><h4 id="1-MAE-Masked-Autoencoders-Are-Scalable-Vision-Learners"><a href="#1-MAE-Masked-Autoencoders-Are-Scalable-Vision-Learners" class="headerlink" title="1. MAE:Masked Autoencoders Are Scalable Vision Learners"></a>1. MAE:Masked Autoencoders Are Scalable Vision Learners</h4><p>自监督学习方法，核心思想是以一定比例随机 mask 掉图片中的一些图像块(patch)然后重建这些部分的像素值</p><h4 id="2-SegFormer-Simple-and-Efficient-Design-for-Semantic-Segmentation-with-Transformers"><a href="#2-SegFormer-Simple-and-Efficient-Design-for-Semantic-Segmentation-with-Transformers" class="headerlink" title="2.SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers"></a>2.SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</h4><p>设计多层次backbone MIT，丢弃PE，优化self-att加速推理，此外从SETR痛点出发设计轻量级MLP解码器</p><h4 id="3-Early-Convolutions-Help-Transformers-See-Better"><a href="#3-Early-Convolutions-Help-Transformers-See-Better" class="headerlink" title="3.Early Convolutions Help Transformers See Better"></a>3.Early Convolutions Help Transformers See Better</h4><p>Vit训练不稳定在于Patch Embedding时使用大卷积核以及大步长导致，进一步提出使用step-wise conv stem进行替换，以此改进vit训练稳定性问题</p><h4 id="4-Visformer-The-Vision-friendly-Transformer"><a href="#4-Visformer-The-Vision-friendly-Transformer" class="headerlink" title="4.Visformer: The Vision-friendly Transformer"></a>4.Visformer: The Vision-friendly Transformer</h4><p>提升transformer方法的性能下限，即使是小数据集依然可以得到很好的性能</p><h4 id="5-Conditional-Positional-Encodings-for-Vision-Transformers"><a href="#5-Conditional-Positional-Encodings-for-Vision-Transformers" class="headerlink" title="5.Conditional Positional Encodings for Vision Transformers"></a>5.Conditional Positional Encodings for Vision Transformers</h4><p>利用卷积+zero-padding来编码局部位置信息，从而丢弃现有的PE，解决输入大小变化时需要对PE进行插值和fine-tune的问题</p><h4 id="6-MetaFormer-is-Actually-What-You-Need-for-Vision"><a href="#6-MetaFormer-is-Actually-What-You-Need-for-Vision" class="headerlink" title="6.MetaFormer is Actually What You Need for Vision"></a>6.MetaFormer is Actually What You Need for Vision</h4><p>transformer优于cnn在于其结构，而不是attention，即使替换成pooling，也能达到不错的性能</p><h4 id="7-Per-Pixel-Classification-is-Not-All-You-Need-for-Semantic-Segmentation"><a href="#7-Per-Pixel-Classification-is-Not-All-You-Need-for-Semantic-Segmentation" class="headerlink" title="7.Per-Pixel Classification is Not All You Need for Semantic Segmentation"></a>7.Per-Pixel Classification is Not All You Need for Semantic Segmentation</h4><p>提出了一种新的分割范式，解耦分割和分类，统一语义分割和实例分割任务</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;近期断断续续的看了一些transformer相关的paper，看的比较杂，有些是对应领域比较有代表性地工作。偷个懒就不详细介绍每篇Paper，简单地记录一下这些paper大致要解决地问题。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="transformer" scheme="https://blog.nicehuster.cn/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>知识蒸馏[转载]</title>
    <link href="https://blog.nicehuster.cn/2021/11/09/knowledge-distillation/"/>
    <id>https://blog.nicehuster.cn/2021/11/09/knowledge-distillation/</id>
    <published>2021-11-09T11:13:39.000Z</published>
    <updated>2022-04-24T08:30:22.122Z</updated>
    
    <content type="html"><![CDATA[<p>知识蒸馏是一种模型压缩方法，是一种基于“教师-学生网络思想”的训练方法，由于其简单，有效，在工业界被广泛应用。这一技术的理论来自于2015年Hinton发表的一篇神作: <a href="https://arxiv.org/pdf/1503.02531.pdf" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a>。Knowledge Distillation，简称KD，顾名思义，就是将已经训练好的模型包含的知识(”Knowledge”)，蒸馏(“Distill”)提取到另一个模型里面去。</p><a id="more"></a><p>以下内容均转载于：<a href="https://zhuanlan.zhihu.com/p/102038521" target="_blank" rel="noopener">【经典简读】知识蒸馏(Knowledge Distillation) 经典之作</a> ，侵权删。</p><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><h3 id="1-1-论文提出的背景"><a href="#1-1-论文提出的背景" class="headerlink" title="1.1. 论文提出的背景"></a>1.1. 论文提出的背景</h3><p>虽然在一般情况下，我们不会去区分训练和部署使用的模型，但是训练和部署之间存在着一定的不一致性:</p><ul><li>在训练过程中，我们需要使用复杂的模型，大量的计算资源，以便从非常大、高度冗余的数据集中提取出信息。在实验中，效果最好的模型往往规模很大，甚至由多个模型集成得到。而大模型不方便部署到服务中去，常见的瓶颈如下:</li></ul><ol><li>推断速度慢</li><li>对部署资源要求高(内存，显存等)</li></ol><ul><li>在部署时，我们对延迟以及计算资源都有着严格的限制。</li></ul><p>因此，模型压缩（在保证性能的前提下减少模型的参数量）成为了一个重要的问题。而”模型蒸馏“属于模型压缩的一种方法。</p><p><strong>插句题外话</strong>，我们可以从模型参数量和训练数据量之间的相对关系来理解underfitting和overfitting。AI领域的从业者可能对此已经习以为常，但是为了力求让小白也能读懂本文，还是引用我同事的解释（我印象很深）形象地说明一下:</p><blockquote><p>模型就像一个容器，训练数据中蕴含的知识就像是要装进容器里的水。当数据知识量(水量)超过模型所能建模的范围时(容器的容积)，加再多的数据也不能提升效果(水再多也装不进容器)，因为模型的表达空间有限(容器容积有限)，就会造成<strong>underfitting</strong>；而当模型的参数量大于已有知识所需要的表达空间时(容积大于水量，水装不满容器)，就会造成<strong>overfitting</strong>，即模型的variance会增大(想象一下摇晃半满的容器，里面水的形状是不稳定的)。</p></blockquote><h3 id="1-2-“思想歧路”"><a href="#1-2-“思想歧路”" class="headerlink" title="1.2. “思想歧路”"></a>1.2. “思想歧路”</h3><p>上面容器和水的比喻非常经典和贴切，但是会引起一个误解: 人们在直觉上会觉得，要保留相近的知识量，必须保留相近规模的模型。也就是说，一个模型的参数量基本决定了其所能捕获到的数据内蕴含的“知识”的量。</p><p>这样的想法是基本正确的，但是需要注意的是:</p><ol><li>模型的参数量和其所能捕获的“知识“量之间并非稳定的线性关系(下图中的1)，而是接近边际收益逐渐减少的一种增长曲线(下图中的2和3)</li><li>完全相同的模型架构和模型参数量，使用完全相同的训练数据，能捕获的“知识”量并不一定完全相同，另一个关键因素是训练的方法。合适的训练方法可以使得在模型参数总量比较小时，尽可能地获取到更多的“知识”(下图中的3与2曲线的对比).</li></ol><p><img src="https://pic2.zhimg.com/80/v2-f2fc2f02b87a38a9ff34a50664800045_720w.jpg" alt="img"></p><h2 id="2-知识蒸馏的理论依据"><a href="#2-知识蒸馏的理论依据" class="headerlink" title="2. 知识蒸馏的理论依据"></a>2. 知识蒸馏的理论依据</h2><h3 id="2-1-Teacher-Model和Student-Model"><a href="#2-1-Teacher-Model和Student-Model" class="headerlink" title="2.1. Teacher Model和Student Model"></a>2.1. Teacher Model和Student Model</h3><p>知识蒸馏使用的是Teacher—Student模型，其中teacher是“知识”的输出者，student是“知识”的接受者。知识蒸馏的过程分为2个阶段:</p><ol><li>原始模型训练: 训练”Teacher模型”, 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对”Teacher模型”不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值。</li><li>精简模型训练: 训练”Student模型”, 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值。</li></ol><p>在本论文中，作者将问题限定在<strong>分类问题</strong>下，或者其他本质上属于分类问题的问题，该类问题的共同点是模型最后会有一个softmax层，其输出值对应了相应类别的概率值。</p><h3 id="2-2-知识蒸馏的关键点"><a href="#2-2-知识蒸馏的关键点" class="headerlink" title="2.2. 知识蒸馏的关键点"></a>2.2. 知识蒸馏的关键点</h3><p>如果回归机器学习最最基础的理论，我们可以很清楚地意识到一点(而这一点往往在我们深入研究机器学习之后被忽略): <strong>机器学习最根本的目的</strong>在于训练出在某个问题上泛化能力强的模型。</p><ul><li><strong>泛化能力强</strong>: 在某问题的所有数据上都能很好地反应输入和输出之间的关系，无论是训练数据，还是测试数据，还是任何属于该问题的未知数据。</li></ul><p>而现实中，由于我们不可能收集到某问题的所有数据来作为训练数据，并且新数据总是在源源不断的产生，因此我们只能退而求其次，训练目标变成在已有的训练数据集上建模输入和输出之间的关系。由于训练数据集是对真实数据分布情况的采样，训练数据集上的最优解往往会多少偏离真正的最优解(这里的讨论不考虑模型容量)。</p><p>而在知识蒸馏时，由于我们已经有了一个泛化能力较强的Net-T，我们在利用Net-T来蒸馏训练Net-S时，可以直接让Net-S去学习Net-T的泛化能力。</p><p>一个很直白且高效的迁移泛化能力的方法就是：使用softmax层输出的类别的概率来作为“soft target”。</p><p><strong>【KD的训练过程和传统的训练过程的对比】</strong></p><ol><li>传统training过程(<strong>hard targets</strong>): 对ground truth求极大似然</li><li>KD的training过程(<strong>soft targets</strong>): 用large model的class probabilities作为soft targets</li></ol><p><img src="https://pic3.zhimg.com/80/v2-29a851c6fa9cc809e51ce738abbec2ce_720w.jpg" alt="img"></p><p>上图: Hard Target 下图: Soft Target</p><p><strong>KD的训练过程为什么更有效?</strong></p><p>softmax层的输出，除了正例之外，<strong>负标签也带有大量的信息</strong>，比如某些负标签对应的概率远远大于其他负标签。而在传统的训练过程(hard target)中，所有负标签都被统一对待。也就是说，<strong>KD的训练方式使得每个样本给Net-S带来的信息量大于传统的训练方式</strong>。</p><p>【<strong>举个例子】</strong></p><p>在手写体数字识别任务MNIST中，输出类别有10个。</p><p><img src="https://pic3.zhimg.com/80/v2-3d77281f38df62990c47d606dd581ee2_720w.jpg" alt="img"></p><p>假设某个输入的“2”更加形似”3”，softmax的输出值中”3”对应的概率为0.1，而其他负标签对应的值都很小，而另一个”2”更加形似”7”，”7”对应的概率为0.1。这两个”2”对应的hard target的值是相同的，但是它们的soft target却是不同的，由此我们可见soft target蕴含着比hard target多的信息。并且soft target分布的熵相对高时，其soft target蕴含的知识就更丰富。</p><p><img src="https://pic4.zhimg.com/80/v2-a9e90626c5ac6f64a7e04c89f6ce3013_720w.jpg" alt="img"></p><p>两个”2“的hard target相同而soft target不同</p><p>这就解释了为什么通过蒸馏的方法训练出的Net-S相比使用完全相同的模型结构和训练数据只使用hard target的训练方法得到的模型，拥有更好的泛化能力。</p><h3 id="2-3-softmax函数"><a href="#2-3-softmax函数" class="headerlink" title="2.3. softmax函数"></a>2.3. softmax函数</h3><p>先回顾一下原始的softmax函数:</p><script type="math/tex; mode=display">q_{i}=\frac{\exp \left(z_{i}\right)}{\sum_{j} \exp \left(z_{j}\right)}</script><p>但要是直接使用softmax层的输出值作为soft target, 这又会带来一个问题: 当softmax输出的概率分布熵相对较小时，负标签的值都很接近0，对损失函数的贡献非常小，小到可以忽略不计。因此<strong>“温度”</strong>这个变量就派上了用场。</p><p>下面的公式时加了温度这个变量之后的softmax函数:</p><script type="math/tex; mode=display">q_{i}=\frac{\exp \left(z_{i} / T\right)}{\sum_{j} \exp \left(z_{j} / T\right)}</script><ul><li>这里的T就是<strong>温度</strong>。</li><li>原来的softmax函数是T = 1的特例。 T越高，softmax的output probability distribution越趋于平滑，其分布的熵越大，负标签携带的信息会被相对地放大，模型训练将更加关注负标签。</li></ul><h2 id="3-知识蒸馏的具体方法"><a href="#3-知识蒸馏的具体方法" class="headerlink" title="3. 知识蒸馏的具体方法"></a>3. 知识蒸馏的具体方法</h2><h3 id="3-1-通用的知识蒸馏方法"><a href="#3-1-通用的知识蒸馏方法" class="headerlink" title="3.1. 通用的知识蒸馏方法"></a>3.1. 通用的知识蒸馏方法</h3><ul><li><strong>第一步</strong>是训练Net-T；<strong>第二步</strong>是在高温T下，蒸馏Net-T的知识到Net-S</li></ul><p><img src="https://pic2.zhimg.com/80/v2-d01f5142d06aa27bc5e207831b5131d9_720w.jpg" alt="img"></p><p>知识蒸馏示意图(来自<a href="https://nervanasystems.github.io/distiller/knowledge_distillation.html" target="_blank" rel="noopener">https://nervanasystems.github.io/distiller/knowledge_distillation.html</a>)</p><p>训练Net-T的过程很简单，下面详细讲讲第二步:高温蒸馏的过程。高温蒸馏过程的目标函数由distill loss(对应soft target)和student loss(对应hard target)加权得到。示意图如上。</p><script type="math/tex; mode=display">L=\alpha L_{s o f t}+\beta L_{h a r d}</script><ul><li><img src="https://www.zhihu.com/equation?tex=v_i" alt="[公式]">: Net-T的logits</li><li><img src="https://www.zhihu.com/equation?tex=z_i" alt="[公式]">: Net-S的logits</li><li><img src="https://www.zhihu.com/equation?tex=p%5ET_i" alt="[公式]">: Net-T的在温度=T下的softmax输出在第i类上的值</li><li><img src="https://www.zhihu.com/equation?tex=q%5ET_i" alt="[公式]">: Net-S的在温度=T下的softmax输出在第i类上的值</li><li><img src="https://www.zhihu.com/equation?tex=c_i" alt="[公式]">: 在第i类上的ground truth值, <img src="https://www.zhihu.com/equation?tex=c_i%5Cin%5C%7B0%2C1%5C%7D" alt="[公式]">, 正标签取1，负标签取0.</li><li><img src="https://www.zhihu.com/equation?tex=N" alt="[公式]">: 总标签数量</li><li>Net-T 和 Net-S同时输入 transfer set (这里可以直接复用训练Net-T用到的training set), 用Net-T产生的softmax distribution (with high temperature) 来作为soft target，Net-S在相同温度T条件下的softmax输出和soft target的cross entropy就是<strong>Loss函数的第一部分</strong> <img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D" alt="[公式]"></li></ul><p><img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D%3D-%5Csum_j%5EN+p%5ET_j%5Clog%28q%5ET_j%29" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=p%5ET_i%3D%5Cfrac%7B%5Cexp%28v_i%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28v_k%2FT%29%7D" alt="[公式]"> , <img src="https://www.zhihu.com/equation?tex=q%5ET_i%3D%5Cfrac%7B%5Cexp%28z_i%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%2FT%29%7D" alt="[公式]"></p><ul><li>Net-S在T=1的条件下的softmax输出和ground truth的cross entropy就是<strong>Loss函数的第二部分</strong> <img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D" alt="[公式]"> 。</li></ul><p><img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D%3D-%5Csum_j%5EN+c_j%5Clog%28q%5E1_j%29" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=q%5E1_i%3D%5Cfrac%7B%5Cexp%28z_i%29%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%29%7D" alt="[公式]"></p><ul><li>第二部分Loss <img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D" alt="[公式]"> 的必要性其实很好理解: Net-T也有一定的错误率，使用ground truth可以有效降低错误被传播给Net-S的可能。打个比方，老师虽然学识远远超过学生，但是他仍然有出错的可能，而这时候如果学生在老师的教授之外，可以同时参考到标准答案，就可以有效地降低被老师偶尔的错误“带偏”的可能性。</li></ul><p><strong>【讨论】</strong></p><ul><li>实验发现第二部分所占比重比较小的时候，能产生最好的结果，这是一个经验的结论。一个可能的原因是，由于soft target产生的gradient与hard target产生的gradient之间有与 <img src="https://www.zhihu.com/equation?tex=T" alt="[公式]"> 相关的比值。原论文中只是一笔带过，我在下面补充了一些简单的推导。(ps. 下面推导可能有些错误，如果有读者能够正确推出来请私信我～)</li><li><strong>Soft Target:</strong><img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D" alt="[公式]"></li></ul><p><img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D%3D-%5Csum_j%5EN+p%5ET_j%5Clog%28q%5ET_j%29%3D-%5Csum_j%5EN+%5Cfrac%7Bz_j%2FT%5Ctimes%5Cexp%28v_j%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28v_k%2FT%29%7D%5Cleft%28%5Cfrac%7B1%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%2FT%29%7D-%5Cfrac%7B%5Cexp+%28z_j+%2F+T%29+%7D%7B%5Cleft%28++%5Csum_k%5EN+%5Cexp%28z_k%2F+T%29%5Cright%29+%5E+2%7D%5Cright%29" alt="[公式]"></p><p><img src="https://www.zhihu.com/equation?tex=%5Capprox+-%5Cfrac%7B1%7D%7BT%5Csum_k%5EN+%5Cexp%28v_k%2FT%29%7D%5Cleft%28%5Cfrac%7B%5Csum_j%5ENz_j%5Cexp%28v_j%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%2FT%29%7D-%5Cfrac%7B%5Csum_j%5EN+z_j%5Cexp+%28z_j%2F+T%29%5Cexp%28v_j%2FT%29+%7D%7B%5Cleft%28++%5Csum_k%5EN+%5Cexp%28z_k+%2F+T%29%5Cright%29+%5E+2%7D+%5Cright%29" alt="[公式]"></p><ul><li><strong>Hard Target:</strong> <img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D" alt="[公式]"></li></ul><p><img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D%3D-%5Csum_j%5EN+c_j%5Clog%28q%5E1_j%29%3D-%5Cleft%28%5Cfrac%7B%5Csum_j%5EN+c_jz_j+%7D%7B+%5Csum_k%5EN+%5Cexp%28z_k+%29%7D-%5Cfrac%7B%5Csum_j%5EN+c_jz_j%5Cexp+%28z_j%29+%7D%7B%5Cleft%28++%5Csum_k%5EN+%5Cexp%28z_k%29%5Cright%29+%5E+2%7D+%5Cright%29" alt="[公式]"></p><ul><li>由于 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_i%7D" alt="[公式]">的magnitude大约是 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bhard%7D%7D%7B%5Cpartial+z_i%7D" alt="[公式]"> 的 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BT%5E2%7D" alt="[公式]"> ，因此在同时使用soft target和hard target的时候，需要在soft target之前乘上<img src="https://www.zhihu.com/equation?tex=T%5E%7B2%7D" alt="[公式]">的系数，这样才能保证soft target和hard target贡献的梯度量基本一致。</li></ul><p><strong>【注意】</strong> 在Net-S训练完毕后，做inference时其softmax的温度T要恢复到1.</p><h3 id="3-2-一种特殊情形-直接match-logits-不经过softmax"><a href="#3-2-一种特殊情形-直接match-logits-不经过softmax" class="headerlink" title="3.2. 一种特殊情形: 直接match logits(不经过softmax)"></a>3.2. 一种特殊情形: 直接match logits(不经过softmax)</h3><p>直接match logits指的是，直接使用softmax层的输入logits（而不是输出）作为soft targets，需要最小化的目标函数是Net-T和Net-S的logits之间的平方差。</p><p><strong>直接上结论: 直接match logits的做法是</strong> <img src="https://www.zhihu.com/equation?tex=T+%5Crightarrow+%5Cinfty" alt="[公式]"> <strong>的情况下的特殊情形。</strong></p><p>由单个case贡献的loss，推算出对应在Net-S每个logit <img src="https://www.zhihu.com/equation?tex=z_i" alt="[公式]"> 上的gradient:</p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_%7Bi%7D%7D%3D%5Cfrac%7B1%7D%7BT%7D%5Cleft%28q_%7Bi%7D-p_%7Bi%7D%5Cright%29%3D%5Cfrac%7B1%7D%7BT%7D%5Cleft%28%5Cfrac%7Be%5E%7Bz_%7Bi%7D+%2F+T%7D%7D%7B%5Csum_%7Bj%7D+e%5E%7Bz_%7Bj%7D+%2F+T%7D%7D-%5Cfrac%7Be%5E%7Bv_%7Bi%7D+%2F+T%7D%7D%7B%5Csum_%7Bj%7D+e%5E%7Bv_%7Bj%7D+%2F+T%7D%7D%5Cright%29" alt="[公式]"></p><p>当 <img src="https://www.zhihu.com/equation?tex=T+%5Crightarrow+%5Cinfty" alt="[公式]"> 时，我们使用 <img src="https://www.zhihu.com/equation?tex=1%2Bx%2FT" alt="[公式]"> 来近似 <img src="https://www.zhihu.com/equation?tex=e%5E%7Bx%2FT%7D" alt="[公式]"> ，于是得到</p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_%7Bi%7D%7D+%5Capprox+%5Cfrac%7B1%7D%7BT%7D%5Cleft%28%5Cfrac%7B1%2Bz_%7Bi%7D+%2F+T%7D%7BN%2B%5Csum_%7Bj%7D+z_%7Bj%7D+%2F+T%7D-%5Cfrac%7B1%2Bv_%7Bi%7D+%2F+T%7D%7BN%2B%5Csum_%7Bj%7D+v_%7Bj%7D+%2F+T%7D%5Cright%29" alt="[公式]"></p><p>如果再加上logits是零均值的假设</p><p><img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bj%7D+z_%7Bj%7D%3D%5Csum_%7Bj%7D+v_%7Bj%7D%3D0" alt="[公式]"></p><p>那么上面的公式可以简化成</p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_%7Bi%7D%7D+%5Capprox+%5Cfrac%7B1%7D%7BN+T%5E%7B2%7D%7D%5Cleft%28z_%7Bi%7D-v_%7Bi%7D%5Cright%29" alt="[公式]"></p><p>也就是等价于minimise下面的损失函数</p><p><img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D%27%3D1+%2F+2%5Cleft%28z_%7Bi%7D-v_%7Bi%7D%5Cright%29%5E%7B2%7D" alt="[公式]"></p><h2 id="4-关于”温度”的讨论"><a href="#4-关于”温度”的讨论" class="headerlink" title="4. 关于”温度”的讨论"></a>4. 关于”温度”的讨论</h2><p>【问题】 我们都知道“蒸馏”需要在高温下进行，那么这个“蒸馏”的温度代表了什么，又是如何选取合适的温度？</p><p><img src="https://pic2.zhimg.com/80/v2-a120cc4bbb70b96968210b995b2e39d1_720w.jpg" alt="img">随着温度T的增大，概率分布的熵逐渐增大</p><h3 id="4-1-温度的特点"><a href="#4-1-温度的特点" class="headerlink" title="4.1. 温度的特点"></a>4.1. 温度的特点</h3><p>在回答这个问题之前，先讨论一下<strong>温度T的特点</strong></p><ol><li>原始的softmax函数是 <img src="https://www.zhihu.com/equation?tex=T%3D1+" alt="[公式]"> 时的特例， <img src="https://www.zhihu.com/equation?tex=T%3C1" alt="[公式]"> 时，概率分布比原始更“陡峭”， <img src="https://www.zhihu.com/equation?tex=T%3E1" alt="[公式]"> 时，概率分布比原始更“平缓”。</li><li>温度越高，softmax上各个值的分布就越平均（思考极端情况: (i) <img src="https://www.zhihu.com/equation?tex=T%3D%5Cinfty" alt="[公式]"> , 此时softmax的值是平均分布的；(ii) <img src="https://www.zhihu.com/equation?tex=T%5Crightarrow0" alt="[公式]">，此时softmax的值就相当于 <img src="https://www.zhihu.com/equation?tex=argmax" alt="[公式]"> , 即最大的概率处的值趋近于1，而其他值趋近于0）</li><li>不管温度T怎么取值，Soft target都有忽略相对较小的 <img src="https://www.zhihu.com/equation?tex=p_i" alt="[公式]"> 携带的信息的倾向</li></ol><h3 id="4-2-温度代表了什么，如何选取合适的温度？"><a href="#4-2-温度代表了什么，如何选取合适的温度？" class="headerlink" title="4.2. 温度代表了什么，如何选取合适的温度？"></a><strong>4.2. 温度代表了什么，如何选取合适的温度？</strong></h3><p><strong>温度的高低改变的是Net-S训练过程中对负标签的关注程度</strong>: 温度较低时，对负标签的关注，尤其是那些显著低于平均值的负标签的关注较少；而温度较高时，负标签相关的值会相对增大，Net-S会相对多地关注到负标签。</p><p>实际上，负标签中包含一定的信息，尤其是那些值显著<strong>高于</strong>平均值的负标签。但由于Net-T的训练过程决定了负标签部分比较noisy，并且负标签的值越低，其信息就越不可靠。因此温度的选取比较empirical，本质上就是在下面两件事之中取舍:</p><ol><li>从有部分信息量的负标签中学习 —&gt; 温度要高一些</li><li>防止受负标签中噪声的影响 —&gt;温度要低一些</li></ol><p>总的来说，T的选择和Net-S的大小有关，Net-S参数量比较小的时候，相对比较低的温度就可以了（因为参数量小的模型不能capture all knowledge，所以可以适当忽略掉一些负标签的信息）</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;知识蒸馏是一种模型压缩方法，是一种基于“教师-学生网络思想”的训练方法，由于其简单，有效，在工业界被广泛应用。这一技术的理论来自于2015年Hinton发表的一篇神作: &lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Distilling the Knowledge in a Neural Network&lt;/a&gt;。Knowledge Distillation，简称KD，顾名思义，就是将已经训练好的模型包含的知识(”Knowledge”)，蒸馏(“Distill”)提取到另一个模型里面去。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="Knowledge Distillation" scheme="https://blog.nicehuster.cn/tags/Knowledge-Distillation/"/>
    
  </entry>
  
  <entry>
    <title>Swin-transformer</title>
    <link href="https://blog.nicehuster.cn/2021/08/12/swin/"/>
    <id>https://blog.nicehuster.cn/2021/08/12/swin/</id>
    <published>2021-08-12T11:13:39.000Z</published>
    <updated>2022-04-12T04:14:34.226Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/pdf/2103.14030.pdf" target="_blank" rel="noopener">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a><br><strong>代码链接：</strong><a href="https://github.com/microsoft/Swin-Transformer" target="_blank" rel="noopener">https://github.com/microsoft/Swin-Transformer</a><br><strong>整体信息：</strong> Swin Transformer 提出了一种针对视觉任务的通用的 Transformer 架构，Transformer 架构在 NLP 任务中已经算得上一种通用的架构，但是如果想迁移到视觉任务中有一个比较大的困难就是处理数据的尺寸不一样。作者分析表明，Transformer 从 NLP 迁移到 CV 上没有大放异彩主要有两点原因：(1)两个领域涉及的scale不同，NLP的scale是标准固定的，而CV的scale变化范围非常大。(2) CV比起NLP需要更大的分辨率，而且CV中使用Transformer的计算复杂度是图像尺度的平方，这会导致计算量过于庞大。为了解决这两个问题，Swin Transformer相比之前的ViT做了两个改进：1.引入CNN中常用的层次化构建方式构建层次化Transformer 2.引入locality思想，对无重合的window区域内进行self-attention计算。</p><p><img src="/img/swin.png" alt="swin"></p><a id="more"></a><p>相比于ViT，Swin Transfomer计算复杂度大幅度降低，具有输入图像大小线性计算复杂度。Swin Transformer随着深度加深，逐渐合并图像块来构建层次化Transformer，可以作为通用的视觉骨干网络，应用于图像分类、目标检测、语义分割等任务。</p><h3 id="1-整体结构"><a href="#1-整体结构" class="headerlink" title="1. 整体结构"></a>1. 整体结构</h3><p>我们先看下Swin Transformer的整体架构:</p><p><img src="/img/swin-arch.png" alt="swin"></p><p>整个模型采取层次化的设计，一共包含4个Stage，每个stage都会缩小输入特征图的分辨率，像CNN一样逐层扩大感受野。</p><ul><li>在输入开始的时候，做了一个<code>Patch Embedding</code>，将图片切成一个个图块，并嵌入到<code>Embedding</code>。</li><li>在每个Stage里，由<code>Patch Merging</code>和多个Block组成。</li><li>其中<code>Patch Merging</code>模块主要在每个Stage一开始降低图片分辨率。</li><li>而Block具体结构如右图所示，主要是<code>LayerNorm</code>，<code>MLP</code>，<code>Window Attention</code> 和 <code>Shifted Window Attention</code>组成 (为了方便讲解，我会省略掉一些参数)</li></ul><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SwinTransformer</span>(<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(...)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">super</span>().__init_<span class="number">_</span>()</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># absolute position embedding</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">ape:</span></span><br><span class="line">            <span class="keyword">self</span>.absolute_pos_embed = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches, embed_dim))</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">self</span>.pos_drop = nn.Dropout(p=drop_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># build layers</span></span><br><span class="line">        <span class="keyword">self</span>.layers = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i_layer <span class="keyword">in</span> range(<span class="keyword">self</span>.num_layers)<span class="symbol">:</span></span><br><span class="line">            layer = BasicLayer(...)</span><br><span class="line">            <span class="keyword">self</span>.layers.append(layer)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.norm = norm_layer(<span class="keyword">self</span>.num_features)</span><br><span class="line">        <span class="keyword">self</span>.avgpool = nn.AdaptiveAvgPool1d(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.head = nn.Linear(<span class="keyword">self</span>.num_features, num_classes) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_features</span><span class="params">(<span class="keyword">self</span>, x)</span></span><span class="symbol">:</span></span><br><span class="line">        x = <span class="keyword">self</span>.patch_embed(x)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">ape:</span></span><br><span class="line">            x = x + <span class="keyword">self</span>.absolute_pos_embed</span><br><span class="line">        x = <span class="keyword">self</span>.pos_drop(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="keyword">self</span>.<span class="symbol">layers:</span></span><br><span class="line">            x = layer(x)</span><br><span class="line"></span><br><span class="line">        x = <span class="keyword">self</span>.norm(x)  <span class="comment"># B L C</span></span><br><span class="line">        x = <span class="keyword">self</span>.avgpool(x.transpose(<span class="number">1</span>, <span class="number">2</span>))  <span class="comment"># B C 1</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(<span class="keyword">self</span>, x)</span></span><span class="symbol">:</span></span><br><span class="line">        x = <span class="keyword">self</span>.forward_features(x)</span><br><span class="line">        x = <span class="keyword">self</span>.head(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>其中有几个地方处理方法与ViT不同：</p><blockquote><ul><li>ViT在输入会给embedding进行位置编码。而Swin-T这里则是作为一个<strong>可选项</strong>（<code>self.ape</code>），Swin-T是在计算Attention的时候做了一个<code>相对位置编码</code>;</li><li>ViT会单独加上一个可学习参数，作为分类的token。而Swin-T则是<strong>直接做平均</strong>，输出分类，有点类似CNN最后的全局平均池化层;</li></ul></blockquote><h3 id="2-Patch-Embedding"><a href="#2-Patch-Embedding" class="headerlink" title="2. Patch Embedding"></a>2. Patch Embedding</h3><p>在输入进Block前，我们需要将图片切成一个个patch，然后嵌入向量。具体做法是对原始图片裁成一个个 <code>window_size * window_size</code>的窗口大小，然后进行嵌入。这里可以通过二维卷积层，<strong>将stride，kernelsize设置为window_size大小</strong>。设定输出通道来确定嵌入向量的大小。最后将H,W维度展开，并移动到第一维度:</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> torch</span><br><span class="line"><span class="built_in">import</span> torch.nn as nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class PatchEmbed(nn.Module):</span><br><span class="line">    def __init__(self, <span class="attr">img_size=224,</span> <span class="attr">patch_size=4,</span> <span class="attr">in_chans=3,</span> <span class="attr">embed_dim=96,</span> <span class="attr">norm_layer=None):</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="attr">img_size</span> = to_2tuple(img_size) <span class="comment"># -&gt; (img_size, img_size)</span></span><br><span class="line">        <span class="attr">patch_size</span> = to_2tuple(patch_size) <span class="comment"># -&gt; (patch_size, patch_size)</span></span><br><span class="line">        <span class="attr">patches_resolution</span> = [img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>]]</span><br><span class="line">        self.<span class="attr">img_size</span> = img_size</span><br><span class="line">        self.<span class="attr">patch_size</span> = patch_size</span><br><span class="line">        self.<span class="attr">patches_resolution</span> = patches_resolution</span><br><span class="line">        self.<span class="attr">num_patches</span> = patches_resolution[<span class="number">0</span>] * patches_resolution[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        self.<span class="attr">in_chans</span> = in_chans</span><br><span class="line">        self.<span class="attr">embed_dim</span> = embed_dim</span><br><span class="line"></span><br><span class="line">        self.<span class="attr">proj</span> = nn.Conv2d(in_chans, embed_dim, <span class="attr">kernel_size=patch_size,</span> <span class="attr">stride=patch_size)</span></span><br><span class="line">        <span class="keyword">if</span> norm_layer is not None:</span><br><span class="line">            self.<span class="attr">norm</span> = norm_layer(embed_dim)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.<span class="attr">norm</span> = None</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        <span class="comment"># 假设采取默认参数</span></span><br><span class="line">        <span class="attr">x</span> = self.proj(x) <span class="comment"># 出来的是(N, 96, 224/4, 224/4) </span></span><br><span class="line">        <span class="attr">x</span> = torch.flatten(x, <span class="number">2</span>) <span class="comment"># 把HW维展开，(N, 96, 56*56)</span></span><br><span class="line">        <span class="attr">x</span> = torch.transpose(x, <span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 把通道维放到最后 (N, 56*56, 96)</span></span><br><span class="line">        <span class="keyword">if</span> self.norm is not None:</span><br><span class="line">            <span class="attr">x</span> = self.norm(x)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure><h3 id="3-Patch-Merging"><a href="#3-Patch-Merging" class="headerlink" title="3.Patch Merging"></a>3.Patch Merging</h3><p>该模块的作用是在每个Stage开始前做降采样，用于缩小分辨率，调整通道数 进而形成层次化的设计，同时也能节省一定运算量。在CNN中，则是在每个Stage开始前用<code>stride=2</code>的卷积/池化层来降低分辨率。每次降采样是两倍，因此<strong>在行方向和列方向上，间隔2选取元素</strong>。然后拼接在一起作为一整个张量，最后展开。<strong>此时通道维度会变成原先的4倍</strong>（因为H,W各缩小2倍），此时再通过一个<strong>全连接层再调整通道维度为原来的两倍</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PatchMerging</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_resolution, dim, norm_layer=nn.LayerNorm)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.input_resolution = input_resolution</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.reduction = nn.Linear(<span class="number">4</span> * dim, <span class="number">2</span> * dim, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.norm = norm_layer(<span class="number">4</span> * dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        x: B, H*W, C</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        H, W = self.input_resolution</span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">"input feature has wrong size"</span></span><br><span class="line">        <span class="keyword">assert</span> H % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> W % <span class="number">2</span> == <span class="number">0</span>, <span class="string">f"x size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) are not even."</span></span><br><span class="line"></span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line"></span><br><span class="line">        x0 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x1 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x2 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x3 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x = torch.cat([x0, x1, x2, x3], <span class="number">-1</span>)  <span class="comment"># B H/2 W/2 4*C</span></span><br><span class="line">        x = x.view(B, <span class="number">-1</span>, <span class="number">4</span> * C)  <span class="comment"># B H/2*W/2 4*C</span></span><br><span class="line"></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.reduction(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>下面是一个示意图（输入张量N=1, H=W=8, C=1，不包含最后的全连接层调整）</p><p><img src="/img/patch_merging.png" alt="swin"></p><h3 id="3-Window-Attention"><a href="#3-Window-Attention" class="headerlink" title="3. Window Attention"></a>3. Window Attention</h3><p>这是这篇文章的关键。传统的Transformer都是<strong>基于全局来计算注意力的</strong>，因此计算复杂度十分高。而Swin Transformer则将<strong>注意力的计算限制在每个窗口内</strong>，进而减少了计算量。我们先简单看下公式：</p><script type="math/tex; mode=display">\operatorname{Attention}(Q, K, V)=\operatorname{SoftMax}\left(Q K^{T} / \sqrt{d}+B\right) V</script><p>主要区别是在原始计算Attention的公式中的Q,K时<strong>加入了相对位置编码</strong>。后续实验有证明相对位置编码的加入提升了模型性能。</p><h3 id="4-Shifted-Window-Attention"><a href="#4-Shifted-Window-Attention" class="headerlink" title="4. Shifted Window Attention"></a>4. Shifted Window Attention</h3><p>前面的Window Attention是在每个窗口下计算注意力的，为了更好的和其他window进行信息交互，Swin Transformer还引入了shifted window操作。</p><p><img src="/img/shifted.png" alt="swin"></p><p>左边是没有重叠的Window Attention，而右边则是将窗口进行移位的Shift Window Attention。可以看到移位后的窗口包含了原本相邻窗口的元素。但这也引入了一个新问题，即<strong>window的个数翻倍了</strong>，由原本四个窗口变成了9个窗口。在实际代码里，我们是<strong>通过对特征图移位，并给Attention设置mask来间接实现的</strong>。能在<strong>保持原有的window个数下</strong>，最后的计算结果等价。</p><p><img src="/img/partition.png" alt="swin"></p><h4 id="4-1-特征图移位操作"><a href="#4-1-特征图移位操作" class="headerlink" title="4.1 特征图移位操作"></a>4.1 特征图移位操作</h4><p>代码里对特征图移位是通过<code>torch.roll</code>来实现的，下面是示意图</p><p><img src="/img/shift_opt.png" alt="swin"></p><blockquote><p>第一位操作是针对行进行移位，第二位操作时针对列进行移位操作。如果需要<code>reverse cyclic shift</code>的话只需把参数<code>shifts</code>设置为对应的正数值。</p></blockquote><h4 id="4-2-attention-mask"><a href="#4-2-attention-mask" class="headerlink" title="4.2 attention mask"></a>4.2 attention mask</h4><p>我认为这是Swin Transformer的精华，通过设置合理的mask，让<code>Shifted Window Attention</code>在与<code>Window Attention</code>相同的窗口个数下，达到等价的计算结果。首先我们对Shift Window后的每个窗口都给上index，并且做一个<code>roll</code>操作（window_size=2, shift_size=1）</p><p><img src="/img/shift_index.jpg" alt="swin"></p><p>我们希望在计算Attention的时候，<strong>让具有相同index QK进行计算，而忽略不同index QK计算结果</strong>。最后正确的结果如下图所示.</p><p><strong>例1：</strong>比如右上角这个 window，如下图所示。它由4个 patch 组成，所以应该计算出的 attention map是4×4的。但是6和4是2个不同的 sub-window，我们又不想让它们的 attention 发生交叠。所以我们希望的 attention map 和attention  mask如下图所示。</p><p><img src="/img/att_mask.png" alt="swin"></p><p><strong>例2：</strong>比如右下角这个 window，对应的 attention map 和attention  mask是下面这个样子。</p><p><img src="/img/att_mask2.png" alt="swin"></p><h3 id="5-transformer-block-整体结构"><a href="#5-transformer-block-整体结构" class="headerlink" title="5. transformer block 整体结构"></a>5. transformer block 整体结构</h3><p><img src="/img/block.png" alt="swin"></p><p>两个连续的Block架构如上图所示，需要注意的是一个Stage包含的Block个数必须是偶数，因为需要交替包含一个含有<code>Window Attention</code>的Layer和含有<code>Shifted Window Attention</code>的Layer。我们看下Block的前向代码：</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, x):</span><br><span class="line">    H, <span class="attr">W</span> = self.input_resolution</span><br><span class="line">    B, L, <span class="attr">C</span> = x.shape</span><br><span class="line">    <span class="keyword">assert</span> <span class="attr">L</span> == H * W, <span class="string">"input feature has wrong size"</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">shortcut</span> = x</span><br><span class="line">    <span class="attr">x</span> = self.norm1(x)</span><br><span class="line">    <span class="attr">x</span> = x.view(B, H, W, C)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># cyclic shift</span></span><br><span class="line">    <span class="keyword">if</span> self.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="attr">shifted_x</span> = torch.roll(x, <span class="attr">shifts=(-self.shift_size,</span> -self.shift_size), <span class="attr">dims=(1,</span> <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="attr">shifted_x</span> = x</span><br><span class="line"></span><br><span class="line">    <span class="comment"># partition windows</span></span><br><span class="line">    <span class="attr">x_windows</span> = window_partition(shifted_x, self.window_size)  <span class="comment"># nW*B, window_size, window_size, C</span></span><br><span class="line">    <span class="attr">x_windows</span> = x_windows.view(-<span class="number">1</span>, self.window_size * self.window_size, C)  <span class="comment"># nW*B, window_size*window_size, C</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># W-MSA/SW-MSA</span></span><br><span class="line">    <span class="attr">attn_windows</span> = self.attn(x_windows, <span class="attr">mask=self.attn_mask)</span>  <span class="comment"># nW*B, window_size*window_size, C</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># merge windows</span></span><br><span class="line">    <span class="attr">attn_windows</span> = attn_windows.view(-<span class="number">1</span>, self.window_size, self.window_size, C)</span><br><span class="line">    <span class="attr">shifted_x</span> = window_reverse(attn_windows, self.window_size, H, W)  <span class="comment"># B H' W' C</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># reverse cyclic shift</span></span><br><span class="line">    <span class="keyword">if</span> self.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="attr">x</span> = torch.roll(shifted_x, <span class="attr">shifts=(self.shift_size,</span> self.shift_size), <span class="attr">dims=(1,</span> <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="attr">x</span> = shifted_x</span><br><span class="line">    <span class="attr">x</span> = x.view(B, H * W, C)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># FFN</span></span><br><span class="line">    <span class="attr">x</span> = shortcut + self.drop_path(x)</span><br><span class="line">    <span class="attr">x</span> = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line"></span><br><span class="line">    return x</span><br></pre></td></tr></table></figure><p>整体流程如下：</p><ul><li>先对特征图进行LayerNorm</li><li>通过<code>self.shift_size</code>决定是否需要对特征图进行shift</li><li>然后将特征图切成一个个窗口</li><li>计算Attention，通过<code>self.attn_mask</code>来区分<code>Window Attention</code>还是<code>Shift Window Attention</code></li><li>将各个窗口合并回来</li><li>如果之前有做shift操作，此时进行<code>reverse shift</code>，把之前的shift操作恢复</li><li>做dropout和残差连接</li><li>再通过一层LayerNorm+全连接层，以及dropout和残差连接</li></ul><h3 id="6-experiments"><a href="#6-experiments" class="headerlink" title="6. experiments"></a>6. experiments</h3><p><img src="/img/ink21.png" alt="swin"></p><p>在ImageNet22K数据集上，准确率能达到惊人的86.4%。另外在检测，分割等任务上表现也很优异，感兴趣的可以翻看论文最后的实验部分。</p><h3 id="7-conclusion"><a href="#7-conclusion" class="headerlink" title="7. conclusion"></a>7. conclusion</h3><p>这篇文章创新点很棒，引入window这一个概念，将CNN的局部性引入，还能控制模型整体计算量。在Shift Window Attention部分，用一个mask和移位操作，很巧妙的实现计算等价。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2103.14030.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Swin Transformer: Hierarchical Vision Transformer using Shifted Windows&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/microsoft/Swin-Transformer&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/microsoft/Swin-Transformer&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt; Swin Transformer 提出了一种针对视觉任务的通用的 Transformer 架构，Transformer 架构在 NLP 任务中已经算得上一种通用的架构，但是如果想迁移到视觉任务中有一个比较大的困难就是处理数据的尺寸不一样。作者分析表明，Transformer 从 NLP 迁移到 CV 上没有大放异彩主要有两点原因：(1)两个领域涉及的scale不同，NLP的scale是标准固定的，而CV的scale变化范围非常大。(2) CV比起NLP需要更大的分辨率，而且CV中使用Transformer的计算复杂度是图像尺度的平方，这会导致计算量过于庞大。为了解决这两个问题，Swin Transformer相比之前的ViT做了两个改进：1.引入CNN中常用的层次化构建方式构建层次化Transformer 2.引入locality思想，对无重合的window区域内进行self-attention计算。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/swin.png&quot; alt=&quot;swin&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="transformer" scheme="https://blog.nicehuster.cn/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>Transformer 在图像领域应用的开拓者VIT</title>
    <link href="https://blog.nicehuster.cn/2021/07/21/vit/"/>
    <id>https://blog.nicehuster.cn/2021/07/21/vit/</id>
    <published>2021-07-21T11:13:39.000Z</published>
    <updated>2022-04-12T04:11:36.659Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/pdf/2010.11929.pdf" target="_blank" rel="noopener">An image is worth 16x16 words: Transformers for image recognition at scale</a><br><strong>代码链接：</strong><a href="https://github.com/google-research/vision_transformer" target="_blank" rel="noopener">https://github.com/google-research/vision_transformer</a><br><strong>整体信息：</strong>ViT（vision transformer）是Google在2020年提出的直接将transformer应用在图像分类的模型，后面很多的工作都是基于ViT进行改进的。ViT的思路很简单：直接把图像分成固定大小的patchs，然后通过线性变换得到patch embedding，这就类比NLP的words和word embedding，由于transformer的输入就是a sequence of token embeddings，所以将图像的patch embeddings送入transformer后就能够进行特征提取从而分类了。</p><p><img src="/img/arch.png" alt="vit"></p><a id="more"></a><p>ViT模型原理如上图所示，其实ViT模型只是用了transformer的Encoder来提取特征（原始的transformer还有decoder部分，用于实现sequence to sequence，比如机器翻译）。下面将分别对各个部分做详细的介绍。</p><h3 id="1-Patch-Embedding"><a href="#1-Patch-Embedding" class="headerlink" title="1.Patch Embedding"></a>1.Patch Embedding</h3><p>对于ViT来说，首先要将原始patch形式的2-D图像转换成一系列1-D的patch embeddings，这就好似NLP中的word embedding。输入的2-D图像记为$\mathbf{x} \in \mathbb{R}^{H \times W \times C}$,其中$H$和$W$分别是图像的高和宽，而$C$为通道数，对于RGB图像为3。如果将图像分为大小为$P \times P$的patchs,可以通过reshape等操作得到一系列patchs：$\mathbf{x}_{p} \in \mathbb{R}^{N \times\left(P^{2} \cdot C\right)}$ ,总共可以得到的patch数是$N=H W / P^{2}$,这个就是序列的长度。注意这里直接将patch拉平为1-D，其特征大小为$P^{2} \cdot C$ .然后通过一个简单的线性变换将patchs映射成D大小的维度，这就是patch embeddings:$\mathbf{x}_{\mathbf{p}}^{\prime} \in \mathbb{R}^{N \times D}$, 在实现上等同于对于x进行一个$P \times P$ 且stride为$P$ 的卷积操作。下面是具体的实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PatchEmbed</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" Image to Patch Embedding</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, embed_dim=<span class="number">768</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        img_size = to_2tuple(img_size)</span><br><span class="line">        patch_size = to_2tuple(patch_size)</span><br><span class="line">        num_patches = (img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>]) * (img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>])</span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.num_patches = num_patches</span><br><span class="line"></span><br><span class="line">        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        <span class="comment"># FIXME look at relaxing size constraints</span></span><br><span class="line">        <span class="keyword">assert</span> H == self.img_size[<span class="number">0</span>] <span class="keyword">and</span> W == self.img_size[<span class="number">1</span>], \</span><br><span class="line">            <span class="string">f"Input image size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) doesn't match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>*<span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>)."</span></span><br><span class="line">        x = self.proj(x).flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="2-Position-Embedding"><a href="#2-Position-Embedding" class="headerlink" title="2. Position Embedding"></a>2. Position Embedding</h3><p>除了patch embeddings，模型还需要另外一个特殊的position embedding。transformer和CNN不同，需要position embedding来编码tokens的位置信息，这主要是因为self-attention是permutation-invariant，即打乱sequence里的tokens的顺序并不会改变结果。如果不给模型提供patch的位置信息，那么模型就需要通过patchs的语义来学习拼图，这就额外增加了学习成本。ViT论文中对比了几种不同的position embedding方案(如下），最后发现如果不提供positional embedding效果会差，但其它各种类型的positional embedding效果都接近，这主要是因为ViT的输入是相对较大的patchs而不是pixels，所以学习位置信息相对容易很多。</p><ul><li>无positional embedding</li><li>1-D positional embedding：把2-D的patchs看成1-D序列</li><li>2-D positional embedding：考虑patchs的2-D位置（x, y）</li><li>Relative positional embeddings：patchs的相对位置</li></ul><p>在ViT中默认采用学习（训练的）的1-D positional embedding，在输入transformer的encoder之前直接将patch embeddings和positional embedding相加:</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里多1是为了后面要说的class token，embed_dim即patch embed_dim</span></span><br><span class="line"><span class="keyword">self</span>.pos_embed = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches + <span class="number">1</span>, embed_dim)) </span><br><span class="line"></span><br><span class="line"><span class="comment"># patch emded + pos_embed</span></span><br><span class="line">x = x + <span class="keyword">self</span>.pos_embed</span><br></pre></td></tr></table></figure><p>论文中也对学习到的positional embedding进行了可视化，发现相近的patchs的positional embedding比较相似，而且同行或同列的positional embedding也相近：</p><p><img src="/img/position_embedding.png" alt="vit"></p><p>这里额外要注意的一点，如果改变图像的输入大小，ViT不会改变patchs的大小，那patch的数量$N$也会发生变化，那么之前学习的pos_embed就维度对不上了，ViT采用的方案是通过插值来解决这个问题。但是这种情形一般会造成性能少许损失，可以通过finetune模型来解决。另外最新的论文<a href="https://arxiv.org/pdf/2102.10882.pdf" target="_blank" rel="noopener">CPVT</a>通过implicit Conditional Position encoding来解决这个问题（插入Conv来隐式编码位置信息，zero padding让Conv学习到绝对位置信息）。</p><h3 id="3-Class-Token"><a href="#3-Class-Token" class="headerlink" title="3. Class Token"></a>3. Class Token</h3><p>除了patch token，ViT借鉴BERT还增加了一个特殊的class token。后面会说，transformer的encoder输入是a sequence patch embeddings，输出也是同样长度的a sequence patch features，但图像分类最后需要获取image feature，简单的策略是采用pooling，比如求patch features的平均来获取image feature，但是ViT并没有采用类似的pooling策略，而是直接增加一个特殊的class token，其最后输出的特征加一个linear classifier就可以实现对图像的分类（ViT的pre-training时是接一个MLP head），所以输入ViT的sequence长度是$N+1$,class token对应的embedding在训练时随机初始化，然后通过训练得到，具体实现如下：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机初始化</span></span><br><span class="line">self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Classifier head</span></span><br><span class="line">self.head = nn.Linear(self.num_features, num_classes) if num_classes &gt; 0 <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 具体forward过程</span></span><br><span class="line">B = x.shape[0]</span><br><span class="line">x = self.patch_embed(x)</span><br><span class="line">cls_tokens = self.cls_token.expand(B, -1, -1)  <span class="comment"># stole cls_tokens impl from Phil Wang, thanks</span></span><br><span class="line">x = torch.cat((cls_tokens, x), dim=1)</span><br><span class="line">x = x + self.pos_embed</span><br></pre></td></tr></table></figure><h3 id="4-Transformer-Encoder"><a href="#4-Transformer-Encoder" class="headerlink" title="4. Transformer Encoder"></a>4. Transformer Encoder</h3><p>transformer最核心的操作就是self-attention，其实attention机制很早就在NLP和CV领域应用了，比如带有attention机制的seq2seq模型，但是transformer完全摒弃RNN或LSTM结构，直接采用attention机制反而取得了更好的效果：attention is all you need！简单来说，attention就是根据当前查询对输入信息赋予不同的权重来聚合信息，从操作上看就是一种“加权平均”。attention中共有3个概念：query, key和value，其中key和value是成对的，对于一个给定的query向量$q \in \mathbb{R}^{d}$,通过计算内积来匹配k个key向量(维度也是d,$K \in \mathbb{R}^{k \times d}$),得到的内积通过softmax来归一化得到k个权重，那么对于query其attention的输出就是k个key向量对应的value向量（即矩阵$V \in \mathbb{R}^{k \times d}$),对于一系列的N个query(即矩阵$Q \in \mathbb{R}^{N \times d}$)，可以通过矩阵计算它们的attention输出：</p><script type="math/tex; mode=display">\operatorname{Attention}(Q, K, V)=\operatorname{Softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><p>这里的$\sqrt{d_{k}}$为缩放因子以避免点击带来的方差影响。上述的Attention机制称为<strong>Scaled dot product attention</strong>，其实attention机制的变种有很多，但基本原理是相似的。如果$Q,K,V$都是从一个包含$N$个向量的sequence($X \in \mathbb{R}^{N \times D}$)变换得到：</p><script type="math/tex; mode=display">Q=X W_{Q}, K=X W_{K}, V=X W_{V}</script><p>那么此时就变成了<strong>self-attention</strong>，这个时候就有N个(key,value)对,self-attention是transformer最核心部分，self-attention其实就是输入向量之间进行相互attention来学习到新特征。前面说过我们已经得到图像的patch sequence，那么送入self-attention就能到同样size的sequence输出，只不过特征改变了。更进一步，transformer采用的是<strong>multi-head self-attention (MSA）</strong>，所谓的MSA就是采用定义h个attention heads，即采用h个self-attention应用在输入sequence上，在操作上可以将sequence拆分成h个size为$N \times d$，这里$D=h d$,不同的heads得到的输出concat在一起然后通过线性变换得到最终的输出,size也是$N \times D$:</p><script type="math/tex; mode=display">\operatorname{MSA}(\mathbf{z})=\left[\mathrm{SA}_{1}(z) ; \mathrm{SA}_{2}(z) ; \cdots ; \mathrm{SA}_{k}(z)\right] \mathrm{U}_{m s a} \quad \mathrm{U}_{m s a} \in \mathbb{R}^{k \cdot D_{h} \times D}</script><p>MSA的计算量是和$N^{2}$成比例的，所以ViT的输入是patch embeddings，而不是pixel embeddings，这有计算量上的考虑。在实现上，MSA是可以并行计算各个head的，具体代码如下：</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">Attention</span>(<span class="title">nn</span>.<span class="type">Module</span>):</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>, <span class="title">dim</span>, <span class="title">num_heads</span>=8, <span class="title">qkv_bias</span>=<span class="type">False</span>, <span class="title">qk_scale</span>=<span class="type">None</span>, <span class="title">attn_drop</span>=0., <span class="title">proj_drop</span>=0.):</span></span><br><span class="line"><span class="class">        super().__init__()</span></span><br><span class="line"><span class="class">        self.num_heads = num_heads</span></span><br><span class="line"><span class="class">        head_dim = dim // num_heads</span></span><br><span class="line"><span class="class">    </span></span><br><span class="line"><span class="class">        self.scale = qk_scale or head_dim ** -0.5</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        self.qkv = nn.<span class="type">Linear</span>(<span class="title">dim</span>, <span class="title">dim</span> * 3, <span class="title">bias</span>=<span class="title">qkv_bias</span>)</span></span><br><span class="line"><span class="class">        self.attn_drop = nn.<span class="type">Dropout</span>(<span class="title">attn_drop</span>)</span></span><br><span class="line"><span class="class">        self.proj = nn.<span class="type">Linear</span>(<span class="title">dim</span>, <span class="title">dim</span>)</span></span><br><span class="line"><span class="class">        # 这里包含了dropout</span></span><br><span class="line"><span class="class">        self.proj_drop = nn.<span class="type">Dropout</span>(<span class="title">proj_drop</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>, <span class="title">x</span>):</span></span><br><span class="line"><span class="class">        <span class="type">B</span>, <span class="type">N</span>, <span class="type">C</span> = x.shape</span></span><br><span class="line"><span class="class">        qkv = self.qkv(<span class="title">x</span>).reshape(<span class="type">B</span>, <span class="type">N</span>, 3, <span class="title">self</span>.<span class="title">num_heads</span>, <span class="type">C</span> // <span class="title">self</span>.<span class="title">num_heads</span>).permute(2, 0, 3, 1, 4)</span></span><br><span class="line"><span class="class">        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (<span class="title">cannot</span> <span class="title">use</span> <span class="title">tensor</span> <span class="title">as</span> <span class="title">tuple</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        attn = (<span class="title">q</span> @ <span class="title">k</span>.<span class="title">transpose</span>(-2, -1)) * self.scale</span></span><br><span class="line"><span class="class">        attn = attn.softmax(<span class="title">dim</span>=-1)</span></span><br><span class="line"><span class="class">        attn = self.attn_drop(<span class="title">attn</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        x = (<span class="title">attn</span> @ <span class="title">v</span>).transpose(1, 2).reshape(<span class="type">B</span>, <span class="type">N</span>, <span class="type">C</span>)</span></span><br><span class="line"><span class="class">        x = self.proj(<span class="title">x</span>)</span></span><br><span class="line"><span class="class">        x = self.proj_drop(<span class="title">x</span>)</span></span><br><span class="line"><span class="class">        return x</span></span><br></pre></td></tr></table></figure><p>在transformer中，MSA后跟一个FFN（Feed-forward network），这个FFN包含两个FC层，第一个FC层将特征从维度D变换成4D，后一个FC层将特征从维度4D变成D，中间的非线性激活函数采用GeLU，其实这就是一个MLP，具体实现如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mlp</span>(<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=<span class="number">0</span>.)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">super</span>().__init_<span class="number">_</span>()</span><br><span class="line">        out_features = out_features <span class="keyword">or</span> in_features</span><br><span class="line">        hidden_features = hidden_features <span class="keyword">or</span> in_features</span><br><span class="line">        <span class="keyword">self</span>.fc1 = nn.Linear(in_features, hidden_features)</span><br><span class="line">        <span class="keyword">self</span>.act = act_layer()</span><br><span class="line">        <span class="keyword">self</span>.fc2 = nn.Linear(hidden_features, out_features)</span><br><span class="line">        <span class="keyword">self</span>.drop = nn.Dropout(drop)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(<span class="keyword">self</span>, x)</span></span><span class="symbol">:</span></span><br><span class="line">        x = <span class="keyword">self</span>.fc1(x)</span><br><span class="line">        x = <span class="keyword">self</span>.act(x)</span><br><span class="line">        x = <span class="keyword">self</span>.drop(x)</span><br><span class="line">        x = <span class="keyword">self</span>.fc2(x)</span><br><span class="line">        x = <span class="keyword">self</span>.drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>那么一个完成transformer encoder block就包含一个MSA后面接一个FFN，其实MSA和FFN均包含和ResNet一样的skip connection，另外MSA和FFN后面都包含layer norm层，具体实现如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class Block(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, dim, num_heads, <span class="attribute">mlp_ratio</span>=4., <span class="attribute">qkv_bias</span>=<span class="literal">False</span>, <span class="attribute">qk_scale</span>=None, <span class="attribute">drop</span>=0., <span class="attribute">attn_drop</span>=0.,</span><br><span class="line">                 <span class="attribute">drop_path</span>=0., <span class="attribute">act_layer</span>=nn.GELU, <span class="attribute">norm_layer</span>=nn.LayerNorm):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.norm1 = norm_layer(dim)</span><br><span class="line">        self.attn = Attention(</span><br><span class="line">            dim, <span class="attribute">num_heads</span>=num_heads, <span class="attribute">qkv_bias</span>=qkv_bias, <span class="attribute">qk_scale</span>=qk_scale, <span class="attribute">attn_drop</span>=attn_drop, <span class="attribute">proj_drop</span>=drop)</span><br><span class="line">        # NOTE: drop path <span class="keyword">for</span> stochastic depth, we shall see <span class="keyword">if</span> this is better than dropout here</span><br><span class="line">        self.drop_path = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; 0. <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.norm2 = norm_layer(dim)</span><br><span class="line">        mlp_hidden_dim = int(dim * mlp_ratio)</span><br><span class="line">        self.mlp = Mlp(<span class="attribute">in_features</span>=dim, <span class="attribute">hidden_features</span>=mlp_hidden_dim, <span class="attribute">act_layer</span>=act_layer, <span class="attribute">drop</span>=drop)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = x + self.drop_path(self.attn(self.norm1(x)))</span><br><span class="line">        x = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure><h3 id="5-ViT"><a href="#5-ViT" class="headerlink" title="5. ViT"></a>5. ViT</h3><p>对于ViT模型来说，就类似CNN那样，不断堆积transformer encoder blocks，最后提取class token对应的特征用于图像分类，论文中也给出了模型的公式表达，其中（1）就是提取图像的patch embeddings，然后和class token对应的embedding拼接在一起并加上positional embedding；（2）是MSA，而（3）是MLP，（2）和（3）共同组成了一个transformer encoder block，共有L层；（4)是对class token对应的输出做layer norm，然后就可以用来图像分类。</p><p><img src="/img/vit-procedu.png" alt="vit"></p><p>ViT模型的超参数主要包括以下，这些超参数直接影响模型参数以及计算量：</p><blockquote><ol><li>Layers：block的数量；</li><li>Hidden size D：隐含层特征，D在各个block是一直不变的；</li><li>MLP size：一般设置为4D大小；</li><li>Heads：MSA中的heads数量；</li><li>Patch size：模型输入的patch size，ViT中共有两个设置：14x14和16x16，这个只影响计算量；</li></ol></blockquote><p>类似BERT，ViT共定义了3中不同大小的模型：Base，Large和Huge，其对应的模型参数不同，如下所示。如ViT-L/16指的是采用Large结构，输入的patch size为16x16。类似BERT，ViT共定义了3中不同大小的模型：Base，Large和Huge，其对应的模型参数不同，如下所示。如ViT-L/16指的是采用Large结构，输入的patch size为16x16。</p><p><img src="/img/models.png" alt="vit"></p><h3 id="6-Experiments"><a href="#6-Experiments" class="headerlink" title="6. Experiments"></a>6. Experiments</h3><p>ViT并不像CNN那样具有inductive bias，论文中发现如果如果直接在ImageNet上训练，同level的ViT模型效果要差于ResNet，但是如果在比较大的数据集上petraining，然后再finetune，效果可以超越ResNet。比如ViT在Google私有的300M JFT数据集上pretrain后，在ImageNet上的最好Top-1 acc可达88.55%，这已经和ImageNet上的SOTA相当了（Noisy Student EfficientNet-L2效果为88.5%，Google最新的SOTA是Meta Pseudo Labels，效果可达90.2%）：</p><p><img src="/img/exp.png" alt="vit"></p><p>那么ViT至少需要多大的数据量才能和CNN旗鼓相当呢？这个论文也做了实验，结果如下图所示，从图上所示这个预训练所使用的数据量要达到100M时才能显示ViT的优势。transformer的一个特色是它的scalability：当模型和数据量提升时，性能持续提升。在大数据面前，ViT可能会发挥更大的优势。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2010.11929.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;An image is worth 16x16 words: Transformers for image recognition at scale&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/google-research/vision_transformer&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/google-research/vision_transformer&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;ViT（vision transformer）是Google在2020年提出的直接将transformer应用在图像分类的模型，后面很多的工作都是基于ViT进行改进的。ViT的思路很简单：直接把图像分成固定大小的patchs，然后通过线性变换得到patch embedding，这就类比NLP的words和word embedding，由于transformer的输入就是a sequence of token embeddings，所以将图像的patch embeddings送入transformer后就能够进行特征提取从而分类了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/arch.png&quot; alt=&quot;vit&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="transformer" scheme="https://blog.nicehuster.cn/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>unbiased teacher-半监督目标检测</title>
    <link href="https://blog.nicehuster.cn/2021/06/05/unbiased-teacher/"/>
    <id>https://blog.nicehuster.cn/2021/06/05/unbiased-teacher/</id>
    <published>2021-06-05T11:13:39.000Z</published>
    <updated>2022-04-24T07:41:54.687Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/abs/2102.09480" target="_blank" rel="noopener">Unbiased Teacher for Semi-Supervised Object Detection</a><br><strong>代码链接：</strong><a href="https://github.com/facebookresearch/unbiased-teacher" target="_blank" rel="noopener">https://github.com/facebookresearch/unbiased-teacher</a><br><strong>整体信息：</strong>这是facebook research在半监督目标检测上的工作。以往的半监督学习多聚焦与分类任务上，在检测任务上少有涉及。解决半监督目标检测的一个直接方法是应用半监督分类方法，但是由于目标检测的特性，图像分类方法并不适合，主要原因在于目标检测任务中的类别不平衡严重阻碍了伪标签的使用。</p><p><img src="/img/unbiased-teacher-ssod.png" alt="img"></p><a id="more"></a><p><img src="/img/unbiased-teacher-overfit.png" alt="img"></p><p><strong>目标检测中存在前景背景不平衡和前景之间类不平衡，这些不平衡导致半监督训练中产生有偏差的预测</strong>。伪标签作为图像分类半监督中最有效的方法之一，在目标检测中因为倾向置信度高的类别和区域导致产生偏差。<strong>将偏差伪标签加入训练会加重类不平衡问题和产生过拟合</strong>。如上图所示。</p><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p><img src="/img/unbiased-teacher-pipeline.png" alt="img"></p><p>如上图所示，Unbiased Teacher框架包含俩个部分，Burn-In 和 Teacher-Student Mutual Learning。在Burn-In 阶段，使用所有有标注数据训练目标检测器。在第二阶段，我们先将Burn-In训练好的检测器复制给两个模型（Teacher和Student）。第二阶段的目标是通过一种共同学习的机制来更新Teacher和Student模型，Teacher模型产生伪标签用于训练Student模型，然后student模型学到的知识再用于更新teacher模型，因此用于训练student模型的伪标签本身也在自我更新。</p><h4 id="1-Burn-In"><a href="#1-Burn-In" class="headerlink" title="1. Burn In"></a>1. Burn In</h4><p>该阶段使用所有标注数据来训练检测模型，使用$L_{sup}$ 优化模型参数：</p><script type="math/tex; mode=display">\mathcal{L}_{s u p}=\sum_{i} \mathcal{L}_{\text {cls }}^{r p n}\left(\boldsymbol{x}_{i}^{s}, \boldsymbol{y}_{i}^{s}\right)+\mathcal{L}_{r e g}^{r p n}\left(\boldsymbol{x}_{i}^{s}, \boldsymbol{y}_{i}^{s}\right)+\mathcal{L}_{\text {cls }}^{r o i}\left(\boldsymbol{x}_{i}^{s}, \boldsymbol{y}_{i}^{s}\right)+\mathcal{L}_{r e g}^{r o i}\left(\boldsymbol{x}_{i}^{s}, \boldsymbol{y}_{i}^{s}\right)</script><p>在作者的appendix有验证burn-in阶段的必要性，在早期阶段，burn-in可以使模型获得更准确的伪标签，而且可以加快模型收敛。</p><h4 id="2-Teacher-Student-Mutual-Learning"><a href="#2-Teacher-Student-Mutual-Learning" class="headerlink" title="2. Teacher-Student Mutual Learning"></a>2. Teacher-Student Mutual Learning</h4><p>teacher模型生成伪标签用于训练student模型，student模型的权重转移给teacher模型以更新teacher模型，在teacher和student的迭代训练中提高了检测准确率。随着检测准确率的提高，teacher产生更为准确和稳定的伪标签，这是性能改善的关键。我们也可以将teacher视为student在不同的时间步里进行temporal ensemble所得，这与我们的研究一致，即teacher的准确率高于student。与先前的研究相比，<strong>改善teacher的关键因素就是student模型的多样性</strong>，所以我们采用强增强图片作为student的输入，使用弱增强图片作为teacher的输入以提供可信赖的伪标签.</p><h5 id="Student-Learning-with-Pseudo-Labeling"><a href="#Student-Learning-with-Pseudo-Labeling" class="headerlink" title="Student Learning with Pseudo-Labeling"></a>Student Learning with Pseudo-Labeling</h5><p>我们首先设置一个置信度阈值 $\delta$，用于过滤低置信度的预测框，这些低置信度的预测框极有可能是错误的正样本。此外，含噪的伪标签会涌向teacher模型，所以我们将teacher和student分开训练。在获得伪标签后只有student的权重可以通过反向传播更新:</p><script type="math/tex; mode=display">\theta_{s} \leftarrow \theta_{s}+\gamma \frac{\partial\left(\mathcal{L}_{s u p}+\boldsymbol{\lambda}_{u} \mathcal{L}_{\text {unsup }}\right)}{\partial \theta_{s}}, \quad \mathcal{L}_{\text {unsup }}=\sum_{i} \mathcal{L}_{c l s}^{r p n}\left(\boldsymbol{x}_{i}^{u}, \hat{\boldsymbol{y}}_{i}^{u}\right)+\mathcal{L}_{c l s}^{r o i}\left(\boldsymbol{x}_{i}^{u}, \hat{\boldsymbol{y}}_{i}^{u}\right)</script><h5 id="Teacher-Refinement-via-Exponential-Moving-Average"><a href="#Teacher-Refinement-via-Exponential-Moving-Average" class="headerlink" title="Teacher Refinement via Exponential Moving Average"></a>Teacher Refinement via Exponential Moving Average</h5><p>为了获得稳定的伪标签，我们应用EMA来逐步更新teacher模型。</p><script type="math/tex; mode=display">\theta_{t} \leftarrow \alpha \theta_{t}+(1-\alpha) \theta_{s}</script><h4 id="3-Bias-in-pseudo-label"><a href="#3-Bias-in-pseudo-label" class="headerlink" title="3. Bias in pseudo-label"></a>3. Bias in pseudo-label</h4><p>理论上来说，基于伪标签的方法可以解决由于标签匮乏带来的问题，但是目标检测任务中的不平衡属性影响了该方法的有效性。目标检测中存在前景-背景不平衡和前景中类不平衡问题，如果在训练数据不充足的情况下使用标准CE，模型会倾向于预测主要类别，这会导致预测偏向数量较多的类别,生成类不平衡的伪标签(偏差伪标签)。在训练时使用偏差伪标签会使不平衡预测问题恶化。</p><p>为了解决这个问题，我们考虑一种简单但是高效的方法，在ROIhead的多类别分类loss中，我们使用Focal loss代替CE loss。Focal loss将给置信度较低的目标实例分配更多的loss权重，这样模型会看重hard目标而不是那些极有可能是主要类别的简单目标。</p><p>另一方面，采用EMA机制可以防止决策边界急剧的向少数类别倾斜，teacher模型的权重可表示为</p><script type="math/tex; mode=display">\theta_{t}^{i}=\hat{\theta}-\gamma \sum_{k=1}^{i-1}\left(1-\alpha^{-k+(i-1)}\right) \frac{\partial\left(\mathcal{L}_{s u p}+\boldsymbol{\lambda}_{u} \mathcal{L}_{u n s u p}\right)}{\partial \theta_{s}^{k}}</script><p>EMA-training使得Teacher模型有利于产生更稳定的伪标签，解决SS-OD中的类失衡问题。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>以FasterRCNN作为目标检测器，置信度阈值$\delta = 0.7$；<strong>弱增强</strong>：随机水平翻转flip；强增强：color jitter, grayscale,gaussian blur, cutout等等。</p><h4 id="COCO-standard"><a href="#COCO-standard" class="headerlink" title="COCO-standard"></a>COCO-standard</h4><p><img src="/img/unbiased-teacher-coco-standard.png" alt="img"></p><p>从上述表格可以看出，有标签数据越少，本文提出的方法改善越大。在作者看来，主要原因在于：（1）伪标签更准确；（2）EMA和Focal loss有效地解决伪标签类失衡问题；</p><h4 id="COCO-additional-and-VOC"><a href="#COCO-additional-and-VOC" class="headerlink" title="COCO-additional and VOC"></a>COCO-additional and VOC</h4><p>作者还有进一步验证在100%标注数据下，使用额外无标注数据能否进一步提升模型性能。</p><p><img src="/img/unbiased-teacher-coco-addition.png" alt="img"></p><h4 id="ablation-study"><a href="#ablation-study" class="headerlink" title="ablation study"></a>ablation study</h4><p><strong>EMA:</strong> teacher和student同步更新，目前半监督图像分类中最优的模型Fixmatch也是同步更新。</p><p><strong>Focal loss：</strong>有使用Focal loss的模型产生的伪标签分布与真是标签分布更相似，将KL散度从1.7915（无EMA无Focal loss）改善到0.2001（无EMA有Focal loss）</p><p><img src="/img/unbiased-teacher-ablation-study.png" alt="img"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这篇文章分析了直接将半监督分类方法应用与目标检测任务时存在的俩个问题：类失衡和过拟合，并提出了unbiased-teacher，简单高效的解决了上述俩个问题。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2102.09480&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Unbiased Teacher for Semi-Supervised Object Detection&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/facebookresearch/unbiased-teacher&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/facebookresearch/unbiased-teacher&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这是facebook research在半监督目标检测上的工作。以往的半监督学习多聚焦与分类任务上，在检测任务上少有涉及。解决半监督目标检测的一个直接方法是应用半监督分类方法，但是由于目标检测的特性，图像分类方法并不适合，主要原因在于目标检测任务中的类别不平衡严重阻碍了伪标签的使用。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/unbiased-teacher-ssod.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="semi-supervised learning" scheme="https://blog.nicehuster.cn/tags/semi-supervised-learning/"/>
    
  </entry>
  
  <entry>
    <title>fixmatch-基于数据增强实现半监督学习</title>
    <link href="https://blog.nicehuster.cn/2021/06/04/fixmatch/"/>
    <id>https://blog.nicehuster.cn/2021/06/04/fixmatch/</id>
    <published>2021-06-04T11:13:39.000Z</published>
    <updated>2022-04-24T07:42:11.780Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong><a href="https://arxiv.org/abs/2001.07685" target="_blank" rel="noopener">FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence</a><br><strong>代码链接：</strong><a href="https://github.com/google-research/fixmatch" target="_blank" rel="noopener">https://github.com/google-research/fixmatch</a><br><strong>整体信息：</strong>这在实际业务场景中，大量的标注数据对于模型性能的提升至关重要，但是获取标注数据是一个耗时耗力的过程，例如工业场景中由于机台型号的更换导致模型性能下降，花费大量时间对新数据进行重新标注大概率会导致模型上线时间delay，而半监督学习(Semi-Supervised Learning, SSL)探究了如何利用大量未标注数据和部分标注数据来提升模型性能。而本文，谷歌提出的fixmatch，是对现有SSL方法进行显著简化的算法。</p><a id="more"></a><p><img src="/img/fixmatch.jpg" alt="preview"></p><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>如上图所示，训练过程包括两个部分，有监督训练和无监督训练。有label的数据，执行有监督训练，和普通分类任务训练没有区别。没有label的数据，经过首先经过弱增强获取伪标签。然后利用该伪标签去监督强增强的输出值，只有大于一定阈值条件才执行伪标签的生成。无监督的训练过程包含两种思想在里面，即一致性正则化和伪标签训练。</p><h4 id="1-一致性正则"><a href="#1-一致性正则" class="headerlink" title="1. 一致性正则"></a>1. 一致性正则</h4><p>一致性正则化是当前半监督SOTA工作中一个重要的组件，其建立在一个基本假设：<strong>相同图片经过不同扰动（增强）经过网络会输出相同预测结果</strong>，因此对这二者进行loss计算便可以对网络进行监督训练，又被称为自监督训练。loss计算如下：</p><script type="math/tex; mode=display">\sum_{b=1}^{\mu B}\left\|p_{\mathrm{m}}\left(y \mid \alpha\left(u_{b}\right)\right)-p_{\mathrm{m}}\left(y \mid \alpha\left(u_{b}\right)\right)\right\|_{2}^{2}</script><p>其中，$\alpha$ 表示随机的弱增强操作。</p><h4 id="2-伪标签"><a href="#2-伪标签" class="headerlink" title="2. 伪标签"></a>2. 伪标签</h4><p>伪标签是利用模型本身为未标记数据获取人工标签的思想。通常是使用“hard”标签，也就是argmax获取的onehot标签，仅保留最大类概率超过阈值的标签。计算loss的时如下：</p><script type="math/tex; mode=display">\frac{1}{\mu B} \sum_{b=1}^{\mu B} \mathbb{1}\left(\max \left(q_{b}\right) \geq \tau\right) \mathrm{H}\left(\hat{q}_{b}, q_{b}\right)</script><p>其中，$\hat{q}_{b}=\arg \max \left(q_{b}\right)$ , $\tau$ 是阈值。</p><h4 id="3-为什么work？"><a href="#3-为什么work？" class="headerlink" title="3.为什么work？"></a>3.为什么work？</h4><p>无监督训练过程实际上是一个孪生网络，可以提取到图片的有用特征。弱增强不至于图像失真，再加上输出伪标签阈值的设置，极大程度上降低了引入错误标签噪声的可能性。而仅仅使用弱增强可能会导致训练过拟合，无法提取到本质的特征，所以使用强增强。强增强带来图片的严重失真，但是依然是保留足够可以辨认类别的特征。有监督和无监督混合训练，逐步提高模型的表达能力。</p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p><img src="/img/fixmatch-algo.png" alt="img"></p><blockquote><p>1.输入：有标签数据，无标签数据，另外需要设定一些超参，包括置信度阈值，无标签采样比例，loss权重。</p><p>2.对有标签数据进行监督训练，使Cross-Entropy loss；</p><p>3.遍历无标签数据，利用弱增强获取伪标签；</p><p>4.利用获取的伪标签对无标签数据进行训练，同样利用Cross-Entropy loss；</p><p>5.基于loss权重，对俩者loss进行融合；</p></blockquote><h3 id="loss设计"><a href="#loss设计" class="headerlink" title="loss设计"></a>loss设计</h3><p>loss包含俩部分：有标注数据的监督训练$L_s$和无标注数据的伪标签监督训练$L_u$。</p><script type="math/tex; mode=display">\ell_{s}=\frac{1}{B} \sum_{b=1}^{B} \mathrm{H}\left(p_{b}, p_{\mathrm{m}}\left(y \mid \alpha\left(x_{b}\right)\right)\right)</script><script type="math/tex; mode=display">\ell_{u}=\frac{1}{\mu B} \sum_{b=1}^{\mu B} \mathbb{1}\left(\max \left(q_{b}\right) \geq \tau\right) \mathrm{H}\left(\hat{q}_{b}, p_{\mathrm{m}}\left(y \mid \mathcal{A}\left(u_{b}\right)\right)\right)</script><p>其中，$\alpha(.)$ 表示弱增强，一般为flip翻转，shift平移；$\mathcal{A}$(.)为强增强，一般为颜色变换，对比度增强等等。</p><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>在fixmatch中包含俩种数据增强：weak aug和strong aug. weak aug为标准的flip-and-shift增强策略，50%的概率进行flip和12.5%的概率进行shift，包括水平和竖直方向。对于strong aug，论文主要应用RandAugment和CTAugment两种策略，都是为提高模型表现而提出的增强策略。</p><p><img src="/img/fixmatch-aug.png" alt="img"></p><p>对于RandAugment：(1)从这个列表里随机选出N个增强，例如N为2；(2)然后选择一个随机的幅度M，例如50%之类的；（3）将所选的增强应用于图像，每种增强都有50％的可能性被应用。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>作者在CIFRAR，SVHN，STL数据集上做了详尽的实验，从实验结果来看，均优于以前的方法。</p><p><img src="/img/fixmatch-exp.png" alt="img"></p><p>在CIFAR-10和SVHN上选用的是Wide ResNet-28-2模型， CIFAR-100选用的是WRN-28-8，STL-10选用的是WRN-37-2。在每个类只有四张图片的情况下，fixmatch明显优于其他方法。</p><p>对于极端缺少标注的场景，仅仅使用每个类别使用1张图片，共10张标注的图片就可以达到78%的最大accuracy，当然这种做法和挑选的样本质量有关，作者也做了相关实验论证。不过也证明本文的方法的确work。</p><p><img src="/img/fixmatch-cifar.png" alt="img"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>fixmatch是SSL领域的一篇经典论文，做法简单有效，利用少量的标注图片就可以达到一个不错的效果，这对于获取标注困难的场景非常有意义。很值得在业务场景试一下。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2001.07685&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/google-research/fixmatch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/google-research/fixmatch&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这在实际业务场景中，大量的标注数据对于模型性能的提升至关重要，但是获取标注数据是一个耗时耗力的过程，例如工业场景中由于机台型号的更换导致模型性能下降，花费大量时间对新数据进行重新标注大概率会导致模型上线时间delay，而半监督学习(Semi-Supervised Learning, SSL)探究了如何利用大量未标注数据和部分标注数据来提升模型性能。而本文，谷歌提出的fixmatch，是对现有SSL方法进行显著简化的算法。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="semi-supervised learning" scheme="https://blog.nicehuster.cn/tags/semi-supervised-learning/"/>
    
  </entry>
  
  <entry>
    <title>目标检测上的自监督学习</title>
    <link href="https://blog.nicehuster.cn/2021/05/30/InsLoc/"/>
    <id>https://blog.nicehuster.cn/2021/05/30/InsLoc/</id>
    <published>2021-05-30T11:13:39.000Z</published>
    <updated>2022-04-24T04:06:06.073Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/abs/2102.08318" target="_blank" rel="noopener">Instance Localization for Self-supervised Detection Pretraining</a><br><strong>代码链接：</strong><a href="https://github.com/limbo0000/InstanceLoc" target="_blank" rel="noopener">https://github.com/limbo0000/InstanceLoc</a><br><strong>整体信息：</strong>这是被CVPR2021的文章，现有的自监督学习方法极大地提升了在图像分类任务上的指标上限，然而在目标检测任务上的迁移性能却与分类任务不一致。论文将这种迁移学习中存在的misalignment问题归咎为:(1)预训练网络结构和目标检测网络结构不一致；(2)现有的对比学习自监督方式是一个分类问题，没有对位置建模。</p><a id="more"></a><p><img src="/img/ins-loc.png" alt="img"></p><p> 方法很直观，将同一个Instance随机crop得到patch，随机粘贴到不同的背景中，使用对比学习进行自监督学习。其中对比学习框架上是按照MoCo来的，其中包括queue设计、动量更新等。</p><h4 id="Bounding-Box-Augmentation"><a href="#Bounding-Box-Augmentation" class="headerlink" title="Bounding-Box Augmentation"></a>Bounding-Box Augmentation</h4><p>在对比学习中，通常需要构建俩个不同view下的输入，在InsLoc这篇论文中，采用的是anchor的方式生成的。</p><p><img src="/img/ins-loc-box-aug.png" alt="img"></p><p>给定gt box，可以计算所有anchors和gt的iou，选取IOU&gt;0.5的anchor作为aug box构造输入对，进行对比学习。</p><script type="math/tex; mode=display">\begin{aligned}v_{q}^{\prime} &=\operatorname{RoIAlign}\left(f\left(I_{q}^{\prime}\right), b_{q}\right) \\v_{k_{+}}^{\prime} &=\operatorname{RoIAlign}\left(f\left(I_{k_{+}}^{\prime}\right), b_{k_{+}}\right)\end{aligned}</script><p>gt box和aug box经过roi align之后得到roi特征，再经由一个俩层的MLP层得到最终特征表示，用于后续的contrastive learning。</p><h4 id="Experients"><a href="#Experients" class="headerlink" title="Experients"></a>Experients</h4><p><img src="/img/ins-loc-ap-acc.png" alt="img"></p><p>从上图可以看出，相比于其他方法，insloc牺牲了很多分类上的acc，但都贡献在检测的ap上。</p><p><img src="/img/ins-loc-loc-eval.png" alt="img"></p><p>在论文中还提到如何评估自监督方法是否具有关注定位的能力。<strong>转化为patch块的位置预测。</strong> 如上图所示。下面是对应方法的实验结果。IncLoc对于定位能力是最强。</p><p><img src="/img/ins-loc-sl.png" alt="image-20220424103025560"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2102.08318&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Instance Localization for Self-supervised Detection Pretraining&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/limbo0000/InstanceLoc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/limbo0000/InstanceLoc&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这是被CVPR2021的文章，现有的自监督学习方法极大地提升了在图像分类任务上的指标上限，然而在目标检测任务上的迁移性能却与分类任务不一致。论文将这种迁移学习中存在的misalignment问题归咎为:(1)预训练网络结构和目标检测网络结构不一致；(2)现有的对比学习自监督方式是一个分类问题，没有对位置建模。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="self-supervised learning" scheme="https://blog.nicehuster.cn/tags/self-supervised-learning/"/>
    
  </entry>
  
  <entry>
    <title>长尾目标检测-Seesaw Loss[转载]</title>
    <link href="https://blog.nicehuster.cn/2021/05/30/seesaw-loss/"/>
    <id>https://blog.nicehuster.cn/2021/05/30/seesaw-loss/</id>
    <published>2021-05-30T11:13:39.000Z</published>
    <updated>2022-04-22T09:06:42.087Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/abs/2008.10032" target="_blank" rel="noopener">Seesaw Loss for Long-Tailed Instance Segmentation</a><br><strong>代码链接：</strong><a href="https://github.com/open-mmlab/mmdetection/tree/master/configs/seesaw_loss" target="_blank" rel="noopener">https://github.com/open-mmlab/mmdetection/tree/master/configs/seesaw_loss</a><br><strong>整体信息：</strong>这是 MMDet 团队参加 LVIS 2020 竞赛获得rank1时提出地损失函数，这篇文章指出了限制检测器在长尾分布数据上性能的一个关键原因：施加在尾部类别（tail class上的<strong>正负样本梯度的比例</strong>是不均衡的。因此，我们提出 <strong>Seesaw Loss</strong> <strong>来动态地抑制尾部类别上过量的负样本梯度，同时补充对误分类样本的惩罚</strong>。 Seesaw Loss 显著提升了尾部类别的分类准确率，进而为检测器在长尾数据集上的整体性能带来可观的增益。由于原作在<a href="https://zhuanlan.zhihu.com/p/339126633" target="_blank" rel="noopener">知乎</a>上已有讲解，我就不班门弄斧，直接搬运过来，侵权删。</p><a id="more"></a><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>在长尾分布的数据集中（例如：LVIS），大部分训练样本来自头部类别（head class），而只有少量样本来自尾部类别（tail class）。因此在训练过程中，来自头部类别的样本会对尾部类别施加过量的负样本梯度，淹没了来自尾部类别自身的正样本梯度。这种不平衡的学习过程导致分类器倾向于给予尾部类别很低的响应，以降低训练的loss。如下图所示，我们统计了在 LVIS v1.0 上训练Mask R-CNN过程中，施加在每个类别的分类器上正负样本累计梯度的分布。</p><p><img src="/img/seesaw-distribution.png" alt="img"></p><p>显然，头部类别获得的正负样本梯度比例接近1.0，而越是稀有的尾部类别，其获得的正负样本梯度的比例就越小。由此带来的结果就是分类的准确率随着样本数的减少而急剧下降，进而严重影响了检测器的性能。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>为了方便直观理解，我们可以把正负样本梯度不均衡的问题，类比于一个一边放有较重物体而另一边放有较轻物体的跷跷板（Seesaw），如下图所示。为了平衡这个跷跷板，一个简单可行的方案就是缩短重物一侧跷跷板的臂长，即减少重物的重量在平衡过程中的权重。回到正负样本梯度不均衡的问题，我们提出了 Seesaw Loss 来动态地减少由头部类别施加在尾部类别上过量的负样本梯度的权重，从而达到正负样本梯度相对平衡的效果。</p><p><img src="/img/seesaw-ce.png" alt="img"></p><p>Seesaw Loss的数学表达如下，</p><script type="math/tex; mode=display">\begin{aligned}&L_{\text {seesaw }}(\mathbf{z})=-\sum_{i=1}^{C} y_{i} \log \left(\widehat{\sigma}_{i}\right) \\&\text { with } \widehat{\sigma}_{i}=\frac{e^{z_{i}}}{\sum_{j \neq i}^{C} \mathcal{S}_{i j} e^{z_{j}}+e^{z_{i}}}\end{aligned}</script><p>$y_{i} \in\{0,1\}$ 是one-hot label, $\mathbf{z}=\left[z_{1}, z_{2}, \ldots, z_{C}\right]$ ,是每一类预测的 logit。此时，对于一个第 i类的样本，它施加在第 j类上的负样本梯度为</p><script type="math/tex; mode=display">\frac{\partial L_{\text {seesaw }}(\mathbf{z})}{\partial z_{j}}=\mathcal{S}_{i j} \frac{e^{z_{j}}}{e^{z_{i}}} \widehat{\sigma}_{i}</script><p>这里我们可以发现 $S_{ij}$就像一个平衡系数，通过调节 $S_{ij}$，我们可以达到放大或者缩小第 i类施加在第 j 类上的负样本梯度的效果。这样，我们就可以通过选择合适 $S_{ij}$来达到平衡正负样本梯度的目的。</p><p>在 Seesaw Loss 的设计中，我们考虑了两方面的因素，一方面我们需要考虑类别间样本分布的关系（class-wise），并据此减少头部类别对尾部类别的”惩罚” （负样本梯度）；另一方面，盲目减少对尾部类别的惩罚会增加错误分类的风险，因为部分误分类的样本受到的惩罚变小了，因此对于那些在训练过程中误分类的样本我们需要保证其受到足够的”惩罚”。据此， $S_{ij}$由两项相乘得到，</p><script type="math/tex; mode=display">\mathcal{S}_{i j}=\mathcal{M}_{i j} \cdot \mathcal{C}_{i j}</script><p>$M_{ij}$ <strong>（Mitigation Factor）</strong>用来缓解尾部类别上过量的负样本梯度, $C_{ij}$ (<strong>Compensation Factor）</strong>用来补充那些错误分类样本上的”惩罚”。</p><h4 id="1-Mitigation-Factor"><a href="#1-Mitigation-Factor" class="headerlink" title="1. Mitigation Factor"></a>1. Mitigation Factor</h4><p><img src="/img/seesaw-mitigation.png" alt="img"></p><p>既然正负样本梯度不平衡的问题来自于样本数量的不平衡，那么一种直接有效的办法就是根据不同类别之间样本数量的相对比例来进行调节。在训练过程中，Seesaw Loss在线地统计每一类的累计训练样本数量 $N_{ij}$ 并根据如下公式计算$M_{ij}$ ,</p><script type="math/tex; mode=display">\mathcal{M}_{i j}=\left\{\begin{array}{cc}1, & \text { if } N_{i} \leq N_{j} \\\left(\frac{N_{j}}{N_{i}}\right)^{p}, & \text { if } N_{i}>N_{j}\end{array}\right.</script><p>也就是说当第 i类比第j类出现地 就会自动根据两类之间不平衡的程度来减少第 i类对第j类施加的负样本梯度。此外，我们在线地累计样本数量，而非使用预先统计的数据集样本分布，这样的设计主要是因为一些高级的样本 sampling 方式会改变数据集的分布（例如：repeat factor sampler, class balanced sampler 等)。在这种情况下，预先统计的方式无法反映训练过程中数据的真实分布。</p><h4 id="2-Compensation-Factor"><a href="#2-Compensation-Factor" class="headerlink" title="2. Compensation Factor"></a>2. Compensation Factor</h4><p><img src="/img/seesaw-compensation.png" alt="img"></p><p>为了防止过度减少负样本梯度而带来的分类错误，Seesaw Loss会增加对那些错误分类样本的惩罚。具体来说，如果一个第i 类的样本错误分给了第 j类，Seesaw Loss会根据两类之间的分类置信度的相对比值来适当的增加对第 j类的惩罚。$C_{ij}$的计算如下，</p><script type="math/tex; mode=display">\mathcal{C}_{i j}=\left\{\begin{array}{cc}1, & \text { if } \sigma_{j} \leq \sigma_{i} \\\left(\frac{\sigma_{j}}{\sigma_{i}}\right)^{q}, & \text { if } \sigma_{j}>\sigma_{i}\end{array}\right.</script><h4 id="3-Normalized-Linear-Activation"><a href="#3-Normalized-Linear-Activation" class="headerlink" title="3. Normalized Linear Activation"></a>3. Normalized Linear Activation</h4><p>受到face recognition，few-shot learning等领域的启发，Seesaw Loss在预测分类logit的时候对weight和feature进行了归一化处理，即</p><script type="math/tex; mode=display">\begin{gathered}z=\tau \widetilde{\mathcal{W}}^{T} \tilde{x}+b \\\widetilde{\mathcal{W}}_{i}=\frac{\mathcal{W}_{i}}{\left\|\mathcal{W}_{i}\right\|_{2}}, i \in C, \tilde{x}=\frac{x}{\|x\|_{2}}\end{gathered}</script><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p><img src="/img/seesaw-exp.png" alt="img"></p><p>Seesaw相比于 EQL 和 BAGS 两种专门为 LVIS 数据设计的方法取得了显著的性能优势，在 end-to-end 训练的情况下在 test-dev 上取得高达30.0 AP的精度。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2008.10032&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Seesaw Loss for Long-Tailed Instance Segmentation&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmdetection/tree/master/configs/seesaw_loss&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/open-mmlab/mmdetection/tree/master/configs/seesaw_loss&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这是 MMDet 团队参加 LVIS 2020 竞赛获得rank1时提出地损失函数，这篇文章指出了限制检测器在长尾分布数据上性能的一个关键原因：施加在尾部类别（tail class上的&lt;strong&gt;正负样本梯度的比例&lt;/strong&gt;是不均衡的。因此，我们提出 &lt;strong&gt;Seesaw Loss&lt;/strong&gt; &lt;strong&gt;来动态地抑制尾部类别上过量的负样本梯度，同时补充对误分类样本的惩罚&lt;/strong&gt;。 Seesaw Loss 显著提升了尾部类别的分类准确率，进而为检测器在长尾数据集上的整体性能带来可观的增益。由于原作在&lt;a href=&quot;https://zhuanlan.zhihu.com/p/339126633&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;知乎&lt;/a&gt;上已有讲解，我就不班门弄斧，直接搬运过来，侵权删。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="Long-tailed object detection" scheme="https://blog.nicehuster.cn/tags/Long-tailed-object-detection/"/>
    
  </entry>
  
  <entry>
    <title>VFNet高性能的密集目标检测器</title>
    <link href="https://blog.nicehuster.cn/2021/04/12/vfnet/"/>
    <id>https://blog.nicehuster.cn/2021/04/12/vfnet/</id>
    <published>2021-04-12T11:13:39.000Z</published>
    <updated>2022-04-12T04:03:05.352Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_VarifocalNet_An_IoU-Aware_Dense_Object_Detector_CVPR_2021_paper.pdf" target="_blank" rel="noopener">VarifocalNet  An IoU-aware Dense Object Detector</a><br><strong>代码链接：</strong><a href="https://github.com/hyz-xmaster/VarifocalNet" target="_blank" rel="noopener">https://github.com/hyz-xmaster/VarifocalNet</a></p><p><img src="/img/vfnet.png" alt="vfnet"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_VarifocalNet_An_IoU-Aware_Dense_Object_Detect
      
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://blog.nicehuster.cn/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>TTFNet 快速训练版的CenterNet</title>
    <link href="https://blog.nicehuster.cn/2021/03/13/ttfnet/"/>
    <id>https://blog.nicehuster.cn/2021/03/13/ttfnet/</id>
    <published>2021-03-13T11:13:39.000Z</published>
    <updated>2022-04-12T03:54:48.981Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/pdf/1909.00700.pdf" target="_blank" rel="noopener">Training-Time-Friendly Network for Real-Time Object Detection</a><br><strong>代码链接：</strong><a href="https://github.com/ZJULearning/ttfnet" target="_blank" rel="noopener">https://github.com/ZJULearning/ttfnet</a><br><strong>问题</strong>： 这篇论文主要是解决CenterNet存在的一些问题，CenterNet在推理速度精度上表现不差，但存在训练耗时的问题，CenterNet官方代码训练coco需要2.5天，因此这篇论文主要是如何改进CenterNet训练耗时长的问题。作者基于CenterNet，在回归box时，作者认为由于CenterNet进行回归时仅仅使用中心点的那些目标是次优的选择，同时作者借助了FCOS是使用Ground Truth回归所有样本的思想，这里作者想到使用高斯核来选择回归样本，一方面可以增加回归的信息量，提升检测精度，同时作者指出这样有增加batch size的效果，可以使用更大的学习率以及有更短的训练时间。并且没有引入FCOS中耗时的NMS后处理过程，使得Inference时间减小。</p><a id="more"></a><p>该方法优势：</p><blockquote><ol><li>使用高斯核编码训练样本，使得网络能够更快地收敛；</li><li>能够在不引入其他组件，比如FPN的基础上，降低ambiguous 以及low-quality的样本;</li><li>不需要any offset predictions来矫正结果;<br><img src="https://github.com/ZJULearning/ttfnet/raw/master/imgs/structure.png" alt="undefined"><br>具体思路：<br>给定一张图片，利用网络分别预测物体中心在什么位置 $\hat{H} \in R^{N \times C \times \frac{H}{r} \times \frac{W}{r}}$和回归box size的向量 $\hat{S} \in R^{N \times 4 \times \frac{\dot{H}}{r} \times \frac{W}{r}}$ 在分类和回归时同时使用高斯核，并通过 $\alpha$ 和 $\beta$  控制高斯核的尺寸。这里和CenterNet一样，将目标检测分为两个部分：object localization和 size regression；</li></ol></blockquote><h3 id="1-object-localization"><a href="#1-object-localization" class="headerlink" title="1.object  localization"></a>1.object  localization</h3><p>假设一张图上有M个annotated box，其中第m个标注box属于$c_m$  类别，二维的高斯核：</p><script type="math/tex; mode=display">K_{m}(x, y)=\exp \left(-\frac{\left(x-x_{0}^{2}\right)}{2 \sigma_{2}^{x}}-\frac{\left(y-y_{0}^{2}\right)}{2 \sigma_{2}^{y}}\right)</script><p>高斯分布的峰值，也就是box中心，被视为正目标，而任何其他像素被视为负目标。对那些与较大的分配值对应的负目标的惩罚将会减轻。使用修正 focal loss.</p><h3 id="2-size-regression"><a href="#2-size-regression" class="headerlink" title="2. size regression"></a>2. size regression</h3><p><img src="/img/ttfnet_sample_define.png" alt="img"><br>给定特征图尺度上的m个ground truth box，采用另一个高斯核生成 $S_{m} \in R^{1 \times \frac{H}{r} \times \frac{W}{r}}$,核大小是由β 如上所述控制。Sm中的非零部分称为高斯区域Am，如图3所示。由于Am总是在它们的框中，所以在本文的其余部分中，它也被称为子区域。将子区域中的每个像素作为训练样本。损失函数部分使用的是GIOU loss。</p><h3 id="3-experiment"><a href="#3-experiment" class="headerlink" title="3. experiment"></a>3. experiment</h3><p><img src="/img/ttfnet_exp.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/1909.00700.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Training-Time-Friendly Network for Real-Time Object Detection&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/ZJULearning/ttfnet&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/ZJULearning/ttfnet&lt;/a&gt;&lt;br&gt;&lt;strong&gt;问题&lt;/strong&gt;： 这篇论文主要是解决CenterNet存在的一些问题，CenterNet在推理速度精度上表现不差，但存在训练耗时的问题，CenterNet官方代码训练coco需要2.5天，因此这篇论文主要是如何改进CenterNet训练耗时长的问题。作者基于CenterNet，在回归box时，作者认为由于CenterNet进行回归时仅仅使用中心点的那些目标是次优的选择，同时作者借助了FCOS是使用Ground Truth回归所有样本的思想，这里作者想到使用高斯核来选择回归样本，一方面可以增加回归的信息量，提升检测精度，同时作者指出这样有增加batch size的效果，可以使用更大的学习率以及有更短的训练时间。并且没有引入FCOS中耗时的NMS后处理过程，使得Inference时间减小。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://blog.nicehuster.cn/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>Generalized Focal Loss &amp;&amp; Generalized Focal Loss V2</title>
    <link href="https://blog.nicehuster.cn/2021/03/12/gfl/"/>
    <id>https://blog.nicehuster.cn/2021/03/12/gfl/</id>
    <published>2021-03-12T11:13:39.000Z</published>
    <updated>2022-04-12T03:38:57.426Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/pdf/2006.04388.pdf" target="_blank" rel="noopener">Generalized Focal Loss</a> &amp;&amp; <a href="https://arxiv.org/pdf/2011.12885.pdf" target="_blank" rel="noopener">Generalized Focal Loss V2</a><br><strong>代码链接：</strong> <a href="https://github.com/implus/GFocal" target="_blank" rel="noopener">https://github.com/implus/GFocal</a> &amp;&amp; <a href="https://github.com/implus/GFocalV2" target="_blank" rel="noopener">https://github.com/implus/GFocalV2</a><br><strong>整体信息：</strong>这倆篇论文都是出自南理工的李翔，从定位质量估计和边框表示的角度上对现有检测方法进行改进。在任意one-stage检测器上都能涨1-2点。GFLv1解决的是目前检测方法存在的两个问题：1）在训练和推理的时候，分类和质量估计的不一致性；2）狄拉克分布针对复杂场景下（模糊和不确定性边界）存在不灵活的问题。GFLv2是在v1的基础上，进一步地，使用概率分布的方式评估检测框质量，相比v1更有效，v2在ATSS上可以无痛涨点2-3点。</p><a id="more"></a><h3 id="GFLv1"><a href="#GFLv1" class="headerlink" title="GFLv1"></a>GFLv1</h3><p>方法的出发点主要是解决俩个问题：1）classification score 和 IoU/centerness score 训练测试不一致；2）bbox regression 采用的表示不够灵活，没有办法建模复杂场景下的uncertainty；</p><p><strong>问题1：训练测试不一致性</strong></p><p><img src="/img/gflv1_problem.png" alt="img"></p><p>不一致主要体先在俩方面：</p><blockquote><ol><li>用法不一致。训练的时候，分类和质量估计各自训记几个儿的，但测试的时候却又是乘在一起作为NMS score排序的依据，这个操作显然没有end-to-end，必然存在一定的gap；</li><li>对象不一致。借助Focal Loss的力量，分类分支能够使得少量的正样本和大量的负样本一起成功训练，但是质量估计（回归）通常就只针对正样本训练；</li></ol></blockquote><p><strong>问题2：bbox regression不灵活，无法建模复杂场景</strong><br>在复杂场景中，边界框的表示具有很强的不确定性，而现有的框回归本质都是建模了非常单一的狄拉克分布，非常不flexible。我们希望用一种general的分布去建模边界框的表示。问题二如图所示（比如被水模糊掉的滑板，以及严重遮挡的大象）：<br><img src="/img/gflv1_problem2.png" alt="img"></p><p>解决方法：</p><p><img src="/img/gflv1_solution.png" alt="img"></p><p>具体地，</p><h4 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h4><p>在分类分支上不再使用one-hot型类别标签监督，而是使用soft-one-hot 标签，标签值使用地iou值；如此，完美的将类别和位置信息结合利用起来了；传统地交叉熵、focal loss只能处理离散标签；因此作者在focal loss基础上提出 Quality Focal Loss，具体形式如下：</p><script type="math/tex; mode=display">\mathbf{Q F L}(\sigma)=-|y-\sigma|^{\beta}((1-y) \log (1-\sigma)+y \log (\sigma))</script><h4 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h4><p>考虑到真实的分布通常不会距离标注的位置太远，作者又额外加了个loss，希望网络能够快速地聚焦到标注位置附近的数值，使得他们概率尽可能大。Distribution Focal Loss：</p><script type="math/tex; mode=display">\mathbf{D F L}\left(\mathcal{S}_{i}, \mathcal{S}_{i+1}\right)=-\left(\left(y_{i+1}-y\right) \log \left(\mathcal{S}_{i}\right)+\left(y-y_{i}\right) \log \left(\mathcal{S}_{i+1}\right)\right)</script><p>其形式上与QFL的右半部分很类似，含义是以类似交叉熵的形式去优化与标签y最接近的一左一右两个位置的概率，从而让网络快速地聚焦到目标位置的邻近区域的分布中去。简单来讲，就是预测目标框边界附近地n个位置地概率值，然后基于概率值和n个位置进行加权，得到目标框边界值；目标的四个边框均采用上述操作，然后基于组合的目标框使用GIOU loss进行优化。</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><script type="math/tex; mode=display">\mathcal{L}=\frac{1}{N_{\text {pos }}} \sum_{z} \mathcal{L}_{\mathcal{Q}}+\frac{1}{N_{\text {pos }}} \sum_{z} \mathbf{1}_{\left\{c_{z}^{*}>0\right\}}\left(\lambda_{0} \mathcal{L}_{\mathcal{B}}+\lambda_{1} \mathcal{L}_{\mathcal{D}}\right)</script><p>上面是训练loss函数，第一部分是分类分支的QFL；第二部分是回归分支的GIOU loss和Distribution Focal Loss；</p><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p><img src="/img/gflv1_exp.png" alt="img"></p><h3 id="GFLv2"><a href="#GFLv2" class="headerlink" title="GFLv2"></a>GFLv2</h3><p>问题：在GFLv1中，作者提出了对边界框进行一个一般化的分布表示建模。有了这个可以学习的表示之后，基本上那些非常清晰明确的边界，它的分布都很尖锐；而模糊定义不清的边界它们学习到的分布基本上会平下来，而且有的时候还经常出现双峰的情况。作者为了充分利用这分布，提出了GFLv2，利用分布形状的统计量去知道最终最终定位质量的估计。具体网络结构如下：<br><img src="/img/gflv2_framework.png" alt="img"></p><h4 id="核心思路"><a href="#核心思路" class="headerlink" title="核心思路"></a>核心思路</h4><p>直接取学习到的分布（分布是用离散化的多个和为1的回归数值表示的，详情参考GFLV1）的Topk数值。其实理解起来也不难，因为所有数值和为1，如果分布非常尖锐的话，Topk这几个数通常就会很大；反之Topk就会比较小。选择Topk还有一个重要的原因就是它可以使得我们的特征与对象的scale尽可能无关，如下图所示：<br><img src="/img/gflv2_topk.png" alt="img"><br>简单来说就是长得差不多形状的分布要出差不多结果的数值，不管它峰值时落在小scale还是大scale。我们把4条边的分布的Topk concat在一起形成一个维度非常低的输入特征向量（可能只有10+或20+），用这个向量再接一个非常小的fc层（通常维度为32、64)，最后再变成一个Sigmoid之后的scalar乘到原来的分类表征中。具体model参考上图，其中红色框就是比GFLV1多出来的Distribution-Guided Quality Predictor部分，这也就是本文的核心。</p><h4 id="实验结果-1"><a href="#实验结果-1" class="headerlink" title="实验结果"></a>实验结果</h4><p><img src="/img/gflv2_exp.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2006.04388.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Generalized Focal Loss&lt;/a&gt; &amp;amp;&amp;amp; &lt;a href=&quot;https://arxiv.org/pdf/2011.12885.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Generalized Focal Loss V2&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt; &lt;a href=&quot;https://github.com/implus/GFocal&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/implus/GFocal&lt;/a&gt; &amp;amp;&amp;amp; &lt;a href=&quot;https://github.com/implus/GFocalV2&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/implus/GFocalV2&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这倆篇论文都是出自南理工的李翔，从定位质量估计和边框表示的角度上对现有检测方法进行改进。在任意one-stage检测器上都能涨1-2点。GFLv1解决的是目前检测方法存在的两个问题：1）在训练和推理的时候，分类和质量估计的不一致性；2）狄拉克分布针对复杂场景下（模糊和不确定性边界）存在不灵活的问题。GFLv2是在v1的基础上，进一步地，使用概率分布的方式评估检测框质量，相比v1更有效，v2在ATSS上可以无痛涨点2-3点。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://blog.nicehuster.cn/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>ATSS自适应选择正负样本的目标检测方法</title>
    <link href="https://blog.nicehuster.cn/2021/02/23/atss/"/>
    <id>https://blog.nicehuster.cn/2021/02/23/atss/</id>
    <published>2021-02-23T11:13:39.000Z</published>
    <updated>2021-02-24T08:09:44.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/abs/1912.02424" target="_blank" rel="noopener">Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</a><br><strong>代码链接：</strong><a href="https://github.com/sfzhang15/ATSS" target="_blank" rel="noopener">https://github.com/sfzhang15/ATSS</a><br><strong>整体信息：</strong>这篇论文是中科院张士峰发表在CVPR2020上的一篇文章，论文通过大量实验证明指出anchor-based方法和anchor-free方法的性能差异主要来自于正负样本的选取上，基于此提出ATSS(Adaptive Training Sample Selection)方法，该方法通过自动统计标注GT上的相关统计特征自适应的选择合适的anchor box作为正负样本，在不带来额外计算量和参数的情况下，能够大幅提升模型的性能。</p><a id="more"></a><h3 id="1-差异分析"><a href="#1-差异分析" class="headerlink" title="1.差异分析"></a>1.差异分析</h3><p>在不考虑各种trick的情况下，anchor-based方法RetinaNet和anchor-free方法FCOS主要存在两个差异：1）正负样本选取；2）回归起始状态；为公平比较，降低两者方法的差异性，在实验中将RetinaNet的anchor数改为1降低差异性，方便与FCOS比较，作者后续在实验中也验证不同anchor数对模型性能的影响。</p><h4 id="1-Inconsistency-Removal"><a href="#1-Inconsistency-Removal" class="headerlink" title="1).Inconsistency Removal"></a>1).Inconsistency Removal</h4><p><img src="/img/atss_Inconsistency%20Removal.png" alt="atss_Inconsistency Removal"></p><p>由于FCOS加入了很多trick，为了与RetinaNet进行公平比较，对FCOS上的trick进行对齐，包括GroupNorm、GIoU loss、限制正样本必须在GT内、Centerness branch以及添加可学习的标量控制FPN的各层的尺寸。具体结果如上面表格所示，最终的RetinaNet与FCOS相差无几，但仍然存在0.8个点的差异。</p><h4 id="2-Essential-Difference"><a href="#2-Essential-Difference" class="headerlink" title="2).Essential Difference"></a>2).Essential Difference</h4><p>在经过上述实验对齐之后，RetinaNet和FCOS仅存在两个差异：1）正负样本选取；2）回归起始状态；针对这两个差异，论文通过实验做进一步分析。</p><p><img src="/img/image-20210224115350117.png" alt="image-20210224115350117"></p><ul><li><p><strong>Classification</strong></p><p>在RetinaNet中，通过划分IOU阈值区间来区分正负样本，$\mathrm{IoU}&gt;\theta_{p}$ 置为正样本，$\mathrm{IoU}&lt;\theta_{n}$ 置为负样本，至于区间内所有样本忽略；在FCOS使用空间尺寸和尺寸限制来区分正负anchor point，正样本首先必须在GT box内，其次需要是GT尺寸对应的层，其余均为负样本；</p></li></ul><p><img src="/img/image-20210224115743063.png" alt="image-20210224115743063"></p><ul><li><p><strong>Regression</strong></p><p>在RetinaNet中，模型预测的是相对anchor box的4个偏移量，通过4个偏移量对anchor box进行调整；而在FCOS则预测的是相对于anchor point的4个偏移量，基于anchor point和4个偏移量组成bbox；</p><p><img src="/img/image-20210224143241390.png" alt="image-20210224143241390"></p></li><li><p><strong>Conclusion</strong></p><p>基于上述两个差异，论文做了实验进行交叉对比，实验结果如上表格2所示，在相同正负样本定义下的RetinaNet和FCOS性能几乎一样，不同的定义方法性能差异较大，具体可以看下表格的纵向对比结果；而从横向对比结果上来看，无论是基于box或基于Point的形式，指标基本不发生变化，这也印证了回归初始状态的不同对模型性能影响不大。所以，基本可以确定正负样本的选取是影响模型性能的关键；</p></li></ul><h3 id="2-自适应样本选取（Adaptive-Training-Sample-Selection）"><a href="#2-自适应样本选取（Adaptive-Training-Sample-Selection）" class="headerlink" title="2. 自适应样本选取（Adaptive Training Sample Selection）"></a>2. 自适应样本选取（Adaptive Training Sample Selection）</h3><p><img src="/img/image-20210224151209904.png" alt="image-20210224151209904"></p><h4 id="1-算法流程"><a href="#1-算法流程" class="headerlink" title="1).算法流程"></a>1).算法流程</h4><p>论文提出基于统计特征自适应样本选取的方法具体流程如上图所示，具体来说，对于每个标注gt框g，首先针对每个FPN层级基于L2距离找到与目标中心点最近的k个anchor box，计算anchor box与gt box的IOU值，计算所有iou值的均值$m_{g}$和标准差$v_{g}$，基于均值和阈值的计算结果，得到IOU阈值 $t_{g}=m_{g}+v_{g}$ ，最后选择阈值大于等于$t_{g}$ 且中心点位于gt box内的box作为最后的输出。如果anchor box对应多个gt，则选择IoU最大的gt。</p><h4 id="2-算法思想"><a href="#2-算法思想" class="headerlink" title="2).算法思想"></a>2).算法思想</h4><p>从思想上来看，自适应样本选取的方法遵循如下几点：</p><blockquote><ul><li>基于目标中心距离远近程度来选择样本；在RetinaNet中，anchor box与gt中心点越近一般IoU越高，而在FCOS中，中心点越近一般预测的质量越高；</li><li>使用使用iou的统计量均值和方差之和作为iou阈值；均反映的是预设的anchor box与gt的匹配程度，均值高则应当提高阈值来调整正样本，均值低则应当降低阈值来调整正样本。标准差反映的是适合GT的FPN层数，标准差高则表示高质量的anchor box集中在一个层中，应将阈值加上标准差来过滤其他层的anchor box，低则表示多个层都适合该gt，将阈值加上标准差来选择合适的层的anchor box，均值和标准差结合作为IoU阈值能够很好地自动选择对应的特征层上合适的anchor box；</li><li>正样本的中心必须位于目标内；若anchor box的中心点不在gt区域内，则其会使用非gt区域的背景特征进行预测，影响模型性能，这种情况应予以剔除；</li><li>几乎无超参数；从上面的算法流程可以看出，唯一的超参数就是k，论文后面也通过实验验证ATSS算法对于k值的选取不明感；</li></ul></blockquote><h4 id="3-ATSS实验对比"><a href="#3-ATSS实验对比" class="headerlink" title="3).ATSS实验对比"></a>3).ATSS实验对比</h4><p><img src="/img/image-20210224154804455.png" alt="image-20210224154804455"></p><p>将ATSS应用到RetinaNet和FCOS上测试效果：</p><ul><li>将RetinaNet中的正负样本替换为ATSS，AP提升了2.3%；</li><li>在FCOS上的应用主要用两种：lite版本采用ATSS的思想，从选取GT内的anchor point改为选取每层离GT最近的topk个候选anchor point，提升了0.8%AP；full版本将FCOS的anchor point改为长宽为8S的anchor box来根据ATSS选择正负样本，但仍然使用原始的回归方法，提升了1.4%AP。两种方法找到的anchor point在空间位置上大致相同，但是在FPN层上的选择不太一样。从结果来看，自适应的选择方法比固定的方法更有效。至于与其他SOTA方法的对比，可以直接参考论文。</li></ul><h3 id="3-Discussion"><a href="#3-Discussion" class="headerlink" title="3.Discussion"></a>3.Discussion</h3><p>上面的实验对比可以看出RetinaNet实验中没有涉及任何关于anchor宽高比ratio和尺度scale相关的参数，而且在实验中仅使用了一个anchor,在原始RetinaNet中，一个目标选择的是9个预定义的不同尺度和宽高比的anchor box，因此论文补充实验测试了在不同尺度、不同宽高比以及不同anchors数量下的实验结果。具体实验结果，可以直接参考原文，从论文po出来的实验结果来看，ATSS对于anchor尺度和宽高比并不敏感，而且在每个位置设定多个anchor box是无用的操作，关键在于选择合适的正样本。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/1912.02424&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/sfzhang15/ATSS&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/sfzhang15/ATSS&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这篇论文是中科院张士峰发表在CVPR2020上的一篇文章，论文通过大量实验证明指出anchor-based方法和anchor-free方法的性能差异主要来自于正负样本的选取上，基于此提出ATSS(Adaptive Training Sample Selection)方法，该方法通过自动统计标注GT上的相关统计特征自适应的选择合适的anchor box作为正负样本，在不带来额外计算量和参数的情况下，能够大幅提升模型的性能。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://blog.nicehuster.cn/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读： A Simple and Strong Anchor-freeObject Detector(FCOS_imprv)</title>
    <link href="https://blog.nicehuster.cn/2021/02/22/FCOS_improv/"/>
    <id>https://blog.nicehuster.cn/2021/02/22/FCOS_improv/</id>
    <published>2021-02-22T11:13:39.000Z</published>
    <updated>2021-02-23T02:46:30.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> Zhi Tian,Adelaide<br><strong>代码链接：</strong><a href="https://github.com/aim-uofa/AdelaiDet" target="_blank" rel="noopener">https://github.com/aim-uofa/AdelaiDet</a><br><strong>整体框架：</strong> 这篇论文是fcos原作者重新修改后发表在PAMI上面的一篇文章，fcos_imprv和fcos整体思想是一致的，只是在原作上做了部分修改，是网络性能得到进一步提升，在backbone为ResNet-101-FPN上面的性能从41.0提升到43.2，提升明显。本文主要是记录一下fcos_imprv相比原方法的一些改进和提升方法。<br><a id="more"></a></p><h3 id="改进点"><a href="#改进点" class="headerlink" title="改进点"></a>改进点</h3><p>对于原始fcos方法的原理介绍，可以参考<a href="https://nicehuster.github.io/2019/04/15/FCOS/" target="_blank" rel="noopener">这里</a> ，这里详细说一下改进的地方：</p><h4 id="1-正负样本"><a href="#1-正负样本" class="headerlink" title="1.正负样本"></a>1.正负样本</h4><p>作者对正负样本的指定做了修改，在原始fcos中，对于目标被的所有特征点均指定为正样本，以及该特征点到目标边界的距离满足所处的FPN层的约束；而在FCOS_imprv中则要求指定只有在目标中心区域的特征点才为正样本；目标中心区域的大小为：</p><script type="math/tex; mode=display">\left(c_{x}-r s, c_{y}-r s, c_{x}+r s, c_{y}+r s\right)</script><p>其中 $\left(c_{x}, c_{y}\right)$ 为目标中心，s为当前层的stride，r 为超参数，在论文中r设置为1.5。这个代码的实现可以看一下mmdetection中FCOS的实现：<a href="mmdet/models/dense_heads/fcos_head.py">fcos_head.py</a> 其中center_sample_radius参数默认设置为1.5.</p><h4 id="2-回归目标修改"><a href="#2-回归目标修改" class="headerlink" title="2.回归目标修改"></a>2.回归目标修改</h4><p>Fcos的回归目标直接是特征点到目标边界的距离，由于Head是共用的，所以在预测时为每个level预设一个可学习的scale因子，而fcos_imprv则加入stride，变得更适应FPN的尺寸，可学习的scale因子依然使用，具体形式如下：</p><script type="math/tex; mode=display">\begin{array}{l}l^{*}=\left(x-x_{0}^{(i)}\right) / s, \quad t^{*}=\left(y-y_{0}^{(i)}\right) / s \\r^{*}=\left(x_{1}^{(i)}-x\right) / s, \quad b^{*}=\left(y_{1}^{(i)}-y\right) / s\end{array}</script><p>这一部分的代码实现，可以看一下这个地方：<a href="https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/dense_heads/fcos_head.py#L147" target="_blank" rel="noopener">fcos_head.py#L147</a> 其中scale为可学习参数。</p><h4 id="3-centerness-修改"><a href="#3-centerness-修改" class="headerlink" title="3.centerness 修改"></a>3.centerness 修改</h4><p>在原始fcos中，centerness是和分类分支放在一起预测，而在fcos_imprv中，将该分支移至回归分支，以便更好的利用位置相关信息；</p><p><img src="/img/fcos_imprv-3999865.png" alt="fcos_imprv"></p><h4 id="4-回归损失函数"><a href="#4-回归损失函数" class="headerlink" title="4.回归损失函数"></a>4.回归损失函数</h4><p>在fcos方法中，训练损失函数如下：</p><script type="math/tex; mode=display">\begin{aligned}L\left(\left\{\boldsymbol{p}_{x, y}\right\},\left\{\boldsymbol{t}_{x, y}\right\}\right) &=\frac{1}{N_{\mathrm{pos}}} \sum_{x, y} L_{\mathrm{cls}}\left(\boldsymbol{p}_{x, y}, c_{x, y}^{*}\right) \\&+\frac{\lambda}{N_{\mathrm{pos}}} \sum_{x, y} \mathbb{1}_{\left\{c_{x, y}^{*}>0\right\}} L_{\mathrm{reg}}\left(\boldsymbol{t}_{x, y}, \boldsymbol{t}_{x, y}^{*}\right)\end{aligned}</script><p>第一项是分类分支的损失函数，使用的是的focal loss，第二项是回归分支的损失函数。在原始fcos中，使用的IOU loss，而在fcos_imprv中，使用的GIOU loss。这里顺便介绍一下IOU loss和GIOU loss的区别。</p><blockquote><ul><li>IOU loss，原始smooth l1 loss在计算目标检测的 bbox loss时，都是独立的求出4个点的 loss，然后相加得到最终的 bbox loss。这种做法的默认4个点是相互独立的，而目标检测评价的是IOU指标，与实际不符。IOU loss计算如下：</li></ul><script type="math/tex; mode=display">\text { IoU loss }=-\ln \operatorname{IoU}\left(bbox_\text {gt }, b b o x_{\text {pred }}\right)</script><ul><li><p>GIOU loss，iou loss计算的是预测框与gt框存在重叠情况下的损失函数，而对于不重叠情况下，损失函数不可导，无法优化两个框不相交的情况。此外，如果iou是确定的，其iou值是无法确定两个框是如何相交的。GIOU 计算如下：</p><script type="math/tex; mode=display">G I o U=I o U-\frac{|C \backslash(A \cup B)|}{|C|}</script><p>GIoU 的实现方式如上式，其中 C 为 A 和 B 的外接矩形。用 C 减去 A 和 B 的并集除以 C 得到一个数值，然后再用 A 和 B 的 IoU 减去这个数值即可得到 GIoU 的值。可以看出：1）GIoU 取值范围为 [-1, 1]，在两框重合时取最大值1，在两框无限远的时候取最小值-1；2）与 IoU 只关注重叠区域不同，GIoU不仅关注重叠区域，还关注其他的非重合区域，能更好的反映两者的重合度。</p><p>GIOU loss计算如下：</p><script type="math/tex; mode=display">\mathcal{L}_{G I o U}= 1-GIOU</script></li></ul></blockquote><h4 id="5-分数计算"><a href="#5-分数计算" class="headerlink" title="5.分数计算"></a>5.分数计算</h4><p>最终分数的计算，fcos采用分类分数以及center-ness之积，fcos_imprv则采用分类分数以及center-ness之积的平方根：</p><script type="math/tex; mode=display">\boldsymbol{s}_{x, y}=\sqrt{\boldsymbol{p}_{x, y} \times o_{x, y}}</script><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/img/fcos_exp.png" alt="fcos_exp"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; Zhi Tian,Adelaide&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/aim-uofa/AdelaiDet&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/aim-uofa/AdelaiDet&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体框架：&lt;/strong&gt; 这篇论文是fcos原作者重新修改后发表在PAMI上面的一篇文章，fcos_imprv和fcos整体思想是一致的，只是在原作上做了部分修改，是网络性能得到进一步提升，在backbone为ResNet-101-FPN上面的性能从41.0提升到43.2，提升明显。本文主要是记录一下fcos_imprv相比原方法的一些改进和提升方法。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://blog.nicehuster.cn/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>DETR 基于transformer的目标检测方法</title>
    <link href="https://blog.nicehuster.cn/2020/10/27/detr/"/>
    <id>https://blog.nicehuster.cn/2020/10/27/detr/</id>
    <published>2020-10-27T11:13:39.000Z</published>
    <updated>2020-10-28T08:49:02.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/pdf/2005.12872.pdf" target="_blank" rel="noopener">End to End Object Detection with Transformers</a><br><strong>代码链接：</strong> <a href="https://github.com/facebookresearch/detr" target="_blank" rel="noopener">https://github.com/facebookresearch/detr</a><br><strong>整体信息：</strong> 这是FAIR最近新出了一篇用transformer做检测的文章。相比于以往所有的检测方法不同的是，没有使用先验知识比如anchor设计，以及后处理步骤比如nms等操作。而是使用transformer预测集合的形式，直接输出目标位置及类别信息。在object detection上DETR准确率和运行时间上和Faster RCNN相当；将模型generalize到panoptic segmentation任务上，DETR表现甚至还超过了其他的baseline.</p><a id="more"></a><h3 id="1-概览"><a href="#1-概览" class="headerlink" title="1.概览"></a>1.概览</h3><p><img src="/img/detr.png" alt="detr"></p><p>上图展示的是DETR的整体结构图。如上图所示，可以看出两个关键的部分：</p><blockquote><ul><li>使用transformer 的encoder-decoder结构生成N个 box predictions；其中N为事先设计的、远大于图像中object数的一个整数；</li><li>设计了bipartite matching loss，基于预测的box和gt box的二分图匹配计算loss，从而使得预测的box更接近gt box；</li></ul></blockquote><p>这篇文章的撰写风格比较诡异，不是按照正常的网络结构的先后顺序来介绍各个部分。而是先介绍了bipartite matching loss，再介绍transformer的encoder和decoder。为了便于理解，我这里还是按照网络结构的先后顺序来介绍各个模块。</p><h3 id="2-Transformer"><a href="#2-Transformer" class="headerlink" title="2.Transformer"></a>2.Transformer</h3><p><img src="/img/detr-transformer.png" alt="detr-transformer"></p><p>上图展示的是transformer的结构，其中包括encoder、decoder以及FFN三部分。</p><h4 id="2-1-Transformer-Encoder"><a href="#2-1-Transformer-Encoder" class="headerlink" title="2.1 Transformer Encoder"></a>2.1 Transformer Encoder</h4><p>如上图左侧所示，在DETR中，首先输入图片 $x_{\mathrm{img}} \in \mathbb{R}^{3 \times H_{0} \times W_{0}}$ 经过CNN backbone处理后，输出feature map$f \in \mathbb{R}^{C \times H \times W}$ ，一般 $C=2048,H, W=\frac{H_{0}}{32}, \frac{W_{0}}{32}$ 。然后将backbone输出的feature map和position encoding相加，输入Transformer Encoder中处理，得到用于输入到Transformer Decoder的image embedding。由于Transformer的输入为序列化数据，因此会对backbone输出的feature map做进一步处理转化为序列化数据，具体处理包括如下：</p><blockquote><ul><li><strong>维度压缩</strong>：使用1x1卷积将feature map 维度从C压缩为d,生成新的feature map $z_{0} \in \mathbb{R}^{d \times H \times W}$ ；</li><li><strong>转化为序列化数据：</strong>将空间的维度（高和宽）压缩为一个维度，即把上一步得到的$d \times H \times W$维的feature map通过reshape成$d \times H W$维的feature map；</li><li><strong>加上positoin encoding:</strong> 由于transformer模型是permutation-invariant（转置不变性，也可以理解为和位置无关），而显然图像中目标是具有空间信息的，与原图的位置有关，所以需要加上position encoding反映位置信息。具体生成的方法后续补充讲解。</li></ul></blockquote><h4 id="2-2-Transformer-Decoder"><a href="#2-2-Transformer-Decoder" class="headerlink" title="2.2 Transformer Decoder"></a>2.2 Transformer Decoder</h4><p>这部分的话，有两个输入，一个是Transformer Encoder的输出，另一个是object queries。这里讲一下object queries，object queries有N个，N是一个事先设定的、比远远大于image中object个数的一个整数），输入Transformer Decoder后分别得到N个 output embedding，然后经过FFN处理之后，输出N个box的位置和类别信息。object queries具体是什么，论文阐述的很模糊。</p><h4 id="2-3-FFN"><a href="#2-3-FFN" class="headerlink" title="2.3 FFN"></a>2.3 FFN</h4><p>这个是网络结构最后的输出部分，在DETR中，其实有两种FFN：一种预测bounding box的中心位置、高和宽，第二种预测class标签。下图是更细节的DETR Transformer结构，大家感兴趣可以仔细看一下。</p><p><img src="/img/detr-ffn.png" alt="detr-ffn"></p><h3 id="3-Loss设计"><a href="#3-Loss设计" class="headerlink" title="3. Loss设计"></a>3. Loss设计</h3><p>前面讲到transformer经过FFN之后会预测输出N个prediction boxes,其中，N是事先设计好的一个远大于image objects的正整数。通过得到这N个prediction boxes和image objects的最优二分图匹配，通过计算配对的box pair的loss来对模型进行优化。</p><h4 id="3-1-最优二分图匹配"><a href="#3-1-最优二分图匹配" class="headerlink" title="3.1 最优二分图匹配"></a>3.1 最优二分图匹配</h4><p>假设对于一张图来说，image objects的个数为m，由于N是事先设计好的一个远大于m的正整数，所以 N&gt;&gt;m，即生成的prediction boxes的数量会远远大于image objects 数量。这样怎么做匹配？</p><p>为了解决这个问题，作者人为构造了一个新的物体类别 $\varnothing$ (表示没有物体)并加入image objects中，上面所说到的多出来的N-m个个prediction embedding就会和类别 $\varnothing$ 配对。这样就可以将prediction boxes和image objects的配对看作两个等容量的集合的二分图匹配了。</p><p>设计好匹配的cost，就可以使用<a href="https://www.cxyxiaowu.com/874.html" target="_blank" rel="noopener">匈牙利算法</a>快速地找到使cost最小的二分图匹配方案了。cost计算如下：</p><script type="math/tex; mode=display">\hat{\sigma}=\underset{\sigma \in \widetilde{S}_{N}}{\arg \min } \sum_{i}^{N} \mathcal{L}_{\operatorname{match}}\left(y_{i}, \hat{y}_{\sigma(i)}\right)</script><p>对应单个prediction box和image object匹配cost $\mathcal{L}_{\operatorname{match}}\left(y_{i}, \hat{y}_{\sigma(i)}\right)$计算如下：</p><script type="math/tex; mode=display">-\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \hat{p}_{\sigma(i)}\left(c_{i}\right)+\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \mathcal{L}_{\mathrm{box}}\left(b_{i}, \hat{b}_{\sigma(i)}\right)</script><p>其中，</p><p>$c_i$ 第i个image object的class标签，$\sigma(i)$ 表示与第i个image object匹配的prediction box的index;</p><p>$\mathbb{1}_{c_{i} \neq \varnothing}$ 是一个函数，当 $c_{i} \neq \phi$ 时为1，否则为0；</p><p>$\hat{p}_{\sigma(i)}\left(c_{i}\right)$ 表示Transformer预测第$\sigma(i)$ 个prediction box为类别$c_i$ 的概率；</p><p>$b_{i}, \hat{b}_{\sigma(i)}$ 分别为第i个image object和第$\sigma(i)$ 个prediction box的位置向量；</p><p>$\mathcal{L}_{\mathrm{box}}\left(b_{i}, \hat{b}_{\sigma(i)}\right)$ 计算的是ground truth box和prediction box之间的距离；具体计算方式论文有提及：</p><script type="math/tex; mode=display">\lambda_{\text {iou }} \mathcal{L}_{\text {iou }}\left(b_{i}, \hat{b}_{\sigma(i)}\right)+\lambda_{\mathrm{L} 1}\left\|b_{i}-\hat{b}_{\sigma(i)}\right\|_{1}</script><p>计算box的距离实验的IOU loss以及L1 loss。</p><p>这样，我们就完全定义好了每对prediction box和 image object 配对时的cost。再利用匈牙利算法即可得到二分图最优匹配。</p><h4 id="3-2-计算set-prediction-loss"><a href="#3-2-计算set-prediction-loss" class="headerlink" title="3.2 计算set prediction loss"></a>3.2 计算set prediction loss</h4><p>上面我们得到了prediction boxes和image objects之间的最优匹配。这里我们基于这个最优匹配，来计算set prediction loss，即评价Transformer生成这些prediction boxes的效果好坏。 set prediction loss的计算如下：</p><script type="math/tex; mode=display">\mathcal{L}_{\text {Hungarian }}(y, \hat{y})=\sum^{N}\left[-\log \hat{p}_{\hat{\sigma}(i)}\left(c_{i}\right)+\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \mathcal{L}_{\text {box }}\left(b_{i}, \hat{b}_{\hat{\sigma}}(i)\right)\right]</script><p>其中，$\hat{\sigma}$ 为最优匹配。将第 i个image object匹配到第$\hat{\sigma}(i)$ 个prediction boxes。这里值得注意的是，和上面计算cost不太一样的地方是这里计算分类概率使用的是log对数的形式；将匹配到类别 $\varnothing$ 的概率考虑进去了，而前面cost的计算中则直接将其置为0了；</p><h3 id="4-实验"><a href="#4-实验" class="headerlink" title="4.实验"></a>4.实验</h3><p>论文中实验细节较多，而且实验结果也比较solid，这里不细讲，有兴趣的可以直接看原文，这里贴一张在coco上实验结果对比：</p><p><img src="/img/detr-exp.png" alt="detr-exp"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2005.12872.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;End to End Object Detection with Transformers&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt; &lt;a href=&quot;https://github.com/facebookresearch/detr&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/facebookresearch/detr&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt; 这是FAIR最近新出了一篇用transformer做检测的文章。相比于以往所有的检测方法不同的是，没有使用先验知识比如anchor设计，以及后处理步骤比如nms等操作。而是使用transformer预测集合的形式，直接输出目标位置及类别信息。在object detection上DETR准确率和运行时间上和Faster RCNN相当；将模型generalize到panoptic segmentation任务上，DETR表现甚至还超过了其他的baseline.&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://blog.nicehuster.cn/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>抠图神器U^2-Net</title>
    <link href="https://blog.nicehuster.cn/2020/09/14/U-2-Net/"/>
    <id>https://blog.nicehuster.cn/2020/09/14/U-2-Net/</id>
    <published>2020-09-14T11:13:39.000Z</published>
    <updated>2020-09-18T07:17:55.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> Going Deeper with Nested U-Structure for Salient Object Detection<br><strong>代码链接：</strong><a href="https://github.com/NathanUA/U-2-Net" target="_blank" rel="noopener">https://github.com/NathanUA/U-2-Net</a><br><strong>整体信息：</strong> 这是发表在PR2020上的一篇关于显著性检测的文章，作者是秦雪彬。从标题上可以看到本文的一个idea是设计了一个deeper的U型结构的网络解决显著性目标检测问题。作者认为，目前显著性目标检测有两种主流思路，一为多层次深层特征集成multi-level deep feature integration，一为多尺度特征提取Multi-scale feature extraction。多层次深层特征集成方法主要集中在开发更好的多层次特征聚合策略上。而多尺度特征提取这一类方法旨在设计更新的模块，从主干网获取的特征中同时提取局部和全局信息。而几乎所有上述方法，都是为了更好地利用现有的图像分类的Backbones生成的特征映射。而作者另辟蹊径，提出了一种新颖而简单的结构，它直接逐级提取多尺度特征，用于显著目标检测，而不是利用这些主干的特征来开发和添加更复杂的模块和策略。下图是该方法与其他方法的一个比较：</p><a id="more"></a><p><img src="/img/image-20200918105536819.png" alt="image-20200918105536819"></p><h3 id="显著性检测"><a href="#显著性检测" class="headerlink" title="显著性检测"></a>显著性检测</h3><p>在讲这篇文章之前，有必要先了解显著性检测这个任务。</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/115002897" target="_blank" rel="noopener">显著性检测</a></p></blockquote><p><strong>显著性检测</strong>，就是使用图像处理技术和计算机视觉算法来定位图片中最“显著”的区域。显著区域就是指图片中引人注目的区域或比较重要的区域，例如人眼在观看一幅图片时会首先关注的区域。例如下图，我们人眼一眼看过去首先注意到的不是草坪，而是躺在草坪上的内马尔，内马尔所在的区域就是显著性区域。这种自动定位图像或场景重要区域的过程称为<strong>显着性检测</strong>。</p><p><img src="/img/v2-422783ab5d5d66fd0dbf5653166ddbd8_720w.jpg" alt></p><h4 id="显著性检测和图像分割区别"><a href="#显著性检测和图像分割区别" class="headerlink" title="显著性检测和图像分割区别"></a>显著性检测和图像分割区别</h4><p>这个任务和图像分割十分类似，区别在于：</p><blockquote><ul><li>1.目标数量：显著性目标检测一般只检一个目标，一般目标检测不会限制数量；</li><li>目标类别： 显著性目标检测不关心目标类别，只关心显著性强的目标，一般目标检测会得到目标位置和类别；</li><li>问题建模不同：显著性目标检测，很早期的时候是通过对一些共性的特征建模，比如该目标一般在图像中心，一般是什么样的颜色分布等等。一般目标检测问题，则是希望特征能够更好的把目标表达出来，越细节越好，特征越丰富约好，因为要区分类别</li><li>groundtruth定义不同：两者互有交集，也有不同；前者，往往定义的是一些显著性较强的目标，比如行人，动物等等，而且前者是不输出类别标签信息；后者，gt标签定义是明确的，可以严格的输出对应类别标签；</li></ul></blockquote><p>其实，说白了，显著性检测等同于是一个二分类的语义分割模型，此外，显著的区域 不一定就是 目标，目标很可能不是显著的；</p><h3 id="方法设计"><a href="#方法设计" class="headerlink" title="方法设计"></a>方法设计</h3><h4 id="Residual-U-blocks"><a href="#Residual-U-blocks" class="headerlink" title="Residual U-blocks"></a>Residual U-blocks</h4><p>了解了显著性检测这个任务之后，来具体了解一下这篇文章的具体方法设计。作者设计一种Residual U-blocks，用于捕获 intra-stage 的 multi-scales 特征。</p><p><img src="/img/image-20200917201033485.png" alt="image-20200917201033485"></p><p>上图为普通卷积block，Res-like block，Inception-like block，Dense-like block和Residual U-blocks的对比图，明显可以看出Residual U-blocks是受了U-Net的启发。</p><p>Residual U-blocks有以下三部分组成：</p><blockquote><ul><li>一个输入卷积层，它将输入的feature map x (H × W × $C_{in}$)转换成中间feature map $F_1(x)$，$F_1(x)$通道数为$C_{out}$。这是一个用于局部特征提取的普通卷积层。</li><li>一个U-like的对称的encoder-decoder结构，高度为L，以中间feature map $F_1(x)$为输入，去学习提取和编码多尺度文本信息$U(F_1(x))$,U表示类U-Net结构。更大L会得到更深层的U-block（RSU），更多的池操作，更大的感受野和更丰富的局部和全局特征。配置此参数允许从具有任意空间分辨率的输入特征图中提取多尺度特征。从逐渐降采样特征映射中提取多尺度特征，并通过渐进上采样、合并和卷积等方法将其编码到高分辨率的特征图中。这一过程减少了大尺度直接上采样造成的细节损失。</li><li>一种残差连接，它通过求和来融合局部特征和多尺度特征：$F_1(x) + U(F_1(x))$。</li></ul></blockquote><p><img src="/img/image-20200917201132725.png" alt="image-20200917201132725"></p><p>RSU与Res block的主要设计区别在于RSU用U-Net结构代替了普通的单流卷积，用一个权重层(weight layer)形成的局部特征来代替原始特征。这种设计的变更使网络能够从多个尺度直接从每个残差块提取特征。更值得注意的是，U结构的计算开销很小，因为大多数操作都是在下采样的特征映射上进行的。</p><h4 id="U-2-Net的结构"><a href="#U-2-Net的结构" class="headerlink" title="U^2-Net的结构"></a>U^2-Net的结构</h4><p>U^2-Net的网络结构如下：</p><p><img src="/img/image-20200917201255253.png" alt="image-20200917201255253"></p><p>与U-Net的网络结构做一个对比：</p><p><img src="/img/1*TXfEPqTbFBPCbXYh2bstlA.png" alt="Learn How to Train U-Net On Your Dataset | by Sukriti Paul | Coinmonks |  Medium"></p><p>直观上可以发现，U^2-Net的每一个Block都是一个U-Net结构的模块，即上述Residual U-blocks。当然，你也可以继续Going Deeper, 每个Block里面的U-Net的子Block仍然可以是一个U-Net结构，命名为U^3-Net。</p><h3 id="损失函数设计"><a href="#损失函数设计" class="headerlink" title="损失函数设计"></a>损失函数设计</h3><p>类似于HED算法的deep supervision方式，作者设计了如下函数：</p><script type="math/tex; mode=display">\mathcal{L}=\sum_{m=1}^{M} w_{\text {side}}^{(m)} \ell_{\text {side}}^{(m)}+w_{\text {fuse}} \ell_{\text {fuse}}</script><p>其中，M=6, 为U2Net 的 Sup1, Sup2, …, Sup6 stage.$w_{\text {side}}^{(m)}$  $l_{\text {side}}^{(m)}$ 为对应的损失函数输出和权重；$w_{f u s e} \ell_{f u s e}$ 为融合的损失函数和权重;对于每一个$l$使用的都是标准的BCE Loss：</p><script type="math/tex; mode=display">\ell=-\sum_{(r, c)}^{(H, W)}\left[P_{G(r, c)} \log P_{S(r, c)}+\left(1-P_{G(r, c)}\right) \log \left(1-P_{S(r, c)}\right)\right]</script><h3 id="实验可视化"><a href="#实验可视化" class="headerlink" title="实验可视化"></a>实验可视化</h3><p>所提出的模型是使用DUTS-TR数据集进行训练，该数据集包含大约10000个样本图像，并使用标准数据增强技术进行扩充。研究人员在6个用于突出目标检测的基准数据集上评估了该模型：DUT-OMRON、DUTS-TE、HKU-IS、ECSSD、PASCAL-S和SOD。评价结果表明，在这6个基准点上，新模型与现有方法具有相当好的性能。</p><p><img src="/img/image-20200918110749081.png" alt="image-20200918110749081"></p><p>U^2-Net的实现是开源的，并提供了两种不同的预训练模型：U2Net(176.3M的较大模型，在GTX 1080Ti GPU上为30 FPS)，以及U2NetP(4.7M小模型，最高可达到40 FPS)。代码和预训练模型都可以在<a href="https://github.com/NathanUA/U-2-Net" target="_blank" rel="noopener">Github</a>。下面是我直接用作者开源的模型跑出来的结果，抠图效果很好，精细到发丝的那种。</p><p><img src="/img/image-20200917201523395.png" alt="image-20200917201523395"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>该论文的优势在于：</p><blockquote><ul><li>提出RSU模块，融合不同尺度感受野的特征，来捕捉不同尺度的上下文信息；</li><li>基于 RSU 模块的 池化(pooling) 操作，在不显著增加计算成本的前提下，增加了整个网络结构的深度(depth).</li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; Going Deeper with Nested U-Structure for Salient Object Detection&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/NathanUA/U-2-Net&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/NathanUA/U-2-Net&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt; 这是发表在PR2020上的一篇关于显著性检测的文章，作者是秦雪彬。从标题上可以看到本文的一个idea是设计了一个deeper的U型结构的网络解决显著性目标检测问题。作者认为，目前显著性目标检测有两种主流思路，一为多层次深层特征集成multi-level deep feature integration，一为多尺度特征提取Multi-scale feature extraction。多层次深层特征集成方法主要集中在开发更好的多层次特征聚合策略上。而多尺度特征提取这一类方法旨在设计更新的模块，从主干网获取的特征中同时提取局部和全局信息。而几乎所有上述方法，都是为了更好地利用现有的图像分类的Backbones生成的特征映射。而作者另辟蹊径，提出了一种新颖而简单的结构，它直接逐级提取多尺度特征，用于显著目标检测，而不是利用这些主干的特征来开发和添加更复杂的模块和策略。下图是该方法与其他方法的一个比较：&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="saliency detection" scheme="https://blog.nicehuster.cn/tags/saliency-detection/"/>
    
  </entry>
  
</feed>
