<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一起打怪升级呀</title>
  <icon>https://www.gravatar.com/avatar/2555127dc0de830d31ceeb98d8565ac8</icon>
  <subtitle>别整太大鸭力,多鸡立自己qaq</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://blog.nicehuster.cn/"/>
  <updated>2022-09-09T08:35:26.613Z</updated>
  <id>https://blog.nicehuster.cn/</id>
  
  <author>
    <name>nicehuster</name>
    <email>nicehuster@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Oscar&amp;METER方法详解</title>
    <link href="https://blog.nicehuster.cn/2022/08/29/Oscar_METER/"/>
    <id>https://blog.nicehuster.cn/2022/08/29/Oscar_METER/</id>
    <published>2022-08-29T11:13:39.000Z</published>
    <updated>2022-09-09T08:35:26.613Z</updated>
    
    <content type="html"><![CDATA[<p>本文要介绍的是微软的俩篇有关VLP的工作，Oscar和METER，前者是发表在CVPR2020，后者是发表在CVPR2022。论文链接如下：<a href="https://arxiv.org/pdf/2004.06165.pdf" target="_blank" rel="noopener">Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</a>，<a href="https://arxiv.org/pdf/2111.02387.pdf" target="_blank" rel="noopener">An Empirical Study of Training End-to-End Vision-and-Language Transformers</a>，下面大致介绍这俩篇工作的具体内容。</p><a id="more"></a><h3 id="Oscar"><a href="#Oscar" class="headerlink" title="Oscar"></a>Oscar</h3><p>这篇论文中提出了一种新的多模态预训练方法Oscar，把object用作视觉和语言语义层面上的Anchor Point，以简化图像和文本之间的语义对齐的学习任务，在多个下游任务上刷新了SOTA。</p><p><img src="/img/oscar.png" alt="oscar"></p><h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>在此之前，VLP方法都是简单粗暴地将图像区域特征和文本特征连接起来作为模型的输入以进行预训练，并不为模型提供任何线索，希望模型能利用Transformer的自我注意机制，使用蛮力来学习图像文本语义对齐方式。检测器在图像上检测的object通常会出现在对应caption text中，因此作者提出使用检测出来的物体标签对应caption中的词建立一个关联。</p><h4 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h4><p><img src="/img/oscar-pipeline.png" alt="oscar-pipeline"></p><p>上图展示了OSCAR的pipeline，通过将对象标签作为anchor引入，Oscar在两个方面与现有的VLP不同：</p><blockquote><ol><li>输入表示：每个image-text样本定义为一个三元组（单词序列，物体标签，区域特征）。</li><li>目标函数：作者从两个不同的角度设计目标函数： modality视角（Contrastive Loss）和dictionary视角（Masked Token Loss）。</li></ol></blockquote><p>注意，在这里object tag输入的embedding是使用同一词表得到word embedding。</p><h4 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h4><p><img src="/img/oscar-exp.png" alt="oscar-exp"></p><p>Oscar在六项任务上均达到了SOTA。在大多数任务上，Osacr的base model要优于以前方法的large model，其表明Oscar具有很高的参数利用效率，这是因为<strong>object tag的使用大大简化了图像和文本之间语义对齐的学习</strong>。</p><p>在这之后，原班作者在Oscar基础上针对检测模型部分提出了<a href="https://arxiv.org/abs/2101.00529" target="_blank" rel="noopener">VinVL</a>，聚焦于提升检测模型提取视觉语义特征能力。</p><h3 id="METER"><a href="#METER" class="headerlink" title="METER"></a>METER</h3><p>这是微软在CVPR2022上的有关VLP的工作。本文提出了METER，一个end2end的VLP框架，并从visual encoder、text encoder、Multimodal Fusion、结构设计以及与预训练目标函数上对VLP做了详细实验分析。METER在VQAv2上取得sota结果。</p><p><img src="/img/meter-overview.png" alt="meter-overview"></p><h4 id="Glossary-of-VLP-Models"><a href="#Glossary-of-VLP-Models" class="headerlink" title="Glossary of VLP Models"></a>Glossary of VLP Models</h4><p>以往基于OD（object detector）的VLP方法需要freeze object detector，限制了VLP模型能力，而且提取region特征时间代价较大。近期大多使用的ViT做visual encoder的VLP方法相比VinVL(OD)性能存在差距，为缩小差距本文提出METER，探索如何设计VLP模型。作者对现有VLP工作根据visual encoder、text encoder、Multimodal Fusion、Decoder以及预训练目标函数进行分类划分。</p><p><img src="/img/meter-glossary-of-vlp.png" alt="meter-glossary-of-vlp"></p><h4 id="METER-1"><a href="#METER-1" class="headerlink" title="METER"></a>METER</h4><p>总体结构：输入图片和文本对，图片经过visual encoder（CLIP-ViT，Swin，BEiT等）编码，文本经过text encoder（BERT，RoBERTa，DeBERTA等）编码，之后两者经过多模态融合模块（Merged Attention/Co-Attention）进行模态信息交互，最后经过一个可选的解码器输出结果。在论文中，作者对VLP模型各个模块都进行了分类和阐述，在实验部分进行了综合性分析并得到了每个部分最好的一个结构，从和产生METER最终结构。</p><blockquote><p>Vsual Encoder： CLIP-ViT-224/16，Swin Transformer</p><p>Text Encoder：Roberta</p><p>Multimodal Fusion： Co-attention（如下图所示）</p><p>Pre-training Objectives：MLM（masked language modeling ）+ITM（image-text matching）</p></blockquote><p><img src="/img/meter-multimodel-fusion.png" alt="meter-multimodel-fusion"></p><p>上图展示的是多模态融合模块的结构，在METER中，使用的是Co-Attention，Co-Attention中包含了堆叠的6层transformer layers，每层包含了一个self-attention，一个co-attention和一个前馈网络。没有decoder和编码器参数共享。</p><h4 id="Experiments-1"><a href="#Experiments-1" class="headerlink" title="Experiments"></a>Experiments</h4><p>在预训练数据方面，作者仅使用了COCO, Conceptual Captions, SBU Captions and Visual Genome，总共4M图片数据。在多个下游任务上进行验证，其中包括VQAv2，visual reasoning(NLVR2), visual entailment(SNLI-VE)和image-text retrieval(COO, Flickr30k)。作者实验做的非常详细，推荐去看原文。</p><h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>（1）MERTER是Vision transformer + Text transformer结构</p><p>（2）在METER中Vision encoder用CLIP-ViT或者Swim transformer，Text encoder用Roberta，多模态融合用co-attention；</p><p>（3）在目标函数上，MLM+ITM都对VLP模型有帮助，但是MIM会带来负面影响；</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文要介绍的是微软的俩篇有关VLP的工作，Oscar和METER，前者是发表在CVPR2020，后者是发表在CVPR2022。论文链接如下：&lt;a href=&quot;https://arxiv.org/pdf/2004.06165.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks&lt;/a&gt;，&lt;a href=&quot;https://arxiv.org/pdf/2111.02387.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;An Empirical Study of Training End-to-End Vision-and-Language Transformers&lt;/a&gt;，下面大致介绍这俩篇工作的具体内容。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="多模态" scheme="https://blog.nicehuster.cn/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>ALBEF方法详解</title>
    <link href="https://blog.nicehuster.cn/2022/08/18/ALBEF/"/>
    <id>https://blog.nicehuster.cn/2022/08/18/ALBEF/</id>
    <published>2022-08-18T11:13:39.000Z</published>
    <updated>2022-08-25T07:25:09.230Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章介绍一篇多模态预训练相关的论文，<a href="https://arxiv.org/abs/2107.07651" target="_blank" rel="noopener">Align before Fuse: Vision and Language Representation Learning with Momentum Distillation</a>，单位是Salesforce Research，下面大致的介绍一下两篇论文的具体工作。这篇paper提出了一个新的视觉-语言表征学习框架，通过在融合之前首先对齐单模态表征来实现最佳性能。</p><a id="more"></a><p>现有的VLP方法存在如下三个限制：</p><blockquote><p>Limitation 1：以CLIP和ALIGN为代表的方法分别独立学习单模态的图像encoder和文本encoder，缺乏对图像和文本之间的复杂互动进行建模的能力，因此它们不擅长于需要细粒度图像-文本理解的任务；</p><p>Limitation 2：以UNITER为代表的方法使用多模态编码器联合学习图像与文本，然而，从区域中提取的图片特征和文本词向量是没有对齐的；</p><p>Limitation 3：现有用于预训练的数据集大多是由从网络上收集的嘈杂的图像-文本对组成。广泛使用的预训练目标，如掩码语言建模（MLM），容易对噪声文本过度拟合，这将损害表示学习。</p></blockquote><p>为了解决这些限制，我们提出了ALign BEfore Fuse（ALBEF），ALBEF在多个视觉-语言下游任务上取得了SOTA的性能，如图像-文本检索、视觉问题回答（VQA）和自然语言视觉推理（NLVR）。</p><h4 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h4><p><img src="/img/ALBEF.png" alt="ALBEF"></p><p>上图展示了ALBEF的整体框架结构，ALBEF包含一个image encoder（ViT-B/16），一个text encoder（BERT的前6层），以及一个multimodal encoder（BERT的后6层与额外的交叉注意力层）。我们通过共同优化以下三个目标来对ALBEF进行预训练：</p><blockquote><p>Objective 1：图像-文本对比学习应用于单模态的image encoder和text encoder。它使图像特征和文本特征相一致，同时训练单模态编码器更好地理解图像和文本的语义;</p><p>Objective 2：图像-文本匹配应用于多模态编码器，预测一对图像和文本是否匹配。我们还使用了难样本挖掘，选择具有较高相似度的样本进行学习;</p><p>Objective 3：在多模态编码器上应用掩码语言建模（MLM）进行训练；</p></blockquote><h4 id="Momentum-Distillation"><a href="#Momentum-Distillation" class="headerlink" title="Momentum Distillation"></a>Momentum Distillation</h4><p>从网络上收集的图像-文本对往往是弱相关的：文本可能包含与图像无关的词，或者图像可能包含文本中没有描述的实体。为了从嘈杂的数据中学习，我们提出了动量蒸馏法，即使用动量模型为图像-文本对比学习和掩码语言建模生成伪目标。</p><h4 id="下游任务上的应用"><a href="#下游任务上的应用" class="headerlink" title="下游任务上的应用"></a>下游任务上的应用</h4><p>ALBEF在多个下游任务上取得了最先进的性能，如下表所示。在图像-文本检索方面，ALBEF优于在更大数量级的数据集上进行预训练的方法（CLIP[2]和ALIGN[3]）。在VQA、NLVR和VE方面，ALBEF优于那些使用预先训练的物体检测器、额外的物体标签或对抗性数据增强的方法。</p><p><img src="/img/ALBEF-zero-shot-i2tr.png" alt="ALBEF-zero-shot-i2tr"></p><p><img src="/img/ALBEF-VQA.png" alt="ALBEF-VQA"></p><h4 id="Visual-Grounding"><a href="#Visual-Grounding" class="headerlink" title="Visual Grounding"></a>Visual Grounding</h4><p>有意思的是，ALBEF还隐含的学习了物体、属性和关系。使用Grad-CAM对multimodal encoder的交叉注意力进行可视化，在弱监督的visual grounding任务上取得很不错的结果，如下示例：</p><p><img src="/img/ALBEF-vg-vis1.png" alt></p><p><img src="/img/ALBEF-vg-vis2.png" alt></p><p><img src="/img/ALBEF-vg-vis3.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章介绍一篇多模态预训练相关的论文，&lt;a href=&quot;https://arxiv.org/abs/2107.07651&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Align before Fuse: Vision and Language Representation Learning with Momentum Distillation&lt;/a&gt;，单位是Salesforce Research，下面大致的介绍一下两篇论文的具体工作。这篇paper提出了一个新的视觉-语言表征学习框架，通过在融合之前首先对齐单模态表征来实现最佳性能。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="多模态" scheme="https://blog.nicehuster.cn/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>BERT原理详解与HuggingFace使用[转载]</title>
    <link href="https://blog.nicehuster.cn/2022/08/04/BERT/"/>
    <id>https://blog.nicehuster.cn/2022/08/04/BERT/</id>
    <published>2022-08-04T11:13:39.000Z</published>
    <updated>2022-08-25T07:35:29.745Z</updated>
    
    <content type="html"><![CDATA[<p>最近在做一些图文理解相关的工作，顺带了解了一下BERT，自BERT（Bidirectional Encoder Representations from Transformer）出现后，NLP界开启了一个全新的范式。本文主要介绍BERT的原理，以及如何使用HuggingFace提供的 <code>transformers</code> 库完成基于BERT的微调任务。</p><a id="more"></a><h4 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h4><p>BERT在一个较大的语料上进行预训练（Pre-train）。预训练主要是在数据和算力充足的条件下，训练一个大模型，在其他任务上可以利用预训练好的模型进行微调（Fine-tune）。</p><h4 id="训练目标"><a href="#训练目标" class="headerlink" title="训练目标"></a>训练目标</h4><p>BERT使用了维基百科等语料库数据，共几十GB，这是一个庞大的语料库。对于一个GB级的语料库，雇佣人力进行标注成本极高。BERT使用了两个巧妙方法来无监督地训练模型：<strong>Masked Language Modeling</strong>和<strong>Next Sentence Prediction</strong>。这两个方法可以无需花费时间和人力标注数据，以较低成本无监督地得到训练数据。图1就是一个输入输出样例。</p><p>对于Masked Language Modeling，给定一些输入句子（图1中最下面的输入层），BERT将输入句子中的一些单词盖住（图1中Masked层），经过中间的词向量和BERT层后，BERT的目标是让模型能够预测那些刚刚被盖住的词。还记得英语考试中，我们经常遇到“完形填空”题型吗？能把完形填空做对，说明已经理解了文章背后的语言逻辑。BERT的Masked Language Modeling本质上就是在做“完形填空”：预训练时，先将一部分词随机地盖住，经过模型的拟合，如果能够很好地预测那些盖住的词，模型就学到了文本的内在逻辑。</p><p><img src="http://aixingqiu-1258949597.cos.ap-beijing.myqcloud.com/2021-12-18-pretrain.png" alt="img">图1 BERT预训练的输入和输出</p><p>除了“完形填空”，BERT还需要做Next Sentence Prediction任务：预测句子B是否为句子A的下一句。Next Sentence Prediction有点像英语考试中的“段落排序”题，只不过简化到只考虑两句话。如果模型无法正确地基于当前句子预测Next Sentence，而是生硬地把两个不相关的句子拼到一起，两个句子在语义上是毫不相关的，说明模型没有读懂文本背后的意思。</p><h4 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h4><p>在基于深度学习的NLP方法中，文本中的词通常都用一维向量来表示。某两个词向量的 Cosine 距离较小，说明两个词在语义上相似。</p><p>信息</p><p>词向量一般由Token转换而成。英文中，一个句子中的词由空格、句号等标点隔开，我们很容易从句子中获得词。英文的词通常有前缀、后缀、词根等，在获得英文的词后，还需要抽出词根，比如图1所展示的，将“playing”切分为“play”和“##ing”。如果不对英文词进行类似词根抽取，词表过大，不容易拟合。对于英文，“play”和“##ing”分别对应两个Token。</p><p>中文一般由多个字组成一个词，传统的中文文本任务通常使用一些分词工具，得到严格意义上的词。在原始的BERT中，对于中文，并没有使用分词工具，而是直接以字为粒度得到词向量的。所以，原始的中文BERT（bert-base-chinese）输入到BERT模型的是字向量，Token就是字。后续有专门的研究去探讨，是否应该对中文进行必要的分词，以词的形式进行切分，得到向量放入BERT模型。</p><p>为了方面说明，本文不明确区分字向量还是词向量，都统称为词向量。</p><p>我们首先需要将文本中每个Token都转换成一维词向量。假如词向量的维度为<code>hidden_size</code>，句子的Token长度为<code>seq_len</code>，或者说句子共包含<code>seq_len</code>个Token，那么上图中，输入就是<code>seq_len * hidden_size</code>。再加上<code>batch_size</code>，那么输入就是<code>batch_size * seq_len * hidden_size</code>。上图只展示了一个样本，未体现出<code>batch_size</code>，或者可以理解成<code>batch_size = 1</code>，即每次只处理一条文本。</p><p>词向量经过BERT模型一系列复杂的转换后，模型最后仍然以词向量的形式输出，用以对文本进行语义表示。输入的词向量是<code>seq_len * hidden_size</code>，句子共<code>seq_len</code>个Token，将每个Token都转换成词向量，送入BERT模型。经过BERT模型后，得到的输出仍然是<code>seq_len * hidden_size</code>维度。输出仍然是<code>seq_len</code>的长度，其中输出的<code>i</code> 个位置（0 &lt; <code>i</code> &lt; <code>seq_len</code>）的词向量，表示经过了拟合后的第<code>i</code>个Token的语义表示。后续可以用输出中每个位置的词向量来进行一些其他任务，比如命名实体识别等。</p><p>除了使用Masked方法故意盖住一些词外，BERT还加了一些特殊的符号：<code>[CLS]</code>和<code>[SEP]</code>。<code>[CLS]</code>用在句首，是句子序列中<code>i = 0</code>位置的Token。BERT认为输出序列的<code>i = 0</code>位置的Token对应的词向量包含了整个句子的信息，可对整个句子进行分类。<code>[SEP]</code>用在分割前后两个句子上。</p><h4 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h4><p>经过预训练后，得到的模型可以用来微调各类任务。</p><ul><li>单文本分类任务。刚才提到，BERT模型在文本前插入一个<code>[CLS]</code>符号，并将该符号对应的输出向量作为整篇文本的语义表示，用于文本分类，如图2所示。对于<code>[CLS]</code>符号，可以理解为：与文本中已有的其它字/词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个字/词的语义信息。</li></ul><p><img src="http://aixingqiu-1258949597.cos.ap-beijing.myqcloud.com/2021-12-18-single-classification.jpeg" alt="img">图2 单文本分类</p><ul><li>语句对分类任务。语句对分类任务的实际应用场景包括：问答（判断一个问题与一个答案是否匹配）、语句匹配（两句话是否表达同一个意思）等。对于该任务，BERT模型除了添加<code>[CLS]</code>符号并将对应的输出作为文本的语义表示，输入两句话之间用<code>[SEP]</code>符号作分割。</li></ul><p><img src="http://aixingqiu-1258949597.cos.ap-beijing.myqcloud.com/2021-12-18-pair-classification.jpeg" alt="img">图3 语句对分类</p><ul><li>序列标注任务。序列标注任务的实际应用场景包括：命名实体识别、中文分词、新词发现（标注每个字是词的首字、中间字或末字）、答案抽取（答案的起止位置）等。对于该任务，BERT模型利用文本中每个Token对应的输出向量对该Token进行标注（分类），如下图所示(B（Begin）、I（Inside）、E（End）分别表示一个词的第一个字、中间字和最后一个字)。</li></ul><p><img src="http://aixingqiu-1258949597.cos.ap-beijing.myqcloud.com/2021-12-18-seq-tagging.jpeg" alt="img">图4 序列标注</p><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>Transformer是BERT的核心模块，Attention注意力机制又是Transformer中最关键的部分。BERT用到的主要是Transformer的Encoder，没有使用Transformer Decoder。把多个Transformer Encoder组装起来，就构成了BERT。在论文中，作者分别用12个和24个Transformer Encoder组装了两套BERT模型，两套模型的参数总数分别为110M和340M。</p><p><img src="http://aixingqiu-1258949597.cos.ap-beijing.myqcloud.com/2021-12-18-transformer-encoder.jpeg" alt="img"></p><h4 id="HuggingFace-Transformers"><a href="#HuggingFace-Transformers" class="headerlink" title="HuggingFace Transformers"></a>HuggingFace Transformers</h4><p>使用BERT和其他各类Transformer模型，绕不开<a href="https://huggingface.co/" target="_blank" rel="noopener">HuggingFace</a>提供的Transformers生态。HuggingFace提供了各类BERT的API（<code>transformers</code>库）、训练好的模型（HuggingFace Hub）还有数据集（<code>datasets</code>）。最初，HuggingFace用PyTorch实现了BERT，并提供了预训练的模型，后来。越来越多的人直接使用HuggingFace提供好的模型进行微调，将自己的模型共享到HuggingFace社区。HuggingFace的社区越来越庞大，不仅覆盖了PyTorch版，还提供TensorFlow版，主流的预训练模型都会提交到HuggingFace社区，供其他人使用。</p><p>使用<code>transformers</code>库进行微调，主要包括：</p><ul><li>Tokenizer：使用提供好的Tokenizer对原始文本处理，得到Token序列；</li><li>构建模型：在提供好的模型结构上，增加下游任务所需预测接口，构建所需模型；</li><li>微调：将Token序列送入构建的模型，进行训练。</li></ul><h4 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h4><p>下面两行代码会创建 <code>BertTokenizer</code>，并将所需的词表加载进来。首次使用这个模型时，<code>transformers</code> 会帮我们将模型从HuggingFace Hub下载到本地。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-cased'</span>)</span><br></pre></td></tr></table></figure><p>用得到的<code>tokenizer</code>进行分词：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(<span class="string">"我是一句话"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(encoded_input)</span><br><span class="line">&#123;<span class="string">'input_ids'</span>: [<span class="number">101</span>, <span class="number">2769</span>, <span class="number">3221</span>, <span class="number">671</span>, <span class="number">1368</span>, <span class="number">6413</span>, <span class="number">102</span>], </span><br><span class="line"><span class="string">'token_type_ids'</span>: [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], </span><br><span class="line"><span class="string">'attention_mask'</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]&#125;</span><br></pre></td></tr></table></figure><p>得到的一个Python <code>dict</code>。其中，<code>input_ids</code>最容易理解，它表示的是句子中的每个Token在词表中的索引数字。词表（Vocabulary）是一个Token到索引数字的映射。可以使用<code>decode()</code>方法，将索引数字转换为Token。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer.decode(encoded_input[<span class="string">"input_ids"</span>])</span><br><span class="line"><span class="string">'[CLS] 我 是 一 句 话 [SEP]'</span></span><br></pre></td></tr></table></figure><p>可以看到，<code>BertTokenizer</code>在给原始文本处理时，自动给文本加上了<code>[CLS]</code>和<code>[SEP]</code>这两个符号，分别对应在词表中的索引数字为101和102。<code>decode()</code>之后，也将这两个符号反向解析出来了。</p><p><code>token_type_ids</code>主要用于句子对，比如下面的例子，两个句子通过<code>[SEP]</code>分割，0表示Token对应的<code>input_ids</code>属于第一个句子，1表示Token对应的<code>input_ids</code>属于第二个句子。不是所有的模型和场景都用得上<code>token_type_ids</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(<span class="string">"您贵姓?"</span>, <span class="string">"免贵姓李"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(encoded_input)</span><br><span class="line">&#123;<span class="string">'input_ids'</span>: [<span class="number">101</span>, <span class="number">2644</span>, <span class="number">6586</span>, <span class="number">1998</span>, <span class="number">136</span>, <span class="number">102</span>, <span class="number">1048</span>, <span class="number">6586</span>, <span class="number">1998</span>, <span class="number">3330</span>, <span class="number">102</span>], </span><br><span class="line"><span class="string">'token_type_ids'</span>: [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], </span><br><span class="line"><span class="string">'attention_mask'</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]&#125;</span><br></pre></td></tr></table></figure><p>句子通常是变长的，多个句子组成一个Batch时，<code>attention_mask</code>就起了至关重要的作用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>batch_sentences = [<span class="string">"我是一句话"</span>, <span class="string">"我是另一句话"</span>, <span class="string">"我是最后一句话"</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>batch = tokenizer(batch_sentences, padding=<span class="keyword">True</span>, return_tensors=<span class="string">"pt"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(batch)</span><br><span class="line">&#123;<span class="string">'input_ids'</span>: </span><br><span class="line"> tensor([[ <span class="number">101</span>, <span class="number">2769</span>, <span class="number">3221</span>,  <span class="number">671</span>, <span class="number">1368</span>, <span class="number">6413</span>,  <span class="number">102</span>,    <span class="number">0</span>,    <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">101</span>, <span class="number">2769</span>, <span class="number">3221</span>, <span class="number">1369</span>,  <span class="number">671</span>, <span class="number">1368</span>, <span class="number">6413</span>,  <span class="number">102</span>,    <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">101</span>, <span class="number">2769</span>, <span class="number">3221</span>, <span class="number">3297</span>, <span class="number">1400</span>,  <span class="number">671</span>, <span class="number">1368</span>, <span class="number">6413</span>,  <span class="number">102</span>]]), </span><br><span class="line"> <span class="string">'token_type_ids'</span>: </span><br><span class="line"> tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]]), </span><br><span class="line"> <span class="string">'attention_mask'</span>: </span><br><span class="line"> tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]])&#125;</span><br></pre></td></tr></table></figure><p>对于这种<code>batch_size = 3</code>的场景，不同句子的长度是不同的，<code>padding=True</code>表示短句子的结尾会被填充<code>[PAD]</code>符号，<code>return_tensors=&quot;pt&quot;</code>表示返回PyTorch格式的<code>Tensor</code>。<code>attention_mask</code>告诉模型，哪些Token需要被模型关注而加入到模型训练中，哪些Token是被填充进去的无意义的符号，模型无需关注。</p><h4 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h4><p>下面两行代码会创建<code>BertModel</code>，并将所需的模型参数加载进来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>model = BertModel.from_pretrained(<span class="string">"bert-base-chinese"</span>)</span><br></pre></td></tr></table></figure><p><code>BertModel</code>是一个PyTorch中用来包裹网络结构的<code>torch.nn.Module</code>，<code>BertModel</code>里有<code>forward()</code>方法，<code>forward()</code>方法中实现了将Token转化为词向量，再将词向量进行多层的Transformer Encoder的复杂变换。<code>forward()</code>方法的入参有<code>input_ids</code>、<code>attention_mask</code>、<code>token_type_ids</code>等等，这些参数基本上是刚才Tokenizer部分的输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>bert_output = model(input_ids=batch[<span class="string">'input_ids'</span>])</span><br></pre></td></tr></table></figure><p><code>forward()</code>方法返回模型预测的结果，返回结果是一个<code>tuple(torch.FloatTensor)</code>，即多个<code>Tensor</code>组成的<code>tuple</code>。<code>tuple</code>默认返回两个重要的<code>Tensor</code>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>len(bert_output)</span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure><ul><li><strong>last_hidden_state</strong>：输出序列每个位置的语义向量，形状为：(batch_size, sequence_length, hidden_size)。</li><li><strong>pooler_output</strong>：<code>[CLS]</code>符号对应的语义向量，经过了全连接层和tanh激活；该向量可用于下游分类任务。</li></ul><h4 id="下游任务"><a href="#下游任务" class="headerlink" title="下游任务"></a>下游任务</h4><p>BERT可以进行很多下游任务，<code>transformers</code>库中实现了一些下游任务，我们也可以参考<code>transformers</code>中的实现，来做自己想做的任务。比如单文本分类，<code>transformers</code>库提供了<code>BertForSequenceClassification</code>类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertForSequenceClassification</span><span class="params">(BertPreTrainedModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        super().__init__(config)</span><br><span class="line">        self.num_labels = config.num_labels</span><br><span class="line">        self.config = config</span><br><span class="line"></span><br><span class="line">        self.bert = BertModel(config)</span><br><span class="line">        classifier_dropout = ...</span><br><span class="line">        self.dropout = nn.Dropout(classifier_dropout)</span><br><span class="line">        self.classifier = nn.Linear(config.hidden_size, config.num_labels)</span><br><span class="line"></span><br><span class="line">        ...</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        ...</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        outputs = self.bert(...)</span><br><span class="line">        pooled_output = outputs[<span class="number">1</span>]</span><br><span class="line">        pooled_output = self.dropout(pooled_output)</span><br><span class="line">        logits = self.classifier(pooled_output)</span><br><span class="line"></span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><p>在这段代码中，<code>BertForSequenceClassification</code>在<code>BertModel</code>基础上，增加了<code>nn.Dropout</code>和<code>nn.Linear</code>层，在预测时，将<code>BertModel</code>的输出放入<code>nn.Linear</code>，完成一个分类任务。除了<code>BertForSequenceClassification</code>，还有<code>BertForQuestionAnswering</code>用于问答，<code>BertForTokenClassification</code>用于序列标注，比如命名实体识别。<code>transformers</code> 中的各个API还有很多其他参数设置，比如得到每一层Transformer Encoder的输出等等，可以访问他们的<a href="https://huggingface.co/docs/transformers/" target="_blank" rel="noopener">文档</a>查看使用方法。</p><p>注：以上内容均转载自 <a href="https://lulaoshi.info/machine-learning/attention/bert" target="_blank" rel="noopener">BERT原理解析及HuggingFace transformers使用入门</a>，侵权删</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在做一些图文理解相关的工作，顺带了解了一下BERT，自BERT（Bidirectional Encoder Representations from Transformer）出现后，NLP界开启了一个全新的范式。本文主要介绍BERT的原理，以及如何使用HuggingFace提供的 &lt;code&gt;transformers&lt;/code&gt; 库完成基于BERT的微调任务。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="bert" scheme="https://blog.nicehuster.cn/tags/bert/"/>
    
  </entry>
  
  <entry>
    <title>Deformable-DETR详解与代码解读</title>
    <link href="https://blog.nicehuster.cn/2022/07/26/Deformable%20DETR/"/>
    <id>https://blog.nicehuster.cn/2022/07/26/Deformable DETR/</id>
    <published>2022-07-26T11:13:39.000Z</published>
    <updated>2022-08-26T02:16:35.536Z</updated>
    
    <content type="html"><![CDATA[<p>DETR是第一个end2end的目标检测器，不需要众多手工设计组件（anchor，iou匹配，nms后处理等），但也存在收敛慢，能处理的特征分辨率有限等缺陷。原因大概存在如下：</p><blockquote><ul><li>transformer在初始化时，分配给所有特征像素的注意力权重几乎均等；这就造成了模型需要长时间去学习关注真正有意义的位置，这些位置应该是稀疏的；</li><li>transformer在计算注意力权重时，伴随着高计算量与空间复杂度。特别是在编码器部分，与特征像素点的数量成平方级关系，因此难以处理高分辨率的特征;</li></ul></blockquote><a id="more"></a><p>Deformable DETR的工作就在于解决DETR收敛慢以及高计算复杂度问题。具体做法有：</p><h4 id="多尺度特征-amp-多尺度Embedding"><a href="#多尺度特征-amp-多尺度Embedding" class="headerlink" title="多尺度特征 &amp; 多尺度Embedding"></a>多尺度特征 &amp; 多尺度Embedding</h4><p>在DETR中，由于计算复杂度的问题，仅仅只使用了<strong>单尺度特征</strong>，对于特征点位置信息编码使用三角函数，不同位置对应不同编码值。而在多尺度特征中，位于不同特征层的特征点可能拥有相同的(h,w)坐标，使用一套位置编码是无法区分。因此作者使用多尺度特征时，增加了scale-level embedding，用于区分不同特征层。不同于三角函数使用固定公式计算编码，scale-level embedding是可学习的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeformableTransformer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model=<span class="number">256</span>, nhead=<span class="number">8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_encoder_layers=<span class="number">6</span>, num_decoder_layers=<span class="number">6</span>, dim_feedforward=<span class="number">1024</span>, dropout=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 activation=<span class="string">"relu"</span>, return_intermediate_dec=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_feature_levels=<span class="number">4</span>, dec_n_points=<span class="number">4</span>,  enc_n_points=<span class="number">4</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 two_stage=False, two_stage_num_proposals=<span class="number">300</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># scale level embedding，对4个特征层分别附加d_model维度的embedding，用于区分query对应的具体特征层</span></span><br><span class="line">        self.level_embed = nn.Parameter(torch.Tensor(num_feature_levels, d_model))</span><br><span class="line">        ...</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, srcs, masks, pos_embeds, query_embed=None)</span>:</span></span><br><span class="line">            ...</span><br><span class="line">            <span class="keyword">for</span> lvl, (src, mask, pos_embed) <span class="keyword">in</span> enumerate(zip(srcs, masks, pos_embeds)):</span><br><span class="line">                ...</span><br><span class="line">                <span class="comment"># (bs,c,h,w) --&gt; (bs,h*w,c)</span></span><br><span class="line">                pos_embed = pos_embed.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">                <span class="comment">#与position embedding相加，且同一特征层，所有query的scale-level embedding是相同的</span></span><br><span class="line">                lvl_pos_embed = pos_embed + self.level_embed[lvl].view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">                ...</span><br></pre></td></tr></table></figure><h4 id="Deformable-Attention"><a href="#Deformable-Attention" class="headerlink" title="Deformable Attention"></a>Deformable Attention</h4><p>通俗地来讲，可变性注意力即，query不是和全局每个位置的key都计算注意力权重，而仅在全局位置中采样部分位置的key，并且value也是基于这些位置进行采样插值得到，最后将局部&amp;稀疏的注意力权重施加在对应的value上。</p><p><img src="/img/deformAttn.png" alt></p><p>如上图所示，每个query在每个head上采样K个位置，只需和这些位置的特征进行交互，不同于detr那样，每个query需要与全局位置进行交互。需要注意的是，位置偏移量$\Delta p_{mqx}$ 是由query经过全连接得到的，注意力权重也是由query经全连接层得到的，同时在K个采样点之间进行权重归一化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MSDeformAttn</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model=<span class="number">256</span>, n_levels=<span class="number">4</span>, n_heads=<span class="number">8</span>, n_points=<span class="number">4</span>)</span>:</span></span><br><span class="line">        ...</span><br><span class="line">        self.n_points = n_points</span><br><span class="line"><span class="comment"># 采样点的坐标偏移量，每个query在每个head，level上都需要采样n_points个点（x,y）。</span></span><br><span class="line">        self.sampling_offsets = nn.Linear(d_model, n_heads * n_levels * n_points * <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 每个query对应的所有采样点的注意力权重</span></span><br><span class="line">        self.attention_weights = nn.Linear(d_model, n_heads * n_levels * n_points)</span><br><span class="line">        <span class="comment"># 线性变换得到value</span></span><br><span class="line">        self.value_proj = nn.Linear(d_model, d_model)</span><br><span class="line">        self.output_proj = nn.Linear(d_model, d_model)</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, reference_points, input_flatten, input_spatial_shapes, input_level_start_index, input_padding_mask=None)</span>:</span></span><br><span class="line">        </span><br><span class="line">       ...</span><br><span class="line">        <span class="comment"># (N,len_in, d_model=256)</span></span><br><span class="line">        value = self.value_proj(input_flatten)</span><br><span class="line">        <span class="comment"># 将原图padding的部分用0填充</span></span><br><span class="line">        <span class="keyword">if</span> input_padding_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            value = value.masked_fill(input_padding_mask[..., <span class="keyword">None</span>], float(<span class="number">0</span>))</span><br><span class="line">        <span class="comment"># 拆分成多个head,(N,Len_in,8,64)</span></span><br><span class="line">        value = value.view(N, Len_in, self.n_heads, self.d_model // self.n_heads)</span><br><span class="line">        <span class="comment"># 预测采样点偏移量，(N,Len_in,8,4,4,2)</span></span><br><span class="line">        sampling_offsets = self.sampling_offsets(query).view(N, Len_q, self.n_heads, self.n_levels, self.n_points, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 预测采样点注意力权重，(N,Len_in,8,4*4)</span></span><br><span class="line">        attention_weights = self.attention_weights(query).view(N, Len_q, self.n_heads, self.n_levels * self.n_points)</span><br><span class="line">        <span class="comment"># 权重归一化，对4个特征层分别采样的4个特征点，合计16个点，进行归一化</span></span><br><span class="line">        attention_weights = F.softmax(attention_weights, <span class="number">-1</span>).view(N, Len_q, self.n_heads, self.n_levels, self.n_points)</span><br><span class="line">        <span class="comment"># N, Len_q, n_heads, n_levels, n_points, 2</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> reference_points.shape[<span class="number">-1</span>] == <span class="number">2</span>: </span><br><span class="line">            <span class="comment"># (4，2) 其中每个是w,h</span></span><br><span class="line">            offset_normalizer = torch.stack([input_spatial_shapes[..., <span class="number">1</span>], input_spatial_shapes[..., <span class="number">0</span>]], <span class="number">-1</span>)</span><br><span class="line">            <span class="comment"># 对坐标偏移量使用对应特征层的宽高进行归一化然后和参考点坐标相加得到采样点坐标</span></span><br><span class="line">            sampling_locations = reference_points[:, :, <span class="keyword">None</span>, :, <span class="keyword">None</span>, :] \</span><br><span class="line">                                 + sampling_offsets / offset_normalizer[<span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span>, :, <span class="keyword">None</span>, :]</span><br><span class="line">        <span class="keyword">elif</span> reference_points.shape[<span class="number">-1</span>] == <span class="number">4</span>:</span><br><span class="line">            <span class="comment"># 最后一维度是4表示(cx,cy,w,h)</span></span><br><span class="line">            sampling_locations = reference_points[:, :, <span class="keyword">None</span>, :, <span class="keyword">None</span>, :<span class="number">2</span>] \</span><br><span class="line">                                 + sampling_offsets / self.n_points * reference_points[:, :, <span class="keyword">None</span>, :, <span class="keyword">None</span>, <span class="number">2</span>:] * <span class="number">0.5</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(</span><br><span class="line">                <span class="string">'Last dim of reference_points must be 2 or 4, but get &#123;&#125; instead.'</span>.format(reference_points.shape[<span class="number">-1</span>]))</span><br><span class="line">        <span class="comment"># 将注意力权重与value进行计算，是调用的self.im2col_step函数</span></span><br><span class="line">        output = MSDeformAttnFunction.apply(</span><br><span class="line">            value, input_spatial_shapes, input_level_start_index, sampling_locations, attention_weights, self.im2col_step)</span><br><span class="line">        <span class="comment"># 做线性变换得到最终输出结果</span></span><br><span class="line">        output = self.output_proj(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><h4 id="Deformable-Transformer"><a href="#Deformable-Transformer" class="headerlink" title="Deformable Transformer"></a>Deformable Transformer</h4><p>与DETR大体一致，主要区别在于用Deformable Attention替换了Encoder中的self-attn和Decoder中的cross-attn。</p><h5 id="Encoder前处理"><a href="#Encoder前处理" class="headerlink" title="Encoder前处理"></a>Encoder前处理</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeformableTransformer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    </span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, srcs, masks, pos_embeds, query_embed=None)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> self.two_stage <span class="keyword">or</span> query_embed <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># prepare input for encoder</span></span><br><span class="line">        src_flatten = []</span><br><span class="line">        mask_flatten = []</span><br><span class="line">        lvl_pos_embed_flatten = []</span><br><span class="line">        spatial_shapes = []</span><br><span class="line">        <span class="keyword">for</span> lvl, (src, mask, pos_embed) <span class="keyword">in</span> enumerate(zip(srcs, masks, pos_embeds)):</span><br><span class="line">            bs, c, h, w = src.shape</span><br><span class="line">            spatial_shape = (h, w)</span><br><span class="line">            spatial_shapes.append(spatial_shape)</span><br><span class="line">            src = src.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            mask = mask.flatten(<span class="number">1</span>)</span><br><span class="line">            pos_embed = pos_embed.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            lvl_pos_embed = pos_embed + self.level_embed[lvl].view(<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>)</span><br><span class="line">            lvl_pos_embed_flatten.append(lvl_pos_embed)</span><br><span class="line">            src_flatten.append(src)</span><br><span class="line">            mask_flatten.append(mask)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#多尺度特征flatten后进行拼接</span></span><br><span class="line">        src_flatten = torch.cat(src_flatten, <span class="number">1</span>)</span><br><span class="line">        <span class="comment">#多尺度mask图flatten后进行拼接</span></span><br><span class="line">        mask_flatten = torch.cat(mask_flatten, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 多尺度位置信息flatten后进行拼接</span></span><br><span class="line">        lvl_pos_embed_flatten = torch.cat(lvl_pos_embed_flatten, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 记录每个特征图的尺度信息</span></span><br><span class="line">        spatial_shapes = torch.as_tensor(spatial_shapes, dtype=torch.long, device=src_flatten.device)</span><br><span class="line">        <span class="comment"># 记录每个尺度特征图拼接后的起始索引</span></span><br><span class="line">        level_start_index = torch.cat((spatial_shapes.new_zeros((<span class="number">1</span>, )), spatial_shapes.prod(<span class="number">1</span>).cumsum(<span class="number">0</span>)[:<span class="number">-1</span>]))</span><br><span class="line">        <span class="comment"># 计算各个尺度特征图中非padding部分的边长占其边长的比例</span></span><br><span class="line">        valid_ratios = torch.stack([self.get_valid_ratio(m) <span class="keyword">for</span> m <span class="keyword">in</span> masks], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># encoder</span></span><br><span class="line">        memory = self.encoder(src_flatten, spatial_shapes, level_start_index, valid_ratios, lvl_pos_embed_flatten, mask_flatten)</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure><p>在DeformableTransformer的前向处理过程中，首先会对多尺度相关的元素进行flatten，这些输入元素包括：多尺度特征图、各尺度特征图对应的mask（指示哪些部分属于padding）、各尺度特征图对应的位置信息（<strong>position embedding + scale-level embedding</strong>），另外还有些辅助信息，比如：各尺度特征图的宽高、不同尺度特征对应于被flatten的那个维度的起始索引、各尺度特征图中非padding部分的边长占其边长的比例。</p><h5 id="Encoder编码"><a href="#Encoder编码" class="headerlink" title="Encoder编码"></a>Encoder编码</h5><p>经过Encoder前处理之后的信息就会经过Encoder进行编码，输出memory。下面代码展示的是Encoder的处理过程：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeformableTransformerEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder_layer, num_layers)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.layers = _get_clones(encoder_layer, num_layers)</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_reference_points</span><span class="params">(spatial_shapes, valid_ratios, device)</span>:</span></span><br><span class="line">        reference_points_list = []</span><br><span class="line">        <span class="keyword">for</span> lvl, (H_, W_) <span class="keyword">in</span> enumerate(spatial_shapes):</span><br><span class="line"></span><br><span class="line">            ref_y, ref_x = torch.meshgrid(torch.linspace(<span class="number">0.5</span>, H_ - <span class="number">0.5</span>, H_, dtype=torch.float32, device=device),</span><br><span class="line">                                          torch.linspace(<span class="number">0.5</span>, W_ - <span class="number">0.5</span>, W_, dtype=torch.float32, device=device))</span><br><span class="line">            ref_y = ref_y.reshape(<span class="number">-1</span>)[<span class="keyword">None</span>] / (valid_ratios[:, <span class="keyword">None</span>, lvl, <span class="number">1</span>] * H_)</span><br><span class="line">            ref_x = ref_x.reshape(<span class="number">-1</span>)[<span class="keyword">None</span>] / (valid_ratios[:, <span class="keyword">None</span>, lvl, <span class="number">0</span>] * W_)</span><br><span class="line">            ref = torch.stack((ref_x, ref_y), <span class="number">-1</span>)</span><br><span class="line">            reference_points_list.append(ref)</span><br><span class="line">        reference_points = torch.cat(reference_points_list, <span class="number">1</span>)</span><br><span class="line">        reference_points = reference_points[:, :, <span class="keyword">None</span>] * valid_ratios[:, <span class="keyword">None</span>]</span><br><span class="line">        <span class="keyword">return</span> reference_points</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, spatial_shapes, level_start_index, valid_ratios, pos=None, padding_mask=None)</span>:</span></span><br><span class="line">        output = src</span><br><span class="line">        <span class="comment"># 参考点初始化，以0.5为步长，在特征图上密集采样所有点作为初始参考点</span></span><br><span class="line">        reference_points = self.get_reference_points(spatial_shapes, valid_ratios, device=src.device)</span><br><span class="line">        <span class="keyword">for</span> _, layer <span class="keyword">in</span> enumerate(self.layers):</span><br><span class="line">            output = layer(output, pos, reference_points, spatial_shapes, level_start_index, padding_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>输出memory（编码后的特征表示），shape是 (bs, h_lvl1<em>w_lvl1+h_lvl2</em>w_lvl2+.., c=256)，其中h_lvli和w_lvli分别代表第i层特征图的高和宽，于是第二个维度就是所有特征点的数量。编码后，特征的最后一个维度hidden_dim=256.</p><h5 id="Decoder前处理"><a href="#Decoder前处理" class="headerlink" title="Decoder前处理"></a>Decoder前处理</h5><p>对encoder的输出进行处理，得到参考点reference_points，需要说明下，在2-stage模式下，参考点和输入到Decoder的object query及query embedding的生成方式和形式会有所不同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeformableTransformer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">   ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gen_encoder_output_proposals</span><span class="params">(self, memory, memory_padding_mask, spatial_shapes)</span>:</span></span><br><span class="line">        N_, S_, C_ = memory.shape</span><br><span class="line">        base_scale = <span class="number">4.0</span></span><br><span class="line">        proposals = []</span><br><span class="line">        _cur = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> lvl, (H_, W_) <span class="keyword">in</span> enumerate(spatial_shapes):</span><br><span class="line">            mask_flatten_ = memory_padding_mask[:, _cur:(_cur + H_ * W_)].view(N_, H_, W_, <span class="number">1</span>)</span><br><span class="line">            valid_H = torch.sum(~mask_flatten_[:, :, <span class="number">0</span>, <span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line">            valid_W = torch.sum(~mask_flatten_[:, <span class="number">0</span>, :, <span class="number">0</span>], <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">            grid_y, grid_x = torch.meshgrid(torch.linspace(<span class="number">0</span>, H_ - <span class="number">1</span>, H_, dtype=torch.float32, device=memory.device),</span><br><span class="line">                                            torch.linspace(<span class="number">0</span>, W_ - <span class="number">1</span>, W_, dtype=torch.float32, device=memory.device))</span><br><span class="line">            grid = torch.cat([grid_x.unsqueeze(<span class="number">-1</span>), grid_y.unsqueeze(<span class="number">-1</span>)], <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">            scale = torch.cat([valid_W.unsqueeze(<span class="number">-1</span>), valid_H.unsqueeze(<span class="number">-1</span>)], <span class="number">1</span>).view(N_, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            grid = (grid.unsqueeze(<span class="number">0</span>).expand(N_, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>) + <span class="number">0.5</span>) / scale</span><br><span class="line">            wh = torch.ones_like(grid) * <span class="number">0.05</span> * (<span class="number">2.0</span> ** lvl)</span><br><span class="line">            proposal = torch.cat((grid, wh), <span class="number">-1</span>).view(N_, <span class="number">-1</span>, <span class="number">4</span>)</span><br><span class="line">            proposals.append(proposal)</span><br><span class="line">            _cur += (H_ * W_)</span><br><span class="line">        output_proposals = torch.cat(proposals, <span class="number">1</span>)</span><br><span class="line">        output_proposals_valid = ((output_proposals &gt; <span class="number">0.01</span>) &amp; (output_proposals &lt; <span class="number">0.99</span>)).all(<span class="number">-1</span>, keepdim=<span class="keyword">True</span>)</span><br><span class="line">        output_proposals = torch.log(output_proposals / (<span class="number">1</span> - output_proposals))</span><br><span class="line">        output_proposals = output_proposals.masked_fill(memory_padding_mask.unsqueeze(<span class="number">-1</span>), float(<span class="string">'inf'</span>))</span><br><span class="line">        output_proposals = output_proposals.masked_fill(~output_proposals_valid, float(<span class="string">'inf'</span>))</span><br><span class="line"></span><br><span class="line">        output_memory = memory</span><br><span class="line">        output_memory = output_memory.masked_fill(memory_padding_mask.unsqueeze(<span class="number">-1</span>), float(<span class="number">0</span>))</span><br><span class="line">        output_memory = output_memory.masked_fill(~output_proposals_valid, float(<span class="number">0</span>))</span><br><span class="line">        output_memory = self.enc_output_norm(self.enc_output(output_memory))</span><br><span class="line">        <span class="keyword">return</span> output_memory, output_proposals</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, srcs, masks, pos_embeds, query_embed=None)</span>:</span></span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># encoder</span></span><br><span class="line">        memory = self.encoder(src_flatten, spatial_shapes, level_start_index, valid_ratios, lvl_pos_embed_flatten, mask_flatten)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># prepare input for decoder</span></span><br><span class="line">        bs, _, c = memory.shape</span><br><span class="line">        <span class="keyword">if</span> self.two_stage:</span><br><span class="line">            <span class="comment"># 生成proposal，对encoder的输出进行处理（全连接层+归一化），output_proposals对应于特征图上各个初始参考点位置(固定的)</span></span><br><span class="line">            output_memory, output_proposals = self.gen_encoder_output_proposals(memory, mask_flatten, spatial_shapes)</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">            <span class="comment"># hack implementation for two-stage Deformable DETR</span></span><br><span class="line">            <span class="comment"># 借助于decoder的最后一层的class_embed和bbox_embed获取分数和proposal</span></span><br><span class="line">            enc_outputs_class = self.decoder.class_embed[self.decoder.num_layers](output_memory)</span><br><span class="line">            <span class="comment"># bbox_embed预测的是相对于初始参考点位置的偏移量，所以需要加上初始参考点位置</span></span><br><span class="line">            enc_outputs_coord_unact = self.decoder.bbox_embed[self.decoder.num_layers](output_memory) + output_proposals</span><br><span class="line"></span><br><span class="line">            topk = self.two_stage_num_proposals</span><br><span class="line">            <span class="comment"># 选取分数topk的proposal</span></span><br><span class="line">            topk_proposals = torch.topk(enc_outputs_class[..., <span class="number">0</span>], topk, dim=<span class="number">1</span>)[<span class="number">1</span>]</span><br><span class="line">            topk_coords_unact = torch.gather(enc_outputs_coord_unact, <span class="number">1</span>, topk_proposals.unsqueeze(<span class="number">-1</span>).repeat(<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">            topk_coords_unact = topk_coords_unact.detach()</span><br><span class="line">            reference_points = topk_coords_unact.sigmoid()</span><br><span class="line">            init_reference_out = reference_points</span><br><span class="line">            pos_trans_out = self.pos_trans_norm(self.pos_trans(self.get_proposal_pos_embed(topk_coords_unact)))</span><br><span class="line">            query_embed, tgt = torch.split(pos_trans_out, c, dim=<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            query_embed, tgt = torch.split(query_embed, c, dim=<span class="number">1</span>)</span><br><span class="line">            query_embed = query_embed.unsqueeze(<span class="number">0</span>).expand(bs, <span class="number">-1</span>, <span class="number">-1</span>)</span><br><span class="line">            tgt = tgt.unsqueeze(<span class="number">0</span>).expand(bs, <span class="number">-1</span>, <span class="number">-1</span>)</span><br><span class="line">            reference_points = self.reference_points(query_embed).sigmoid()</span><br><span class="line">            init_reference_out = reference_points</span><br><span class="line"></span><br><span class="line">        <span class="comment"># decoder</span></span><br><span class="line">        hs, inter_references = self.decoder(tgt, reference_points, memory,</span><br><span class="line">                                            spatial_shapes, level_start_index, valid_ratios, query_embed, mask_flatten)</span><br><span class="line"></span><br><span class="line">        inter_references_out = inter_references</span><br><span class="line">        <span class="keyword">if</span> self.two_stage:</span><br><span class="line">            <span class="keyword">return</span> hs, init_reference_out, inter_references_out, enc_outputs_class, enc_outputs_coord_unact</span><br><span class="line">        <span class="keyword">return</span> hs, init_reference_out, inter_references_out, <span class="keyword">None</span>, <span class="keyword">None</span></span><br></pre></td></tr></table></figure><blockquote><ul><li>如果是two-stage 模式下，参考点由Encoder预测topk得分最高的proposal box（这时的参考点是4d，bbox形式），然后对参考点进行position embedding来生成Decoder需要的object query和对应的query embedding；</li><li>非two-stage模式下，Decoder的 object query(target )和 query embedding 就是预设的embedding，然后将query embedding经过全连接层输出2d参考点，这时的参考点是归一化的中心坐标形式。</li></ul></blockquote><p>另外，两种情况下生成的参考点数量可能不同：2-stage时是有top-k（作者设置为300）个，而1-stage时是<em>num_queries</em>(作者也设置为300)个，也就是和object query的数量一致(可以理解为，此时参考点就是object query本身的位置)。</p><h5 id="Decoder解码"><a href="#Decoder解码" class="headerlink" title="Decoder解码"></a>Decoder解码</h5><p>这里与Transformer中主要的区别在于使用可变形注意力替代了原生的交叉注意力。类似地，每层的解码过程是self-attention+cross-attention+ffn，下一层输入的object query是上一层输出的解码特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DeformableTransformerDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, tgt, reference_points, src, src_spatial_shapes, src_level_start_index, src_valid_ratios,</span></span></span><br><span class="line"><span class="function"><span class="params">                query_pos=None, src_padding_mask=None)</span>:</span></span><br><span class="line">        output = tgt</span><br><span class="line"></span><br><span class="line">        intermediate = []</span><br><span class="line">        intermediate_reference_points = []</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> lid, layer <span class="keyword">in</span> enumerate(self.layers):</span><br><span class="line">            <span class="keyword">if</span> reference_points.shape[<span class="number">-1</span>] == <span class="number">4</span>:</span><br><span class="line">                reference_points_input = reference_points[:, :, <span class="keyword">None</span>] \</span><br><span class="line">                                         * torch.cat([src_valid_ratios, src_valid_ratios], <span class="number">-1</span>)[:, <span class="keyword">None</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">assert</span> reference_points.shape[<span class="number">-1</span>] == <span class="number">2</span></span><br><span class="line">                reference_points_input = reference_points[:, :, <span class="keyword">None</span>] * src_valid_ratios[:, <span class="keyword">None</span>]</span><br><span class="line">            output = layer(output, query_pos, reference_points_input, src, src_spatial_shapes, src_level_start_index, src_padding_mask)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># hack implementation for iterative bounding box refinement</span></span><br><span class="line">            <span class="comment"># 使用iterative bbox refinement</span></span><br><span class="line">            <span class="keyword">if</span> self.bbox_embed <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="comment"># 得到相对初始参考点的偏移量</span></span><br><span class="line">                tmp = self.bbox_embed[lid](output)</span><br><span class="line">                <span class="comment"># 得到归一化坐标点</span></span><br><span class="line">                <span class="keyword">if</span> reference_points.shape[<span class="number">-1</span>] == <span class="number">4</span>:</span><br><span class="line">                    new_reference_points = tmp + inverse_sigmoid(reference_points)</span><br><span class="line">                    new_reference_points = new_reference_points.sigmoid()</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">assert</span> reference_points.shape[<span class="number">-1</span>] == <span class="number">2</span></span><br><span class="line">                    new_reference_points = tmp</span><br><span class="line">                    new_reference_points[..., :<span class="number">2</span>] = tmp[..., :<span class="number">2</span>] + inverse_sigmoid(reference_points)</span><br><span class="line">                    new_reference_points = new_reference_points.sigmoid()</span><br><span class="line">                <span class="comment"># 在输入下一层之前取消梯度，将当前层的预测bbox作为下一层的初始参考点</span></span><br><span class="line">                reference_points = new_reference_points.detach()</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.return_intermediate:</span><br><span class="line">                intermediate.append(output)</span><br><span class="line">                intermediate_reference_points.append(reference_points)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.return_intermediate:</span><br><span class="line">            <span class="keyword">return</span> torch.stack(intermediate), torch.stack(intermediate_reference_points)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output, reference_points</span><br></pre></td></tr></table></figure><p>进行Decoder解码完之后就是接class_embed和bbox_embed得到最后box分数和坐标。在上面需要注意的一点是，<strong>每次refine后的bbox梯度是不会传递到下一层</strong>。</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><ol><li>相比于DETR，使用了多尺度特征+scale-level embedding，用于区分不同特征层;</li><li>使用了多尺度可变形注意力替代Encoder中的selfattn和Decoder中的crossattn，减小计算量；</li><li>引入了参考点，类似引入先验知识；</li><li>设计了两阶段模式和iteraitive box refinement策略；</li><li>检测头回归分支预测是bbox相对参考点的偏移量而非绝对坐标值；</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DETR是第一个end2end的目标检测器，不需要众多手工设计组件（anchor，iou匹配，nms后处理等），但也存在收敛慢，能处理的特征分辨率有限等缺陷。原因大概存在如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;transformer在初始化时，分配给所有特征像素的注意力权重几乎均等；这就造成了模型需要长时间去学习关注真正有意义的位置，这些位置应该是稀疏的；&lt;/li&gt;
&lt;li&gt;transformer在计算注意力权重时，伴随着高计算量与空间复杂度。特别是在编码器部分，与特征像素点的数量成平方级关系，因此难以处理高分辨率的特征;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="目标检测" scheme="https://blog.nicehuster.cn/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>DETR源码解读</title>
    <link href="https://blog.nicehuster.cn/2022/07/24/DETR%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/"/>
    <id>https://blog.nicehuster.cn/2022/07/24/DETR代码解读/</id>
    <published>2022-07-24T11:13:39.000Z</published>
    <updated>2022-08-26T02:16:49.016Z</updated>
    
    <content type="html"><![CDATA[<p>transformer由encoder和decoder俩部分组成。</p><a id="more"></a><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p>一个encoder由多个encoder_layer组成，在detr中默认是6层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder_layer, num_layers, norm=None)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.layers = _get_clones(encoder_layer, num_layers) <span class="comment"># Encoder包含num层，每层具有相同结构encoder_layer</span></span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line"></span><br><span class="line">        self.norm = norm <span class="comment"># 归一化</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src,</span></span></span><br><span class="line"><span class="function"><span class="params">                mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                src_key_padding_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                pos: Optional[Tensor] = None)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># src 对应backbone最后一层输出的feature maps,并且维度已经映射到(h*w,bs, hidden_dim)</span></span><br><span class="line">        <span class="comment"># mask 一般为空</span></span><br><span class="line">        <span class="comment"># pos 对应backbone最后一层输出的feature maps对应的位置编码，shape是(h*w,bs,c)</span></span><br><span class="line">        <span class="comment"># src_key_padding_mask 对应backbone最后一层输出的feature maps对应的mask，shape是(bs,h*w)</span></span><br><span class="line"></span><br><span class="line">        output = src</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            output = layer(output, src_mask=mask,</span><br><span class="line">                           src_key_padding_mask=src_key_padding_mask, pos=pos)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            output = self.norm(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>EncoderLayer 的前向过程分为两种情况，一种是在输入多头自注意力层和前向反馈层前先进行归一化，另一种则是在这两个层输出后再进行归一化操作。对应实现可以参考如下图左侧部分：</p><p><img src="/img/detr-trans.png" alt="detr"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerEncoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, nhead, dim_feedforward=<span class="number">2048</span>, dropout=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 activation=<span class="string">"relu"</span>, normalize_before=False)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#多头自注意力模块</span></span><br><span class="line">        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)</span><br><span class="line">        <span class="comment"># Implementation of Feedforward model</span></span><br><span class="line">        self.linear1 = nn.Linear(d_model, dim_feedforward)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.linear2 = nn.Linear(dim_feedforward, d_model)</span><br><span class="line"></span><br><span class="line">        self.norm1 = nn.LayerNorm(d_model)</span><br><span class="line">        self.norm2 = nn.LayerNorm(d_model)</span><br><span class="line">        self.dropout1 = nn.Dropout(dropout)</span><br><span class="line">        self.dropout2 = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.activation = _get_activation_fn(activation)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 是否需要在输入多头自注意力层之前进行归一化</span></span><br><span class="line">        self.normalize_before = normalize_before</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">with_pos_embed</span><span class="params">(self, tensor, pos: Optional[Tensor])</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tensor <span class="keyword">if</span> pos <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> tensor + pos</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_post</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                     src,</span></span></span><br><span class="line"><span class="function"><span class="params">                     src_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                     src_key_padding_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                     pos: Optional[Tensor] = None)</span>:</span></span><br><span class="line">        q = k = self.with_pos_embed(src, pos)</span><br><span class="line">        src2 = self.self_attn(q, k, value=src, attn_mask=src_mask,</span><br><span class="line">                              key_padding_mask=src_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">        src = src + self.dropout1(src2)</span><br><span class="line">        src = self.norm1(src)</span><br><span class="line">        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))</span><br><span class="line">        src = src + self.dropout2(src2)</span><br><span class="line">        src = self.norm2(src)</span><br><span class="line">        <span class="keyword">return</span> src</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_pre</span><span class="params">(self, src,</span></span></span><br><span class="line"><span class="function"><span class="params">                    src_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    src_key_padding_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    pos: Optional[Tensor] = None)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 输入多头自注意力层前进行归一化</span></span><br><span class="line">        src2 = self.norm1(src)</span><br><span class="line">        <span class="comment"># q,k在输入attn之前需要结合位置编码</span></span><br><span class="line">        q = k = self.with_pos_embed(src2, pos)</span><br><span class="line">        <span class="comment"># self.self_attn是nn.MultiheadAttention的实例，其前向过程返回两部分，第一个是自注意力层的输出，第二个是自注意力权重，因此这里取了输出索引为0的部分即代表自注意力层的输出。</span></span><br><span class="line">        src2 = self.self_attn(q, k, value=src2, attn_mask=src_mask,</span><br><span class="line">                              key_padding_mask=src_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">                              </span><br><span class="line">        src = src + self.dropout1(src2)</span><br><span class="line">        src2 = self.norm2(src)</span><br><span class="line">        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))</span><br><span class="line">        src = src + self.dropout2(src2)</span><br><span class="line">        <span class="keyword">return</span> src</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src,</span></span></span><br><span class="line"><span class="function"><span class="params">                src_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                src_key_padding_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                pos: Optional[Tensor] = None)</span>:</span></span><br><span class="line">                </span><br><span class="line">         <span class="comment"># 俩种不同的前向过程</span></span><br><span class="line">        <span class="keyword">if</span> self.normalize_before:</span><br><span class="line">            <span class="keyword">return</span> self.forward_pre(src, src_mask, src_key_padding_mask, pos)</span><br><span class="line">        <span class="keyword">return</span> self.forward_post(src, src_mask, src_key_padding_mask, pos)</span><br></pre></td></tr></table></figure><p>需要注意的是，在输入多头自注意力层时需要先进行位置嵌入，即结合位置编码。注意仅对query和key实施，而value不需要。query和key是在图像特征中各个位置之间计算相关性，而value作为原图像特征，使用计算出来的相关性加权上去，得到各位置结合了全局相关性（增强/削弱）后的特征表示。</p><h4 id="Query-Embedding"><a href="#Query-Embedding" class="headerlink" title="Query Embedding"></a>Query Embedding</h4><p>在解析Decoder前，有必要先简要地谈谈query embedding，因为它是Decoder的主要输入之一。query embedding 有点anchor的味道，而且是自学习的anchor，作者使用了<em>nn.Embedding</em>实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.query_embed = nn.Embedding(num_queries, hidden_dim)</span><br></pre></td></tr></table></figure><p>其中num_queries 代表图像中有多少个目标（位置），默认是100个，对这些目标（位置）全部进行嵌入，维度映射到 <em>hidden_dim</em>，将 <strong>query_embedding 的权重</strong>作为参数输入到Transformer的前向过程，使用时与position encoding的方式相同：直接相加。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[<span class="number">-1</span>])[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>而这个query embedding应该加在哪呢？当然是我们需要预测的目标（query object）咯！可是网络一开始还没有输出，我们都不知道预测目标在哪里呀，如何将它实体化？作者也不知道，于是就简单粗暴地直接将它初始化为全0，shape和query embedding 的权重一致（从而可以element-wise add）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model=<span class="number">512</span>, nhead=<span class="number">8</span>, num_encoder_layers=<span class="number">6</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_decoder_layers=<span class="number">6</span>, dim_feedforward=<span class="number">2048</span>, dropout=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 activation=<span class="string">"relu"</span>, normalize_before=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 return_intermediate_dec=False)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, mask, query_embed, pos_embed)</span>:</span></span><br><span class="line">        </span><br><span class="line">...</span><br><span class="line">        <span class="comment"># (num_queries,bs,hidden_dim)</span></span><br><span class="line">        tgt = torch.zeros_like(query_embed) <span class="comment">#</span></span><br><span class="line">        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)</span><br><span class="line">        hs = self.decoder(tgt, memory, memory_key_padding_mask=mask,</span><br><span class="line">                          pos=pos_embed, query_pos=query_embed)</span><br><span class="line">        <span class="keyword">return</span> hs.transpose(<span class="number">1</span>, <span class="number">2</span>), memory.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).view(bs, c, h, w)</span><br></pre></td></tr></table></figure><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p>Decoder的结构和Encoder十分类似。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, decoder_layer, num_layers, norm=None, return_intermediate=False)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.layers = _get_clones(decoder_layer, num_layers)</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.norm = norm</span><br><span class="line">        self.return_intermediate = return_intermediate</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, tgt, memory,</span></span></span><br><span class="line"><span class="function"><span class="params">                tgt_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                memory_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                tgt_key_padding_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                memory_key_padding_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                pos: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                query_pos: Optional[Tensor] = None)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># tgt 是query embedding，shape是(num_queries,bs,hidden_dim)</span></span><br><span class="line">        <span class="comment"># query_pos 是对应tgt的位置编码，shape和tgt一致</span></span><br><span class="line">        <span class="comment"># memory是encoder的输出，shape是(h*w,bs,hidden_dim)</span></span><br><span class="line">        <span class="comment"># memory_key_padding_mask是对应encoder的src_key_padding_mask，shape是(bs,h*w)</span></span><br><span class="line">        <span class="comment"># pos 对应输入到encoder的位置编码，这里代表memory的位置编码，shape和memory一致</span></span><br><span class="line">        </span><br><span class="line">        output = tgt</span><br><span class="line"></span><br><span class="line">        intermediate = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            output = layer(output, memory, tgt_mask=tgt_mask,</span><br><span class="line">                           memory_mask=memory_mask,</span><br><span class="line">                           tgt_key_padding_mask=tgt_key_padding_mask,</span><br><span class="line">                           memory_key_padding_mask=memory_key_padding_mask,</span><br><span class="line">                           pos=pos, query_pos=query_pos)</span><br><span class="line">            <span class="keyword">if</span> self.return_intermediate:</span><br><span class="line">                intermediate.append(self.norm(output))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            output = self.norm(output)</span><br><span class="line">            <span class="keyword">if</span> self.return_intermediate:</span><br><span class="line">                intermediate.pop()</span><br><span class="line">                intermediate.append(output)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.return_intermediate:</span><br><span class="line">            <span class="keyword">return</span> torch.stack(intermediate)</span><br><span class="line">        <span class="keyword">return</span> output.unsqueeze(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>注意，在detr中，tgt_mask和memory_mask并未使用。需要注意的是intermediate中记录的是每层输出后的归一化结果，而每一层的输入是前一层输出（没有归一化）的结果。</p><p>DecoderLayer与Encoder的实现类似，只不过多了一层cross attention，其实质也是多头自注意力层，但是key和value来自于Encoder的输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerDecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, nhead, dim_feedforward=<span class="number">2048</span>, dropout=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 activation=<span class="string">"relu"</span>, normalize_before=False)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)</span><br><span class="line">        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)</span><br><span class="line">        <span class="comment"># Implementation of Feedforward model</span></span><br><span class="line">        self.linear1 = nn.Linear(d_model, dim_feedforward)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.linear2 = nn.Linear(dim_feedforward, d_model)</span><br><span class="line"></span><br><span class="line">        self.norm1 = nn.LayerNorm(d_model)</span><br><span class="line">        self.norm2 = nn.LayerNorm(d_model)</span><br><span class="line">        self.norm3 = nn.LayerNorm(d_model)</span><br><span class="line">        self.dropout1 = nn.Dropout(dropout)</span><br><span class="line">        self.dropout2 = nn.Dropout(dropout)</span><br><span class="line">        self.dropout3 = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.activation = _get_activation_fn(activation)</span><br><span class="line">        self.normalize_before = normalize_before</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">with_pos_embed</span><span class="params">(self, tensor, pos: Optional[Tensor])</span>:</span></span><br><span class="line">        <span class="keyword">return</span> tensor <span class="keyword">if</span> pos <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> tensor + pos</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_post</span><span class="params">(self, tgt, memory,</span></span></span><br><span class="line"><span class="function"><span class="params">                     tgt_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                     memory_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                     tgt_key_padding_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                     memory_key_padding_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                     pos: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                     query_pos: Optional[Tensor] = None)</span>:</span></span><br><span class="line">        q = k = self.with_pos_embed(tgt, query_pos)</span><br><span class="line">        tgt2 = self.self_attn(q, k, value=tgt, attn_mask=tgt_mask,</span><br><span class="line">                              key_padding_mask=tgt_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">        tgt = tgt + self.dropout1(tgt2)</span><br><span class="line">        tgt = self.norm1(tgt)</span><br><span class="line">        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt, query_pos),</span><br><span class="line">                                   key=self.with_pos_embed(memory, pos),</span><br><span class="line">                                   value=memory, attn_mask=memory_mask,</span><br><span class="line">                                   key_padding_mask=memory_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">        tgt = tgt + self.dropout2(tgt2)</span><br><span class="line">        tgt = self.norm2(tgt)</span><br><span class="line">        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))</span><br><span class="line">        tgt = tgt + self.dropout3(tgt2)</span><br><span class="line">        tgt = self.norm3(tgt)</span><br><span class="line">        <span class="keyword">return</span> tgt</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_pre</span><span class="params">(self, tgt, memory,</span></span></span><br><span class="line"><span class="function"><span class="params">                    tgt_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    memory_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    tgt_key_padding_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    memory_key_padding_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    pos: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    query_pos: Optional[Tensor] = None)</span>:</span></span><br><span class="line">        </span><br><span class="line">        tgt2 = self.norm1(tgt)</span><br><span class="line">        <span class="comment"># 进行位置嵌入</span></span><br><span class="line">        q = k = self.with_pos_embed(tgt2, query_pos)</span><br><span class="line">        <span class="comment"># 多头自注意力层，输入不包含encoder的输出</span></span><br><span class="line">        tgt2 = self.self_attn(q, k, value=tgt2, attn_mask=tgt_mask,</span><br><span class="line">                              key_padding_mask=tgt_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">        tgt = tgt + self.dropout1(tgt2)</span><br><span class="line">        tgt2 = self.norm2(tgt)</span><br><span class="line">        <span class="comment"># cross attention，key,value来自encoder，query来自上一层输出</span></span><br><span class="line">        <span class="comment"># key,query均需进行位置嵌入</span></span><br><span class="line">        tgt2 = self.multihead_attn(query=self.with_pos_embed(tgt2, query_pos),</span><br><span class="line">                                   key=self.with_pos_embed(memory, pos),</span><br><span class="line">                                   value=memory, attn_mask=memory_mask,</span><br><span class="line">                                   key_padding_mask=memory_key_padding_mask)[<span class="number">0</span>]</span><br><span class="line">        tgt = tgt + self.dropout2(tgt2)</span><br><span class="line">        tgt2 = self.norm3(tgt)</span><br><span class="line">        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))</span><br><span class="line">        tgt = tgt + self.dropout3(tgt2)</span><br><span class="line">        <span class="keyword">return</span> tgt</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, tgt, memory,</span></span></span><br><span class="line"><span class="function"><span class="params">                tgt_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                memory_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                tgt_key_padding_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                memory_key_padding_mask: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                pos: Optional[Tensor] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                query_pos: Optional[Tensor] = None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.normalize_before:</span><br><span class="line">            <span class="keyword">return</span> self.forward_pre(tgt, memory, tgt_mask, memory_mask,</span><br><span class="line">                                    tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)</span><br><span class="line">        <span class="keyword">return</span> self.forward_post(tgt, memory, tgt_mask, memory_mask,</span><br><span class="line">                                 tgt_key_padding_mask, memory_key_padding_mask, pos, query_pos)</span><br></pre></td></tr></table></figure><p>注意，在tgt在输入到self_attn之前，需要经过position embedding，tgt+query_pos。在第二个多头注意力模块multihead_attn上，key和value均来自Encoder的输出。同样地，query和key要进行位置嵌入（而value不用）。这里cross attention计算的相关性是目标物体与图像特征各位置的相关性，然后再把这个相关性系数加权到Encoder编码后的图像特征（value）上，相当于获得了object features的意思，更好地表征了图像中的各个物体。从上面encoder和decoder的实现可以看出，作者非常强调位置嵌入的作用，每次进行attention计算前都需要进行position embedding，究其原因是因为transformer的转置不变性，即对排列和位置是不care的，然而在detection任务中却是十分重要的。</p><h4 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h4><p>将Encoder和Decoder封装在一起构成Transformer。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model=<span class="number">512</span>, nhead=<span class="number">8</span>, num_encoder_layers=<span class="number">6</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 num_decoder_layers=<span class="number">6</span>, dim_feedforward=<span class="number">2048</span>, dropout=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 activation=<span class="string">"relu"</span>, normalize_before=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 return_intermediate_dec=False)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line"><span class="comment"># 构建encoder layer</span></span><br><span class="line">        encoder_layer = TransformerEncoderLayer(d_model, nhead, dim_feedforward,</span><br><span class="line">                                                dropout, activation, normalize_before)</span><br><span class="line">        encoder_norm = nn.LayerNorm(d_model) <span class="keyword">if</span> normalize_before <span class="keyword">else</span> <span class="keyword">None</span></span><br><span class="line">        self.encoder = TransformerEncoder(encoder_layer, num_encoder_layers, encoder_norm)</span><br><span class="line"><span class="comment">#构建decoder layer</span></span><br><span class="line">        decoder_layer = TransformerDecoderLayer(d_model, nhead, dim_feedforward,</span><br><span class="line">                                                dropout, activation, normalize_before)</span><br><span class="line">        decoder_norm = nn.LayerNorm(d_model)</span><br><span class="line">        self.decoder = TransformerDecoder(decoder_layer, num_decoder_layers, decoder_norm,</span><br><span class="line">                                          return_intermediate=return_intermediate_dec)</span><br><span class="line"></span><br><span class="line">        self._reset_parameters()</span><br><span class="line"></span><br><span class="line">        self.d_model = d_model <span class="comment"># 输入的embedding的特征维度</span></span><br><span class="line">        self.nhead = nhead <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_reset_parameters</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.parameters():</span><br><span class="line">            <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">                nn.init.xavier_uniform_(p)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, mask, query_embed, pos_embed)</span>:</span></span><br><span class="line">        <span class="comment"># flatten NxCxHxW to HWxNxC</span></span><br><span class="line">        bs, c, h, w = src.shape</span><br><span class="line">        <span class="comment"># 将backbone输入的feature maps进行flatten成序列，</span></span><br><span class="line">        <span class="comment"># src: (h*w,bs,c)</span></span><br><span class="line">        src = src.flatten(<span class="number">2</span>).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># pos: (h*w,bs,hidden_dim)</span></span><br><span class="line">        pos_embed = pos_embed.flatten(<span class="number">2</span>).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># query_embed: (num_queries, bs, hidden_dim)</span></span><br><span class="line">        query_embed = query_embed.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, bs, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># mask: (bs, h*w)</span></span><br><span class="line">        mask = mask.flatten(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        tgt = torch.zeros_like(query_embed) <span class="comment"># 每次forward时，tgt都会初始化为0</span></span><br><span class="line">        <span class="comment"># memory: (h*w, bs, c)</span></span><br><span class="line">        memory = self.encoder(src, src_key_padding_mask=mask, pos=pos_embed)</span><br><span class="line">        <span class="comment"># TransformerDecoderLayer中return_intermediate设置为true，因此decoder包含了每层的输出结果，因此hs的shape是（6, num_queries,bs,hidden_dim）</span></span><br><span class="line">        hs = self.decoder(tgt, memory, memory_key_padding_mask=mask,</span><br><span class="line">                          pos=pos_embed, query_pos=query_embed)</span><br><span class="line">        <span class="keyword">return</span> hs.transpose(<span class="number">1</span>, <span class="number">2</span>), memory.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).view(bs, c, h, w)</span><br></pre></td></tr></table></figure><p>注意，tgt是与query embedding形状一直且设置为全0的结果，意为初始化需要预测的目标。因为一开始并不清楚这些目标，所以初始化为全0。其会在Decoder的各层不断被refine，相当于一个coarse-to-fine的过程，但是真正要学习的是query embedding，学习到的是整个数据集中目标物体的统计特征，而tgt在每次迭代训练（一个batch数据刚到来）时会被重新初始化为0。</p><h4 id="DETR"><a href="#DETR" class="headerlink" title="DETR"></a>DETR</h4><p>DETR包含backbone，encoder, decoder, prediction heads四个部分。encoder和decoder通常会用一个transformer来实现。prediction heads部分包括分类和回归。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DETR</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" This is the DETR module that performs object detection """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, backbone, transformer, num_classes, num_queries, aux_loss=False)</span>:</span></span><br><span class="line">        <span class="string">""" Initializes the model.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            backbone: torch module of the backbone to be used. See backbone.py</span></span><br><span class="line"><span class="string">            transformer: torch module of the transformer architecture. See transformer.py</span></span><br><span class="line"><span class="string">            num_classes: number of object classes</span></span><br><span class="line"><span class="string">            num_queries: number of object queries, ie detection slot. This is the maximal number of objects</span></span><br><span class="line"><span class="string">                         DETR can detect in a single image. For COCO, we recommend 100 queries.</span></span><br><span class="line"><span class="string">            aux_loss: True if auxiliary decoding losses (loss at each decoder layer) are to be used.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.num_queries = num_queries</span><br><span class="line">        self.transformer = transformer</span><br><span class="line">        hidden_dim = transformer.d_model</span><br><span class="line">        <span class="comment"># class分类</span></span><br><span class="line">        self.class_embed = nn.Linear(hidden_dim, num_classes + <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># box回归，包含3层nn.linear()，最后一层维度映射为4，代表bbox的中心点横、纵坐标和宽、高。</span></span><br><span class="line">        self.bbox_embed = MLP(hidden_dim, hidden_dim, <span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># query_embed用于在Transformer中对初始化query以及对其编码生成嵌入</span></span><br><span class="line">        self.query_embed = nn.Embedding(num_queries, hidden_dim)</span><br><span class="line"><span class="comment"># input_proj是将CNN提取的特征维度映射到Transformer隐层的维度；</span></span><br><span class="line">        self.input_proj = nn.Conv2d(backbone.num_channels, hidden_dim, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.backbone = backbone</span><br><span class="line">        self.aux_loss = aux_loss</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, samples: NestedTensor)</span>:</span></span><br><span class="line">        <span class="comment"># 将sample转换成nestedTensor类型</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(samples, (list, torch.Tensor)):</span><br><span class="line">            samples = nested_tensor_from_tensor_list(samples)</span><br><span class="line">        <span class="comment"># 输入cnn提取特征，并输出pos encoding  </span></span><br><span class="line">        features, pos = self.backbone(samples)</span><br><span class="line"><span class="comment"># 取出最后一层特征及对应mask</span></span><br><span class="line">        src, mask = features[<span class="number">-1</span>].decompose()</span><br><span class="line">        <span class="keyword">assert</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span></span><br><span class="line">        hs = self.transformer(self.input_proj(src), mask, self.query_embed.weight, pos[<span class="number">-1</span>])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成分类与回归的预测结果</span></span><br><span class="line">        outputs_class = self.class_embed(hs)</span><br><span class="line">        outputs_coord = self.bbox_embed(hs).sigmoid()</span><br><span class="line">        <span class="comment"># 由于hs包含transformer中decoder每层输出，因此索引-1表示取最后一层输出</span></span><br><span class="line">        out = &#123;<span class="string">'pred_logits'</span>: outputs_class[<span class="number">-1</span>], <span class="string">'pred_boxes'</span>: outputs_coord[<span class="number">-1</span>]&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.aux_loss:</span><br><span class="line">            out[<span class="string">'aux_outputs'</span>] = self._set_aux_loss(outputs_class, outputs_coord)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h4 id="Postprocess"><a href="#Postprocess" class="headerlink" title="Postprocess"></a>Postprocess</h4><p>一部分DETR的输出并不是最终预测结果的形式，还需要进行简单的后处理。但是这里的后处理并不是NMS哦！DETR预测的是集合，并且在训练过程中经过匈牙利算法与GT一对一匹配学习，因此不存在重复框的情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PostProcess</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" This module converts the model's output into the format expected by the coco api"""</span></span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, outputs, target_sizes)</span>:</span></span><br><span class="line">      </span><br><span class="line">        out_logits, out_bbox = outputs[<span class="string">'pred_logits'</span>], outputs[<span class="string">'pred_boxes'</span>]</span><br><span class="line">        <span class="keyword">assert</span> len(out_logits) == len(target_sizes)</span><br><span class="line">        <span class="keyword">assert</span> target_sizes.shape[<span class="number">1</span>] == <span class="number">2</span></span><br><span class="line"><span class="comment"># out_logits : (bs, num_queries,num_classes)</span></span><br><span class="line">        prob = F.softmax(out_logits, <span class="number">-1</span>)</span><br><span class="line">        scores, labels = prob[..., :<span class="number">-1</span>].max(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># convert to [x0, y0, x1, y1] format</span></span><br><span class="line">        boxes = box_ops.box_cxcywh_to_xyxy(out_bbox)</span><br><span class="line">        <span class="comment"># and from relative [0, 1] to absolute [0, height] coordinates</span></span><br><span class="line">        img_h, img_w = target_sizes.unbind(<span class="number">1</span>)</span><br><span class="line">        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=<span class="number">1</span>)</span><br><span class="line">        boxes = boxes * scale_fct[:, <span class="keyword">None</span>, :]</span><br><span class="line"></span><br><span class="line">        results = [&#123;<span class="string">'scores'</span>: s, <span class="string">'labels'</span>: l, <span class="string">'boxes'</span>: b&#125; <span class="keyword">for</span> s, l, b <span class="keyword">in</span> zip(scores, labels, boxes)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure><h4 id="Loss-Fuction"><a href="#Loss-Fuction" class="headerlink" title="Loss Fuction"></a>Loss Fuction</h4><p>这一部分主要介绍一下和损失函数相关的部分源码。先看一下与损失函数相关的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">matcher = build_matcher(args)</span><br><span class="line">weight_dict = &#123;<span class="string">'loss_ce'</span>: <span class="number">1</span>, <span class="string">'loss_bbox'</span>: args.bbox_loss_coef&#125;</span><br><span class="line">weight_dict[<span class="string">'loss_giou'</span>] = args.giou_loss_coef</span><br><span class="line"><span class="keyword">if</span> args.masks:</span><br><span class="line">    weight_dict[<span class="string">"loss_mask"</span>] = args.mask_loss_coef</span><br><span class="line">    weight_dict[<span class="string">"loss_dice"</span>] = args.dice_loss_coef</span><br><span class="line"><span class="comment"># TODO this is a hack</span></span><br><span class="line"><span class="keyword">if</span> args.aux_loss:</span><br><span class="line">    aux_weight_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(args.dec_layers - <span class="number">1</span>):</span><br><span class="line">        aux_weight_dict.update(&#123;k + <span class="string">f'_<span class="subst">&#123;i&#125;</span>'</span>: v <span class="keyword">for</span> k, v <span class="keyword">in</span> weight_dict.items()&#125;)</span><br><span class="line">    weight_dict.update(aux_weight_dict)</span><br><span class="line"></span><br><span class="line">losses = [<span class="string">'labels'</span>, <span class="string">'boxes'</span>, <span class="string">'cardinality'</span>]</span><br><span class="line"><span class="keyword">if</span> args.masks:</span><br><span class="line">    losses += [<span class="string">"masks"</span>]</span><br><span class="line">criterion = SetCriterion(num_classes, matcher=matcher, weight_dict=weight_dict,</span><br><span class="line">                         eos_coef=args.eos_coef, losses=losses)</span><br></pre></td></tr></table></figure><p>matcher是将预测结果与gt进行匹配的匈牙利算法，weight_dict是各部分loss设置的权重参数，包括分类与回归损失。分类使用的是CE loss，回归包括l1 loss和giou loss。如果包含分割任务，还有mask相关损失函数，另外如果设置了aux_loss，则代表计算decoder中间层预测结果对应的loss。 loss函数的实例化使用SetCriterion进行构建的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SetCriterion</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" This class computes the loss for DETR.</span></span><br><span class="line"><span class="string">    The process happens in two steps:</span></span><br><span class="line"><span class="string">        1) we compute hungarian assignment between ground truth boxes and the outputs of the model</span></span><br><span class="line"><span class="string">        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes, matcher, weight_dict, eos_coef, losses)</span>:</span></span><br><span class="line">        <span class="string">""" Create the criterion.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            num_classes: number of object categories, omitting the special no-object category</span></span><br><span class="line"><span class="string">            matcher: module able to compute a matching between targets and proposals</span></span><br><span class="line"><span class="string">            weight_dict: dict containing as key the names of the losses and as values their relative weight.</span></span><br><span class="line"><span class="string">            eos_coef: relative classification weight applied to the no-object category</span></span><br><span class="line"><span class="string">            losses: list of all the losses to be applied. See get_loss for list of available losses.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.matcher = matcher</span><br><span class="line">        self.weight_dict = weight_dict</span><br><span class="line">        <span class="comment"># 针对背景分类的loss权重</span></span><br><span class="line">        self.eos_coef = eos_coef</span><br><span class="line">        self.losses = losses</span><br><span class="line">        empty_weight = torch.ones(self.num_classes + <span class="number">1</span>)</span><br><span class="line">        empty_weight[<span class="number">-1</span>] = self.eos_coef</span><br><span class="line">        self.register_buffer(<span class="string">'empty_weight'</span>, empty_weight)</span><br><span class="line"></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_loss</span><span class="params">(self, loss, outputs, targets, indices, num_boxes, **kwargs)</span>:</span></span><br><span class="line">        loss_map = &#123;</span><br><span class="line">            <span class="string">'labels'</span>: self.loss_labels,</span><br><span class="line">            <span class="string">'cardinality'</span>: self.loss_cardinality,</span><br><span class="line">            <span class="string">'boxes'</span>: self.loss_boxes,</span><br><span class="line">            <span class="string">'masks'</span>: self.loss_masks</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">assert</span> loss <span class="keyword">in</span> loss_map, <span class="string">f'do you really want to compute <span class="subst">&#123;loss&#125;</span> loss?'</span></span><br><span class="line">        <span class="keyword">return</span> loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, outputs, targets)</span>:</span></span><br><span class="line">        <span class="string">""" This performs the loss computation.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">             outputs: dict of tensors, see the output specification of the model for the format</span></span><br><span class="line"><span class="string">             targets: list of dicts, such that len(targets) == batch_size.</span></span><br><span class="line"><span class="string">                      The expected keys in each dict depends on the losses applied, see each loss' doc</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        outputs_without_aux = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> outputs.items() <span class="keyword">if</span> k != <span class="string">'aux_outputs'</span>&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Retrieve the matching between the outputs of the last layer and the targets</span></span><br><span class="line">        <span class="comment"># 将预测结果与GT进行匹配，indices是一个与bs长度相等的多元组的list</span></span><br><span class="line">        <span class="comment"># 每个元组为(ind_i,ind_j),前者是匹配的预测预测索引，后者是gt的索引</span></span><br><span class="line">        indices = self.matcher(outputs_without_aux, targets)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the average number of target boxes accross all nodes, for normalization purposes</span></span><br><span class="line">        num_boxes = sum(len(t[<span class="string">"labels"</span>]) <span class="keyword">for</span> t <span class="keyword">in</span> targets)</span><br><span class="line">        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)</span><br><span class="line">        <span class="keyword">if</span> is_dist_avail_and_initialized():</span><br><span class="line">            torch.distributed.all_reduce(num_boxes)</span><br><span class="line">        num_boxes = torch.clamp(num_boxes / get_world_size(), min=<span class="number">1</span>).item()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute all the requested losses</span></span><br><span class="line">        <span class="comment"># 计算所有相关的损失，其中self.losses = ['labels', 'boxes', 'cardinality']</span></span><br><span class="line">        losses = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> loss <span class="keyword">in</span> self.losses:</span><br><span class="line">            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> losses</span><br></pre></td></tr></table></figure><p>从forward函数可以看出，首先进行匈牙利匹配的是decoder最后一层的输出，之后再计算匹配后的损失函数包括losses = [‘labels’, ‘boxes’, ‘cardinality’]，具体计算部分可以看get_loss方法中映射的对应计算方法，其中包括self.loss_labels，self.loss_cardinality，self.loss_boxes。</p><h4 id="匈牙利匹配"><a href="#匈牙利匹配" class="headerlink" title="匈牙利匹配"></a>匈牙利匹配</h4><p>匈牙利算法，在这里用于预测集（prediction set）和GT的匹配，<strong>最终匹配方案是选取“loss总和”最小的分配方式。</strong>注意，这里计算的loss与损失函数中计算loss并不相同，在这里是用来作为代价cost，cost大小决定匹配程度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HungarianMatcher</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">   </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, cost_class: float = <span class="number">1</span>, cost_bbox: float = <span class="number">1</span>, cost_giou: float = <span class="number">1</span>)</span>:</span></span><br><span class="line">        <span class="string">"""Creates the matcher</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            cost_class: This is the relative weight of the classification error in the matching cost</span></span><br><span class="line"><span class="string">            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost</span></span><br><span class="line"><span class="string">            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.cost_class = cost_class</span><br><span class="line">        self.cost_bbox = cost_bbox</span><br><span class="line">        self.cost_giou = cost_giou</span><br><span class="line">        <span class="keyword">assert</span> cost_class != <span class="number">0</span> <span class="keyword">or</span> cost_bbox != <span class="number">0</span> <span class="keyword">or</span> cost_giou != <span class="number">0</span>, <span class="string">"all costs cant be 0"</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, outputs, targets)</span>:</span></span><br><span class="line">        bs, num_queries = outputs[<span class="string">"pred_logits"</span>].shape[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># We flatten to compute the cost matrices in a batch</span></span><br><span class="line">        out_prob = outputs[<span class="string">"pred_logits"</span>].flatten(<span class="number">0</span>, <span class="number">1</span>).softmax(<span class="number">-1</span>)  <span class="comment"># [batch_size * num_queries, num_classes]</span></span><br><span class="line">        out_bbox = outputs[<span class="string">"pred_boxes"</span>].flatten(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># [batch_size * num_queries, 4]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Also concat the target labels and boxes</span></span><br><span class="line">        tgt_ids = torch.cat([v[<span class="string">"labels"</span>] <span class="keyword">for</span> v <span class="keyword">in</span> targets])</span><br><span class="line">        tgt_bbox = torch.cat([v[<span class="string">"boxes"</span>] <span class="keyword">for</span> v <span class="keyword">in</span> targets])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the classification cost. Contrary to the loss, we don't use the NLL,</span></span><br><span class="line">        <span class="comment"># but approximate it in 1 - proba[target class].</span></span><br><span class="line">        <span class="comment"># The 1 is a constant that doesn't change the matching, it can be ommitted.</span></span><br><span class="line">        cost_class = -out_prob[:, tgt_ids]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the L1 cost between boxes</span></span><br><span class="line">        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the giou cost betwen boxes</span></span><br><span class="line">        cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Final cost matrix</span></span><br><span class="line">        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou</span><br><span class="line">        C = C.view(bs, num_queries, <span class="number">-1</span>).cpu()</span><br><span class="line"></span><br><span class="line">        sizes = [len(v[<span class="string">"boxes"</span>]) <span class="keyword">for</span> v <span class="keyword">in</span> targets]</span><br><span class="line">        indices = [linear_sum_assignment(c[i]) <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(C.split(sizes, <span class="number">-1</span>))]</span><br><span class="line">        <span class="keyword">return</span> [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) <span class="keyword">for</span> i, j <span class="keyword">in</span> indices]</span><br></pre></td></tr></table></figure><p>从上面可以看到，匈牙利匹配在前向计算过程中，是不需要梯度的。其中分类cost是直接采用1减去预测概率的形式，同时由于1是常数，于是作者甚至连1都省去了，在box上计算了l1和giou两种cost，之后对各部分进行加权求和得到总的cost。匹配方法使用的是 <em>scipy</em> 优化模块中的 <em>linear_sum_assignment()</em>，其输入是二分图的度量矩阵，该方法是计算这个二分图度量矩阵的最小权重分配方式，返回的是匹配方案对应的矩阵行索引和列索引。</p><h4 id="End"><a href="#End" class="headerlink" title="End"></a>End</h4><p>至此，DETR所有相关源码均已解读完毕。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;transformer由encoder和decoder俩部分组成。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="transformer" scheme="https://blog.nicehuster.cn/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>pix2seq方法详解</title>
    <link href="https://blog.nicehuster.cn/2022/07/18/pix2seq/"/>
    <id>https://blog.nicehuster.cn/2022/07/18/pix2seq/</id>
    <published>2022-07-18T11:13:39.000Z</published>
    <updated>2022-07-19T02:46:10.103Z</updated>
    
    <content type="html"><![CDATA[<p>本文分享seq2seq learning相关的两篇论文，单位是google brain，一作均为Ting Chen（自监督学习方法SimCLR的作者），论文地址：<a href="https://arxiv.org/pdf/2109.10852.pdf" target="_blank" rel="noopener">pix2seq: A Language Modeling Framework for Object Detection</a>,[ICLR2022接收]；<a href="https://arxiv.org/pdf/2206.07669.pdf" target="_blank" rel="noopener">A Unified Sequence Interface for Vision Tasks</a>,[上星期挂arxiv]，后者是对前者在多个视觉任务上的拓展。下面大致的介绍一下两篇论文的具体工作。</p><a id="more"></a><p><img src="/img/image-20220718111615345.png" alt="image-20220718111615345"></p><h3 id="Pix2seq"><a href="#Pix2seq" class="headerlink" title="Pix2seq"></a>Pix2seq</h3><h4 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h4><p>pix2seq将目标检测任务转换为语言建模来处理，以往目标检测任务基于属性进行预测，通常会分为分类预测+回归预测。而该方法是通过将object的类别和box信息构造成序列，对序列进行预测。pix2seq的结构和学习过程由四个部分组成，如下图所示：</p><p><img src="/img/image-20220718112047122.png" alt="image-20220718112047122"></p><p>Pix2seq主要包含四部分：</p><blockquote><ol><li>Image augmentation：图像数据增强，包括random scale+crops;</li><li>Sequence construction &amp; augmentation：构造序列和序列增强，将bbox和label转换为离散的token;</li><li>Architecture：使用encoder-decoder结构将输入图像pixel转换为序列；</li><li>Objective/loss function：常用的softmax 交叉熵损失；</li></ol></blockquote><h4 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h4><p><img src="/img/image-20220718112842096.png" alt="image-20220718112842096"></p><p>这里详细介绍一下box+cls序列化方法，为了和自然语言对齐，它把坐标框（4个值）和类别（1）都拼成一个序列，意味着100个目标对应着长度为500的序列。因为坐标是连续值，作者这里用了一个分桶的机制，把坐标分到n个桶里(bin)，就构成了离散值。具体地，一个目标被表示为一个由五个离散的[token]组成的序列，即[ymin, xmin, ymax, xmax, c]，其中每个连续的角坐标被均匀地离散为[1, nbins]之间的一个整数，c是类索引。我们对所有标记使用共享词汇表，因此词汇量大小等于 bin 数+类别数。对于600x600的图片而言，使用600个bin就可以实现零量化误差，其实整个离散值的范围比起nlp里的字典而言，还是非常非常小的。</p><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p>在构建好序列之后，使用Resnet + 6层transformer encoder + 6层transformer decoder对输入图像进行序列化，然后使用交叉熵计算损失。作者分别在train from scratch和finetune两种setting下进行了一些实验对比。</p><p><img src="/img/image-20220718211018875.png" alt="image-20220718211018875"></p><p>从上面结果来看，相比较而言，在指标上优势并不明显，但足矣证明本文的idea是可行的。在train from scratch的setting下，pix2seq是训练了300epoch，表格上之所以并没表明对比方法训练的epoch数，可能这正是pix2seq的一个缺点，训练收敛慢。</p><h3 id="PixSeq-v2"><a href="#PixSeq-v2" class="headerlink" title="PixSeq v2"></a>PixSeq v2</h3><h4 id="主要方法-1"><a href="#主要方法-1" class="headerlink" title="主要方法"></a>主要方法</h4><p>Pixseq v2是上周Ting Chen挂在arxiv的对pix2seq在多个视觉任务上拓展的一个工作，总的来说，作者并没有对模型层面做进一步改进，但对不同视觉task的输入输出接口做了统一。如下图所示，</p><p><img src="/img/image-20220719094051172.png" alt="image-20220719094051172"></p><p>以往的视觉任务比如，目标检测、实例分割、关键点检测和图像描述等任务都是单独设计不同模型、不同输入、不同损失函数来解决，而本文将每个任务的输出形式化为具有一个统一接口的一个离散的token序列，可以做到在所有这些任务上仅训练一个具有单一模型结构和损失函数的神经网络，而不需要针对特定任务进行模型结构或损失函数的定制。为了解决一个特定的任务，本文使用一个简短的prompt作为该任务的描述，网络的输出序列适应于该prompt，因此模型能够产生特定于任务的输出。</p><blockquote><ol><li>对于目标检测任务，遵循pix2seq做法，通过量化连续图像坐标，将box和cls转换为一系列离散token;</li><li>对于实例分割任务，以图像坐标序列形式预测polygon，与检测任务一样，对坐标进行量化离散为token；</li><li>对于关键点预测任务，给定一个人体实例，将关键点预测为一个量化的图像坐标序列；</li><li>对于图像描述，直接预测文本token。</li></ol></blockquote><p><img src="/img/image-20220719094807354.png" alt="image-20220719094807354"></p><p>值得注意的是，所有四个任务都使用同一个词汇表。 具体的prompt和输出序列如上图所示</p><h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><p>每个任务都有自己的成对图像序列训练数据。有两种方法可以将任务结合起来进行联合训练。作者提出了data mixing和batch mixing两种数据混合方式。</p><p><img src="/img/image-20220719095713926.png" alt="image-20220719095713926"></p><p>data mixing在概念上简单，但是因为数据格式不同，图像增强很难合并比较麻烦，相比较而言，batch mixing对单个任务采样图像后进行相应增强后转换为图像-序列对，模型分别计算每个任务的损失和梯度。作者认为可以将特定任务的每一批数据的梯度以适当的形式加权组合起来。</p><p><img src="/img/image-20220719100348972.png" alt="image-20220719100348972"></p><p>在损失函数上，与pix2seq一样，训练目标是最大化基于图像的token和之前的token的似然性。</p><script type="math/tex; mode=display">\operatorname{maximize} \sum_{j=1}^{L} \boldsymbol{w}_{j} \log P\left(\boldsymbol{y}_{j} \mid \boldsymbol{x}, \boldsymbol{y}_{1: j-1}\right)</script><p>其中，x表示输入图像，y是长度为L的编码序列(监督信号)，序列y的初始部分是一个prompt，为此作者将权重wi设置为零，损失计算时不包括该部分。</p><h4 id="实验结果-1"><a href="#实验结果-1" class="headerlink" title="实验结果"></a>实验结果</h4><p><img src="/img/image-20220719103500099.png" alt="image-20220719103500099"></p><p>从上述表格可以看出，在模型结构和损失函数都没有针对特定任务进行设计的前提下，本文所提出的模型对于每个单独的任务仍然可以获得与专门定制化的baseline相比，依然具有一定的可比性(即使输入图像的尺寸更小)。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文分享seq2seq learning相关的两篇论文，单位是google brain，一作均为Ting Chen（自监督学习方法SimCLR的作者），论文地址：&lt;a href=&quot;https://arxiv.org/pdf/2109.10852.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;pix2seq: A Language Modeling Framework for Object Detection&lt;/a&gt;,[ICLR2022接收]；&lt;a href=&quot;https://arxiv.org/pdf/2206.07669.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;A Unified Sequence Interface for Vision Tasks&lt;/a&gt;,[上星期挂arxiv]，后者是对前者在多个视觉任务上的拓展。下面大致的介绍一下两篇论文的具体工作。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="seq2seq" scheme="https://blog.nicehuster.cn/tags/seq2seq/"/>
    
  </entry>
  
  <entry>
    <title>ViLD基于CLIP模型的zero-shot目标检测方法</title>
    <link href="https://blog.nicehuster.cn/2022/06/13/ViLD/"/>
    <id>https://blog.nicehuster.cn/2022/06/13/ViLD/</id>
    <published>2022-06-13T11:13:39.000Z</published>
    <updated>2022-06-15T03:23:56.100Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息</strong>：<a href="https://arxiv.org/pdf/2104.13921.pdf" target="_blank" rel="noopener">Open-vocabulary Object Detection via Vision and Language Knowledge Distillation</a><br><strong>代码链接</strong>：<a href="https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild" target="_blank" rel="noopener">https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild</a><br><strong>整体信息</strong>：这是google research 发表在ICLR2022上有关CLIP在下游任务-目标检测任务上的应用。使用CLIP模型实现zero-shot场景下的目标检测任务。比较有想象意义的是，通过一句话就可以检测出图像中需要的指定目标。在之前<a href="https://nicehuster.github.io/2022/06/03/CLIP/#more" target="_blank" rel="noopener">CLIP图文多模态对比预训练方法详解</a>中也有提及过这篇工作。</p><a id="more"></a><p><img src="https://camo.githubusercontent.com/238affd721b73ef2ff8f95114352188693623796bcc57b83f94747a7d2b6ff3a/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f636c6f75642d7470752d636865636b706f696e74732f646574656374696f6e2f70726f6a656374732f76696c642f6173736574732f6e65775f7465617365722e706e67" alt></p><p>上图展示的是ViLD的检测结果。其中只有toy类别是训练过程中见到的类别，但zero-shot detection还检测到其他的属性，比如toy种类和颜色等。</p><h3 id="zero-shot-detection"><a href="#zero-shot-detection" class="headerlink" title="zero-shot detection"></a>zero-shot detection</h3><p>顾名思义，这个任务就是，对于任意一个新类别，一张训练图像都不给的情况下，训练出来的检测器也能检测这个类别。zero-shot detection的setting通常是，将数据集依据类别划分为俩部分：base类别和novel类别，base类别用于训练，novel类别在训练过程中不可见。该任务的目标在于，在novel类别上获得较好性能的同时还需要保持base类别的性能；在这篇文章中使用的是LVIS数据集进行实验对比分析，将common和frequency俩类别作为base类，将rare类别作为novel类。</p><h3 id="常规方法"><a href="#常规方法" class="headerlink" title="常规方法"></a>常规方法</h3><p><img src="/img/vild-Vanilla.png" alt="vild-Vanilla"></p><p>如上图展示的是zero-shot detection with cropped regions，具体地，使用在二阶段检测方法比如Mask-RCNN获得proposal之后，对每个proposal都crop &amp; resize 然后输入到CLIP-image-encoder中获得image-embedding，与对应类别的text-embedding进行对比，获取类别信息。该方法的的缺点是比较慢，需要one-by-one地处理每个object proposal，而且CLIP-text-encoder没有充分利用base类别的文本信息。</p><h3 id="ViLD方法"><a href="#ViLD方法" class="headerlink" title="ViLD方法"></a>ViLD方法</h3><p><img src="/img/vild-pipeline.png" alt="vild-pipeline"></p><p>上图展示的是ViLD方法的pipeline。具体地，在ViLD中包含俩部分：<strong>ViLD-text</strong>用于学习文本embedding和<strong>ViLD-image</strong>用于学习图像embedding。在ViLD-text中，将base类别文本送入CLIP-text-encoder中获得text embedding，然后用于classify目标区域，在ViLD-image中会将对应的proposal送入CLIP-image-encoder中获得图像embedding，对经过roi align之后的region embedding 进行知识蒸馏；相比于ViLD-text，ViLD-image蒸馏了base+novel的信息，因为proposal网络输出的proposal可能会包含novel，而ViLD-text只使用了base类的文本信息；</p><p><img src="/img/vild-overview.png" alt="vild-overview"></p><p>上图展示的是ViLD的训练和推理流程。相比于mask-rcnn，修改地是rcnn的分类分支；具体地，在训练过程中，在获取分类监督信号上包括俩部分：用CLIP获得image embedding蒸馏region embedding，以及用CLIP获得text embedding监督region embedding；总的损失如下公式所示：</p><script type="math/tex; mode=display">\mathcal{L}_{\mathrm{ViLD}}=\mathcal{L}_{\text {ViLD-text }}+w \cdot \mathcal{L}_{\mathrm{ViLD}-\mathrm{image}}</script><p>在推理过程，只需要将region embedding和text embedding(base+novel)进行对比即可得到类别信息。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p><strong>数据集</strong>：实验使用的是LVIS v1.0（1203类别），其中frequent(f: 405个类别)和common(c: 461个类别)作为base类，其余rare(r: 337个类别)作为novel类。</p><p><strong>目标proposal</strong>：由于训练过程中只使用了base类训练，下表展示的是仅使用base类训练时的RPN召回率和使用base+novel时的RPN召回率，从上可以看出二者相差1-2个点。因此可以看出RPN是具备从base类上泛化到novel。</p><p><img src="/img/vild-recall.png" alt="vild-recall"></p><p><strong>Ablation</strong>：作者在paper中做了较为详尽的ablation study实验，这里只提及一些证明idea有效的关键实验分析。</p><p><img src="/img/vild-ensemble.png" alt="vild-ensemble"></p><p>上表格中，CLIP on cropped regions就是前面介绍的常规方法，该方法在APr上可以达到13.0，ViLD-text和ViLD-image表示分别使用单一监督信号。ViLD(w=0.5)表示同时使用ViLD-text和ViLD-image监督训练。ViLD-text相比CLIP on cropped regions在APr上下降了3个点，说明使用base类信息监督ViLD-text在novel上的泛化性有所下降。ViLD(w=0.5)相比于ViLD-text和ViLD-image都提升幅度明显。ViLD-ensemble(w=0.5)表示同时使用ViLD-text和ViLD-image监督训练同时，在base预测上，倾向于ViLD-text，在novel预测上使用vice versa投票决定。可以看出ViLD-ensemble(w=0.5)方式在base类别上提升明显。</p><p><strong>Transfer to other detection datasets</strong>：这个是证明在不同数据集之间的一个迁移有效性。只需要替换类别 text embedding，无需进行fine-tune。</p><p><img src="/img/vild-transfer.png" alt="vild-transfer"></p><p>在不进行任何fine-tune下，ViLD在COCO数据集上就可以取得36.6AP，与fine-tune条件下AP只相差不到3个点。</p><h3 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h3><p>作者也在离线交互式检测上也做过一些实验，输入文本信息，就可以检测出对应的目标。这个还挺有意思的，随意说一句话就能检测到图像的指定目标。</p><p><img src="/img/vild-interactive detection.png" alt="vild-interactive detection"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息&lt;/strong&gt;：&lt;a href=&quot;https://arxiv.org/pdf/2104.13921.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Open-vocabulary Object Detection via Vision and Language Knowledge Distillation&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接&lt;/strong&gt;：&lt;a href=&quot;https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息&lt;/strong&gt;：这是google research 发表在ICLR2022上有关CLIP在下游任务-目标检测任务上的应用。使用CLIP模型实现zero-shot场景下的目标检测任务。比较有想象意义的是，通过一句话就可以检测出图像中需要的指定目标。在之前&lt;a href=&quot;https://nicehuster.github.io/2022/06/03/CLIP/#more&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CLIP图文多模态对比预训练方法详解&lt;/a&gt;中也有提及过这篇工作。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="multi-model" scheme="https://blog.nicehuster.cn/tags/multi-model/"/>
    
  </entry>
  
  <entry>
    <title>DeCLIP一种数据高效的CLIP训练方法</title>
    <link href="https://blog.nicehuster.cn/2022/06/09/DeCLIP/"/>
    <id>https://blog.nicehuster.cn/2022/06/09/DeCLIP/</id>
    <published>2022-06-09T11:13:39.000Z</published>
    <updated>2022-06-13T03:56:00.801Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息</strong>：<a href="https://arxiv.org/pdf/2110.05208.pdf" target="_blank" rel="noopener">Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm</a><br><strong>代码链接</strong>：<a href="https://github.com/Sense-GVT/DeCLIP" target="_blank" rel="noopener">https://github.com/Sense-GVT/DeCLIP</a><br><strong>整体信息</strong>：这是商汤科技发表在ICLR2022上关于多模态预训练的工作，在前面的文章中介绍过CLIP，是一种基于对比文本-图像对的预训练方法，该方法需要在大量的图像-文本对数据集进行训练，在CLIP工作上就使用了4亿的图像-文本对数据，数百张卡进行预训练。为了提高训练效率，这篇工作提出了DeCLIP(Data Efficiency CLIP)方法，在较少数据下依旧可以取得不错的效果。</p><a id="more"></a><p><img src="/img/declip-sota.png" alt="declip-sota"></p><h3 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h3><p><img src="/img/clip-vs-declip.png" alt="clip-vs-declip"></p><p>上图，直观地，展示的是CLIP和DeCLIP方法的差异。CLIP是直接学习原始图片与对应的文本信息，使用俩个encoder分别编码图像信息和文本信息。图像encoder一般是resnet或者ViT，文本encoder一般使用transformer。之后将俩个embedding映射到相同的空间中，使用对比学习的思想进行训练。从方法上看，其实只使用了图像-文本对匹配的一种监督信号进行训练。假设batch size是N，共计N个图像-文本对$\left\{\left(x_{i}^{I}, x_{i}^{T}\right)\right\}$，损失函数InfoNCE如下：</p><script type="math/tex; mode=display">L_{I}=-\frac{1}{N} \sum_{i=1}^{N} \log \frac{\exp \left(\operatorname{sim}\left(\boldsymbol{z}_{i}^{I}, \boldsymbol{z}_{i}^{T}\right) / \tau\right)}{\sum_{j=1}^{N} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_{i}^{I}, \boldsymbol{z}_{j}^{T}\right) / \tau\right)}</script><p>不同于CLIP，在DeCLIP方法中，使用了更多的自监督信号：1. 单模态的自监督学习；2. 跨模态的多视角监督学习；3. 最近邻监督学习；具体地，</p><ol><li><p><strong>单模态自监督学习</strong>（self-supervision within each modality, SS），包括使用<strong>SimSiam</strong>作为图像的自监督信号，和使用掩码语言模型<strong>MLM</strong>作为文本的自监督信号；</p><p><img src="/img/declip-ss.png" alt="declip-ss"></p><p>(a)<strong>图像自监督</strong>：同一张图片进过数据增强获得俩个view：$(x^{I}, \tilde{x}^{I})$,将经过数据增强后的结果经过相同的encoder得到俩个embedding向量$(z^{I}, \tilde{z}^{I})$，之后将其中一个embedding向量$x^{I}$再经过一个perd层得到向量$p^{I}$,训练时让$p^{I}$和$\tilde{x}^{I}$ 尽量接近；</p><p>(b)<strong>文本自监督</strong>：文本自监督使用的是MLM方法，即随机mask掉文本中15%的token，然后利用前后token预测被mask掉的token；</p></li><li><p><strong>跨模态多视角监督学习</strong>（Multi-View Supervision, MVS）：CLIP只使用的原始图像-文本对$\left(z^{I}, z^{T}\right)$，计算infoNCE损失，而DeCLIP中使用的是增强后的文本和图像计算infoNCE损失：$\left(z^{I}, z^{T}\right), \quad\left(\tilde{z}^{I}, z^{T}\right),\left(z^{I}, \tilde{z}^{T}\right), \quad\left(\tilde{z}^{I}, \tilde{z}^{T}\right)$ ，相比CLIP多了3个监督信息；</p></li><li><p><strong>最近邻监督学习</strong>（Nearest-Neighbor Supervision, NNS）：考虑到相同的图像可能会有类似的语言描述，因此选择语言描述相似的图文进行对比学习，通过维护一个先入先出的队列来模拟整个数据的分布，从队列中选择最相似的句子作为正样本$z^{T^{\prime}}$，之后使用InfoNCE计算最近邻损失：$\left(z^{I}, z^{T^{\prime}}\right),\left(\tilde{z}^{I}, z^{T^{\prime}}\right)$;</p><p><img src="/img/declip-nss.png" alt="declip-nss"></p></li></ol><p>在损失函数层面上，对以上三种不同监督的损失进行加权求和，得到最终的loss，具体地，如下所示：</p><script type="math/tex; mode=display">L_{D e C L I P}=(1-\alpha-\beta-\gamma) L_{C L I P}+\alpha\left(L_{I S S}+L_{T S S}\right)+\beta L_{M V S}+\gamma L_{N N S}</script><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p><img src="/img/declip-dataset.png" alt="declip-dataset"></p><p>在DeCLIP中，数据集包含俩部分：开源数据集29M和网络下载的数据集59M，总共88M训练数据，相比于CLIP使用的400M数据少很多。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><ol><li><p>Zero-shot准确率;</p><p><img src="/img/declip-zero-shot.png" alt="declip-zero-shot"></p><p>相比于CLIP，使用更少的训练数据，得到了更高的准确率；</p></li><li><p>下游任务表现；</p><p><img src="/img/declip-finetune.png" alt="declip-finetune"></p><p>在resnet和ViT俩种不同的encoder上，都证明了DeCLIP学习到的特征表示相比CLIP要强；</p></li><li><p>Ablation study</p><p><img src="/img/declip-ablation.png" alt="declip-ablation"></p><p>如上图证明了使用多种监督信息可有效的提升zero-shot准确率，而且相比于CLIP，DeCLIP的训练效率更高；</p></li></ol><h3 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h3><p>作者还在DeCLIP的基础上提出了<a href="https://arxiv.org/abs/2203.05796" target="_blank" rel="noopener">CLIP-benchmark</a>，其中包含了高质量的YFCC15M-V2数据集，而且复现了CLIP系列的相关方法(CLIP，DeCLIP，FILIP，DeCLIP，DeFILIP)。目前代码均已开源在<a href="https://github.com/Sense-GVT/DeCLIP" target="_blank" rel="noopener">这里</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息&lt;/strong&gt;：&lt;a href=&quot;https://arxiv.org/pdf/2110.05208.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接&lt;/strong&gt;：&lt;a href=&quot;https://github.com/Sense-GVT/DeCLIP&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/Sense-GVT/DeCLIP&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息&lt;/strong&gt;：这是商汤科技发表在ICLR2022上关于多模态预训练的工作，在前面的文章中介绍过CLIP，是一种基于对比文本-图像对的预训练方法，该方法需要在大量的图像-文本对数据集进行训练，在CLIP工作上就使用了4亿的图像-文本对数据，数百张卡进行预训练。为了提高训练效率，这篇工作提出了DeCLIP(Data Efficiency CLIP)方法，在较少数据下依旧可以取得不错的效果。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="multi-model" scheme="https://blog.nicehuster.cn/tags/multi-model/"/>
    
  </entry>
  
  <entry>
    <title>视觉-语言预训练(Vision-Language Pretraining)综述</title>
    <link href="https://blog.nicehuster.cn/2022/06/05/VLP/"/>
    <id>https://blog.nicehuster.cn/2022/06/05/VLP/</id>
    <published>2022-06-05T11:13:39.000Z</published>
    <updated>2022-06-07T08:33:47.376Z</updated>
    
    <content type="html"><![CDATA[<p>前几天看完CLIP论文后觉得视觉-语言预训练(Vision-Language Pretraining)这个方向还挺有意思，就顺便找了一篇关于VLP的综述文章：<a href="https://arxiv.org/abs/2202.09061" target="_blank" rel="noopener">VLP: A Survey on Vision-Language Pre-training</a>，这篇文章有详细地介绍了VLP领域的最新进展和总结，包括了图像-文本和视频-文本的预训练。对于完整的了解VLP这个领域有很大帮助。</p><a id="more"></a><p>整篇综述从以下5个方面对视觉-语言预训练进行了详细地阐述：</p><blockquote><ol><li>特征提取</li><li>模型架构</li><li>预训练目标</li><li>预训练数据集</li><li>下游任务</li></ol></blockquote><p>VLP主要通过对大规模数据的预训练来学习不同模态之间的语义对应关系。例如，在图文预训练中，我们希望模型将文本中的“狗”与图像中的“狗”联系起来。在视频文本预训练中，我们期望模型将文本中的对象/动作映射到视频中的对象/动作。为了实现这一目标，需要巧妙地设计VLP对象和模型体系结构，以允许模型挖掘不同模式之间的关联。</p><h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><p>特征提取部分分为视觉特征提取和文本特征提取。视觉特征提取包括图像特征提取和视频特征提取。</p><h4 id="图像特征提取"><a href="#图像特征提取" class="headerlink" title="图像特征提取"></a>图像特征提取</h4><blockquote><ol><li>OD-based Region Features (OD-RFs)：使用预训练的目标检测模型识别图像中目标区域，并提取每个区域的表示，来提取视觉特征，常用的是Faster RCNN；</li><li>CNN-based Grid Features (CNN-GFs)：直接用CNN对整张图提取视觉特征获得网格特征；</li><li>ViT-based Patch Features (ViT-PFs)：使用ViT将图片压平成序列，对于transformer类型的encoder的输入比较友好；</li></ol></blockquote><h4 id="视频特征提取"><a href="#视频特征提取" class="headerlink" title="视频特征提取"></a>视频特征提取</h4><p>一般把视频当做M帧组成的图像信息。VLP模型利用上述图像特征提取方法提取frame特征。俩个常用的是CNN-GFs和ViT-PFs。对于CNN-GFs，一般是使用在imagenet上预训练的resnet和预先训练好的SlowFast来提取每个视频帧的2d特征和3d特征。将这些特征串联便可得到视频的视觉特征，通过FC层被投影到与token embedding相同的低维空间；对于ViT-PFs，一段视频clips $V_{i} \in \mathbb{R}^{M \times H \times W \times C}$ 会被分割为 $M \times N$ 的无重叠的时空patchs，大小为$P \times P$, 在这里 $N=\frac{H W}{P^{2}}$.</p><h4 id="文本特征提取"><a href="#文本特征提取" class="headerlink" title="文本特征提取"></a>文本特征提取</h4><p>对于文本特征，一般使用Bert进行特征提取。VLP模型首先将输入的句子分割成一系列字词。然后再序列的开头和结尾处插入序列开始和结束标记。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>模型结构主要从俩个视角进行划分：</p><blockquote><ol><li>从多模态融合上，可以划分为：Single-stream和Dual-stream；</li><li>从整体结构设计上，可以划分为：Encoder-Only和Encoder-Decoder；</li></ol></blockquote><p><img src="/img/vlp-single-stream-vs-dual-stream.png" alt="image-20220607114126559"></p><h4 id="Single-stream-vs-Dual-stream"><a href="#Single-stream-vs-Dual-stream" class="headerlink" title="Single-stream vs Dual-stream"></a>Single-stream vs Dual-stream</h4><p>单流架构是指将文本和视觉特征连接在一起，然后馈送到单个Transformer块中。双流架构是指文本和视觉特征不是串联在一起而是独立地发送到两个不同的Transformer块中。单流架构一般来说更加parameter-efficient。双流架构一般采用上图虚线所表示的cross attention进行特征交互。</p><h4 id="Encoder-Only-vs-Encoder-Decoder"><a href="#Encoder-Only-vs-Encoder-Decoder" class="headerlink" title="Encoder-Only vs Encoder-Decoder"></a>Encoder-Only vs Encoder-Decoder</h4><p>许多VLP模型采用encoder-only的体系结构，其中跨模态表示被直接馈送到输出层以生成最终输出。相比之下，其他VLP模型主张使用encoder-decoder体系结构，其中跨模态表示首先被馈送到decoder，然后被馈送到输出层。</p><h3 id="预训练目标"><a href="#预训练目标" class="headerlink" title="预训练目标"></a>预训练目标</h3><p>在论文中对预训练目标归纳为了四类：Completion、Matching、Temporal和Particular。</p><h4 id="Completion"><a href="#Completion" class="headerlink" title="Completion"></a>Completion</h4><p>这类方法主要是通过对masked的元素进行重建来理解模态信息。包括：Masked Language Modeling、Prefix Language Modeling和 Masked Vision Modeling。</p><h5 id="Masked-Language-Modeling"><a href="#Masked-Language-Modeling" class="headerlink" title="Masked Language Modeling"></a>Masked Language Modeling</h5><p>掩蔽语言建模(MLM)应该是被广泛应用在Bert模型中的一种预训练方式，通过利用上下文的可见的词向量预测masked词向量。而在VLP任务重则是使用上下文可见的词向量和视觉向量表征去预测masked的视觉或者词向量。</p><h5 id="Prefix-Language-Modeling"><a href="#Prefix-Language-Modeling" class="headerlink" title="Prefix Language Modeling"></a>Prefix Language Modeling</h5><p>前缀语言建模(Prefix Language Model，Prefix LM)是屏蔽语言模型和语言建模的统一。前缀模型的提出是为了使模型具有实体生成能力，使得文本诱导的zero-shot具有无需fine-tuning的泛化性。</p><h5 id="Masked-Vision-Modeling"><a href="#Masked-Vision-Modeling" class="headerlink" title="Masked Vision Modeling"></a>Masked Vision Modeling</h5><p>与MLM类似，MVM对视觉(图像或视频)区域或块进行采样，并通常以15%的概率mask其视觉特征。在给定剩余视觉特征和所有文本特征的情况下，VLP模型需要重建mask的视觉特征。</p><h4 id="Matching"><a href="#Matching" class="headerlink" title="Matching"></a>Matching</h4><p>Matching是将视觉和语言统一到一个共享的隐层，生成通用的视觉语言表征。包括Vision-Language Matching， Vision-Language Contrastive Learning， Word-Region Alignment。</p><h5 id="Vision-Language-Matching"><a href="#Vision-Language-Matching" class="headerlink" title="Vision-Language Matching"></a>Vision-Language Matching</h5><p>视觉语言匹配(VLM)是最常见的预训练模型的目标，以实现视觉和语言的匹配。以双流VLP模型为例，在得到视觉表征和文本表征之后将其串联起来，作为俩种模式的融合表征，然后经过FC层和sigmoid函数，以预测0-1分数，0表示视觉-语言不匹配，1表示视觉和语言匹配。</p><h5 id="Vision-Language-Contrastive-Learning"><a href="#Vision-Language-Contrastive-Learning" class="headerlink" title="Vision-Language Contrastive Learning"></a>Vision-Language Contrastive Learning</h5><p>视觉语言对比学习(VLC)从N × N个可能的视觉语言对中预测出匹配的视觉语言对。请注意，在一批训练中有N ~ N个负视觉语言对。VLP模型计算 softmax-normalized 的视觉(图像或视频)到文本的相似性和文本到视觉的相似性，并利用视觉到文本和文本到视觉相似性的交叉熵损失来更新自己。相似度通常用点积来实现。最常见的方法就是CLIP。</p><h5 id="Word-Region-Alignment"><a href="#Word-Region-Alignment" class="headerlink" title="Word-Region Alignment"></a>Word-Region Alignment</h5><p>单词-区域对齐(WRA)是一种无监督的预训练目标，用于对齐视觉区域(视觉patch)和单词。VLP模型利用最优传输来学习视觉和语言之间的对齐。</p><h3 id="预训练数据集"><a href="#预训练数据集" class="headerlink" title="预训练数据集"></a>预训练数据集</h3><p><img src="/img/vlp-datasets.png" alt="image-20220607151527603"></p><h3 id="下游任务"><a href="#下游任务" class="headerlink" title="下游任务"></a>下游任务</h3><p>下游任务主要有分类、回归、检索、生成以及其他任务。</p><blockquote><p><strong>分类任务</strong>：Visual Question Answering (VQA)，Visual Reasoning and Compositional Question Answering (GQA)，Video-Language Inference (VLI)，Natural Language for Visual Reasoning (NLVR)，Visual Entailment (VE)，Visual Commonsense Reasoning (VCR)，Grounding Referring Expressions (GRE)，Category Recognition (CR).<br><strong>回归任务</strong>：Multi-modal Sentiment Analysis (MSA).<br><strong>检索任务</strong>：Vision-Language Retrieval (VLR).<br><strong>生成任务</strong>：Visual Captioning (VC)，Novel Object Captioning at Scale (NoCaps)，Visual Dialogue (VD).<br><strong>其他任务</strong>：Multi-modal Machine Translation (MMT)，Vision-Language Navigation (VLN)，Optical Character Recognition (OCR).</p></blockquote><h3 id="SOTA-VLP-模型"><a href="#SOTA-VLP-模型" class="headerlink" title="SOTA VLP 模型"></a>SOTA VLP 模型</h3><p><img src="/img/vlp-sota.png" alt="vlp-sota"></p><h3 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h3><p>第一篇关于VLP的调研综述。文章从特征提取、模型结构、预训练目标、预训练数据集和下游任务五个方面综述了其最新进展，并对具体的SOTA VLP模型进行了详细的总结。这能够帮助研究人员更好地了解VLP，并启发新的工作来推动这一领域的发展。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前几天看完CLIP论文后觉得视觉-语言预训练(Vision-Language Pretraining)这个方向还挺有意思，就顺便找了一篇关于VLP的综述文章：&lt;a href=&quot;https://arxiv.org/abs/2202.09061&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;VLP: A Survey on Vision-Language Pre-training&lt;/a&gt;，这篇文章有详细地介绍了VLP领域的最新进展和总结，包括了图像-文本和视频-文本的预训练。对于完整的了解VLP这个领域有很大帮助。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="multi-model" scheme="https://blog.nicehuster.cn/tags/multi-model/"/>
    
  </entry>
  
  <entry>
    <title>CLIP图文多模态对比预训练方法详解</title>
    <link href="https://blog.nicehuster.cn/2022/06/03/CLIP/"/>
    <id>https://blog.nicehuster.cn/2022/06/03/CLIP/</id>
    <published>2022-06-03T11:13:39.000Z</published>
    <updated>2022-06-07T08:17:32.448Z</updated>
    
    <content type="html"><![CDATA[<p>CLIP是OpenAI在2021年发表的一种用NLP来监督CV的方法。成功连接文本和图像。CLIP全称是， Contrastive Language–Image Pre-training，一种基于对比文本-图像对的预训练方法。在了解CLIP具体方法之前，可以先看一下该工作的在一些下游任务的应用。<br><a id="more"></a></p><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>1.风格迁移<a href="https://arxiv.org/abs/2103.17249" target="_blank" rel="noopener">styleCLIP</a></p><p><img src="https://github.com/orpatashnik/StyleCLIP/raw/main/img/teaser.png" alt></p><p>styleCLIP很神奇，CLIP可以理解各种抽象的妆容，比如发型，卸妆等等</p><p>2.文本生成图像<a href="https://arxiv.org/abs/2106.14843" target="_blank" rel="noopener">CLIPDraw</a></p><p><img src="https://kvfrans.com/content/images/2021/06/Screen-Shot-2021-06-10-at-8.47.23-PM.png" alt></p><p>这个也是使用CLIP指导模型的生成，甚至无需训练，就可以生成简笔画，颇具有抽象派画风。</p><p>3.开集目标检测<a href="https://arxiv.org/abs/2104.13921" target="_blank" rel="noopener">ViLD</a></p><p><img src="https://camo.githubusercontent.com/238affd721b73ef2ff8f95114352188693623796bcc57b83f94747a7d2b6ff3a/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f636c6f75642d7470752d636865636b706f696e74732f646574656374696f6e2f70726f6a656374732f76696c642f6173736574732f6e65775f7465617365722e706e67" alt></p><p>传统目标检测方法可能只能告诉你以上均是玩具类别，但是基于CLIP还可以知道玩具颜色，玩具种类。</p><p>4.文本视频检索<a href="https://github.com/johanmodin/clifs" target="_blank" rel="noopener">Clifs</a></p><p><img src="https://github.com/johanmodin/clifs/raw/master/media/odwalla.jpg" alt></p><p>CLIP还可以用于视频检索，如上图所示，告知“A truck with the text “odwalla””，可以基于文本内容检索到视频中对应帧的位置。</p><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><p>现有的模型都是基于固定的预定义物体类别集合进行监督训练。比如coco 80类等。但是这种限制性的监督型号限制了模型本身的泛化性，当需要识别新物体类别时需要重新收集新的数据重新训练，因此作者想到使用NLP里面文本信息获取监督信号。</p><h3 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h3><p>CLIP是一种基于对比学习的多模态模型，与CV中的一些对比学习方法如moco和simclr不同的是，CLIP的训练数据是文本-图像对：一张图像和它对应的文本描述，这里希望通过对比学习，模型能够学习到文本-图像对的匹配关系。如下图所示，CLIP包括两个模型：<strong>Text Encoder</strong>和<strong>Image Encoder</strong>，其中Text Encoder用来提取文本的特征，可以采用NLP中常用的text transformer模型；而Image Encoder用来提取图像的特征，可以采用常用CNN模型或者vision transformer。</p><p><img src="https://openaiassets.blob.core.windows.net/$web/clip/draft/20210104b/overview-a.svg" alt></p><p>这里对提取的文本特征和图像特征进行对比学习。对于一个包含N个文本-图像对的训练batch，将N个文本特征和N个图像特征两两组合，CLIP模型会预测出 $N^2$ 个可能的文本-图像对的相似度，这里的相似度直接<strong>计算文本特征和图像特征的余弦相似性（cosine similarity）</strong>，即上图所示的矩阵。这里共有N个正样本，即真正属于一对的文本和图像（矩阵中的对角线元素），而剩余的个$N^2-N$个文本-图像对为负样本，那么CLIP的训练目标就是最大个N正样本的相似度，同时最小化$N^2-N$个负样本的相似度，对应的伪代码实现如下所示：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># image_encoder - ResNet or Vision Transformer</span></span><br><span class="line"><span class="comment"># text_encoder - CBOW or Text Transformer</span></span><br><span class="line"><span class="comment"># I[n, h, w, c] - minibatch of aligned images</span></span><br><span class="line"><span class="comment"># T[n, l] - minibatch of aligned texts</span></span><br><span class="line"><span class="comment"># W_i[d_i, d_e] - learned proj of image to embed</span></span><br><span class="line"><span class="comment"># W_t[d_t, d_e] - learned proj of text to embed</span></span><br><span class="line"><span class="comment"># t - learned temperature parameter</span></span><br><span class="line"><span class="comment"># extract feature representations of each modality</span></span><br><span class="line">I_f = image_encoder(I) <span class="comment">#[n, d_i]</span></span><br><span class="line">T_f = text_encoder(T) <span class="comment">#[n, d_t]</span></span><br><span class="line"><span class="comment"># joint multimodal embedding [n, d_e]</span></span><br><span class="line">I_e = l2_normalize(np.dot(I_f, W_i), axis=<span class="number">1</span>)</span><br><span class="line">T_e = l2_normalize(np.dot(T_f, W_t), axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># scaled pairwise cosine similarities [n, n]</span></span><br><span class="line">logits = np.dot(I_e, T_e.T) * np.exp(t)</span><br><span class="line"><span class="comment"># symmetric loss function</span></span><br><span class="line">labels = np.arange(n)</span><br><span class="line">loss_i = cross_entropy_loss(logits, labels, axis=<span class="number">0</span>)</span><br><span class="line">loss_t = cross_entropy_loss(logits, labels, axis=<span class="number">1</span>)</span><br><span class="line">loss = (loss_i + loss_t)/<span class="number">2</span></span><br></pre></td></tr></table></figure><p>为了训练CLIP，OpenAI从互联网收集了4亿文本-图像对，论文称之为WebImageText。论文中Text Encoder固定选择一个包含63M参数的text transformer模型，而Image Encoder采用了两种的不同的架构，一是常用的CNN架构ResNet，二是基于transformer的ViT，其中ResNet包含5个不同大小的模型：ResNet50<strong>，</strong>ResNet101<strong>，</strong>RN50x4<strong>，</strong>RN50x16和RNx64（后面三个模型是按照EfficientNet缩放规则对ResNet分别增大4x，16x和64x得到），而ViT选择3个不同大小的模型：ViT-B/32<strong>，</strong>ViT-B/16<strong>和</strong>ViT-L/14。所有的模型都训练32个epoch，采用AdamW优化器，而且训练过程采用了一个较大的batch size：32768。由于数据量较大，最大的ResNet模型RN50x64需要在592个V100卡上训练18天，而最大ViT模型ViT-L/14需要在256张V100卡上训练12天，可见要训练CLIP需要耗费多大的资源。对于ViT-L/14，还在336的分辨率下额外finetune了一个epoch来增强性能，论文发现这个模型效果最好，记为ViT-L/14@336，论文中进行对比实验的CLIP模型也采用这个。</p><p>值得思考的是，在CLIP中，作者没有采用预测式的目标函数优化模型，而是采样对比学习的方式进行优化，在这一块，原文有介绍，使用对比学习的方式训练模型，放宽了约束，模型也更容易收敛，相比于预测型监督，对比式监督可以提升4倍的训练效率。</p><h3 id="使用CLIP实现zero-shot分类"><a href="#使用CLIP实现zero-shot分类" class="headerlink" title="使用CLIP实现zero-shot分类"></a>使用CLIP实现zero-shot分类</h3><p>当CLIP预训练好之后，有俩个编码器Text Encoder和Image Encoder，基于这俩训练好的encoder可以直接实现zero-shot分类，即无需任何训练数据，就能在某个下游任务上实现分类。具体步骤：</p><p><img src="https://openaiassets.blob.core.windows.net/$web/clip/draft/20210104b/overview-b.svg" alt></p><blockquote><ol><li>根据分类任务的分类标签构建每个类别的描述文本：A photo of {label}, 然后将这些文本送入Text Encoder获得对应的文本特征，如果类别数目为N，那么得到N个文本特征。</li><li>将要预测的图像送入Image Encoder得到图像特征，然后与N个文本特征计算余弦相似度，将相似度大的作为图像预测类别。</li></ol></blockquote><p>在zero-shot分类任务上，CLIP在多个数据集上取得优于在imagenet上fine的res101，具体可以看下图的结果对比：</p><p><img src="/img/zero-shot-cls.png" alt></p><p>在zero-shot任务上，作者在linear probe基础上做了详尽的实验，首先作者通过和经过强监督学习的Resnet-50提取的特征对比，任务都是分类任务，因此作者基于Resnet-50和CLIP提取出的特征，只是训练了最后的分类器，分类结果如下图所示。可以发现仅仅通过无监督的对比学习预训练得到的特征，即便是和强监督模型特征相比也是不分伯仲的。同时可以发现，zero-shot CLIP在一些动作识别任务中，比如Kinetics 700，UCF 101中有着比较大的提高，作者认为这可能是因为目前的文本描述中有很多以动词，动作为中心的句子导致的。</p><p><img src="/img/zero-shot-clip-res50.png" alt="image-20220606165016300"></p><h3 id="prompt-engineering"><a href="#prompt-engineering" class="headerlink" title="prompt engineering"></a>prompt engineering</h3><p>考虑到以单词作为标签存在歧义情况，比如在Oxford-IIIT Pet dataset 数据集中<code>boxer</code>表示斗牛犬，而在其他数据集中则可能表示拳击运动；在ImageNet中，<code>crane</code>同时表示了起重机和鹤。这种词语的多义显然对是因为缺少对标签的上下文描述导致的。为了解决这种问题，作者在指示上下文中添加了一些提示标签类型的词语，比如<code>A photo of a &lt;LABEL&gt;, a type of pet.</code>。作者将这个方法称之为“prompt engineering”。在合适地选取了不同的指示上下文，并且将其打分进行ensemble之后。作者发现这些Tricks竟能在zero-shot实验上提高5个绝对百分位，如Fig 2.3所示。</p><p><img src="/img/prompt-eng.png" alt></p><h3 id="limitation"><a href="#limitation" class="headerlink" title="limitation"></a>limitation</h3><p>这部分是最容易被忽略，但个人认为这部分往往更引人深思，这里简单的总结一下我关注的几个CLIP的limitation：</p><ol><li>CLIP在zero-shot上的性能虽然总体上比supervised baseline res50要好，但是在很多任务上是比不过sota，因此CLIP的迁移学习有待挖掘；</li><li>CLIP在一下几种task上的性能不好：细粒度分类，计数等任务；</li><li>CLIP本质上还是在有限的类别中进行对比推理，无法像image caption那样完全地灵活地生成新的概念，不同于生成模型；</li><li>CLIP依旧没有解决深度学习poor data efficiency的问题；</li></ol><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>CLIP可以再预训练阶段学习到更多通用的视觉语义信息，并且给下游任务提供帮助。而且相比于以往的训练方法，打破了之前的固定种类标签的范式。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CLIP是OpenAI在2021年发表的一种用NLP来监督CV的方法。成功连接文本和图像。CLIP全称是， Contrastive Language–Image Pre-training，一种基于对比文本-图像对的预训练方法。在了解CLIP具体方法之前，可以先看一下该工作的在一些下游任务的应用。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="multi-model" scheme="https://blog.nicehuster.cn/tags/multi-model/"/>
    
  </entry>
  
  <entry>
    <title>Diffusion Model详解</title>
    <link href="https://blog.nicehuster.cn/2022/06/03/Diffusion/"/>
    <id>https://blog.nicehuster.cn/2022/06/03/Diffusion/</id>
    <published>2022-06-03T11:13:39.000Z</published>
    <updated>2022-07-05T01:16:08.179Z</updated>
    
    <content type="html"><![CDATA[<p>扩散模型(Diffusion Model)是一种新的图像生成范式，有着与其他生成方法比如GAN、VAE以及Flow-based models不同而且有意思的特性。最近也有许多基于diffusion model的图像生成方法涌现出来，比如DALLE2、Imagen、GLIDE，    CogView1&amp;2，本文主要依据<a href="https://lilianweng.github.io/" target="_blank" rel="noopener">Lil’Log</a> 的博客进行翻译整理，方便学习记录。可以先看一下基于diffusion model的一些下游方法基于text-to-image图像生成结果。<br><a id="more"></a></p><h3 id="图像生成-text2image"><a href="#图像生成-text2image" class="headerlink" title="图像生成(text2image)"></a>图像生成(text2image)</h3><p>上面各类基于diffusion模型生成的图片都非常真实，而且有些生成出来的风格画很有意境。接下来就开始介绍diffusion model，在介绍diffusion model之前首先看一下各类生成模型的对比：</p><p><img src="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/generative-overview.png" alt></p><p>上图对比了GAN，VAE、以及基于Flow的模型，它们在生成高质量样本方面显示出巨大的成功，但每一种都有其自身的一些局限性。GAN模型由于其对抗性训练的性质，以潜在的不稳定的训练和较少的生成多样性而闻名。VAE依赖于代用损失。基于Flow的模型必须使用专门的架构来构建可逆变换。</p><h3 id="什么是Diffusion-Models"><a href="#什么是Diffusion-Models" class="headerlink" title="什么是Diffusion Models?"></a>什么是Diffusion Models?</h3><p><img src="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/DDPM.png" alt></p><p>如图所示，diffusion model通常包含俩个过程，从信号到噪声的正向过程/扩散过程(forward/diffusion process)以及从噪声逐步到信号的逆向/重建过程(reverse process)，这里主要介绍Google 的DDPM(Denoising Diffusion Probabilistic Model)，近些面出来的基于diffusion model的方法大都是基于DDPM的框架所设计。</p><h4 id="正向-扩散过程"><a href="#正向-扩散过程" class="headerlink" title="正向/扩散过程"></a>正向/扩散过程</h4><p>正向过程采用的是一个固定的Markov chain形式(每个时刻$t$只与$t-1$时刻有关)，即逐步地向图片中添加高斯噪声的过程：</p><script type="math/tex; mode=display">q\left(x_{t} \mid x_{t-1}\right)=\mathcal{N}\left(x_{t} ; \sqrt{1-\beta_{t}} x_{t-1}, \beta_{t} \mathbf{I}\right), q\left(x_{1: T} \mid x_{0}\right)=\prod_{t=1}^{T} q\left(x_{t} \mid x_{t-1}\right)</script><p>上面公式表示的是，$x_t$ 采样自 $x_{t-1}$并服从高斯分布$N(\sqrt{1-\beta_{t}},\beta_{t})$，在这个过程中随着$t$的增大，$x_t$ 越来越接近纯噪声，当$T \rightarrow \infty$ ，$x_T$ 是完全的高斯噪声(isotropic Gaussian distribution)。在实际应用中$\beta_{t}$是随着t增大而递增的，在DDPM中，$\beta_{t}$是预先设置的常量参数。在介绍diffusion的实现和推导过程中需要了解俩个重要特性：</p><blockquote><p><strong>特性1：重参数技巧（re-parameterization trick）</strong></p><p>如果我们要从某个分布中随机采样(高斯分布)一个样本，这个过程是无法反传梯度的。而这个通过高斯噪声采样得到$x_t$的过程在diffusion中到处都是，因此我们需要通过重参数技巧来使得他可微。最通常的做法是把随机性通过一个独立的随机变量$\epsilon$引导过去。举个例子，如果要从高斯分布$z \sim \mathcal{N}\left(z ; \mu_{\theta}, \sigma_{\theta}^{2} \mathbf{I}\right)$采样一个z，我们可以写成:$z=\mu_{\theta}+\sigma_{\theta} \odot \epsilon, \epsilon \sim \mathcal{N}(0, \mathbf{I})$；</p><p>上式的z依旧是有随机性的， 且满足均值$\mu_{\theta}$为方差为$\sigma_{\theta}^{2}$的高斯分布。这里的($\mu_{\theta}$,$\sigma_{\theta}^{2}$)可以是由参数 $\theta$ 的神经网络推断得到的。整个“采样”过程依旧梯度可导，随机性被转嫁到了$\epsilon$上。</p><p><strong>特性2：任意时刻的$x_t$可以有$x_0$和$\beta$ 表示</strong></p><p>将$\alpha_{t}=1-\beta_{t}$以及$\bar{\alpha}_{t}=\prod_{t=1}^{T} \alpha_{t}$ ，则可以得到：$q\left(x_{t} \mid x_{0}\right):=\mathcal{N}\left(x_{t} ; \sqrt{\bar{\alpha}_{t}} x_{0},\left(1-\bar{\alpha}_{t}\right) \boldsymbol{I}\right)$</p><p>该公式意味着我们可以直接获得任意程度的加噪图片，方便后续的训练。</p></blockquote><p>扩散过程有一个重要的特性是，可以直接采样任意时刻t下的加噪结果$x_t$。</p><p><a href="https://zhuanlan.zhihu.com/p/525106459" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/525106459</a></p><p><a href="https://zhuanlan.zhihu.com/p/449284962" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/449284962</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;扩散模型(Diffusion Model)是一种新的图像生成范式，有着与其他生成方法比如GAN、VAE以及Flow-based models不同而且有意思的特性。最近也有许多基于diffusion model的图像生成方法涌现出来，比如DALLE2、Imagen、GLIDE，    CogView1&amp;amp;2，本文主要依据&lt;a href=&quot;https://lilianweng.github.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Lil’Log&lt;/a&gt; 的博客进行翻译整理，方便学习记录。可以先看一下基于diffusion model的一些下游方法基于text-to-image图像生成结果。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="Diffusion" scheme="https://blog.nicehuster.cn/tags/Diffusion/"/>
    
  </entry>
  
  <entry>
    <title>Real-ESRGAN详解</title>
    <link href="https://blog.nicehuster.cn/2022/05/12/real-esrgan/"/>
    <id>https://blog.nicehuster.cn/2022/05/12/real-esrgan/</id>
    <published>2022-05-12T11:13:39.000Z</published>
    <updated>2022-06-06T08:58:10.581Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong><a href="https://arxiv.org/pdf/2107.10833.pdf" target="_blank" rel="noopener">Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data</a><br><strong>代码链接：</strong> <a href="https://github.com/xinntao/Real-ESRGAN" target="_blank" rel="noopener">https://github.com/xinntao/Real-ESRGAN</a><br><strong>整体信息：</strong> Real-ESRGAN目前超分算法中比较热门应用较广的算法，在了解该算法前，可以先了解一下该方法的一个发展历程，SRCNN-&gt;SRGAN-&gt;ESRGAN-&gt;Real-ESRGAN。</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/pdf/2107.10833.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt; &lt;a href=&quot;https://github.com/xinntao/Real-ESRGAN&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/xinntao/Real-ESRGAN&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt; Real-ESRGAN目前超分算法中比较热门应用较广的算法，在了解该算法前，可以先了解一下该方法的一个发展历程，SRCNN-&amp;gt;SRGAN-&amp;gt;ESRGAN-&amp;gt;Real-ESRGAN。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="multi-model" scheme="https://blog.nicehuster.cn/tags/multi-model/"/>
    
  </entry>
  
  <entry>
    <title>Mask Transfiner详解</title>
    <link href="https://blog.nicehuster.cn/2022/05/08/mask-transfiner/"/>
    <id>https://blog.nicehuster.cn/2022/05/08/mask-transfiner/</id>
    <published>2022-05-08T11:13:39.000Z</published>
    <updated>2022-05-09T10:11:58.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong><a href="https://arxiv.org/abs/2111.13673" target="_blank" rel="noopener">Mask Transfiner for High-Quality Instance Segmentation</a><br><strong>代码链接：</strong><a href="https://github.com/SysCV/transfiner" target="_blank" rel="noopener">https://github.com/SysCV/transfiner</a><br><strong>整体信息：</strong>这是ETH和港科大合作发表在CVPR2022上有关实例分割的论文，该论文中提出的Mask Transfiner通过引入 Incoherent Regions检测机制的方式，在不产生额外计算成本的情况下，有效地改善目标分割mask。在COCO，Cityscapes和BDD100K上均取得了明显的性能提升。</p><p><img src="/img/mask-transfiner.png" alt="img"></p><a id="more"></a><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p><img src="/img/mask-transfiner-motivation.png" alt="img"></p><p>现有的实例分割方法在高频区域的分割依旧比较粗糙，边缘像素点存在大量错分的情况，要获取精细化的mask，需要大分辨率的深度特征图来保留物体细节，与此同时，大尺寸会带来高计算量和内存消耗，因此如何在不显著提高计算成本的条件下，获取精细化mask极具挑战。</p><p>大致做法是，如上图所示，首先识别容易出错并需要优化的pixel区域（黄色区域）,在论文里面称Incoherent Regions，这些区域往往出现在物体边界或者高频区域。只需要对这些pixel区域进行优化就行，优化的具体措施下面详细介绍。当然作者也有分析，实际预测错误的所有pixel中，有43%是分布在Incoherent Regions，作者将这部分区域的pixel直接替换成gt，coco mask ap可以提升倒51.</p><p><img src="/img/mask-transfiner-property-incoherent-region.png" alt="img"></p><h4 id="Incoherent-Regions定义"><a href="#Incoherent-Regions定义" class="headerlink" title="Incoherent Regions定义"></a>Incoherent Regions定义</h4><p>实例分割的边缘错误很多是空间分辨率过低产生的，例如对物体标注mask的下采样、过小的RoI池化、和基于PCA/DCT的系数压缩等。这些分割方法把低分辨率特征作为输入，由于丢失了高分辨率图上的物体细节，使准确地分割物体细节非常困难。而Incoherent Regions可以很好的计算mask由于分辨率下降而导致信息丢失的区域。</p><p><img src="/img/mask-transfiner-incoherent-region.png" alt="img"></p><h3 id="Mask-Transfiner框架"><a href="#Mask-Transfiner框架" class="headerlink" title="Mask Transfiner框架"></a>Mask Transfiner框架</h3><p><img src="/img/mask-transfiner-pipeline.png" alt="img"></p><h4 id="1-Incoherent-Regions检测"><a href="#1-Incoherent-Regions检测" class="headerlink" title="1. Incoherent Regions检测"></a>1. Incoherent Regions检测</h4><p>Incoherent Regions的检测遵循由低到高的级联设计（cascaded design）。如图上图右侧，为了检测RoI金字塔上的不同层级上信息损失节点，Transfiner先将最低层的RoI特征（28x28）和初始的物体mask预测作为输入，采用一个简单的全卷积网络（四个3×3 卷积）预测四叉树的根节点。每个根结点会分解到临近更高RoI层对应的4个子节点，例如从RoI大小28x28延伸到56x56。对于高层的RoI特征，Transfiner对上一层损失区域检测的mask做上采样后与RoI特征拼接，并使用单个1×1卷积层预测更精细的信息损失节点，以保持检测模块的轻量化。</p><h4 id="2-四叉树构造"><a href="#2-四叉树构造" class="headerlink" title="2. 四叉树构造"></a>2. 四叉树构造</h4><p>四叉树的结构如上图的 Point Quadtree部分所示，来自低层级的RoI特征（如分辨率28×28）的信息损失节点在其相邻的更高层的RoI中（如分辨率56×56）中有四个对应子节点。为了减少计算量，我们只将预测为损失节点的像素点向上层进一步分解，并把树的最大深度设为3。更为具体地，我们把从最低层级（28x28）检测到的信息损失点作为根节点，从上到下递归扩展四个子象限点，构建了一个多层次的四叉树。在更高层的特征图上选取子象限点，是因为大尺度特征具有更高的分辨率和更多的物体局部细节。</p><h4 id="3-节点编码"><a href="#3-节点编码" class="headerlink" title="3. 节点编码"></a>3. 节点编码</h4><p>节点编码器（Node Encoder）使用四种不同的信息线索对每个四叉树节点进行编码：</p><ol><li>从 FPN 金字塔的相应位置和层级提取的细粒度深度特征。</li><li>初始检测器的粗略掩码预测提供高层的语义信息。</li><li>相对位置编码，补充节点在RoI中的距离相关性。</li><li>每个节点的周围临近点信息来补充局部细节。</li></ol><h4 id="4-编码和像素解码"><a href="#4-编码和像素解码" class="headerlink" title="4. 编码和像素解码"></a>4. 编码和像素解码</h4><p>四叉树节点经编码器编码后，为了建模点之间的关联，序列编码器(Sequence Encoder)中的多头注意力模块会对输入序列进行点之间的特征融合及更新。相较于MLP，Transformer可以执行序列上的全局跨尺度推理。序列编码器的每一层都由多头自注意力模块和全连接的前馈网络（FFN）组成。为了给输入序列补充足够的前景和背景信息，我们还将RoI金字塔中最低层大小为14x14的196个特征点输入。与标准Transformer解码器不同，Transfiner 的像素解码器(Pixel Decoder)是一个简单的两层 MLP，不具有多头注意力模块。它对树中每个节点的输出查询进行解码，以预测最终的实例标签。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>1.在coco数据集上的SOTA结果对比</p><p><img src="/img/mask-transfiner-coco-exp.png" alt="img"></p><ol><li>可视化分析比较</li></ol><p><img src="/img/mask-transfiner-com.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.13673&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Mask Transfiner for High-Quality Instance Segmentation&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/SysCV/transfiner&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/SysCV/transfiner&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这是ETH和港科大合作发表在CVPR2022上有关实例分割的论文，该论文中提出的Mask Transfiner通过引入 Incoherent Regions检测机制的方式，在不产生额外计算成本的情况下，有效地改善目标分割mask。在COCO，Cityscapes和BDD100K上均取得了明显的性能提升。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/mask-transfiner.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="instance segmentation" scheme="https://blog.nicehuster.cn/tags/instance-segmentation/"/>
    
  </entry>
  
  <entry>
    <title>增量目标检测方法 Faster ILOD</title>
    <link href="https://blog.nicehuster.cn/2022/03/24/Faster_ILOD/"/>
    <id>https://blog.nicehuster.cn/2022/03/24/Faster_ILOD/</id>
    <published>2022-03-24T11:13:39.000Z</published>
    <updated>2022-04-13T04:30:08.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/abs/2003.03901" target="_blank" rel="noopener">Faster ILOD: Incremental Learning for Object Detectors based on Faster RCNN</a><br><strong>代码链接：</strong><a href="https://github.com/CanPeng123/Faster-ILOD" target="_blank" rel="noopener">https://github.com/CanPeng123/Faster-ILOD</a><br><strong>整体信息：</strong>这是发表在PRL2020上的一篇文章关于增量目标检测的文章，作者是来自The University of Queensland，这篇文章基于Faster RCNN，使用multi-network 自适应蒸馏，设计了一种end2end的增量目标检测方法。</p><a id="more"></a><h2 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h2><p>增量目标检测包含S个增量steps，在每个增量step，训练数据只包含新类别Cn ，给定一个在旧类别 C0 上已训练好的目标检测模型，增量目标检测的任务是在新类别 Cn 数据上重新训练模型(一般是fine-tune)，同时维持模型在旧类别 C0 上的性能，在增量训练过程中，旧类别 C0 不可见。</p><p><img src="/img/faster_ilod_1.png" alt="img"></p><p>上图展示的是一个在VOC数据集上的增量目标检测的示例，模型首先在前15类训练，然后逐步增加每个类别。增量训练过程中，只提供当前新类别的标注，其他类别不可见。Normal Training是使用所有数据（旧数据和新数据）从头开始重新训练模型，该模型在测试集上的指标即为增量目标检测的指标上界。 Catastrophic forgetting是使用已训练好的旧类模型直接在新类数据上fine-tune的结果，可以看到在不断进行增量训练时，总体指标在逐渐下降，即出现了灾难性遗忘的问题( Catastrophic forgetting)。</p><h2 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h2><p><img src="/img/faster_ilod_method.png" alt="img"></p><p>作者使用Multi-network自适应蒸馏的方法来解决增量目标检测中出现的Catastrophic forgetting问题。具体做法如上图所示，上面是旧模型即teacher模型T，下面是新模型即student模型S，中间为蒸馏过程。</p><p>（1）<strong>特征蒸馏</strong>，所有feature map进行减均值归一化，然后使用L1 loss进行蒸馏。具体地，针对特征图上每个激活值大小进行比较，确定是否蒸馏，如下公式所示，</p><script type="math/tex; mode=display">\mathcal{L}_{F_{-} D i s t}=\frac{1}{\mathcal{M}} \sum \begin{cases}\left\|\tilde{f}_{t e}-\tilde{f}_{s t}\right\|_{1}, & \text { if } \tilde{f}_{t e}>\tilde{f}_{s t} \\ 0, & \text { otherwise }\end{cases}</script><p>（2）<strong>RPN蒸馏</strong>，同样地，使用T模型RPN的输出作为下界进行自适应蒸馏，使用的是L2 loss。</p><script type="math/tex; mode=display">\begin{aligned} &\mathcal{L}_{R P N_{-} D i s t}=\frac{1}{\mathcal{N}} \sum \begin{cases}\left\|q_{t e}-q_{s t}\right\|_{2}^{2}+\beta\left\|r_{t e}-r_{s t}\right\|_{2}^{2}, & \text { if } q_{t e}>q_{s t} \\ 0, & \text { otherwise }\end{cases} \\ &\text { where } \\ &\beta= \begin{cases}1, & \text { if } q_{t e}>\left(q_{s t}+\mathcal{T}\right) \\ 0, & \text { otherwise. }\end{cases} \end{aligned}</script><p>（3）<strong>RCNN蒸馏</strong>，具体做法和ILOD做法一致，在T模型中背景分数最小的128个ROI中随机选择64个proposal进行蒸馏，即只蒸馏背景信息，新类别不参与RCNN蒸馏，具体地，如下公式所示，</p><script type="math/tex; mode=display">\mathcal{L}_{R C N_{-} D i s t}=\frac{1}{\mathcal{K} \times C_{o}} \sum\left[\left\|\tilde{p}_{t e}-\tilde{p}_{s t}\right\|_{2}^{2}+\left\|t_{t e}-t_{s t}\right\|_{2}^{2}\right]</script><p>最后，总的loss为三者相加，</p><script type="math/tex; mode=display">\mathcal{L}_{\text {total }}=\mathcal{L}_{R C N N}+\lambda_{1} \mathcal{L}_{F_{-} \text {Dist }}+\lambda_{2} \mathcal{L}_{R P N_{-} \text {Dist }}+\lambda_{3} \mathcal{L}_{R C N_{-} \text {Dist }}</script><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ul><li>数据集(指标)：PASCAL VOC(AP@0.5)，COCO(mAP)</li><li>settings：one-step 和 multi-step</li><li><p>对比方法，ILOD</p></li><li><p>对比方法，ILOD</p></li></ul><p><img src="/img/faster_ilod_exp1.png" alt="img"></p><p><img src="/img/faster_ilod_exp2.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2003.03901&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Faster ILOD: Incremental Learning for Object Detectors based on Faster RCNN&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/CanPeng123/Faster-ILOD&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/CanPeng123/Faster-ILOD&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这是发表在PRL2020上的一篇文章关于增量目标检测的文章，作者是来自The University of Queensland，这篇文章基于Faster RCNN，使用multi-network 自适应蒸馏，设计了一种end2end的增量目标检测方法。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="Continual learning" scheme="https://blog.nicehuster.cn/tags/Continual-learning/"/>
    
  </entry>
  
  <entry>
    <title>增量目标检测方法调研</title>
    <link href="https://blog.nicehuster.cn/2022/03/21/IOD/"/>
    <id>https://blog.nicehuster.cn/2022/03/21/IOD/</id>
    <published>2022-03-21T11:13:39.000Z</published>
    <updated>2022-04-12T11:36:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近参加了CVPR2022 有关增量学习的一个Workshop：<a href="https://sites.google.com/view/clvision2022/overview" target="_blank" rel="noopener">Workshop on Continual Learning in Computer Vision</a>，这个workshop有三个赛道：(1)Instance Classification Track，(2)Category Detection Track；(3)Instance Detection Track，三个赛道的任务介绍可以看官网。个人是做检测方向出身，因此关注了一下track2，并基于增量目标检测方向做了部分调研，并简单介绍其中部分算法。</p><a id="more"></a><h3 id="增量目标检测方法调研"><a href="#增量目标检测方法调研" class="headerlink" title="增量目标检测方法调研"></a>增量目标检测方法调研</h3><p><img src="/img/iod_survey.png" alt="img"></p><h3 id="部分方法介绍"><a href="#部分方法介绍" class="headerlink" title="部分方法介绍"></a>部分方法介绍</h3><p><img src="/img/IOD_1.PNG" alt="IOD (1)"></p><p><img src="/img/IOD_2.PNG" alt="IOD (2)"></p><p><img src="/img/IOD_3.PNG" alt="IOD (3)"></p><p><img src="/img/IOD_4.PNG" alt="IOD (4)"></p><p><img src="/img/IOD_5.PNG" alt="IOD (5)"></p><p><img src="/img/IOD_6.PNG" alt="IOD (6)"></p><p><img src="/img/IOD_7.PNG" alt="IOD (7)"></p><p><img src="/img/IOD_8.PNG" alt="IOD (8)"></p><p><img src="/img/IOD_9.PNG" alt="IOD (9)"></p><p><img src="/img/IOD_10.PNG" alt="IOD (10)"></p><p><img src="/img/IOD_11.PNG" alt="IOD (11)"></p><p><img src="/img/IOD_12.PNG" alt="IOD (12)"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近参加了CVPR2022 有关增量学习的一个Workshop：&lt;a href=&quot;https://sites.google.com/view/clvision2022/overview&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Workshop on Continual Learning in Computer Vision&lt;/a&gt;，这个workshop有三个赛道：(1)Instance Classification Track，(2)Category Detection Track；(3)Instance Detection Track，三个赛道的任务介绍可以看官网。个人是做检测方向出身，因此关注了一下track2，并基于增量目标检测方向做了部分调研，并简单介绍其中部分算法。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="Continual learning" scheme="https://blog.nicehuster.cn/tags/Continual-learning/"/>
    
  </entry>
  
  <entry>
    <title>Panoptic Segmentation详解</title>
    <link href="https://blog.nicehuster.cn/2022/01/07/PSeg/"/>
    <id>https://blog.nicehuster.cn/2022/01/07/PSeg/</id>
    <published>2022-01-07T11:13:39.000Z</published>
    <updated>2022-04-12T04:35:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近在知乎上频繁地刷到有关Mask2Former地帖子，这么多人吹捧地必是精品，就跟一波风看了一下Mask2Former，顺带地也了解一下Panoptic Segmentation这个任务。众所周知，图像分割主要有两个方向：</p><ul><li><strong>语义分割（semantic segmentation）</strong>，常用来识别天空、草地、道路等没有固定形状的不可数<strong>事物（stuff）</strong>。语义分割的标记方法通常是给每个像素加上标签。</li><li><strong>实例分割（instance segmentation）</strong>，人、动物或工具等可数的、独立的明显物体<strong>（things）</strong>。实例分割通常用包围盒或分割掩码标记目标。</li></ul><p><strong>全景分割（Panoptic Segmentation）</strong>其实就是把这两个方向结合起来，生成统一的、全局的分割图像，既识别事物，也识别物体。</p><p><img src="/img/ps.png" alt="img"></p><a id="more"></a><p><strong>标记方法</strong></p><p>全景分割的标记方法结合了语义分割和实例分割，给每个像素加上标签$\left(l_{i}, z_{i}\right)$,其中i表示第i个像素，l表示语义类别，z表示实例ID。语义类别由两部分组成，事物类别$L^{ST}$和$L^{TH}$分别为stuff和thing的简写）。当$l_{i} \in L^{S T}$，忽略$z_i$（事物类别）；</p><p><strong>评估标准</strong></p><p>首先是常规的IoU &gt; 0.5，然后结合TF、FN、FP搞出了一个PQ标准。（PQ是Panoptic Quality，即全景质量的简称。)</p><p><img src="/img/metric.png" alt="img"></p><p>PQ的具体公式为：</p><script type="math/tex; mode=display">\mathrm{PQ}=\frac{\sum_{(p, g) \in T P} \operatorname{IoU}(p, g)}{|T P|+\frac{1}{2}|F P|+\frac{1}{2}|F N|}</script><p>另外，PQ可以分解为<strong>分割质量（segmentation quality，SQ）</strong>和<strong>识别质量（recognition quality，RQ）</strong>的乘积，便于进一步评估分割和识别环节的表现。</p><script type="math/tex; mode=display">\mathrm{PQ}=\underbrace{\frac{\sum_{(p, g) \in T P} \operatorname{IoU}(p, g)}{|T P|}}_{\text {segmentation quality }(\mathrm{SQ})} \times \underbrace{\frac{|T P|}{|T P|+\frac{1}{2}|F P|+\frac{1}{2}|F N|}}_{\text {recognition quality }(\mathrm{RQ})}</script><p><strong>数据集</strong></p><p>全景分割数据集需要既有语义分割标注，也有实例分割标注。</p><ul><li>Cityscapes(19classes)：5000张街景图片，97%的图片有像素标注，共有19个类别，其中8个类别符合语义分割的特征；</li><li>ADE20k(150classes)：图像总量超过25000张，并经过公开标注。其中包括100种物体和59种事物。</li><li>Mapillary Vistas(65classes)：25000张分辨率不同的街景照片。其中98%的图片都经过了像素标注，涵盖28种事物与37种物体。</li><li>COCO：知名数据集COCO最近加入了全景分割标注。</li></ul><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h4 id="1-MaskFormer"><a href="#1-MaskFormer" class="headerlink" title="1.MaskFormer"></a>1.<strong>MaskFormer</strong></h4><p>Per-Pixel Classification is Not All You Need for Semantic Segmentation</p><p>篇文章提出了一个新的分割输出端范式，传统方法会将每个像素点预测成一种类别来完成分割任务，而本文则是会输出很多个二分类分割图，如下图所示：</p><p><img src="/img/mask_cls.png" alt="img"></p><p>上图左侧为传统方法，右侧为本文方法，不是只输出 K 个类别二值分类图，而是提前设定一个较大值，同时与分类图一同预测的还有一个预测类别，这个类别可以为空，即该二值分类图没有用。因此损失函数由两个组成，一个是预测分类图与真值图的损失，另一个是预测类别的交叉熵损失.</p><script type="math/tex; mode=display">\mathcal{L}_{\text {mask-cls }}\left(z, z^{\mathrm{gt}}\right)=\sum_{j=1}^{N}\left[-\log p_{\sigma(j)}\left(c_{j}^{\mathrm{gt}}\right)+\mathbb{1}_{c_{j}^{\mathrm{gt}} \neq \varnothing} \mathcal{L}_{\text {mask }}\left(m_{\sigma(j)}, m_{j}^{\mathrm{gt}}\right)\right]</script><p>匹配策略上依旧采用匈牙利匹配，匹配成本：</p><script type="math/tex; mode=display">-p_{i}\left(c_{j}^{\mathrm{gt}}\right)+\mathcal{L}_{\text {mask }}\left(m_{i}, m_{j}^{\mathrm{gt}}\right)</script><p>其中，$L_{mask}$为二类交叉熵损失。MaskFormer具体结构如下：</p><p><img src="/img/maskformer.png" alt="img"></p><h4 id="2-Mask2Former"><a href="#2-Mask2Former" class="headerlink" title="2.Mask2Former"></a>2.Mask2Former</h4><p>Masked-attention Mask Transformer for Universal Image Segmentation</p><p>一个model去建模所有的分割任务：语义分割，实例分割和以及全景分割。一个模型取得三个不同分割任务STOA.。具体而言本文方法沿用了上一篇的分割图分别产生方式，另外有两个主要创新点，包括每个transformer decoder层都会使用pixel-decoder的金字塔对应结构，以及 Mask Attention 形式。可参考下图：</p><p><img src="/img/mask2former.png" alt="img"></p><p>因此，作者基于上述的问题和最初的motivation(一个模型取得三个不同分割任务STOA)，提出了几个改进，节省训练的时间(MaskFormer训练需要300epoch)和cost的同时能够提升性能。</p><p><strong>第一个改进</strong>：Mask Attention加速收敛,相比于之前的cross attention，这个里面的attention affinity是一种稀疏的attention，其实就是将上一层预测的分割图使用阈值0.5转换成[0,1]mask图，将转换后的mask进一步转换成[-inf,0]，然后和原始att相加，过softmax得到att_mask，相当于不计算原始分割图中为0的区域att.</p><p>standard cross-attention:  $\mathbf{X}_{l}=\operatorname{softmax}\left(\mathbf{Q}_{l} \mathbf{K}_{l}^{\mathrm{T}}\right) \mathbf{V}_{l}+\mathbf{X}_{l-1}$</p><p> masked cross-attention: </p><script type="math/tex; mode=display">\mathbf{X}_{l}=\operatorname{softmax}\left(\mathcal{M}_{l-1}+\mathbf{Q}_{l} \mathbf{K}_{l}^{\mathrm{T}}\right) \mathbf{V}_{l}+\mathbf{X}_{l-1}</script><script type="math/tex; mode=display">\mathcal{M}_{l-1}(x, y)=\left\{\begin{array}{ll}0 & \text { if } \mathbf{M}_{l-1}(x, y)=1 \\-\infty & \text { otherwise }\end{array} .\right.</script><p>具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiScaleMaskedTransformerDecoder</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">       ...</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask_features, mask = None)</span>:</span></span><br><span class="line">       ...</span><br><span class="line">       outputs_class, outputs_mask, attn_mask = self.forward_prediction_heads(output, mask_features, attn_mask_target_size=size_list[<span class="number">0</span>])</span><br><span class="line">       <span class="comment">#print(outputs_class.shape,outputs_mask.shape,attn_mask.shape)</span></span><br><span class="line">       predictions_class.append(outputs_class)</span><br><span class="line">       predictions_mask.append(outputs_mask)</span><br><span class="line"></span><br><span class="line">       <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_layers):</span><br><span class="line">           level_index = i % self.num_feature_levels</span><br><span class="line">           attn_mask[torch.where(attn_mask.sum(<span class="number">-1</span>) == attn_mask.shape[<span class="number">-1</span>])] = <span class="keyword">False</span></span><br><span class="line">           <span class="comment"># attention: cross-attention first</span></span><br><span class="line">           output = self.transformer_cross_attention_layers[i](</span><br><span class="line">               output, src[level_index],</span><br><span class="line">               memory_mask=attn_mask,</span><br><span class="line">               memory_key_padding_mask=<span class="keyword">None</span>,  <span class="comment"># here we do not apply masking on padded region</span></span><br><span class="line">               pos=pos[level_index], query_pos=query_embed</span><br><span class="line">           )</span><br><span class="line"></span><br><span class="line">           output = self.transformer_self_attention_layers[i](</span><br><span class="line">               output, tgt_mask=<span class="keyword">None</span>,</span><br><span class="line">               tgt_key_padding_mask=<span class="keyword">None</span>,</span><br><span class="line">               query_pos=query_embed</span><br><span class="line">           )</span><br><span class="line"></span><br><span class="line">           <span class="comment"># FFN</span></span><br><span class="line">           output = self.transformer_ffn_layers[i](</span><br><span class="line">               output</span><br><span class="line">           )</span><br><span class="line"></span><br><span class="line">           outputs_class, outputs_mask, attn_mask = self.forward_prediction_heads(output, mask_features, attn_mask_target_size=size_list[(i + <span class="number">1</span>) % self.num_feature_levels])</span><br><span class="line">           predictions_class.append(outputs_class)</span><br><span class="line">           predictions_mask.append(outputs_mask)</span><br><span class="line">           ...</span><br><span class="line">           </span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward_prediction_heads</span><span class="params">(self, output, mask_features, attn_mask_target_size)</span>:</span></span><br><span class="line">       decoder_output = self.decoder_norm(output)</span><br><span class="line">       decoder_output = decoder_output.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">       outputs_class = self.class_embed(decoder_output)</span><br><span class="line">       mask_embed = self.mask_embed(decoder_output)</span><br><span class="line">       outputs_mask = torch.einsum(<span class="string">"bqc,bchw-&gt;bqhw"</span>, mask_embed, mask_features)</span><br><span class="line"></span><br><span class="line">       <span class="comment"># <span class="doctag">NOTE:</span> prediction is of higher-resolution</span></span><br><span class="line">       <span class="comment"># [B, Q, H, W] -&gt; [B, Q, H*W] -&gt; [B, h, Q, H*W] -&gt; [B*h, Q, HW]</span></span><br><span class="line">       attn_mask = F.interpolate(outputs_mask, size=attn_mask_target_size, mode=<span class="string">"bilinear"</span>, align_corners=<span class="keyword">False</span>)</span><br><span class="line">       <span class="comment">#print(outputs_mask.shape,attn_mask.shape)</span></span><br><span class="line">       <span class="comment"># must use bool type</span></span><br><span class="line">       <span class="comment"># If a BoolTensor is provided, positions with ``True`` are not allowed to attend while ``False`` values will be unchanged.</span></span><br><span class="line">       attn_mask = (attn_mask.sigmoid().flatten(<span class="number">2</span>).unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, self.num_heads, <span class="number">1</span>, <span class="number">1</span>).flatten(<span class="number">0</span>, <span class="number">1</span>) &lt; <span class="number">0.5</span>).bool()</span><br><span class="line">       attn_mask = attn_mask.detach()</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> outputs_class, outputs_mask, attn_mask</span><br></pre></td></tr></table></figure><p><strong>第二个改进是多尺度特征改善小目标分割</strong>，对应于pixel-decoder，作者使用了类似于Deformable DETR decoder端的设置，在decoder端采用了multi scale的特征输入做attention。这个步骤对于提升small object的segmentation帮助很大。具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MSDeformAttnPixelDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_features</span><span class="params">(self, features)</span>:</span> <span class="comment">#[res3,res4,res5]</span></span><br><span class="line">        srcs = []</span><br><span class="line">        pos = []</span><br><span class="line">        <span class="comment"># Reverse feature maps into top-down order (from low to high resolution)</span></span><br><span class="line">        <span class="keyword">for</span> idx, f <span class="keyword">in</span> enumerate(self.transformer_in_features[::<span class="number">-1</span>]):</span><br><span class="line">            x = features[f].float()  <span class="comment"># deformable detr does not support half precision</span></span><br><span class="line">            srcs.append(self.input_proj[idx](x))</span><br><span class="line">            pos.append(self.pe_layer(x))</span><br><span class="line"></span><br><span class="line">        y, spatial_shapes, level_start_index = self.transformer(srcs, pos) <span class="comment">#MSDeformAttnTransformerEncoderOnly</span></span><br><span class="line"></span><br><span class="line">        bs = y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        split_size_or_sections = [<span class="keyword">None</span>] * self.transformer_num_feature_levels</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.transformer_num_feature_levels):</span><br><span class="line">            <span class="keyword">if</span> i &lt; self.transformer_num_feature_levels - <span class="number">1</span>:</span><br><span class="line">                split_size_or_sections[i] = level_start_index[i + <span class="number">1</span>] - level_start_index[i]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                split_size_or_sections[i] = y.shape[<span class="number">1</span>] - level_start_index[i]</span><br><span class="line">        y = torch.split(y, split_size_or_sections, dim=<span class="number">1</span>) [x3,x4,x5]</span><br><span class="line">        </span><br><span class="line">        out = []</span><br><span class="line">        multi_scale_features = []</span><br><span class="line">        num_cur_levels = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, z <span class="keyword">in</span> enumerate(y):</span><br><span class="line">            zz = z.transpose(<span class="number">1</span>, <span class="number">2</span>).view(bs, <span class="number">-1</span>, spatial_shapes[i][<span class="number">0</span>], spatial_shapes[i][<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            out.append(zz)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># append `out` with extra FPN levels</span></span><br><span class="line">        <span class="comment"># Reverse feature maps into top-down order (from low to high resolution)</span></span><br><span class="line">        <span class="keyword">for</span> idx, f <span class="keyword">in</span> enumerate(self.in_features[:self.num_fpn_levels][::<span class="number">-1</span>]):</span><br><span class="line"></span><br><span class="line">            x = features[f].float()</span><br><span class="line"></span><br><span class="line">            lateral_conv = self.lateral_convs[idx]</span><br><span class="line">            output_conv = self.output_convs[idx]</span><br><span class="line">            cur_fpn = lateral_conv(x)</span><br><span class="line">            <span class="comment"># Following FPN implementation, we use nearest upsampling here</span></span><br><span class="line">            y = cur_fpn + F.interpolate(out[<span class="number">-1</span>], size=cur_fpn.shape[<span class="number">-2</span>:], mode=<span class="string">"bilinear"</span>, align_corners=<span class="keyword">False</span>)</span><br><span class="line">            y = output_conv(y)</span><br><span class="line">            out.append(y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> o <span class="keyword">in</span> out:</span><br><span class="line">            <span class="keyword">if</span> num_cur_levels &lt; self.maskformer_num_feature_levels:</span><br><span class="line">                multi_scale_features.append(o)</span><br><span class="line">                num_cur_levels += <span class="number">1</span>      </span><br><span class="line">        <span class="keyword">return</span> self.mask_features(out[<span class="number">-1</span>]), out[<span class="number">0</span>], multi_scale_features</span><br></pre></td></tr></table></figure><p>此外，文章还额外提出了三个小改进：</p><ul><li>将 Self 和 Cross Attention 的顺序做一个变换，让 Cross 在前面，因为在图像特征没加入计算时自身做 Self 效率会较低；</li><li>文章将 Transformer 解码器的初始序列设置为可学版本；</li><li>整个模型将不使用 dropout 操作。</li><li>使用point-rend head 改善分割边界质量，同时降低显存；</li></ul><p>mask2former/modeling/meta_arch/mask_former_head.py </p><p><strong>backbone:res50</strong></p><p>800x800(short size=800) —&gt;res2[1, 256, 200, 200],res3[1, 512, 100, 100],res4[1, 1024, 50, 50],res5[1, 2048, 25, 25],</p><p><strong>pixel decoder:MSDeformAttnPixelDecoder</strong>(6 layers)</p><p> res3,res4,res5—&gt;MSDeformAttnTransformerEncoderOnly—&gt;x1[1, 256, 25, 25],x2[1, 256, 50, 50],x3[1, 256, 100, 100],</p><p>x1,x2,x3+FPN(res2) —-&gt;x1[1, 256, 25, 25],x2[1, 256, 50, 50],x3[1, 256, 100, 100],x4[1, 256, 200, 200]</p><p> res3,res4,res5—&gt;  <strong>mask_features</strong>:conv[x4],<strong>transformer_encoder_features</strong>:x4,<strong>multi_scale_features</strong>:[x1,x2,x3]</p><p><strong>Transformer decoder:MultiScaleMaskedTransformerDecoder</strong></p><p>input：multi_scale_features,mask_features </p><p>query_feat(100x256)，mask_features[1, 256, 200, 200]  —&gt;forward_prediction_heads—&gt;outputs_class[1, 100, 134],outputs_mask[1, 100, 200, 200],attn_mask[8, 100, 625]，attn_mask是有outputs_mask插值降采样得到</p><h4 id="3-PointRend"><a href="#3-PointRend" class="headerlink" title="3.PointRend"></a>3.PointRend</h4><p>文中使用了各种术语，比如rend、subdivision、ray-tracing，是想说明一个问题: 图像是对真实目标的一个离散化表达，真实目标的一些属性，如区域联通性、边缘连续性，在图像中同样存在。那么分割问题就可以看作预测一个真实目标在离散化后的图像中所占的区域，即：point-wise label prediction（点对点分类）。连续性体现在：图像中的像素可以通过插值得到与真实目标一一对应的坐标点。分割是在离散化的网格区域点对点的分类，但是有些点很难分类的准确，这些点大部分处在目标的边缘。pointrend方法的提出是对这些模糊分割的点，做更进一步的预测，即：精细分割。主要分成3步，如下图所示：1）候选点（或模糊分割点）选取；2）点特征提取；3）点分割预测。</p><p><img src="/img/pointRend.png" alt="img"></p><h5 id="1-候选点选取"><a href="#1-候选点选取" class="headerlink" title="1.候选点选取"></a><strong>1.候选点选取</strong></h5><p>候选点选取在训练和测试过程是不一样的，其中推断过程是通过迭代的方式从一个低分辨率的分割图像得到一个高分辨率的图像，这种方式不适合训练过程中的梯度反传，故采用另一种方式。</p><p><strong>Point select for training</strong></p><ul><li>随机生成kN个点，其中k&gt;1。</li><li>估计这kN个点的不确定程度，并根据不确定程度，筛选前βN个点，β取值范围为[0, 1]。其中不确定程度的估计方式与推断过程估计方式相同，使用低分辨率的分割置信度。</li><li>在剩余的点中，均匀采用得到（1-β）N个点。</li></ul><p><img src="/img/point_select_train.png" alt="img"></p><p>在PointRend中，采用了k=3和$\beta=0.75$的采样策略参数，采样得到$14^{2}$个点；从 coarse prediction 中插值的 GT 类别的概率和 0.5 之间的距离作为 point-wise 不确定性度量.</p><p><strong>Point select for inference</strong></p><p>推断过程是迭代进行的，具体过程如下图所示，首先通过上采样，将模型直接输出的低分辨率的分割图的长宽各扩大2倍得到高分辨的分割图，如从4x4的特征图上采样得到8x8的特征图；然后在高分辨分割图中，筛选N个分割模糊的点，即分割置信度（若置信度区间为[0,1]）在0.5左右的点，如在下图8x8的高分辨率分割图中，点集中在边缘附近。这N个点为最终筛选出来进行再次确认的点。以此类推，逐步迭代，得到最终目标分辨率的分割图。</p><p><img src="/img/subdivision.png" alt="img"></p><p>在PointRend中，对于预测类别 c 的矩形框，除非特别说明，均采用 adaptive subdivision 来通过 5 steps 将 7x7 coarse 预测精细化到 224x224. 每一次迭代，选择并更新 $N=28^2$个基于预测值和 0.5 之间的差异性距离得到最不确定的点.</p><h5 id="2-点特征提取"><a href="#2-点特征提取" class="headerlink" title="2.点特征提取"></a><strong>2.点特征提取</strong></h5><p>点特征提取包括fine-grained特征和coarse特征，其中coarse为K维，来自低分辨的分割mask，fine-grained特征来自原cnn backbone某个stage或者多个stage的组合特征。文中使用P2层的卷积特征作为fine-grained特征提取的特征图，具体流程如下图。</p><p><img src="/img/pointRend-ppl.png" alt="img"></p><h5 id="3-点分割预测"><a href="#3-点分割预测" class="headerlink" title="3. 点分割预测"></a>3. 点分割预测</h5><p>如上图所示紫色箭头，得到候选点的特征表达后，经过一组MLP，来得到最后的N个点的分割预测结果。其中，在实验中使用了4个全连接层，其中<strong>每个全连接层的输出，都联合coarse feature</strong>，作为下一个全连接层的输入。</p><h5 id="5-Results"><a href="#5-Results" class="headerlink" title="5. Results"></a>5. Results</h5><p><img src="/img/pointRend-res.png" alt="img"></p><p>其中左边一列图为mask rcnn的结果，右边一列图为PointRend的结果，从视觉效果看，PointRend对物体边缘描述更加精细化。</p><h4 id="4-Panoptic-SegFormer"><a href="#4-Panoptic-SegFormer" class="headerlink" title="4.Panoptic SegFormer"></a>4.Panoptic SegFormer</h4><p> Delving Deeper into Panoptic Segmentation with Transformers</p><p><img src="/img/PSFormer-ppl.png" alt="img"></p><p>该结构与DETR类似，不同之处在于：</p><p>（1）backbone使用多尺度特征(C3,C4,C5);</p><p>（2）针对decoder中query做了进一步精细划分,解耦location和Mask。针对Thing Query使用location Decoder捕获thing类别位置信息对Thing Query进行refine；此后使用refine后的Thing Query和Stuff Query作为Mask Decoder 输出mask结果。其中location Decoders使用bbox信息辅助监督，可以加速网络收敛；</p><p>（3）后处理部分，使用Mask-wise merge策略融合things和stuff获取最终的mask结果.</p><p>下面详细讲一下location decoder、mask decoder和mask-wise merge部分。</p><p><strong>Location Decoder</strong></p><p>给定N个初始化queries，训练阶段，在location decoder后面添加一个辅助MLP来预测位置和尺寸，location decoder的输出称为location-aware queries；推理阶段，去除辅助MLP。这一个辅助loss，可以帮助网络快速收敛，每个query关注区域指向性更明确。</p><p><strong>Mask Decoder</strong></p><p>mask decoder将location decoder的输出location-wise queries当作query，和MaskFormer预测mask和类别不同的是，Panoptic SegFormer预测mask需要先将attention map拆分成A3，A4，A5，然后都上采样到H/8xW/8的分辨率，concat在一起得到A_fuse，最后通过1x1卷积得到mask预测结果。</p><p><strong>Mask-wise merge</strong></p><p><img src="/img/mask-wise-merge.png" alt="img"></p><p>之前的分割去重，一般都是使用pixel-wise argmax策略，也就是重叠部分保留预测分数最大的类别。本文提出的mask-wise merge策略，对于重叠部分进行舍弃，上图是伪代码。</p><p>很喜欢作者在conclusion里面提到的一句话， Given the similarities and differences among the various segmentation tasks, “seek common ground while reserving differences” is a more reasonable guiding ideology.  完全的统一框架不见得是最好的选择，“求同存异”才是一个更合理的指导思想。</p><p>代码链接：<a href="https://github.com/zhiqi-li/Panoptic-SegFormer" target="_blank" rel="noopener">https://github.com/zhiqi-li/Panoptic-SegFormer</a></p><p>结果复现：</p><div class="table-container"><table><thead><tr><th>Method</th><th>PQ</th><th>SQ</th><th>RQ</th><th>N</th></tr></thead><tbody><tr><td>All(paper)</td><td>49.600</td><td>81.600</td><td>59.900</td><td>133</td></tr><tr><td>All(rep)</td><td>49.900</td><td>81.500</td><td>60.200</td><td>133</td></tr></tbody></table></div><h4 id="5-K-Net"><a href="#5-K-Net" class="headerlink" title="5. K-Net"></a>5. K-Net</h4><p>K-Net: Towards Unified Image Segmentation</p><p><img src="/img/knet-simple.png" alt="img"></p><p>K-Net的目标也是在于统一实例分割和语义分割。如上图所示，语义分割核心结构就是由一组kernel来负责语义mask的生成，让kernel数量与数据集类别数据保持一致，每个kernel负责一个固定类别masker的生成。受此启发，在实例分割中，可以通过同样方式引入一组卷积核来负责 mask 的生成，限定一个 kernel 只分割一个物体，每个kernel负责分割不同的物体，实例分割任务统一到一个框架内。</p><p><strong>Group-Aware Kernels</strong></p><p>理论上一组instance kernel就可以得到实例分割结果，但实验结果却相差甚远，与sem seg 相比，ins seg需要的kernel要求更高：</p><p>(1)在sem seg中，每个单独的sem kernel与类别(sem class)是绑定的，在每张图上都可以学习分割同一个类别，而ins seg不具备，而是通过Bipartite matching 来做的 target assignment,这导致每个kernel在每张图上学习的目标是根据当前的预测情况动态分配的。</p><p>(2)ins kernel需要区分appearence和scale变化的物体，需要具备更强的判别特性；</p><p>基于此，作者设计了<strong>Kernel Update Head</strong> 基于 mask 和特征图来将 kernel 动态化；如下图所示，Kernel Update Head 首先获得每个 kernel 对应 pixel group 的 feature，然后以某种方式动态地更新当前kernel。</p><p><img src="/img/kernel-update-head.png" alt="img"></p><p>此外，为了使得kernel可以modeling全局信息，作者还新增kernel interaction模块，最终得到的特征可用于class prediction, dynamic kernels 和mask predictions. </p><p><img src="/img/knet-ppl.png" alt="img"></p><p>为了得到更精细化的mask，可以通过叠加多个Kernel Update Head对mask和kernel进行迭代式refine.最终K-Net pipeline如上图所示，在论文中使用了3个Kernel Update Head和100个ins kernel.</p><p>注：在COCO-Panoptic上多尺度训练36epoch，训练一个K-Net，使用16张V1100需要两天半，在两台机器的情况下，训练时间有点长。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在知乎上频繁地刷到有关Mask2Former地帖子，这么多人吹捧地必是精品，就跟一波风看了一下Mask2Former，顺带地也了解一下Panoptic Segmentation这个任务。众所周知，图像分割主要有两个方向：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;语义分割（semantic segmentation）&lt;/strong&gt;，常用来识别天空、草地、道路等没有固定形状的不可数&lt;strong&gt;事物（stuff）&lt;/strong&gt;。语义分割的标记方法通常是给每个像素加上标签。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实例分割（instance segmentation）&lt;/strong&gt;，人、动物或工具等可数的、独立的明显物体&lt;strong&gt;（things）&lt;/strong&gt;。实例分割通常用包围盒或分割掩码标记目标。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;全景分割（Panoptic Segmentation）&lt;/strong&gt;其实就是把这两个方向结合起来，生成统一的、全局的分割图像，既识别事物，也识别物体。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/ps.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="transformer" scheme="https://blog.nicehuster.cn/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>自监督学习--基于contrastive learning方法</title>
    <link href="https://blog.nicehuster.cn/2021/12/13/self-sup-contrastive-learning/"/>
    <id>https://blog.nicehuster.cn/2021/12/13/self-sup-contrastive-learning/</id>
    <published>2021-12-13T11:13:39.000Z</published>
    <updated>2022-04-23T13:38:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>基于contrastive learning的自监督学习方法从2020-2021年涌现了许多工作，这里不一一列举，但是会简单介绍一些高引用的方法：MoCo，SimCLR，MoCov2，BYOL，SwAV，SimSiam，MoCov3。</p><p><img src="/img/contrastive learning.svg" alt="contrastive learning"></p><a id="more"></a><h3 id="1-MoCo-CVPR2020"><a href="#1-MoCo-CVPR2020" class="headerlink" title="1. MoCo(CVPR2020)"></a><a href="https://arxiv.org/abs/1911.05722" target="_blank" rel="noopener">1. MoCo(CVPR2020)</a></h3><p><img src="/img/MoCo.svg" alt="MoCo"></p><h3 id="2-SimCLR-ICML2020"><a href="#2-SimCLR-ICML2020" class="headerlink" title="2. SimCLR(ICML2020)"></a><a href="https://arxiv.org/abs/2002.05709" target="_blank" rel="noopener">2. SimCLR(ICML2020)</a></h3><p><img src="/img/SimCLR.svg" alt="SimCLR"></p><h3 id="3-MoCov2"><a href="#3-MoCov2" class="headerlink" title="3. MoCov2"></a><a href="https://arxiv.org/abs/2003.04297" target="_blank" rel="noopener">3. MoCov2</a></h3><p><img src="/img/MoCov2.svg" alt="MoCov2"></p><h3 id="4-BYOL-NIPS2020"><a href="#4-BYOL-NIPS2020" class="headerlink" title="4. BYOL(NIPS2020)"></a><a href="https://arxiv.org/abs/2006.07733" target="_blank" rel="noopener">4. BYOL(NIPS2020)</a></h3><p>待更新。。。</p><h3 id="5-SwAV-NIPS2020"><a href="#5-SwAV-NIPS2020" class="headerlink" title="5. SwAV(NIPS2020)"></a><a href="https://arxiv.org/abs/2006.07733" target="_blank" rel="noopener">5. SwAV(NIPS2020)</a></h3><p>待更新。。。</p><h3 id="6-SimSiam-CVPR2021"><a href="#6-SimSiam-CVPR2021" class="headerlink" title="6. SimSiam(CVPR2021)"></a><a href="https://arxiv.org/abs/2011.10566" target="_blank" rel="noopener">6. SimSiam(CVPR2021)</a></h3><p>待更新。。。</p><h3 id="7-MoCov3-ICCV2021"><a href="#7-MoCov3-ICCV2021" class="headerlink" title="7.MoCov3(ICCV2021)"></a><a href="https://arxiv.org/abs/2104.02057" target="_blank" rel="noopener">7.MoCov3(ICCV2021)</a></h3><p>待更新。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;基于contrastive learning的自监督学习方法从2020-2021年涌现了许多工作，这里不一一列举，但是会简单介绍一些高引用的方法：MoCo，SimCLR，MoCov2，BYOL，SwAV，SimSiam，MoCov3。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/contrastive learning.svg&quot; alt=&quot;contrastive learning&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="self-supervised" scheme="https://blog.nicehuster.cn/tags/self-supervised/"/>
    
  </entry>
  
  <entry>
    <title>自监督学习--基于pretext-task方法</title>
    <link href="https://blog.nicehuster.cn/2021/12/10/self-sup-pretext-task/"/>
    <id>https://blog.nicehuster.cn/2021/12/10/self-sup-pretext-task/</id>
    <published>2021-12-10T11:13:39.000Z</published>
    <updated>2022-04-23T13:11:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>自从2019年MoCo横空出世，掀起了一股自监督学习的浪潮，随后SimCLR,MoCo,BYOL,SwAV等一系列优秀的工作被提出，2021年底，何凯明的MAE更是将自监督学习带到另外一个高度。自监督学习的背后一个强大的动机就是，打破目前神经网络训练对于标注数据的依赖，即使在没有标注数据的情况下，也可以高效的训练网络。自监督学习的核心在于合理构建有利于模型学习的任务，其大致可分为三类：</p><p><img src="/img/self-sup-categories.png" alt="img"></p><a id="more"></a><p>基于pretext task的自监督学习方法大概有四篇经典的工作：</p><h3 id="1-Relative-Location-CVPR2015"><a href="#1-Relative-Location-CVPR2015" class="headerlink" title="1. Relative Location(CVPR2015)"></a>1. <a href="https://arxiv.org/abs/1505.05192" target="_blank" rel="noopener">Relative Location(CVPR2015)</a></h3><p><img src="/img/self-sup-rl.png" alt="img"></p><h3 id="2-Colorization-ECCV2016"><a href="#2-Colorization-ECCV2016" class="headerlink" title="2. Colorization(ECCV2016)"></a>2. <a href="https://arxiv.org/abs/1603.08511" target="_blank" rel="noopener">Colorization(ECCV2016)</a></h3><p><img src="/img/self-sup-color.png" alt="img"></p><h3 id="3-Context-Encoders（CVPR2016）"><a href="#3-Context-Encoders（CVPR2016）" class="headerlink" title="3. Context Encoders（CVPR2016）"></a>3. <a href="https://arxiv.org/abs/1604.07379" target="_blank" rel="noopener">Context Encoders（CVPR2016）</a></h3><p><img src="/img/self-sup-ce.png" alt="img"></p><h3 id="4-Rotation-Prediction-ICLR2018"><a href="#4-Rotation-Prediction-ICLR2018" class="headerlink" title="4. Rotation Prediction(ICLR2018)"></a>4. <a href="https://arxiv.org/abs/1803.07728" target="_blank" rel="noopener">Rotation Prediction(ICLR2018)</a></h3><p><img src="/img/self-sup-rp.png" alt="img"></p><p>从上面4种方法可以看出，基于pretext task的自监督学习方法都具备俩个特点：</p><blockquote><ol><li>良好的任务定义，比如预测旋转角度，或者相对位置</li><li>合理的限制条件，避免模型出现无效解</li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;自从2019年MoCo横空出世，掀起了一股自监督学习的浪潮，随后SimCLR,MoCo,BYOL,SwAV等一系列优秀的工作被提出，2021年底，何凯明的MAE更是将自监督学习带到另外一个高度。自监督学习的背后一个强大的动机就是，打破目前神经网络训练对于标注数据的依赖，即使在没有标注数据的情况下，也可以高效的训练网络。自监督学习的核心在于合理构建有利于模型学习的任务，其大致可分为三类：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/self-sup-categories.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="self-supervised learning" scheme="https://blog.nicehuster.cn/tags/self-supervised-learning/"/>
    
  </entry>
  
  <entry>
    <title>Transformer 杂记</title>
    <link href="https://blog.nicehuster.cn/2021/12/05/transformer-soup/"/>
    <id>https://blog.nicehuster.cn/2021/12/05/transformer-soup/</id>
    <published>2021-12-05T11:13:39.000Z</published>
    <updated>2022-04-12T11:39:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>近期断断续续的看了一些transformer相关的paper，看的比较杂，有些是对应领域比较有代表性地工作。偷个懒就不详细介绍每篇Paper，简单地记录一下这些paper大致要解决地问题。</p><a id="more"></a><h4 id="1-MAE-Masked-Autoencoders-Are-Scalable-Vision-Learners"><a href="#1-MAE-Masked-Autoencoders-Are-Scalable-Vision-Learners" class="headerlink" title="1. MAE:Masked Autoencoders Are Scalable Vision Learners"></a>1. MAE:Masked Autoencoders Are Scalable Vision Learners</h4><p>自监督学习方法，核心思想是以一定比例随机 mask 掉图片中的一些图像块(patch)然后重建这些部分的像素值</p><h4 id="2-SegFormer-Simple-and-Efficient-Design-for-Semantic-Segmentation-with-Transformers"><a href="#2-SegFormer-Simple-and-Efficient-Design-for-Semantic-Segmentation-with-Transformers" class="headerlink" title="2.SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers"></a>2.SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</h4><p>设计多层次backbone MIT，丢弃PE，优化self-att加速推理，此外从SETR痛点出发设计轻量级MLP解码器</p><h4 id="3-Early-Convolutions-Help-Transformers-See-Better"><a href="#3-Early-Convolutions-Help-Transformers-See-Better" class="headerlink" title="3.Early Convolutions Help Transformers See Better"></a>3.Early Convolutions Help Transformers See Better</h4><p>Vit训练不稳定在于Patch Embedding时使用大卷积核以及大步长导致，进一步提出使用step-wise conv stem进行替换，以此改进vit训练稳定性问题</p><h4 id="4-Visformer-The-Vision-friendly-Transformer"><a href="#4-Visformer-The-Vision-friendly-Transformer" class="headerlink" title="4.Visformer: The Vision-friendly Transformer"></a>4.Visformer: The Vision-friendly Transformer</h4><p>提升transformer方法的性能下限，即使是小数据集依然可以得到很好的性能</p><h4 id="5-Conditional-Positional-Encodings-for-Vision-Transformers"><a href="#5-Conditional-Positional-Encodings-for-Vision-Transformers" class="headerlink" title="5.Conditional Positional Encodings for Vision Transformers"></a>5.Conditional Positional Encodings for Vision Transformers</h4><p>利用卷积+zero-padding来编码局部位置信息，从而丢弃现有的PE，解决输入大小变化时需要对PE进行插值和fine-tune的问题</p><h4 id="6-MetaFormer-is-Actually-What-You-Need-for-Vision"><a href="#6-MetaFormer-is-Actually-What-You-Need-for-Vision" class="headerlink" title="6.MetaFormer is Actually What You Need for Vision"></a>6.MetaFormer is Actually What You Need for Vision</h4><p>transformer优于cnn在于其结构，而不是attention，即使替换成pooling，也能达到不错的性能</p><h4 id="7-Per-Pixel-Classification-is-Not-All-You-Need-for-Semantic-Segmentation"><a href="#7-Per-Pixel-Classification-is-Not-All-You-Need-for-Semantic-Segmentation" class="headerlink" title="7.Per-Pixel Classification is Not All You Need for Semantic Segmentation"></a>7.Per-Pixel Classification is Not All You Need for Semantic Segmentation</h4><p>提出了一种新的分割范式，解耦分割和分类，统一语义分割和实例分割任务</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;近期断断续续的看了一些transformer相关的paper，看的比较杂，有些是对应领域比较有代表性地工作。偷个懒就不详细介绍每篇Paper，简单地记录一下这些paper大致要解决地问题。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="transformer" scheme="https://blog.nicehuster.cn/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>知识蒸馏[转载]</title>
    <link href="https://blog.nicehuster.cn/2021/11/09/knowledge-distillation/"/>
    <id>https://blog.nicehuster.cn/2021/11/09/knowledge-distillation/</id>
    <published>2021-11-09T11:13:39.000Z</published>
    <updated>2022-04-24T08:30:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>知识蒸馏是一种模型压缩方法，是一种基于“教师-学生网络思想”的训练方法，由于其简单，有效，在工业界被广泛应用。这一技术的理论来自于2015年Hinton发表的一篇神作: <a href="https://arxiv.org/pdf/1503.02531.pdf" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a>。Knowledge Distillation，简称KD，顾名思义，就是将已经训练好的模型包含的知识(”Knowledge”)，蒸馏(“Distill”)提取到另一个模型里面去。</p><a id="more"></a><p>以下内容均转载于：<a href="https://zhuanlan.zhihu.com/p/102038521" target="_blank" rel="noopener">【经典简读】知识蒸馏(Knowledge Distillation) 经典之作</a> ，侵权删。</p><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><h3 id="1-1-论文提出的背景"><a href="#1-1-论文提出的背景" class="headerlink" title="1.1. 论文提出的背景"></a>1.1. 论文提出的背景</h3><p>虽然在一般情况下，我们不会去区分训练和部署使用的模型，但是训练和部署之间存在着一定的不一致性:</p><ul><li>在训练过程中，我们需要使用复杂的模型，大量的计算资源，以便从非常大、高度冗余的数据集中提取出信息。在实验中，效果最好的模型往往规模很大，甚至由多个模型集成得到。而大模型不方便部署到服务中去，常见的瓶颈如下:</li></ul><ol><li>推断速度慢</li><li>对部署资源要求高(内存，显存等)</li></ol><ul><li>在部署时，我们对延迟以及计算资源都有着严格的限制。</li></ul><p>因此，模型压缩（在保证性能的前提下减少模型的参数量）成为了一个重要的问题。而”模型蒸馏“属于模型压缩的一种方法。</p><p><strong>插句题外话</strong>，我们可以从模型参数量和训练数据量之间的相对关系来理解underfitting和overfitting。AI领域的从业者可能对此已经习以为常，但是为了力求让小白也能读懂本文，还是引用我同事的解释（我印象很深）形象地说明一下:</p><blockquote><p>模型就像一个容器，训练数据中蕴含的知识就像是要装进容器里的水。当数据知识量(水量)超过模型所能建模的范围时(容器的容积)，加再多的数据也不能提升效果(水再多也装不进容器)，因为模型的表达空间有限(容器容积有限)，就会造成<strong>underfitting</strong>；而当模型的参数量大于已有知识所需要的表达空间时(容积大于水量，水装不满容器)，就会造成<strong>overfitting</strong>，即模型的variance会增大(想象一下摇晃半满的容器，里面水的形状是不稳定的)。</p></blockquote><h3 id="1-2-“思想歧路”"><a href="#1-2-“思想歧路”" class="headerlink" title="1.2. “思想歧路”"></a>1.2. “思想歧路”</h3><p>上面容器和水的比喻非常经典和贴切，但是会引起一个误解: 人们在直觉上会觉得，要保留相近的知识量，必须保留相近规模的模型。也就是说，一个模型的参数量基本决定了其所能捕获到的数据内蕴含的“知识”的量。</p><p>这样的想法是基本正确的，但是需要注意的是:</p><ol><li>模型的参数量和其所能捕获的“知识“量之间并非稳定的线性关系(下图中的1)，而是接近边际收益逐渐减少的一种增长曲线(下图中的2和3)</li><li>完全相同的模型架构和模型参数量，使用完全相同的训练数据，能捕获的“知识”量并不一定完全相同，另一个关键因素是训练的方法。合适的训练方法可以使得在模型参数总量比较小时，尽可能地获取到更多的“知识”(下图中的3与2曲线的对比).</li></ol><p><img src="https://pic2.zhimg.com/80/v2-f2fc2f02b87a38a9ff34a50664800045_720w.jpg" alt="img"></p><h2 id="2-知识蒸馏的理论依据"><a href="#2-知识蒸馏的理论依据" class="headerlink" title="2. 知识蒸馏的理论依据"></a>2. 知识蒸馏的理论依据</h2><h3 id="2-1-Teacher-Model和Student-Model"><a href="#2-1-Teacher-Model和Student-Model" class="headerlink" title="2.1. Teacher Model和Student Model"></a>2.1. Teacher Model和Student Model</h3><p>知识蒸馏使用的是Teacher—Student模型，其中teacher是“知识”的输出者，student是“知识”的接受者。知识蒸馏的过程分为2个阶段:</p><ol><li>原始模型训练: 训练”Teacher模型”, 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对”Teacher模型”不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值。</li><li>精简模型训练: 训练”Student模型”, 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值。</li></ol><p>在本论文中，作者将问题限定在<strong>分类问题</strong>下，或者其他本质上属于分类问题的问题，该类问题的共同点是模型最后会有一个softmax层，其输出值对应了相应类别的概率值。</p><h3 id="2-2-知识蒸馏的关键点"><a href="#2-2-知识蒸馏的关键点" class="headerlink" title="2.2. 知识蒸馏的关键点"></a>2.2. 知识蒸馏的关键点</h3><p>如果回归机器学习最最基础的理论，我们可以很清楚地意识到一点(而这一点往往在我们深入研究机器学习之后被忽略): <strong>机器学习最根本的目的</strong>在于训练出在某个问题上泛化能力强的模型。</p><ul><li><strong>泛化能力强</strong>: 在某问题的所有数据上都能很好地反应输入和输出之间的关系，无论是训练数据，还是测试数据，还是任何属于该问题的未知数据。</li></ul><p>而现实中，由于我们不可能收集到某问题的所有数据来作为训练数据，并且新数据总是在源源不断的产生，因此我们只能退而求其次，训练目标变成在已有的训练数据集上建模输入和输出之间的关系。由于训练数据集是对真实数据分布情况的采样，训练数据集上的最优解往往会多少偏离真正的最优解(这里的讨论不考虑模型容量)。</p><p>而在知识蒸馏时，由于我们已经有了一个泛化能力较强的Net-T，我们在利用Net-T来蒸馏训练Net-S时，可以直接让Net-S去学习Net-T的泛化能力。</p><p>一个很直白且高效的迁移泛化能力的方法就是：使用softmax层输出的类别的概率来作为“soft target”。</p><p><strong>【KD的训练过程和传统的训练过程的对比】</strong></p><ol><li>传统training过程(<strong>hard targets</strong>): 对ground truth求极大似然</li><li>KD的training过程(<strong>soft targets</strong>): 用large model的class probabilities作为soft targets</li></ol><p><img src="https://pic3.zhimg.com/80/v2-29a851c6fa9cc809e51ce738abbec2ce_720w.jpg" alt="img"></p><p>上图: Hard Target 下图: Soft Target</p><p><strong>KD的训练过程为什么更有效?</strong></p><p>softmax层的输出，除了正例之外，<strong>负标签也带有大量的信息</strong>，比如某些负标签对应的概率远远大于其他负标签。而在传统的训练过程(hard target)中，所有负标签都被统一对待。也就是说，<strong>KD的训练方式使得每个样本给Net-S带来的信息量大于传统的训练方式</strong>。</p><p>【<strong>举个例子】</strong></p><p>在手写体数字识别任务MNIST中，输出类别有10个。</p><p><img src="https://pic3.zhimg.com/80/v2-3d77281f38df62990c47d606dd581ee2_720w.jpg" alt="img"></p><p>假设某个输入的“2”更加形似”3”，softmax的输出值中”3”对应的概率为0.1，而其他负标签对应的值都很小，而另一个”2”更加形似”7”，”7”对应的概率为0.1。这两个”2”对应的hard target的值是相同的，但是它们的soft target却是不同的，由此我们可见soft target蕴含着比hard target多的信息。并且soft target分布的熵相对高时，其soft target蕴含的知识就更丰富。</p><p><img src="https://pic4.zhimg.com/80/v2-a9e90626c5ac6f64a7e04c89f6ce3013_720w.jpg" alt="img"></p><p>两个”2“的hard target相同而soft target不同</p><p>这就解释了为什么通过蒸馏的方法训练出的Net-S相比使用完全相同的模型结构和训练数据只使用hard target的训练方法得到的模型，拥有更好的泛化能力。</p><h3 id="2-3-softmax函数"><a href="#2-3-softmax函数" class="headerlink" title="2.3. softmax函数"></a>2.3. softmax函数</h3><p>先回顾一下原始的softmax函数:</p><script type="math/tex; mode=display">q_{i}=\frac{\exp \left(z_{i}\right)}{\sum_{j} \exp \left(z_{j}\right)}</script><p>但要是直接使用softmax层的输出值作为soft target, 这又会带来一个问题: 当softmax输出的概率分布熵相对较小时，负标签的值都很接近0，对损失函数的贡献非常小，小到可以忽略不计。因此<strong>“温度”</strong>这个变量就派上了用场。</p><p>下面的公式时加了温度这个变量之后的softmax函数:</p><script type="math/tex; mode=display">q_{i}=\frac{\exp \left(z_{i} / T\right)}{\sum_{j} \exp \left(z_{j} / T\right)}</script><ul><li>这里的T就是<strong>温度</strong>。</li><li>原来的softmax函数是T = 1的特例。 T越高，softmax的output probability distribution越趋于平滑，其分布的熵越大，负标签携带的信息会被相对地放大，模型训练将更加关注负标签。</li></ul><h2 id="3-知识蒸馏的具体方法"><a href="#3-知识蒸馏的具体方法" class="headerlink" title="3. 知识蒸馏的具体方法"></a>3. 知识蒸馏的具体方法</h2><h3 id="3-1-通用的知识蒸馏方法"><a href="#3-1-通用的知识蒸馏方法" class="headerlink" title="3.1. 通用的知识蒸馏方法"></a>3.1. 通用的知识蒸馏方法</h3><ul><li><strong>第一步</strong>是训练Net-T；<strong>第二步</strong>是在高温T下，蒸馏Net-T的知识到Net-S</li></ul><p><img src="https://pic2.zhimg.com/80/v2-d01f5142d06aa27bc5e207831b5131d9_720w.jpg" alt="img"></p><p>知识蒸馏示意图(来自<a href="https://nervanasystems.github.io/distiller/knowledge_distillation.html" target="_blank" rel="noopener">https://nervanasystems.github.io/distiller/knowledge_distillation.html</a>)</p><p>训练Net-T的过程很简单，下面详细讲讲第二步:高温蒸馏的过程。高温蒸馏过程的目标函数由distill loss(对应soft target)和student loss(对应hard target)加权得到。示意图如上。</p><script type="math/tex; mode=display">L=\alpha L_{s o f t}+\beta L_{h a r d}</script><ul><li><img src="https://www.zhihu.com/equation?tex=v_i" alt="[公式]">: Net-T的logits</li><li><img src="https://www.zhihu.com/equation?tex=z_i" alt="[公式]">: Net-S的logits</li><li><img src="https://www.zhihu.com/equation?tex=p%5ET_i" alt="[公式]">: Net-T的在温度=T下的softmax输出在第i类上的值</li><li><img src="https://www.zhihu.com/equation?tex=q%5ET_i" alt="[公式]">: Net-S的在温度=T下的softmax输出在第i类上的值</li><li><img src="https://www.zhihu.com/equation?tex=c_i" alt="[公式]">: 在第i类上的ground truth值, <img src="https://www.zhihu.com/equation?tex=c_i%5Cin%5C%7B0%2C1%5C%7D" alt="[公式]">, 正标签取1，负标签取0.</li><li><img src="https://www.zhihu.com/equation?tex=N" alt="[公式]">: 总标签数量</li><li>Net-T 和 Net-S同时输入 transfer set (这里可以直接复用训练Net-T用到的training set), 用Net-T产生的softmax distribution (with high temperature) 来作为soft target，Net-S在相同温度T条件下的softmax输出和soft target的cross entropy就是<strong>Loss函数的第一部分</strong> <img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D" alt="[公式]"></li></ul><p><img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D%3D-%5Csum_j%5EN+p%5ET_j%5Clog%28q%5ET_j%29" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=p%5ET_i%3D%5Cfrac%7B%5Cexp%28v_i%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28v_k%2FT%29%7D" alt="[公式]"> , <img src="https://www.zhihu.com/equation?tex=q%5ET_i%3D%5Cfrac%7B%5Cexp%28z_i%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%2FT%29%7D" alt="[公式]"></p><ul><li>Net-S在T=1的条件下的softmax输出和ground truth的cross entropy就是<strong>Loss函数的第二部分</strong> <img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D" alt="[公式]"> 。</li></ul><p><img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D%3D-%5Csum_j%5EN+c_j%5Clog%28q%5E1_j%29" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=q%5E1_i%3D%5Cfrac%7B%5Cexp%28z_i%29%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%29%7D" alt="[公式]"></p><ul><li>第二部分Loss <img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D" alt="[公式]"> 的必要性其实很好理解: Net-T也有一定的错误率，使用ground truth可以有效降低错误被传播给Net-S的可能。打个比方，老师虽然学识远远超过学生，但是他仍然有出错的可能，而这时候如果学生在老师的教授之外，可以同时参考到标准答案，就可以有效地降低被老师偶尔的错误“带偏”的可能性。</li></ul><p><strong>【讨论】</strong></p><ul><li>实验发现第二部分所占比重比较小的时候，能产生最好的结果，这是一个经验的结论。一个可能的原因是，由于soft target产生的gradient与hard target产生的gradient之间有与 <img src="https://www.zhihu.com/equation?tex=T" alt="[公式]"> 相关的比值。原论文中只是一笔带过，我在下面补充了一些简单的推导。(ps. 下面推导可能有些错误，如果有读者能够正确推出来请私信我～)</li><li><strong>Soft Target:</strong><img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D" alt="[公式]"></li></ul><p><img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D%3D-%5Csum_j%5EN+p%5ET_j%5Clog%28q%5ET_j%29%3D-%5Csum_j%5EN+%5Cfrac%7Bz_j%2FT%5Ctimes%5Cexp%28v_j%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28v_k%2FT%29%7D%5Cleft%28%5Cfrac%7B1%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%2FT%29%7D-%5Cfrac%7B%5Cexp+%28z_j+%2F+T%29+%7D%7B%5Cleft%28++%5Csum_k%5EN+%5Cexp%28z_k%2F+T%29%5Cright%29+%5E+2%7D%5Cright%29" alt="[公式]"></p><p><img src="https://www.zhihu.com/equation?tex=%5Capprox+-%5Cfrac%7B1%7D%7BT%5Csum_k%5EN+%5Cexp%28v_k%2FT%29%7D%5Cleft%28%5Cfrac%7B%5Csum_j%5ENz_j%5Cexp%28v_j%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%2FT%29%7D-%5Cfrac%7B%5Csum_j%5EN+z_j%5Cexp+%28z_j%2F+T%29%5Cexp%28v_j%2FT%29+%7D%7B%5Cleft%28++%5Csum_k%5EN+%5Cexp%28z_k+%2F+T%29%5Cright%29+%5E+2%7D+%5Cright%29" alt="[公式]"></p><ul><li><strong>Hard Target:</strong> <img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D" alt="[公式]"></li></ul><p><img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D%3D-%5Csum_j%5EN+c_j%5Clog%28q%5E1_j%29%3D-%5Cleft%28%5Cfrac%7B%5Csum_j%5EN+c_jz_j+%7D%7B+%5Csum_k%5EN+%5Cexp%28z_k+%29%7D-%5Cfrac%7B%5Csum_j%5EN+c_jz_j%5Cexp+%28z_j%29+%7D%7B%5Cleft%28++%5Csum_k%5EN+%5Cexp%28z_k%29%5Cright%29+%5E+2%7D+%5Cright%29" alt="[公式]"></p><ul><li>由于 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_i%7D" alt="[公式]">的magnitude大约是 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bhard%7D%7D%7B%5Cpartial+z_i%7D" alt="[公式]"> 的 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BT%5E2%7D" alt="[公式]"> ，因此在同时使用soft target和hard target的时候，需要在soft target之前乘上<img src="https://www.zhihu.com/equation?tex=T%5E%7B2%7D" alt="[公式]">的系数，这样才能保证soft target和hard target贡献的梯度量基本一致。</li></ul><p><strong>【注意】</strong> 在Net-S训练完毕后，做inference时其softmax的温度T要恢复到1.</p><h3 id="3-2-一种特殊情形-直接match-logits-不经过softmax"><a href="#3-2-一种特殊情形-直接match-logits-不经过softmax" class="headerlink" title="3.2. 一种特殊情形: 直接match logits(不经过softmax)"></a>3.2. 一种特殊情形: 直接match logits(不经过softmax)</h3><p>直接match logits指的是，直接使用softmax层的输入logits（而不是输出）作为soft targets，需要最小化的目标函数是Net-T和Net-S的logits之间的平方差。</p><p><strong>直接上结论: 直接match logits的做法是</strong> <img src="https://www.zhihu.com/equation?tex=T+%5Crightarrow+%5Cinfty" alt="[公式]"> <strong>的情况下的特殊情形。</strong></p><p>由单个case贡献的loss，推算出对应在Net-S每个logit <img src="https://www.zhihu.com/equation?tex=z_i" alt="[公式]"> 上的gradient:</p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_%7Bi%7D%7D%3D%5Cfrac%7B1%7D%7BT%7D%5Cleft%28q_%7Bi%7D-p_%7Bi%7D%5Cright%29%3D%5Cfrac%7B1%7D%7BT%7D%5Cleft%28%5Cfrac%7Be%5E%7Bz_%7Bi%7D+%2F+T%7D%7D%7B%5Csum_%7Bj%7D+e%5E%7Bz_%7Bj%7D+%2F+T%7D%7D-%5Cfrac%7Be%5E%7Bv_%7Bi%7D+%2F+T%7D%7D%7B%5Csum_%7Bj%7D+e%5E%7Bv_%7Bj%7D+%2F+T%7D%7D%5Cright%29" alt="[公式]"></p><p>当 <img src="https://www.zhihu.com/equation?tex=T+%5Crightarrow+%5Cinfty" alt="[公式]"> 时，我们使用 <img src="https://www.zhihu.com/equation?tex=1%2Bx%2FT" alt="[公式]"> 来近似 <img src="https://www.zhihu.com/equation?tex=e%5E%7Bx%2FT%7D" alt="[公式]"> ，于是得到</p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_%7Bi%7D%7D+%5Capprox+%5Cfrac%7B1%7D%7BT%7D%5Cleft%28%5Cfrac%7B1%2Bz_%7Bi%7D+%2F+T%7D%7BN%2B%5Csum_%7Bj%7D+z_%7Bj%7D+%2F+T%7D-%5Cfrac%7B1%2Bv_%7Bi%7D+%2F+T%7D%7BN%2B%5Csum_%7Bj%7D+v_%7Bj%7D+%2F+T%7D%5Cright%29" alt="[公式]"></p><p>如果再加上logits是零均值的假设</p><p><img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bj%7D+z_%7Bj%7D%3D%5Csum_%7Bj%7D+v_%7Bj%7D%3D0" alt="[公式]"></p><p>那么上面的公式可以简化成</p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_%7Bi%7D%7D+%5Capprox+%5Cfrac%7B1%7D%7BN+T%5E%7B2%7D%7D%5Cleft%28z_%7Bi%7D-v_%7Bi%7D%5Cright%29" alt="[公式]"></p><p>也就是等价于minimise下面的损失函数</p><p><img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D%27%3D1+%2F+2%5Cleft%28z_%7Bi%7D-v_%7Bi%7D%5Cright%29%5E%7B2%7D" alt="[公式]"></p><h2 id="4-关于”温度”的讨论"><a href="#4-关于”温度”的讨论" class="headerlink" title="4. 关于”温度”的讨论"></a>4. 关于”温度”的讨论</h2><p>【问题】 我们都知道“蒸馏”需要在高温下进行，那么这个“蒸馏”的温度代表了什么，又是如何选取合适的温度？</p><p><img src="https://pic2.zhimg.com/80/v2-a120cc4bbb70b96968210b995b2e39d1_720w.jpg" alt="img">随着温度T的增大，概率分布的熵逐渐增大</p><h3 id="4-1-温度的特点"><a href="#4-1-温度的特点" class="headerlink" title="4.1. 温度的特点"></a>4.1. 温度的特点</h3><p>在回答这个问题之前，先讨论一下<strong>温度T的特点</strong></p><ol><li>原始的softmax函数是 <img src="https://www.zhihu.com/equation?tex=T%3D1+" alt="[公式]"> 时的特例， <img src="https://www.zhihu.com/equation?tex=T%3C1" alt="[公式]"> 时，概率分布比原始更“陡峭”， <img src="https://www.zhihu.com/equation?tex=T%3E1" alt="[公式]"> 时，概率分布比原始更“平缓”。</li><li>温度越高，softmax上各个值的分布就越平均（思考极端情况: (i) <img src="https://www.zhihu.com/equation?tex=T%3D%5Cinfty" alt="[公式]"> , 此时softmax的值是平均分布的；(ii) <img src="https://www.zhihu.com/equation?tex=T%5Crightarrow0" alt="[公式]">，此时softmax的值就相当于 <img src="https://www.zhihu.com/equation?tex=argmax" alt="[公式]"> , 即最大的概率处的值趋近于1，而其他值趋近于0）</li><li>不管温度T怎么取值，Soft target都有忽略相对较小的 <img src="https://www.zhihu.com/equation?tex=p_i" alt="[公式]"> 携带的信息的倾向</li></ol><h3 id="4-2-温度代表了什么，如何选取合适的温度？"><a href="#4-2-温度代表了什么，如何选取合适的温度？" class="headerlink" title="4.2. 温度代表了什么，如何选取合适的温度？"></a><strong>4.2. 温度代表了什么，如何选取合适的温度？</strong></h3><p><strong>温度的高低改变的是Net-S训练过程中对负标签的关注程度</strong>: 温度较低时，对负标签的关注，尤其是那些显著低于平均值的负标签的关注较少；而温度较高时，负标签相关的值会相对增大，Net-S会相对多地关注到负标签。</p><p>实际上，负标签中包含一定的信息，尤其是那些值显著<strong>高于</strong>平均值的负标签。但由于Net-T的训练过程决定了负标签部分比较noisy，并且负标签的值越低，其信息就越不可靠。因此温度的选取比较empirical，本质上就是在下面两件事之中取舍:</p><ol><li>从有部分信息量的负标签中学习 —&gt; 温度要高一些</li><li>防止受负标签中噪声的影响 —&gt;温度要低一些</li></ol><p>总的来说，T的选择和Net-S的大小有关，Net-S参数量比较小的时候，相对比较低的温度就可以了（因为参数量小的模型不能capture all knowledge，所以可以适当忽略掉一些负标签的信息）</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;知识蒸馏是一种模型压缩方法，是一种基于“教师-学生网络思想”的训练方法，由于其简单，有效，在工业界被广泛应用。这一技术的理论来自于2015年Hinton发表的一篇神作: &lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Distilling the Knowledge in a Neural Network&lt;/a&gt;。Knowledge Distillation，简称KD，顾名思义，就是将已经训练好的模型包含的知识(”Knowledge”)，蒸馏(“Distill”)提取到另一个模型里面去。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="Knowledge Distillation" scheme="https://blog.nicehuster.cn/tags/Knowledge-Distillation/"/>
    
  </entry>
  
</feed>
