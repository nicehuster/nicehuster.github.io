<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>nicehuster&#39;s blog</title>
  <icon>https://www.gravatar.com/avatar/2555127dc0de830d31ceeb98d8565ac8</icon>
  <subtitle>不积跬步，无以至千里</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://nicehuster.github.io/"/>
  <updated>2020-09-17T12:44:41.312Z</updated>
  <id>https://nicehuster.github.io/</id>
  
  <author>
    <name>nicehuster</name>
    <email>nicehuster@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>mmdetection详解指北 (四)</title>
    <link href="https://nicehuster.github.io/2020/09/08/2020-09-06-mmdetection-4/"/>
    <id>https://nicehuster.github.io/2020/09/08/2020-09-06-mmdetection-4/</id>
    <published>2020-09-08T11:16:39.000Z</published>
    <updated>2020-09-17T12:44:41.312Z</updated>
    
    <content type="html"><![CDATA[<h3 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h3><p>训练流程的包装过程大致如下:tools/train.py-&gt;apis/train.py-&gt;mmcv/runner.py-&gt;mmcv/hook.py(后面是分散的), 其中 runner 维护了数据信息,优化器, 日志系统, 训练 loop 中的各节点信息, 模型保存, 学习率等. 另外补充一点, 以上包装过程, 在 mmdet 中无处不在, 包括 mmcv 的代码也是对日常频繁使用的函数进行了统一封装。</p><a id="more"></a><h4 id="训练逻辑"><a href="#训练逻辑" class="headerlink" title="训练逻辑"></a>训练逻辑</h4><p><img src="/img/image-20200917203908378.png" alt="image-20200917203908378"></p><p>图见2, 注意它的四个层级. 代码上, 主要查看 apis/train.py, mmcv 中的runner 相关文件. 核心围绕 Runner,Hook 两个类. Runner 将模型, 批处理函数 batch_pro cessor, 优化器作为基本属性, 训练过程中与训练状态, 各节点相关的信息被记录在mode,_hooks,_epoch,_iter,_inner_iter,_max_epochs, _max_iters中，这些信息维护了训练过程中插入不同 hook 的操作方式. 理清训练流程只需看 Runner 的成员函数 run. 在 run 里会根据 mode 按配置中 workflow 的 epoch 循环调用 train 和 val 函数, 跑完所有的 epoch. 比如train:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EpochBasedRunner</span>(<span class="title">BaseRunner</span>):</span></span><br><span class="line">    <span class="string">""</span><span class="string">"Epoch-based Runner.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This runner train models epoch by epoch.</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(<span class="keyword">self</span>, data_loader, **kwargs)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.model.train()</span><br><span class="line">        <span class="keyword">self</span>.mode = <span class="string">'train'</span></span><br><span class="line">        <span class="keyword">self</span>.data_loader = data_loader</span><br><span class="line">        <span class="keyword">self</span>._max_iters = <span class="keyword">self</span>._max_epochs * len(<span class="keyword">self</span>.data_loader) <span class="comment"># 最大迭代次数</span></span><br><span class="line">        <span class="keyword">self</span>.call_hook(<span class="string">'before_train_epoch'</span>)<span class="comment"># 根据名字获取hook对象函数</span></span><br><span class="line">        time.sleep(<span class="number">2</span>)  <span class="comment"># Prevent possible deadlock during epoch transition</span></span><br><span class="line">        <span class="keyword">for</span> i, data_batch <span class="keyword">in</span> enumerate(<span class="keyword">self</span>.data_loader)<span class="symbol">:</span></span><br><span class="line">            <span class="keyword">self</span>._inner_iter = i <span class="comment"># 记录当前训练迭代次数</span></span><br><span class="line">            <span class="keyword">self</span>.call_hook(<span class="string">'before_train_iter'</span>) <span class="comment">#一个batch 前向操作开始</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">self</span>.batch_processor is <span class="symbol">None:</span></span><br><span class="line">                outputs = <span class="keyword">self</span>.model.train_step(data_batch, <span class="keyword">self</span>.optimizer,</span><br><span class="line">                                                **kwargs)</span><br><span class="line">            <span class="symbol">else:</span></span><br><span class="line">                outputs = <span class="keyword">self</span>.batch_processor(</span><br><span class="line">                    <span class="keyword">self</span>.model, data_batch, train_mode=True, **kwargs) </span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> isinstance(outputs, dict)<span class="symbol">:</span></span><br><span class="line">                raise TypeError(<span class="string">'"batch_processor()" or "model.train_step()"'</span></span><br><span class="line">                                <span class="string">' must return a dict'</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'log_vars'</span> <span class="keyword">in</span> <span class="symbol">outputs:</span></span><br><span class="line">                <span class="keyword">self</span>.log_buffer.update(outputs[<span class="string">'log_vars'</span>],</span><br><span class="line">                                       outputs[<span class="string">'num_samples'</span>])</span><br><span class="line">            <span class="keyword">self</span>.outputs = outputs</span><br><span class="line">            <span class="keyword">self</span>.call_hook(<span class="string">'after_train_iter'</span>)<span class="comment">#一个batch 前向操作结束</span></span><br><span class="line">            <span class="keyword">self</span>._iter += <span class="number">1</span> <span class="comment"># 方便resume，知道从哪次开始优化</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.call_hook(<span class="string">'after_train_epoch'</span>) <span class="comment">#一个epoch结束</span></span><br><span class="line">        <span class="keyword">self</span>._epoch += <span class="number">1</span> <span class="comment">#记录训练epoch状态，方便resume</span></span><br></pre></td></tr></table></figure><p>上面需要说明的是自定义 hook 类, 自定义 hook 类需继承 mmcv 的Hook 类, 其默认了 6+8+4 个成员函数, 也即2所示的 6 个层级节点, 外加 2*4 个区分 train 和 val 的节点记录函数, 以及 4 个边界检查函数. 从train.py 中容易看出, 在训练之前, 已经将需要的 hook 函数注册到 Runner的 self._hook 中了, 包括从配置文件解析的优化器, 学习率调整函数, 模型保存, 一个 batch 的时间记录等 (注册 hook 算子在 self._hook 中按优先级升序排列). 这里的 call_hook 函数定义如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseRunner</span><span class="params">(metaclass=ABCMeta)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call_hook</span><span class="params">(self, fn_name)</span>:</span></span><br><span class="line">        <span class="string">"""Call all hooks.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            fn_name (str): The function name in each hook to be called, such as</span></span><br><span class="line"><span class="string">                "before_train_epoch".</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> hook <span class="keyword">in</span> self._hooks:</span><br><span class="line">            getattr(hook, fn_name)(self)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>容易看出, 在训练的不同节点, 将从注册列表中调用实现了该节点函数的类成员函数. 比如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@HOOKS.register_module()</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OptimizerHook</span><span class="params">(Hook)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, grad_clip=None)</span>:</span></span><br><span class="line">        self.grad_clip = grad_clip</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">clip_grads</span><span class="params">(self, params)</span>:</span></span><br><span class="line">        params = list(</span><br><span class="line">            filter(<span class="keyword">lambda</span> p: p.requires_grad <span class="keyword">and</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, params))</span><br><span class="line">        <span class="keyword">if</span> len(params) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> clip_grad.clip_grad_norm_(params, **self.grad_clip)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">after_train_iter</span><span class="params">(self, runner)</span>:</span></span><br><span class="line">        runner.optimizer.zero_grad()</span><br><span class="line">        runner.outputs[<span class="string">'loss'</span>].backward()</span><br><span class="line">        <span class="keyword">if</span> self.grad_clip <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            grad_norm = self.clip_grads(runner.model.parameters())</span><br><span class="line">            <span class="keyword">if</span> grad_norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="comment"># Add grad norm to the logger</span></span><br><span class="line">                runner.log_buffer.update(&#123;<span class="string">'grad_norm'</span>: float(grad_norm)&#125;,</span><br><span class="line">                                         runner.outputs[<span class="string">'num_samples'</span>])</span><br><span class="line">        runner.optimizer.step()</span><br></pre></td></tr></table></figure><p>将在每个 train_iter 后实现反向传播和参数更新。学习率优化相对复杂一点, 其基类 LrUpdaterHook, 实现了 before_run,before_train_epoch, before_train_iter 三个 hook 函数, 意义自明. . 这里选一个余弦式变化, 稍作说明:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">annealing_cos</span><span class="params">(start, end, factor, weight=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Calculate annealing cos learning rate.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Cosine anneal from `weight * start + (1 - weight) * end` to `end` as</span></span><br><span class="line"><span class="string">    percentage goes from 0.0 to 1.0.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        start (float): The starting learning rate of the cosine annealing.</span></span><br><span class="line"><span class="string">        end (float): The ending learing rate of the cosine annealing.</span></span><br><span class="line"><span class="string">        factor (float): The coefficient of `pi` when calculating the current</span></span><br><span class="line"><span class="string">            percentage. Range from 0.0 to 1.0.</span></span><br><span class="line"><span class="string">        weight (float, optional): The combination factor of `start` and `end`</span></span><br><span class="line"><span class="string">            when calculating the actual starting learning rate. Default to 1.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    cos_out = cos(pi * factor) + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> end + <span class="number">0.5</span> * weight * (start - end) * cos_out</span><br><span class="line"></span><br><span class="line"><span class="meta">@HOOKS.register_module()</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CosineAnnealingLrUpdaterHook</span><span class="params">(LrUpdaterHook)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, min_lr=None, min_lr_ratio=None, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> (min_lr <span class="keyword">is</span> <span class="keyword">None</span>) ^ (min_lr_ratio <span class="keyword">is</span> <span class="keyword">None</span>)</span><br><span class="line">        self.min_lr = min_lr</span><br><span class="line">        self.min_lr_ratio = min_lr_ratio</span><br><span class="line">        super(CosineAnnealingLrUpdaterHook, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_lr</span><span class="params">(self, runner, base_lr)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.by_epoch:</span><br><span class="line">            progress = runner.epoch</span><br><span class="line">            max_progress = runner.max_epochs</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            progress = runner.iter</span><br><span class="line">            max_progress = runner.max_iters</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.min_lr_ratio <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            target_lr = base_lr * self.min_lr_ratio</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            target_lr = self.min_lr</span><br><span class="line">        <span class="keyword">return</span> annealing_cos(base_lr, target_lr, progress / max_progress)</span><br></pre></td></tr></table></figure><p>从 get_lr 可以看到, 学习率变换周期有两种,epoch-&gt;max_epoch, 或者更大的 iter-&gt;max_iter, 后者表明一个 epoch 内不同 batch 的学习率可以不同, 因为没有什么理论, 所有这两种方式都行. 其中 base_lr 为初始学习率,target_lr 为学习率衰减的上界, 而当前学习率即为返回值.</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;训练流程&quot;&gt;&lt;a href=&quot;#训练流程&quot; class=&quot;headerlink&quot; title=&quot;训练流程&quot;&gt;&lt;/a&gt;训练流程&lt;/h3&gt;&lt;p&gt;训练流程的包装过程大致如下:tools/train.py-&amp;gt;apis/train.py-&amp;gt;mmcv/runner.py-&amp;gt;mmcv/hook.py(后面是分散的), 其中 runner 维护了数据信息,优化器, 日志系统, 训练 loop 中的各节点信息, 模型保存, 学习率等. 另外补充一点, 以上包装过程, 在 mmdet 中无处不在, 包括 mmcv 的代码也是对日常频繁使用的函数进行了统一封装。&lt;/p&gt;
    
    </summary>
    
      <category term="mmdetection" scheme="https://nicehuster.github.io/categories/mmdetection/"/>
    
    
      <category term="mmdetection" scheme="https://nicehuster.github.io/tags/mmdetection/"/>
    
  </entry>
  
  <entry>
    <title>mmdetection详解指北 (三)</title>
    <link href="https://nicehuster.github.io/2020/09/08/2020-09-06-mmdetection-3/"/>
    <id>https://nicehuster.github.io/2020/09/08/2020-09-06-mmdetection-3/</id>
    <published>2020-09-08T11:15:39.000Z</published>
    <updated>2020-09-17T12:44:33.740Z</updated>
    
    <content type="html"><![CDATA[<h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>数据处理可能是炼丹师接触最为密集的了，因为通常情况，除了数据的离线处理，写个数据类，就可以炼丹了。但本节主要涉及数据的在线处理，更进一步应该是检测分割数据的 pytorch 处理方式。虽然 mmdet 将常用的数据都实现了，而且也实现了中间通用数据格式，但，这和模型，损失函数，性能评估的实现也相关，比如你想把官网的 centernet 完整的改成 mmdet风格，就能看到 (看起来没必要)。</p><a id="more"></a><h4 id="CustomDataset"><a href="#CustomDataset" class="headerlink" title="CustomDataset"></a>CustomDataset</h4><p>看看配置文件，数据相关的有 data dict，里面包含了 train,val,test 的路径信息，用于数据类初始化, 有 pipeline，将各个函数及对应参数以字典形式放到列表里，是对 pytorch 原装的 transforms+compose，在检测，分割相关数据上的一次封装，使得形式更加统一。</p><p>从 builder.py 中 build_dataset 函数能看到，构建数据有三种方式，ConcatDataset，RepeatDataset 和从注册器中提取。其中 dataset_wrappers.py中 ConcatDataset 和 RepeatDataset 意义自明，前者继承自 pytorch 原始的ConcatDataset，将多个数据集整合到一起，具体为把不同序列（ 可参考<a href="https://docs.python.org/zh-cn/3/library/collections.abc.html" target="_blank" rel="noopener">容器的抽象基类</a>) 的长度相加<strong>getitem</strong> 函数对应 index 替换一下。后者就是单个数据类 (序列) 的多次重复。就功能来说，前者提高数据丰富度，后者可解决数据太少使得 loading 时间长的问题 (见代码注释)。而被注册的数据类在 datasets 下一些熟知的数据名文件中。其中，基类为 custom.py 中的 CustomDataset，coco 继承自它，cityscapes 继承自 coco，xml_style 的XMLDataset 继承 CustomDataset，然后 wider_face，voc 均继承自 XMLDataset。因此这里先分析一下CustomDataset。</p><p>CustomDataset 记录数据路径等信息，解析标注文件，将每一张图的所有信息以字典作为数据结构存在 results 中，然后进入pipeline: 数据增强相关操作，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@DATASETS.register_module()</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(...)</span>:</span></span><br><span class="line">        ...</span><br><span class="line">        self.pipeline = Compose(pipeline)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pre_pipeline</span><span class="params">(self, results)</span>:</span></span><br><span class="line">        <span class="string">"""Prepare results dict for pipeline."""</span></span><br><span class="line">        results[<span class="string">'img_prefix'</span>] = self.img_prefix</span><br><span class="line">        results[<span class="string">'seg_prefix'</span>] = self.seg_prefix</span><br><span class="line">        results[<span class="string">'proposal_file'</span>] = self.proposal_file</span><br><span class="line">        results[<span class="string">'bbox_fields'</span>] = []</span><br><span class="line">        results[<span class="string">'mask_fields'</span>] = []</span><br><span class="line">        results[<span class="string">'seg_fields'</span>] = []</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare_train_img</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="string">"""Get training data and annotations after pipeline.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            idx (int): Index of data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            dict: Training data and annotation after pipeline with new keys \</span></span><br><span class="line"><span class="string">                introduced by pipeline.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        img_info = self.data_infos[idx]</span><br><span class="line">        ann_info = self.get_ann_info(idx)</span><br><span class="line">        <span class="comment"># 基本信息，初始化字典</span></span><br><span class="line">        results = dict(img_info=img_info, ann_info=ann_info)</span><br><span class="line">        <span class="keyword">if</span> self.proposals <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            results[<span class="string">'proposals'</span>] = self.proposals[idx]</span><br><span class="line">        self.pre_pipeline(results)</span><br><span class="line">        <span class="keyword">return</span> self.pipeline(results) <span class="comment"># 数据增强等</span></span><br></pre></td></tr></table></figure><p>这里数据结构的选取需要注意一下，字典结构，在数据增强库 albu 中也是如此处理，因此可以快速替换为 albu 中的算法。另外每个数据类增加了各自的 evaluate 函数。evaluate 基础函数在 mmdet.core.evaluation 中，后做补充。</p><p>mmdet 的数<strong>据处理，字典结构，pipeline，evaluate</strong> 是三个关键部分。其他所有类的文件解析部分，数据筛选等，看看即可。因为我们知道，pytorch读取数据，是将序列转化为迭代器后进行 io 操作，所以在 dataset 下除了pipelines 外还有 loader 文件夹，里面实现了分组，分布式分组采样方法，以及调用了 mmcv 中的 collate 函数 (此处为 1.x 版本，2.0 版本将 loader移植到了 builder.py 中)，且 build_dataloader 封装的 DataLoader 最后在train_detector 中被调用，这部分将在后面补充，这里说说 pipelines。</p><h4 id="data-config"><a href="#data-config" class="headerlink" title="data_config"></a>data_config</h4><p>返回 maskrcnn 的配置文件 (1.x,2.0 看 base config)，可以看到训练和测试的不同之处：LoadAnnotations，MultiScaleFlipAug，DefaultFormatBundle 和 Collect。额外提示，虽然测试没有 LoadAnnotations，根据 CustomDataset 可知，它仍需标注文件，这和 inference 的 pipeline 不同，也即这里的 test 实为 evaluate。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 序列中的dict可以随意删减，增加，属于数据增强调参内容</span></span><br><span class="line">train_pipeline = [</span><br><span class="line">    dict(type=<span class="string">'LoadImageFromFile'</span>),</span><br><span class="line">    dict(type=<span class="string">'LoadAnnotations'</span>, with_bbox=<span class="keyword">True</span>),</span><br><span class="line">    dict(type=<span class="string">'Resize'</span>, img_scale=(<span class="number">1333</span>, <span class="number">800</span>), keep_ratio=<span class="keyword">True</span>),</span><br><span class="line">    dict(type=<span class="string">'RandomFlip'</span>, flip_ratio=<span class="number">0.5</span>),</span><br><span class="line">    dict(type=<span class="string">'Normalize'</span>, **img_norm_cfg),</span><br><span class="line">    dict(type=<span class="string">'Pad'</span>, size_divisor=<span class="number">32</span>),</span><br><span class="line">    dict(type=<span class="string">'DefaultFormatBundle'</span>),</span><br><span class="line">    dict(type=<span class="string">'Collect'</span>, keys=[<span class="string">'img'</span>, <span class="string">'gt_bboxes'</span>, <span class="string">'gt_labels'</span>]),</span><br><span class="line">]</span><br><span class="line">test_pipeline = [</span><br><span class="line">    dict(type=<span class="string">'LoadImageFromFile'</span>),</span><br><span class="line">    dict(</span><br><span class="line">        type=<span class="string">'MultiScaleFlipAug'</span>,</span><br><span class="line">        img_scale=(<span class="number">1333</span>, <span class="number">800</span>),</span><br><span class="line">        flip=<span class="keyword">False</span>,</span><br><span class="line">        transforms=[</span><br><span class="line">            dict(type=<span class="string">'Resize'</span>, keep_ratio=<span class="keyword">True</span>),</span><br><span class="line">            dict(type=<span class="string">'RandomFlip'</span>),</span><br><span class="line">            dict(type=<span class="string">'Normalize'</span>, **img_norm_cfg),</span><br><span class="line">            dict(type=<span class="string">'Pad'</span>, size_divisor=<span class="number">32</span>),</span><br><span class="line">            dict(type=<span class="string">'ImageToTensor'</span>, keys=[<span class="string">'img'</span>]),</span><br><span class="line">            dict(type=<span class="string">'Collect'</span>, keys=[<span class="string">'img'</span>]),</span><br><span class="line">        ])</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>最后这些所有操作被 Compose 串联起来，代码如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">@PIPELINES.register_module()</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Compose</span>(<span class="title">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, transforms)</span></span><span class="symbol">:</span></span><br><span class="line">        assert isinstance(transforms, collections.abc.Sequence) <span class="comment">#列表是序列结构</span></span><br><span class="line">        <span class="keyword">self</span>.transforms = []</span><br><span class="line">        <span class="keyword">for</span> transform <span class="keyword">in</span> <span class="symbol">transforms:</span></span><br><span class="line">            <span class="keyword">if</span> isinstance(transform, dict)<span class="symbol">:</span></span><br><span class="line">                transform = build_from_cfg(transform, PIPELINES)</span><br><span class="line">                <span class="keyword">self</span>.transforms.append(transform)</span><br><span class="line">            elif callable(transform)<span class="symbol">:</span></span><br><span class="line">                <span class="keyword">self</span>.transforms.append(transform)</span><br><span class="line">            <span class="symbol">else:</span></span><br><span class="line">                raise TypeError(<span class="string">'transform must be callable or a dict'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(<span class="keyword">self</span>, data)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="keyword">self</span>.<span class="symbol">transforms:</span></span><br><span class="line">            data = t(data)</span><br><span class="line">            <span class="keyword">if</span> data is <span class="symbol">None:</span></span><br><span class="line">                <span class="keyword">return</span> None</span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        format_string = <span class="keyword">self</span>.__class_<span class="number">_</span>.__name_<span class="number">_</span> + <span class="string">'('</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="keyword">self</span>.<span class="symbol">transforms:</span></span><br><span class="line">            format_string += <span class="string">'\n'</span></span><br><span class="line">            format_string += f<span class="string">'    &#123;t&#125;'</span></span><br><span class="line">        format_string += <span class="string">'\n)'</span></span><br><span class="line">        <span class="keyword">return</span> format_string</span><br></pre></td></tr></table></figure><p>上面代码能看到，配置文件中 pipeline 中的字典传入 build_from_cfg 函数，逐一实现了各个增强类 (方法)。扩展的增强类均需实现 <strong>call</strong> 方法，这和 pytorch 原始方法是一致的。有了以上认识，重新梳理一下 pipelines 的逻辑，由三部分组成，load，transforms，和 format。load 相关的 LoadImageFromFile，LoadAnnotations都是字典 results 进去，字典 results 出来。具体代码看下便知，LoadImageFromFile 增加了’filename’，’img’，’img_shape’，’ori_shape’,’pad_shape’,’scale_factor’,’img_norm_cfg’ 字段。其中 img 是 numpy 格式。LoadAnnotations 从 results[’ann_info’] 中解析出 bboxs,masks,labels 等信息。注意 coco 格式的原始解析来自 pycocotools，包括其评估方法，这里关键是字典结构 (这个和模型损失函数，评估等相关，统一结构，使得代码统一)。transforms 中的类作用于字典的 values，也即数据增强。format 中的 DefaultFormatBundle 是将数据转成 mmcv 扩展的容器类格式 DataContainer。另外 Collect 会根据不同任务的不同配置，从 results 中选取只含 keys 的信息生成新的字典，具体看下该类帮助文档。</p><h4 id="DataContainer"><a href="#DataContainer" class="headerlink" title="DataContainer"></a>DataContainer</h4><p>那么 DataContainer 是什么呢？它是对 tensor 的封装，将 results 中的 tensor 转成 DataContainer 格式，实际上只是增加了几个 property 函数，cpu_only，stack，padding_value，pad_dims，其含义自明，以及 size，dim用来获取数据的维度，形状信息。考虑到序列数据在进入 DataLoader 时，需要以 batch 方式进入模型，那么通常的 collate_fn 会要求 tensor 数据的形状一致。但是这样不是很方便，于是有了 DataContainer。它可以做到载入 GPU 的数据可以保持统一 shape，并被 stack，也可以不 stack，也可以保持原样，或者在非 batch 维度上做 pad。当然这个也要对 default_collate进行改造，mmcv 在 parallel.collate 中实现了这个。</p><p>collate_fn 是 DataLoader 中将序列 dataset 组织成 batch 大小的函数，这里帖三个普通例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn_1</span><span class="params">(batch)</span> :</span></span><br><span class="line">    <span class="comment"># 这 是 默 认 的， 明 显batch中 包 含 相 同 形 状 的img\_tensor和 label</span></span><br><span class="line">    <span class="keyword">return</span> tuple ( zip(*batch))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coco_collate_2</span><span class="params">(batch)</span> :</span></span><br><span class="line">    <span class="comment"># 传 入 的batch数 据 是 被albu增 强 后 的(字 典 结 构)</span></span><br><span class="line">    imgs = [s [ ’image’ ] <span class="keyword">for</span> s <span class="keyword">in</span> batch] </span><br><span class="line">    annots = [s [ ’bboxes’ ] <span class="keyword">for</span> s <span class="keyword">in</span> batch]</span><br><span class="line">    labels = [s [ ’category_id’ ] <span class="keyword">for</span> s <span class="keyword">in</span> batch]</span><br><span class="line">    <span class="comment"># 以 当 前batch中 图 片annot数 量 的 最 大 值 作 为 标 记 数 据 的 第 二 维 度 值， 空 出 的 就补−1</span></span><br><span class="line">    max_num_annots = max( len (annot) <span class="keyword">for</span> annot <span class="keyword">in</span> annots)</span><br><span class="line">    annot_padded = np. ones (( len (annots) , max_num_annots, <span class="number">5</span>))*−<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> max_num_annots &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">for</span> idx , (annot , lab) <span class="keyword">in</span> enumerate( zip (annots , labels )) :</span><br><span class="line">            <span class="keyword">if</span> len (annot) &gt; <span class="number">0</span>:</span><br><span class="line">                annot_padded[idx,:len(annot),:<span class="number">4</span>] = annot</span><br><span class="line">                annot_padded[idx,:len(annot),<span class="number">2</span>] += annot_padded[idx,:len(annot),<span class="number">0</span>]</span><br><span class="line">                annot_padded[idx,:len(annot),<span class="number">3</span>] += annot_padded[idx,:len(annot),<span class="number">1</span>]</span><br><span class="line">                annot_padded[idx,:len(annot),:] /= <span class="number">640</span></span><br><span class="line">    annot_padded[idx,:len(annot),<span class="number">4</span>] = lab</span><br><span class="line">   <span class="keyword">return</span> torch.stack(imgs,<span class="number">0</span>), torch.FloatTensor(annot_padded)   </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detection_collate_3</span><span class="params">(batch)</span> :</span></span><br><span class="line">    targets = []</span><br><span class="line">    imgs = [ ]</span><br><span class="line">    <span class="keyword">for</span> _, sample <span class="keyword">in</span> enumerate(batch) :</span><br><span class="line">        <span class="keyword">for</span> _, img_anno <span class="keyword">in</span> enumerate(sample) :</span><br><span class="line">            <span class="keyword">if</span> torch.is_tensor(img_anno) :</span><br><span class="line">                imgs.append(img_anno)</span><br><span class="line">           <span class="keyword">elif</span> isinstance(img_anno, np.ndarray) :  </span><br><span class="line">                annos = torch.from_numpy(img_anno).float()</span><br><span class="line">                targets.append(annos)</span><br><span class="line">   <span class="keyword">return</span> torch.stack(imgs,<span class="number">0</span>), targets <span class="comment"># 做了stack，DataContainer可以不做 stack</span></span><br></pre></td></tr></table></figure><p>以上就是数据处理的相关内容。最后再用 DataLoader 封装拆成迭代器，其相关细节，sampler 等暂略。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data_loader = DataLoader(</span><br><span class="line">        dataset,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        sampler=sampler,</span><br><span class="line">        num_workers=num_workers,</span><br><span class="line">        collate_fn=partial(collate, samples_per_gpu=samples_per_gpu),</span><br><span class="line">        pin_memory=<span class="keyword">False</span>,</span><br><span class="line">        worker_init_fn=init_fn,</span><br><span class="line">        **kwargs)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;数据处理&quot;&gt;&lt;a href=&quot;#数据处理&quot; class=&quot;headerlink&quot; title=&quot;数据处理&quot;&gt;&lt;/a&gt;数据处理&lt;/h3&gt;&lt;p&gt;数据处理可能是炼丹师接触最为密集的了，因为通常情况，除了数据的离线处理，写个数据类，就可以炼丹了。但本节主要涉及数据的在线处理，更进一步应该是检测分割数据的 pytorch 处理方式。虽然 mmdet 将常用的数据都实现了，而且也实现了中间通用数据格式，但，这和模型，损失函数，性能评估的实现也相关，比如你想把官网的 centernet 完整的改成 mmdet风格，就能看到 (看起来没必要)。&lt;/p&gt;
    
    </summary>
    
      <category term="mmdetection" scheme="https://nicehuster.github.io/categories/mmdetection/"/>
    
    
      <category term="mmdetection" scheme="https://nicehuster.github.io/tags/mmdetection/"/>
    
  </entry>
  
  <entry>
    <title>mmdetection详解指北 (二)</title>
    <link href="https://nicehuster.github.io/2020/09/08/2020-09-06-mmdetection-2/"/>
    <id>https://nicehuster.github.io/2020/09/08/2020-09-06-mmdetection-2/</id>
    <published>2020-09-08T11:14:39.000Z</published>
    <updated>2020-09-17T12:45:02.069Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇博客主要介绍到mmdetection这个检测框架的一些结构设计以及代码的总体逻辑。这篇就主要介绍一下在mmdetection被大量使用的配置和注册。</p><h3 id="配置类"><a href="#配置类" class="headerlink" title="配置类"></a>配置类</h3><p>配置方式支持 python/json/yaml, 从 mmcv 的 Config 解析, 其功能同 maskrcnn-benchmark 的 yacs 类似, 将字典的取值方式属性化. 这里帖部分代码，以供学习。</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Config</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""A facility for config and config files.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It supports common file formats as configs: python/json/yaml. The interface</span></span><br><span class="line"><span class="string">    is the same as a dict object and also allows access config values as</span></span><br><span class="line"><span class="string">    attributes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg = Config(dict(a=1, b=dict(b1=[0, 1])))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg.a</span></span><br><span class="line"><span class="string">        1</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg.b</span></span><br><span class="line"><span class="string">        &#123;'b1': [0, 1]&#125;</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg.b.b1</span></span><br><span class="line"><span class="string">        [0, 1]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg = Config.fromfile('tests/data/config/a.py')</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg.filename</span></span><br><span class="line"><span class="string">        "/home/kchen/projects/mmcv/tests/data/config/a.py"</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg.item4</span></span><br><span class="line"><span class="string">        'test'</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg</span></span><br><span class="line"><span class="string">        "Config [path: /home/kchen/projects/mmcv/tests/data/config/a.py]: "</span></span><br><span class="line"><span class="string">        "&#123;'item1': [1, 2], 'item2': &#123;'a': 0&#125;, 'item3': True, 'item4': 'test'&#125;"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fromfile</span><span class="params">(filename)</span>:</span></span><br><span class="line">        filename = osp.abspath(osp.expanduser(filename))</span><br><span class="line">        check_file_exist(filename)</span><br><span class="line">        <span class="keyword">if</span> filename.endswith(<span class="string">'.py'</span>):</span><br><span class="line">            module_name = osp.basename(filename)[:<span class="number">-3</span>]</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'.'</span> <span class="keyword">in</span> module_name:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">'Dots are not allowed in config file path.'</span>)</span><br><span class="line">            config_dir = osp.dirname(filename)</span><br><span class="line">            sys.path.insert(<span class="number">0</span>, config_dir)</span><br><span class="line">            mod = import_module(module_name)</span><br><span class="line">            sys.path.pop(<span class="number">0</span>)</span><br><span class="line">            cfg_dict = &#123;</span><br><span class="line">                name: value</span><br><span class="line">                <span class="keyword">for</span> name, value <span class="keyword">in</span> mod.__dict__.items()</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> name.startswith(<span class="string">'__'</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">elif</span> filename.endswith((<span class="string">'.yml'</span>, <span class="string">'.yaml'</span>, <span class="string">'.json'</span>)):</span><br><span class="line">            <span class="keyword">import</span> mmcv</span><br><span class="line">            cfg_dict = mmcv.load(filename)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> IOError(<span class="string">'Only py/yml/yaml/json type are supported now!'</span>)</span><br><span class="line">        <span class="keyword">return</span> Config(cfg_dict, filename=filename)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">auto_argparser</span><span class="params">(description=None)</span>:</span></span><br><span class="line">        <span class="string">"""Generate argparser from config file automatically (experimental)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        partial_parser = ArgumentParser(description=description)</span><br><span class="line">        partial_parser.add_argument(<span class="string">'config'</span>, help=<span class="string">'config file path'</span>)</span><br><span class="line">        cfg_file = partial_parser.parse_known_args()[<span class="number">0</span>].config</span><br><span class="line">        cfg = Config.fromfile(cfg_file)</span><br><span class="line">        parser = ArgumentParser(description=description)</span><br><span class="line">        parser.add_argument(<span class="string">'config'</span>, help=<span class="string">'config file path'</span>)</span><br><span class="line">        add_args(parser, cfg)</span><br><span class="line">        <span class="keyword">return</span> parser, cfg</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, cfg_dict=None, filename=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> cfg_dict <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            cfg_dict = dict()</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> isinstance(cfg_dict, dict):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">'cfg_dict must be a dict, but got &#123;&#125;'</span>.format(</span><br><span class="line">                type(cfg_dict)))</span><br><span class="line"></span><br><span class="line">        super(Config, self).__setattr__(<span class="string">'_cfg_dict'</span>, ConfigDict(cfg_dict))</span><br><span class="line">        super(Config, self).__setattr__(<span class="string">'_filename'</span>, filename)</span><br><span class="line">        <span class="keyword">if</span> filename:</span><br><span class="line">            <span class="keyword">with</span> open(filename, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                super(Config, self).__setattr__(<span class="string">'_text'</span>, f.read())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            super(Config, self).__setattr__(<span class="string">'_text'</span>, <span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">filename</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._filename</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">text</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._text</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'Config (path: &#123;&#125;): &#123;&#125;'</span>.format(self.filename,</span><br><span class="line">                                              self._cfg_dict.__repr__())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self._cfg_dict)</span><br><span class="line">    <span class="comment"># 获取key值</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getattr__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> getattr(self._cfg_dict, name)</span><br><span class="line">    <span class="comment"># 序列化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._cfg_dict.__getitem__(name)</span><br><span class="line">    <span class="comment"># 序列化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setattr__</span><span class="params">(self, name, value)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(value, dict):</span><br><span class="line">            value = ConfigDict(value)</span><br><span class="line">        self._cfg_dict.__setattr__(name, value)</span><br><span class="line">    <span class="comment"># 更新key值</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setitem__</span><span class="params">(self, name, value)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(value, dict):</span><br><span class="line">            value = ConfigDict(value)</span><br><span class="line">        self._cfg_dict.__setitem__(name, value)</span><br><span class="line">    <span class="comment"># 迭代器</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> iter(self._cfg_dict)</span><br></pre></td></tr></table></figure><p>主要考虑点是自己怎么实现类似的东西，核心点就是 python 的基本魔法函数的应用，可同时参考 yacs。</p><h4 id="注册器"><a href="#注册器" class="headerlink" title="注册器"></a>注册器</h4><p>把基本对象放到一个继承了字典的对象中，实现了对象的灵活管理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Registry</span>:</span></span><br><span class="line">    <span class="string">"""A registry to map strings to classes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        name (str): Registry name.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self._name = name</span><br><span class="line">        self._module_dict = dict()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self._module_dict)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__contains__</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.get(key) <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        format_str = self.__class__.__name__ + \</span><br><span class="line">                     <span class="string">f'(name=<span class="subst">&#123;self._name&#125;</span>, '</span> \</span><br><span class="line">                     <span class="string">f'items=<span class="subst">&#123;self._module_dict&#125;</span>)'</span></span><br><span class="line">        <span class="keyword">return</span> format_str</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">name</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._name</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">module_dict</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._module_dict</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="string">"""Get the registry record.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            key (str): The class name in string format.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            class: The corresponding class.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self._module_dict.get(key, <span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_register_module</span><span class="params">(self, module_class, module_name=None, force=False)</span>:</span></span><br><span class="line">        <span class="comment"># 校验当前注册的module_class是否是类对象</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> inspect.isclass(module_class):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">'module must be a class, '</span></span><br><span class="line">                            <span class="string">f'but got <span class="subst">&#123;type(module_class)&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> module_name <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            module_name = module_class.__name__</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> force <span class="keyword">and</span> module_name <span class="keyword">in</span> self._module_dict:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(<span class="string">f'<span class="subst">&#123;module_name&#125;</span> is already registered '</span></span><br><span class="line">                           <span class="string">f'in <span class="subst">&#123;self.name&#125;</span>'</span>)</span><br><span class="line">        self._module_dict[module_name] = module_class  <span class="comment"># 类 名 : 类</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deprecated_register_module</span><span class="params">(self, cls=None, force=False)</span>:</span></span><br><span class="line">        warnings.warn(</span><br><span class="line">            <span class="string">'The old API of register_module(module, force=False) '</span></span><br><span class="line">            <span class="string">'is deprecated and will be removed, please use the new API '</span></span><br><span class="line">            <span class="string">'register_module(name=None, force=False, module=None) instead.'</span>)</span><br><span class="line">        <span class="keyword">if</span> cls <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> partial(self.deprecated_register_module, force=force)</span><br><span class="line">        self._register_module(cls, force=force)</span><br><span class="line">        <span class="keyword">return</span> cls</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">register_module</span><span class="params">(self, name=None, force=False, module=None)</span>:</span></span><br><span class="line">        <span class="comment"># 作 为 类 name 的 装 饰 器</span></span><br><span class="line">        <span class="string">"""Register a module.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        A record will be added to `self._module_dict`, whose key is the class</span></span><br><span class="line"><span class="string">        name or the specified name, and value is the class itself.</span></span><br><span class="line"><span class="string">        It can be used as a decorator or a normal function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Example:</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt; backbones = Registry('backbone')</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt; @backbones.register_module()</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt; class ResNet:</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt;     pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            name (str | None): The module name to be registered. If not</span></span><br><span class="line"><span class="string">                specified, the class name will be used.</span></span><br><span class="line"><span class="string">            force (bool, optional): Whether to override an existing class with</span></span><br><span class="line"><span class="string">                the same name. Default: False.</span></span><br><span class="line"><span class="string">            module (type): Module class to be registered.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(force, bool):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">f'force must be a boolean, but got <span class="subst">&#123;type(force)&#125;</span>'</span>)</span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> This is a walkaround to be compatible with the old api,</span></span><br><span class="line">        <span class="comment"># while it may introduce unexpected bugs.</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(name, type):</span><br><span class="line">            <span class="keyword">return</span> self.deprecated_register_module(name, force=force)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># use it as a normal method: x.register_module(module=SomeClass)</span></span><br><span class="line">        <span class="keyword">if</span> module <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self._register_module(</span><br><span class="line">                module_class=module, module_name=name, force=force)</span><br><span class="line">            <span class="keyword">return</span> module</span><br><span class="line"></span><br><span class="line">        <span class="comment"># raise the error ahead of time</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> (name <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">or</span> isinstance(name, str)):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">f'name must be a str, but got <span class="subst">&#123;type(name)&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># use it as a decorator: @x.register_module()</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_register</span><span class="params">(cls)</span>:</span></span><br><span class="line">            self._register_module(</span><br><span class="line">                module_class=cls, module_name=name, force=force)</span><br><span class="line">            <span class="keyword">return</span> cls</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> _register</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_from_cfg</span><span class="params">(cfg, registry, default_args=None)</span>:</span></span><br><span class="line">    <span class="string">"""Build a module from config dict.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        cfg (dict): Config dict. It should at least contain the key "type".</span></span><br><span class="line"><span class="string">        registry (:obj:`Registry`): The registry to search the type from.</span></span><br><span class="line"><span class="string">        default_args (dict, optional): Default initialization arguments.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        object: The constructed object.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(cfg, dict):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">f'cfg must be a dict, but got <span class="subst">&#123;type(cfg)&#125;</span>'</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'type'</span> <span class="keyword">not</span> <span class="keyword">in</span> cfg:</span><br><span class="line">        <span class="keyword">raise</span> KeyError(</span><br><span class="line">            <span class="string">f'the cfg dict must contain the key "type", but got <span class="subst">&#123;cfg&#125;</span>'</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(registry, Registry):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">'registry must be an mmcv.Registry object, '</span></span><br><span class="line">                        <span class="string">f'but got <span class="subst">&#123;type(registry)&#125;</span>'</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> (isinstance(default_args, dict) <span class="keyword">or</span> default_args <span class="keyword">is</span> <span class="keyword">None</span>):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">'default_args must be a dict or None, '</span></span><br><span class="line">                        <span class="string">f'but got <span class="subst">&#123;type(default_args)&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">    args = cfg.copy()</span><br><span class="line">    obj_type = args.pop(<span class="string">'type'</span>)</span><br><span class="line">    <span class="keyword">if</span> is_str(obj_type):</span><br><span class="line">        <span class="comment"># 从 注 册 类 中 拿 出obj_type类</span></span><br><span class="line">        obj_cls = registry.get(obj_type)</span><br><span class="line">        <span class="keyword">if</span> obj_cls <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(</span><br><span class="line">                <span class="string">f'<span class="subst">&#123;obj_type&#125;</span> is not in the <span class="subst">&#123;registry.name&#125;</span> registry'</span>)</span><br><span class="line">    <span class="keyword">elif</span> inspect.isclass(obj_type):</span><br><span class="line">        obj_cls = obj_type</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> TypeError(</span><br><span class="line">            <span class="string">f'type must be a str or valid type, but got <span class="subst">&#123;type(obj_type)&#125;</span>'</span>)</span><br><span class="line">    <span class="comment"># 增 加 一 些 新 的 参 数</span></span><br><span class="line">    <span class="keyword">if</span> default_args <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">for</span> name, value <span class="keyword">in</span> default_args.items():</span><br><span class="line">            args.setdefault(name, value)</span><br><span class="line">    <span class="keyword">return</span> obj_cls(**args)<span class="comment"># **args 是 将 字 典 解 析 成 位 置 参 数(k=v)。</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇博客主要介绍到mmdetection这个检测框架的一些结构设计以及代码的总体逻辑。这篇就主要介绍一下在mmdetection被大量使用的配置和注册。&lt;/p&gt;
&lt;h3 id=&quot;配置类&quot;&gt;&lt;a href=&quot;#配置类&quot; class=&quot;headerlink&quot; title=&quot;配置类&quot;&gt;&lt;/a&gt;配置类&lt;/h3&gt;&lt;p&gt;配置方式支持 python/json/yaml, 从 mmcv 的 Config 解析, 其功能同 maskrcnn-benchmark 的 yacs 类似, 将字典的取值方式属性化. 这里帖部分代码，以供学习。&lt;/p&gt;
    
    </summary>
    
      <category term="mmdetection" scheme="https://nicehuster.github.io/categories/mmdetection/"/>
    
    
      <category term="mmdetection" scheme="https://nicehuster.github.io/tags/mmdetection/"/>
    
  </entry>
  
  <entry>
    <title>mmdetection详解指北 (一)</title>
    <link href="https://nicehuster.github.io/2020/09/08/2020-09-06-mmdetection-1/"/>
    <id>https://nicehuster.github.io/2020/09/08/2020-09-06-mmdetection-1/</id>
    <published>2020-09-08T11:13:39.000Z</published>
    <updated>2020-09-17T12:43:21.284Z</updated>
    
    <content type="html"><![CDATA[<p>平时做的都些检测相关的项目，因此对于各类检测框架使用较多，以及一些不知名的repo都有用过，平时接触的都是业务项目，很少去认真的看一个repo中算法的框架设计，最近项目处于交付阶段，主要是客户和平台开发人员对界面的问题。趁有空mmdetection的代码重新看了一遍，顺便做了一些笔记。</p><h3 id="组件设计"><a href="#组件设计" class="headerlink" title="组件设计"></a>组件设计</h3><blockquote><ul><li>BackBone: 特征提取骨架网络,ResNet,ResneXt,ssd_vgg, hrnet 等。</li><li>Neck: 连接骨架和头部. 多层级特征融合,FPN,BFP,PAFPN 等。</li><li>DenseHead: 处理特征图上的密集框部分, 主要分 AnchorHead。AnchorFreeHead 两大类，分别有 RPNHead, SSDHead,RetinaHead 和 FCOSHead 等。</li><li>RoIHead (BBoxHead/MaskHead): 在特征图上对 roi 做类别分类或位置回归等 (1.x)。</li><li>ROIHead:bbox 或 mask 的 roi_extractor+head(2.0, 合并了 extractor 和 head)</li><li>SingleStage: BackBone + Neck + DenseHead</li><li>TwoStage: BackBone + Neck + DenseHead + RoIHead(2.0)</li></ul></blockquote><a id="more"></a><h3 id="结构设计"><a href="#结构设计" class="headerlink" title="结构设计"></a>结构设计</h3><h4 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h4><blockquote><ul><li><p>configs 网络组件结构等配置信息 </p></li><li><p>tools: 训练和测试的最终包装和一些实用脚本 </p></li><li><p>mmdet:</p><blockquote><ul><li>apis: 分布式环境设定 (1.x,2.0 移植到 mmcv), 推断, 测试, 训练基础代码;</li><li>core: anchor 生成,bbox,mask 编解码, 变换, 标签锚定, 采样等, 模型评估, 加速, 优化器，后处理;</li><li>datasets:coco,voc 等数据类, 数据 pipelines 的统一格式, 数据增强，数据采样;</li><li>models: 模型组件 (backbone,head,loss,neck)，采用注册和组合构建的形式完成模型搭建</li><li>ops: 优化加速代码, 包括 nms,roialign,dcn,masked_conv，focal_loss 等</li></ul></blockquote></li></ul></blockquote><p><img src="/img/image-20200917202833058.png" alt="image-20200917202833058"></p><h4 id="总体逻辑"><a href="#总体逻辑" class="headerlink" title="总体逻辑"></a>总体逻辑</h4><p>从 tools/train.py 中能看到整体可分如下 4 个步骤:</p><blockquote><ul><li><p>1.mmcv.Config.fromfile 从配置文件解析配置信息, 并做适当更新, 包括环境搜集，预加载模型文件, 分布式设置，日志记录等;</p></li><li><p>2.mmdet.models 中的 build_detector 根据配置信息构造模型 ;</p><blockquote><ul><li>2.1 build 系列函数调用 build_from_cfg 函数, 按 type 关键字从注册表中获取相应的对象, 对象的具名参数在注册文件中赋值;</li><li>2.2 registr.py 放置了模型的组件注册器。其中注册器的 register_module 成员函数是一个装饰器功能函数，在具体的类对象 <em>A</em> 头上装饰 @X.register _module，并同时在 <em>A</em> 对象所在包的初始化文件中调用 <em>A</em>，即可将 <em>A</em> 保存到 registry.module_dict 中, 完成注册;_</li><li>2.3 目前包含 BACKBONES,NECKS,ROI_EXTRACTORS,SHARED_ HEADS,HEADS,LOSSES,DETECTORS 七个模型相关注册器，另外还有数据类，优化器等注册器;</li></ul></blockquote></li></ul><ul><li><p>3.build_dataset 根据配置信息获取数据类;</p><blockquote><ul><li>3.1 coco，cityscapes，voc，deepfasion，lvis，wider_face 等数据 (数据类扩展见后续例子)。</li></ul></blockquote></li><li><p>4.train_detector 模型训练流程:</p><blockquote><ul><li>4.1数据 loader 化, 模型分布式化，优化器选取</li><li>4.2 进入 runner 训练流程 (来自 mmcv 库，采用 hook 方式，整合了 pytorch 训练流程)</li><li>4.3 训练 pipelines 具体细节见后续展开。</li></ul></blockquote></li></ul></blockquote><p>后续说说配置文件，注册机制和训练逻辑。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;平时做的都些检测相关的项目，因此对于各类检测框架使用较多，以及一些不知名的repo都有用过，平时接触的都是业务项目，很少去认真的看一个repo中算法的框架设计，最近项目处于交付阶段，主要是客户和平台开发人员对界面的问题。趁有空mmdetection的代码重新看了一遍，顺便做了一些笔记。&lt;/p&gt;
&lt;h3 id=&quot;组件设计&quot;&gt;&lt;a href=&quot;#组件设计&quot; class=&quot;headerlink&quot; title=&quot;组件设计&quot;&gt;&lt;/a&gt;组件设计&lt;/h3&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;BackBone: 特征提取骨架网络,ResNet,ResneXt,ssd_vgg, hrnet 等。&lt;/li&gt;
&lt;li&gt;Neck: 连接骨架和头部. 多层级特征融合,FPN,BFP,PAFPN 等。&lt;/li&gt;
&lt;li&gt;DenseHead: 处理特征图上的密集框部分, 主要分 AnchorHead。AnchorFreeHead 两大类，分别有 RPNHead, SSDHead,RetinaHead 和 FCOSHead 等。&lt;/li&gt;
&lt;li&gt;RoIHead (BBoxHead/MaskHead): 在特征图上对 roi 做类别分类或位置回归等 (1.x)。&lt;/li&gt;
&lt;li&gt;ROIHead:bbox 或 mask 的 roi_extractor+head(2.0, 合并了 extractor 和 head)&lt;/li&gt;
&lt;li&gt;SingleStage: BackBone + Neck + DenseHead&lt;/li&gt;
&lt;li&gt;TwoStage: BackBone + Neck + DenseHead + RoIHead(2.0)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="mmdetection" scheme="https://nicehuster.github.io/categories/mmdetection/"/>
    
    
      <category term="mmdetection" scheme="https://nicehuster.github.io/tags/mmdetection/"/>
    
  </entry>
  
  <entry>
    <title>抠图神器U^2-Net</title>
    <link href="https://nicehuster.github.io/2020/09/07/0064-U-2-Net/"/>
    <id>https://nicehuster.github.io/2020/09/07/0064-U-2-Net/</id>
    <published>2020-09-07T11:13:39.000Z</published>
    <updated>2020-09-18T03:15:23.384Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> Going Deeper with Nested U-Structure for Salient Object Detection<br><strong>代码链接：</strong><a href="https://github.com/NathanUA/U-2-Net" target="_blank" rel="noopener">https://github.com/NathanUA/U-2-Net</a><br><strong>整体信息：</strong> 这是发表在PR2020上的一篇关于显著性检测的文章，作者是秦雪彬。从标题上可以看到本文的一个idea是设计了一个deeper的U型结构的网络解决显著性目标检测问题。作者认为，目前显著性目标检测有两种主流思路，一为多层次深层特征集成multi-level deep feature integration，一为多尺度特征提取Multi-scale feature extraction。多层次深层特征集成方法主要集中在开发更好的多层次特征聚合策略上。而多尺度特征提取这一类方法旨在设计更新的模块，从主干网获取的特征中同时提取局部和全局信息。而几乎所有上述方法，都是为了更好地利用现有的图像分类的Backbones生成的特征映射。而作者另辟蹊径，提出了一种新颖而简单的结构，它直接逐级提取多尺度特征，用于显著目标检测，而不是利用这些主干的特征来开发和添加更复杂的模块和策略。下图是该方法与其他方法的一个比较：</p><a id="more"></a><p><img src="/img/image-20200918105536819.png" alt="image-20200918105536819"></p><h3 id="显著性检测"><a href="#显著性检测" class="headerlink" title="显著性检测"></a>显著性检测</h3><p>在讲这篇文章之前，有必要先了解显著性检测这个任务。</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/115002897" target="_blank" rel="noopener">显著性检测</a></p></blockquote><p><strong>显著性检测</strong>，就是使用图像处理技术和计算机视觉算法来定位图片中最“显著”的区域。显著区域就是指图片中引人注目的区域或比较重要的区域，例如人眼在观看一幅图片时会首先关注的区域。例如下图，我们人眼一眼看过去首先注意到的不是草坪，而是躺在草坪上的内马尔，内马尔所在的区域就是显著性区域。这种自动定位图像或场景重要区域的过程称为<strong>显着性检测</strong>。</p><p><img src="/img/v2-422783ab5d5d66fd0dbf5653166ddbd8_720w.jpg" alt></p><h4 id="显著性检测和图像分割区别"><a href="#显著性检测和图像分割区别" class="headerlink" title="显著性检测和图像分割区别"></a>显著性检测和图像分割区别</h4><p>这个任务和图像分割十分类似，区别在于：</p><blockquote><ul><li>1.目标数量：显著性目标检测一般只检一个目标，一般目标检测不会限制数量；</li><li>目标类别： 显著性目标检测不关心目标类别，只关心显著性强的目标，一般目标检测会得到目标位置和类别；</li><li>问题建模不同：显著性目标检测，很早期的时候是通过对一些共性的特征建模，比如该目标一般在图像中心，一般是什么样的颜色分布等等。一般目标检测问题，则是希望特征能够更好的把目标表达出来，越细节越好，特征越丰富约好，因为要区分类别</li><li>groundtruth定义不同：两者互有交集，也有不同；前者，往往定义的是一些显著性较强的目标，比如行人，动物等等，而且前者是不输出类别标签信息；后者，gt标签定义是明确的，可以严格的输出对应类别标签；</li></ul></blockquote><p>其实，说白了，显著性检测等同于是一个二分类的语义分割模型，此外，显著的区域 不一定就是 目标，目标很可能不是显著的；</p><h3 id="方法设计"><a href="#方法设计" class="headerlink" title="方法设计"></a>方法设计</h3><h4 id="Residual-U-blocks"><a href="#Residual-U-blocks" class="headerlink" title="Residual U-blocks"></a>Residual U-blocks</h4><p>了解了显著性检测这个任务之后，来具体了解一下这篇文章的具体方法设计。作者设计一种Residual U-blocks，用于捕获 intra-stage 的 multi-scales 特征。</p><p><img src="/img/image-20200917201033485.png" alt="image-20200917201033485"></p><p>上图为普通卷积block，Res-like block，Inception-like block，Dense-like block和Residual U-blocks的对比图，明显可以看出Residual U-blocks是受了U-Net的启发。</p><p>Residual U-blocks有以下三部分组成：</p><blockquote><ul><li>一个输入卷积层，它将输入的feature map x (H × W × $C_{in}$)转换成中间feature map $F_1(x)$，$F_1(x)$通道数为$C_{out}$。这是一个用于局部特征提取的普通卷积层。</li><li>一个U-like的对称的encoder-decoder结构，高度为L，以中间feature map $F_1(x)$为输入，去学习提取和编码多尺度文本信息$U(F_1(x))$,U表示类U-Net结构。更大L会得到更深层的U-block（RSU），更多的池操作，更大的感受野和更丰富的局部和全局特征。配置此参数允许从具有任意空间分辨率的输入特征图中提取多尺度特征。从逐渐降采样特征映射中提取多尺度特征，并通过渐进上采样、合并和卷积等方法将其编码到高分辨率的特征图中。这一过程减少了大尺度直接上采样造成的细节损失。</li><li>一种残差连接，它通过求和来融合局部特征和多尺度特征：$F_1(x) + U(F_1(x))$。</li></ul></blockquote><p><img src="/img/image-20200917201132725.png" alt="image-20200917201132725"></p><p>RSU与Res block的主要设计区别在于RSU用U-Net结构代替了普通的单流卷积，用一个权重层(weight layer)形成的局部特征来代替原始特征。这种设计的变更使网络能够从多个尺度直接从每个残差块提取特征。更值得注意的是，U结构的计算开销很小，因为大多数操作都是在下采样的特征映射上进行的。</p><h4 id="U-2-Net的结构"><a href="#U-2-Net的结构" class="headerlink" title="U^2-Net的结构"></a>U^2-Net的结构</h4><p>U^2-Net的网络结构如下：</p><p><img src="/img/image-20200917201255253.png" alt="image-20200917201255253"></p><p>与U-Net的网络结构做一个对比：</p><p><img src="/img/1*TXfEPqTbFBPCbXYh2bstlA.png" alt="Learn How to Train U-Net On Your Dataset | by Sukriti Paul | Coinmonks |  Medium"></p><p>直观上可以发现，U^2-Net的每一个Block都是一个U-Net结构的模块，即上述Residual U-blocks。当然，你也可以继续Going Deeper, 每个Block里面的U-Net的子Block仍然可以是一个U-Net结构，命名为U^3-Net。</p><h3 id="损失函数设计"><a href="#损失函数设计" class="headerlink" title="损失函数设计"></a>损失函数设计</h3><p>类似于HED算法的deep supervision方式，作者设计了如下函数：</p><script type="math/tex; mode=display">\mathcal{L}=\sum_{m=1}^{M} w_{\text {side}}^{(m)} \ell_{\text {side}}^{(m)}+w_{\text {fuse}} \ell_{\text {fuse}}</script><p>其中，M=6, 为U2Net 的 Sup1, Sup2, …, Sup6 stage.$w_{\text {side}}^{(m)}$  $l_{\text {side}}^{(m)}$ 为对应的损失函数输出和权重；$w_{f u s e} \ell_{f u s e}$ 为融合的损失函数和权重;对于每一个$l$使用的都是标准的BCE Loss：</p><script type="math/tex; mode=display">\ell=-\sum_{(r, c)}^{(H, W)}\left[P_{G(r, c)} \log P_{S(r, c)}+\left(1-P_{G(r, c)}\right) \log \left(1-P_{S(r, c)}\right)\right]</script><h3 id="实验可视化"><a href="#实验可视化" class="headerlink" title="实验可视化"></a>实验可视化</h3><p>所提出的模型是使用DUTS-TR数据集进行训练，该数据集包含大约10000个样本图像，并使用标准数据增强技术进行扩充。研究人员在6个用于突出目标检测的基准数据集上评估了该模型：DUT-OMRON、DUTS-TE、HKU-IS、ECSSD、PASCAL-S和SOD。评价结果表明，在这6个基准点上，新模型与现有方法具有相当好的性能。</p><p><img src="/img/image-20200918110749081.png" alt="image-20200918110749081"></p><p>U^2-Net的实现是开源的，并提供了两种不同的预训练模型：U2Net(176.3M的较大模型，在GTX 1080Ti GPU上为30 FPS)，以及U2NetP(4.7M小模型，最高可达到40 FPS)。代码和预训练模型都可以在<a href="https://github.com/NathanUA/U-2-Net" target="_blank" rel="noopener">Github</a>。下面是我直接用作者开源的模型跑出来的结果，抠图效果很好，精细到发丝的那种。</p><p><img src="/img/image-20200917201523395.png" alt="image-20200917201523395"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>该论文的优势在于：</p><blockquote><ul><li>提出RSU模块，融合不同尺度感受野的特征，来捕捉不同尺度的上下文信息；</li><li>基于 RSU 模块的 池化(pooling) 操作，在不显著增加计算成本的前提下，增加了整个网络结构的深度(depth).</li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; Going Deeper with Nested U-Structure for Salient Object Detection&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/NathanUA/U-2-Net&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/NathanUA/U-2-Net&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt; 这是发表在PR2020上的一篇关于显著性检测的文章，作者是秦雪彬。从标题上可以看到本文的一个idea是设计了一个deeper的U型结构的网络解决显著性目标检测问题。作者认为，目前显著性目标检测有两种主流思路，一为多层次深层特征集成multi-level deep feature integration，一为多尺度特征提取Multi-scale feature extraction。多层次深层特征集成方法主要集中在开发更好的多层次特征聚合策略上。而多尺度特征提取这一类方法旨在设计更新的模块，从主干网获取的特征中同时提取局部和全局信息。而几乎所有上述方法，都是为了更好地利用现有的图像分类的Backbones生成的特征映射。而作者另辟蹊径，提出了一种新颖而简单的结构，它直接逐级提取多尺度特征，用于显著目标检测，而不是利用这些主干的特征来开发和添加更复杂的模块和策略。下图是该方法与其他方法的一个比较：&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://nicehuster.github.io/categories/paper-reading/"/>
    
    
      <category term="saliency detection" scheme="https://nicehuster.github.io/tags/saliency-detection/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：Delving Deeper into Anti-aliasing in ConvNets</title>
    <link href="https://nicehuster.github.io/2020/09/06/0065-Adaptive-anti-Aliasing/"/>
    <id>https://nicehuster.github.io/2020/09/06/0065-Adaptive-anti-Aliasing/</id>
    <published>2020-09-06T11:13:39.000Z</published>
    <updated>2020-09-16T09:01:33.472Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> Xueyan Zou,UC Davis,NVIDIA<br><strong>代码链接：</strong><a href="https://github.com/MaureenZOU/" target="_blank" rel="noopener">https://github.com/MaureenZOU/</a><br><strong>整体框架：</strong>这篇文章是前几天BMVC2020获得best paper award 的一篇文章，这篇文章提出了一个plugin  module ，用来提高CNN的鲁棒性。在图像分类、图像分割、目标检测等任务上都能带来1+个点的提升。对于cnn，众所周知存在一个比较明显的缺陷：即使图像移动几个pixel都可能导致分类识别任务结果发生改变，作者发现其中重要原因在于网络中被大量用来降低参数量的下采样层导致混叠(aliasing)问题，即高频信号在采样后退化为完全不同的部分现象。为此提出了这样一个content-aware anti-aliasing的模块，用于缓解下采样过程中带来的高频信息的退化问题。</p><h3 id="下采样的问题"><a href="#下采样的问题" class="headerlink" title="下采样的问题"></a>下采样的问题</h3><p>以一维信号的的降采样为例：</p><script type="math/tex; mode=display">\begin{array}{l}001100110011 \frac{\mathrm{k}=2, \text { stride }=2}{\operatorname{maxpool}} 010101 \\011001100110 \frac{\mathrm{k}=2, \text { stride }=2}{\operatorname{maxpool}} 111111\end{array}</script><a id="more"></a><p>对于k=2,stride=2的maxpool操作而言，对输入信号移动一位数字，经过maxpool之后的输出结果是截然不同的。而在CNN中大量使用降采样来降低参数量，这种aliasing问题更为明显，标准解决方案是在下采样之前应用低通滤波器（例如，高斯模糊）。但是，在整个内容上应用相同的过滤器可能不是最佳选择，因为特征的频率可能会在<strong>空间位置</strong>和<strong>特征通道</strong>之间发生变化。可以看下作者在论文中给出的实验结果：</p><p><img src="/img/image-20200916161719818.png" alt="image-20200916161719818" style="zoom:50%;"></p><p>上图(a)输入图片；(b)直接4x下采样；(c)应用经过调整以匹配噪声频率的单个高斯滤波器后的下采样结果;(d)应用多个空间自适应高斯滤波后的下采样结果（具有很强的背景模糊和边界弱化能力）</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p><img src="/img/image-20200916161851654.png" alt="image-20200916161851654" style="zoom:50%;"></p><p>作者针对空间和通道分别生成低通滤波器用于缓解aliasing问题。基于空间自适应的低通滤波器就是一个简单的分组卷积操作：</p><script type="math/tex; mode=display">Y_{i, j}=\sum_{p, q \in \Omega} w_{i, j}^{p, q} \cdot X_{i+p, j+q}</script><p>在论文中也有提及，为了避免权重为负数，作者取了一个softmax操作。</p><p>之后对上面生成的权重进行分组再和输入进行一次卷积操作，最终输出Y：</p><script type="math/tex; mode=display">Y_{i, j}^{g}=\sum_{p, q \in \Omega} w_{i, j, g}^{p, q} \cdot X_{i+p, j+q}^{c}</script><p>具体可以看下作者放出来的代码：</p><blockquote><p><a href="https://github.com/MaureenZOU/Adaptive-anti-Aliasing/blob/master/models_lpf/layers/pasa.py" target="_blank" rel="noopener">Adaptive-anti-Aliasing/models_lpf/layers/pasa.py</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Downsample_PASA_group_softmax</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, kernel_size, stride=<span class="number">1</span>, pad_type=<span class="string">'reflect'</span>, group=<span class="number">2</span>)</span>:</span></span><br><span class="line">        super(Downsample_PASA_group_softmax, self).__init__()</span><br><span class="line">        self.pad = get_pad_layer(pad_type)(kernel_size//<span class="number">2</span>)</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.kernel_size = kernel_size</span><br><span class="line">        self.group = group</span><br><span class="line"></span><br><span class="line">        self.conv = nn.Conv2d(in_channels, group*kernel_size*kernel_size, kernel_size=kernel_size, stride=<span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.bn = nn.BatchNorm2d(group*kernel_size*kernel_size)</span><br><span class="line">        self.softmax = nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">        nn.init.kaiming_normal_(self.conv.weight, mode=<span class="string">'fan_out'</span>, nonlinearity=<span class="string">'relu'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        sigma = self.conv(self.pad(x))</span><br><span class="line">        sigma = self.bn(sigma)</span><br><span class="line">        sigma = self.softmax(sigma)</span><br><span class="line"></span><br><span class="line">        n,c,h,w = sigma.shape</span><br><span class="line"></span><br><span class="line">        sigma = sigma.reshape(n,<span class="number">1</span>,c,h*w)</span><br><span class="line"></span><br><span class="line">        n,c,h,w = x.shape</span><br><span class="line">        x = F.unfold(self.pad(x), kernel_size=self.kernel_size).reshape((n,c,self.kernel_size*self.kernel_size,h*w))</span><br><span class="line"></span><br><span class="line">        n,c1,p,q = x.shape</span><br><span class="line">        x = x.permute(<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>).reshape(self.group, c1//self.group, n, p, q).permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        n,c2,p,q = sigma.shape</span><br><span class="line">        sigma = sigma.permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>).reshape((p//(self.kernel_size*self.kernel_size), self.kernel_size*self.kernel_size,n,c2,q)).permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        x = torch.sum(x*sigma, dim=<span class="number">3</span>).reshape(n,c1,h,w)</span><br><span class="line">        <span class="keyword">return</span> x[:,:,torch.arange(h)%self.stride==<span class="number">0</span>,:][:,:,:,torch.arange(w)%self.stride==<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>作者分别在分类，检测，分割任务上均有验证其有效性，均能提高1个点左右。具体实验结果可以看原论文，这里就不贴上来了。</p><h3 id="一致性指标"><a href="#一致性指标" class="headerlink" title="一致性指标"></a>一致性指标</h3><p>作者为了验证该方法的有效性，针对不同任务提出了一系列的一致性指标。比如针对分类任务的一致性指标计算如下：</p><script type="math/tex; mode=display">\text { Consistency }=\mathbb{E}_{X, h_{1}, w_{1}, h_{2}, w_{2}} \mathbb{I}\left\{F\left(X_{h_{1}, w_{1}}\right)=F\left(X_{h_{2}, w_{2}}\right)\right\}</script><p>X表示输入图像，$h_{1}, w_{1}, h_{2}, w_{2}$ 表示偏移量。$F(\cdot)$ 表示模型输出的top1的类别标签。具体代码实现可以看下面：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">output0 = model(input[:,:,off0[<span class="number">0</span>]:off0[<span class="number">0</span>]+args.size,off0[<span class="number">1</span>]:off0[<span class="number">1</span>]+args.size])</span><br><span class="line">output1 = model(input[:,:,off1[<span class="number">0</span>]:off1[<span class="number">0</span>]+args.size,off1[<span class="number">1</span>]:off1[<span class="number">1</span>]+args.size])</span><br><span class="line">cur_agree = agreement_correct(output0, output1, target).type(torch.FloatTensor).to(output0.device)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agreement_correct</span><span class="params">(output0, output1, target)</span>:</span></span><br><span class="line">    pred0 = output0.argmax(dim=<span class="number">1</span>, keepdim=<span class="keyword">False</span>)</span><br><span class="line">    pred1 = output1.argmax(dim=<span class="number">1</span>, keepdim=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    agree = pred0.eq(pred1)</span><br><span class="line">    agree_target_pred0 = pred0.eq(target)</span><br><span class="line">    agree_target_pred1 = pred1.eq(target)</span><br><span class="line"></span><br><span class="line">    correct_or = (agree_target_pred0 + agree_target_pred1) &gt; <span class="number">0</span></span><br><span class="line">    agree = agree * correct_or</span><br><span class="line"></span><br><span class="line">    agree = <span class="number">100.</span>*(torch.sum(agree).float() / (torch.sum(correct_or).float() + <span class="number">1e-10</span>)).to(output0.device)</span><br><span class="line">    <span class="keyword">return</span> agree</span><br></pre></td></tr></table></figure><p><code>agreement_correct</code> 是统计一致性指标的函数，具体可看出来统计方法是分别对输入图像随机偏移<code>off0</code>和<code>off1</code> 然后统计在该偏移量下，两者输出一致且和gt一致的比例。对于检测分割任务，作者也提出了相应的一致性指标计算方法：mAISC和mAISC这两个指标。具体计算如下图。</p><p><img src="/img/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_16002437758073.png" alt="企业微信截图_16002437758073"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; Xueyan Zou,UC Davis,NVIDIA&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/MaureenZOU/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/MaureenZOU/&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体框架：&lt;/strong&gt;这篇文章是前几天BMVC2020获得best paper award 的一篇文章，这篇文章提出了一个plugin  module ，用来提高CNN的鲁棒性。在图像分类、图像分割、目标检测等任务上都能带来1+个点的提升。对于cnn，众所周知存在一个比较明显的缺陷：即使图像移动几个pixel都可能导致分类识别任务结果发生改变，作者发现其中重要原因在于网络中被大量用来降低参数量的下采样层导致混叠(aliasing)问题，即高频信号在采样后退化为完全不同的部分现象。为此提出了这样一个content-aware anti-aliasing的模块，用于缓解下采样过程中带来的高频信息的退化问题。&lt;/p&gt;
&lt;h3 id=&quot;下采样的问题&quot;&gt;&lt;a href=&quot;#下采样的问题&quot; class=&quot;headerlink&quot; title=&quot;下采样的问题&quot;&gt;&lt;/a&gt;下采样的问题&lt;/h3&gt;&lt;p&gt;以一维信号的的降采样为例：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{array}{l}
001100110011 \frac{\mathrm{k}=2, \text { stride }=2}{\operatorname{maxpool}} 010101 \\
011001100110 \frac{\mathrm{k}=2, \text { stride }=2}{\operatorname{maxpool}} 111111
\end{array}&lt;/script&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://nicehuster.github.io/categories/paper-reading/"/>
    
    
      <category term="classification" scheme="https://nicehuster.github.io/tags/classification/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：Minimum Delay Object Detection from Video</title>
    <link href="https://nicehuster.github.io/2020/09/05/0063-minidelay-detection/"/>
    <id>https://nicehuster.github.io/2020/09/05/0063-minidelay-detection/</id>
    <published>2020-09-05T11:13:39.000Z</published>
    <updated>2020-09-15T06:28:52.040Z</updated>
    
    <content type="html"><![CDATA[<p>最近逛知乎上了解到<strong>低延迟目标检测</strong>这个方向，这个方向解决的是视觉任务工程化过程中存在一个痛点问题，如何解决延迟和误报的情况。拿视频中的目标检测问题来讲，我们通常会使用一个单帧检测器来检测视频中每一帧存在目标情况，静态上看可能会存在一些误检漏检情况，动态上表现为检测到的目标一闪一闪的情况，导致视觉效果很不友好。而低延迟目标检测的任务就是通过贝叶斯概率建模的方式融合多帧信息来解决这样的问题。</p><p>总体来说，对于任何检测任务来说，延迟和误报存在如下图的关系：</p><p><img src="/img/v2-2c0d4736cf5fde58e2b58530d7643006_1440w.jpg" alt="img"></p><a id="more"></a><p>这个关系不难理解，不只是视觉问题，世间万物，更长的决策过程（delay）往往能带来更高的准确度，但是这个更长的决策过程也会带来更大的延迟。两者之间的平衡，对很多需要在线决策（online process）的系统来说非常重要。例如生物视觉，假定一个动物检测到掠食者就需要逃跑，如果追求低误报率，就要承担高延迟带来的风险，有可能检测到掠食者时为时已晚无法逃脱；如果追求低延迟，虽然相对安全，但是误报率高，有一点风吹草动就犹如惊弓之鸟。</p><h3 id="低延迟检测思路"><a href="#低延迟检测思路" class="headerlink" title="低延迟检测思路"></a>低延迟检测思路</h3><p>在视频物体检测中，如果使用上一帧的检测结果作为先验，将下一帧的检测结果输入贝叶斯框架，输出后验，那么总体来说，这个后验结果融合了两帧的信息，会比单帧更准。在这个思路下，理论上来说使用的帧数越多，检测越准。如以下单帧和多帧的对比：</p><p><img src="/img/v2-fcf08710643bcb172cb661c687f36fe5_b.webp" alt="img"></p><p>但是同时更多的帧数会造成更长的延迟（延迟 := 物体被检测到的时刻 - 物体出现的时刻）。如何在保证物体检测精度的情况下，尽量降低延迟呢？我们参照QD理论进行如下建模：</p><p>假设一个物体在时刻$t_s$出现在视频中，在 $t_e$离开视频，则这个物体的移动轨迹可以用时序上的一组检测框$b_{t_s,t_e}=(b_{t_s},b_{t_s+1},…,b_{t_e})$表示。这样的一组检测框检测框序列，在目标追踪（Data association / tracking）领域被一些人称作tracklet。简单说，我们的算法目标是以低延迟判断检测框序列内是否含有物体。因此，我们称这样一组检测框序列为一个candidate。在quickest change detection框架下， 可以用如下似然比检验判断$t$时刻物体是否出现在该candidate内：</p><script type="math/tex; mode=display">\begin{aligned}\Lambda_{t}\left(b_{1, t}\right) &=\max _{i} \frac{\mathrm{p}\left(\Gamma_{0, i}<t \mid D_{1, t}, b_{1, t}\right)}{\mathrm{p}\left(\Gamma_{0, i} \geq t \mid D_{1, t}, b_{1, t}\right)} \\&=\max _{i} \max _{t_{c} \geq 1} \frac{\mathrm{p}_{i}\left(D_{t_{c}, t} \mid b_{t_{c}, t}\right)}{\mathrm{p}_{0}\left(D_{t_{c}, t} \mid b_{t_{c}, t}\right)}\end{aligned}</script><p>其中$D_t$代表一个单帧检测器在$I_t$上的检测结果，$T_{0,i}$代表candidate中的内容从背景变为第类$i$物体（如行人）这一事件发生的时刻,$p_i(\bullet )=p(\bullet |l=l_i)$ 代表给定类别$i$的时候，$\bullet$ 事件发生的概率。由条件概率的独立性与,$p_i(\bullet |b_t)$类别$i$和检测框独$b_t$立，继而时序上各时刻检测结果的联合概率变成各时刻概率的连乘：</p><script type="math/tex; mode=display">\Lambda_{t}\left(b_{1, t}\right)=\max _{i} \max _{t_{c} \geq 1} \prod_{j=t_{c}}^{t} \frac{\mathrm{p}_{i}\left(D_{j} \mid b_{j}\right)}{\mathrm{p}_{0}\left(D_{j} \mid b_{j}\right)}</script><p>这里需要明确一下，上边公式中的条件概率并非简单的检测器输出的结果，具体如何计算$p$需要一套比较复杂的建模。由于这里只介绍低延迟检测的整体思路，关于$p$的建模待我有空时会附在文末，有兴趣的朋友可以直接去论文查阅。总之，我们可以对这个似然比取阈值，进行检测。阈值越高，结果越准，但是延迟越大，反之同理。由QD理论中递归算法（CuSum算法），我们可以对上述似然比取log，记为W。最终的检测流程可以参照如下框图。</p><p><img src="/img/v2-bd434876b196828b2db7d14ea5eb8c52_1440w.jpg" alt></p><p>整体流程为：</p><blockquote><ul><li>（1）将已有但似然比未超过阈值的candidate做tracking进入下一帧；</li><li>（2）在下一帧进行单帧检测，生成新的检测框，与前一帧tracking后的检测框合并到一起；</li><li>（3）对这些candidate进行似然比检验，W超出阈值则输出检测结果，W小于零则去除该检测框，W大于零小于阈值则回到（1），进入下一帧。</li></ul></blockquote><p>这样一个检测框架，可以与<strong>任何</strong>单帧检测器结合。</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>以上算法的具体实现过程,作者开源了一个简单的python实现，git地址在<a href="https://github.com/donglao/mindelay/blob/master/detection.py" target="_blank" rel="noopener">这里</a>。这里主要看下核心的代码：</p><p>detection.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">result = toolbox.initialize_result(num_cat) <span class="comment"># 第一步，外循环，根据类别初始化result</span></span><br><span class="line"></span><br><span class="line">result = association.update(result, result_det) <span class="comment">#使用当前帧信息更新历史轨迹，result保存的是历史帧的检测信息，result_det是当前帧的检测信息</span></span><br><span class="line"></span><br><span class="line">result = toolbox.combine_result(result, result_det, <span class="number">0.5</span>) <span class="comment">#对结合的轨迹信息和当前帧信息就行结合，输出最终需要alarm的检测结果；</span></span><br></pre></td></tr></table></figure><p>其中association.update的具体代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> mindelay.toolbox.IoU <span class="keyword">as</span> IoU</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_traj</span><span class="params">(old, det)</span>:</span></span><br><span class="line">    prior = <span class="number">0.5</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(old.shape[<span class="number">0</span>]):</span><br><span class="line">        a = old[i, <span class="number">0</span>:<span class="number">4</span>] * prior</span><br><span class="line">        b = prior</span><br><span class="line">        l = prior * <span class="number">0.5</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(det.shape[<span class="number">0</span>]):</span><br><span class="line">            weight = (IoU(old[i, <span class="number">0</span>:<span class="number">4</span>], det[j, <span class="number">0</span>:<span class="number">4</span>])&gt;<span class="number">0.5</span>)*IoU(old[i, <span class="number">0</span>:<span class="number">4</span>], det[j, <span class="number">0</span>:<span class="number">4</span>])</span><br><span class="line">        <span class="comment"># The bounding box update is here. I've tried different methods in the original Matlab code. </span></span><br><span class="line">        <span class="comment"># But here I just use the previous frame as the guidance of current frame and leave it as-is.</span></span><br><span class="line">        <span class="comment"># I will combine it with trackers (some work to be done!) and update in later version of the code. </span></span><br><span class="line">            a = a + weight * det[j, <span class="number">0</span>:<span class="number">4</span>]</span><br><span class="line">            b = b + weight</span><br><span class="line">            l = l + weight * det[j, <span class="number">4</span>]</span><br><span class="line">        old[i, <span class="number">0</span>:<span class="number">4</span>] = a / b</span><br><span class="line">        l = l / b</span><br><span class="line"></span><br><span class="line">        temp_lr = np.log(l + <span class="number">0.25</span>) - np.log((<span class="number">1</span> - l) + <span class="number">0.25</span>) + old[i, <span class="number">4</span>] - <span class="number">2.5</span>/b</span><br><span class="line">        <span class="comment"># the +0.25 is making the output smoother. If the output of the detector is not ideal you may want to tune it.</span></span><br><span class="line">        <span class="comment"># the 2.5/b is a prior. Instead of setting a fixed prior I am trying to make it adaptive. Feel free to play with it! </span></span><br><span class="line">    </span><br><span class="line">        old[i, <span class="number">4</span>] = max(temp_lr, <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> old</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(result, result_det)</span>:</span></span><br><span class="line">    n = result.__len__()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        result[i] = update_traj(np.array(result[i]), np.array(result_det[i]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br></pre></td></tr></table></figure><p>其中，toolbox.combine_result的实现比较简单，对经过更新的轨迹信息和当前帧信息进行combine后经nms处理一下，代码如下：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def combine_result(result, result_det, thre_nms):</span><br><span class="line">    n = result.__len__()</span><br><span class="line">    output = np.empty((n,), dtype = np.object)</span><br><span class="line">    for i in range(n):</span><br><span class="line">        if result_det<span class="string">[i]</span>.shape<span class="string">[0]</span> &gt; <span class="number">0</span>:</span><br><span class="line">            temp_lr = np.log(result_det<span class="string">[i]</span><span class="string">[:,4]</span>+<span class="number">0</span>.<span class="number">25</span>) - np.log(<span class="number">1</span> - result_det<span class="string">[i]</span><span class="string">[:,4]</span>+<span class="number">0</span>.<span class="number">25</span>)</span><br><span class="line">            temp_lr<span class="string">[temp_lr &lt; 0]</span> = <span class="number">0</span></span><br><span class="line">            result_det<span class="string">[i]</span><span class="string">[:, 4]</span> = temp_lr</span><br><span class="line">        tmp = np.vstack((result<span class="string">[i]</span>, result_det<span class="string">[i]</span>))</span><br><span class="line">        keep = py_cpu_nms(tmp,thre_nms)</span><br><span class="line">        #print(keep,thre_nms,tmp<span class="string">[keep]</span>)</span><br><span class="line">        output<span class="string">[i]</span> = tmp<span class="string">[keep]</span></span><br><span class="line"></span><br><span class="line">    return output</span><br></pre></td></tr></table></figure><p>以上理论内容转自：<a href="https://zhuanlan.zhihu.com/p/212842916" target="_blank" rel="noopener">计算机视觉中低延迟检测的相关理论和应用</a>；</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近逛知乎上了解到&lt;strong&gt;低延迟目标检测&lt;/strong&gt;这个方向，这个方向解决的是视觉任务工程化过程中存在一个痛点问题，如何解决延迟和误报的情况。拿视频中的目标检测问题来讲，我们通常会使用一个单帧检测器来检测视频中每一帧存在目标情况，静态上看可能会存在一些误检漏检情况，动态上表现为检测到的目标一闪一闪的情况，导致视觉效果很不友好。而低延迟目标检测的任务就是通过贝叶斯概率建模的方式融合多帧信息来解决这样的问题。&lt;/p&gt;
&lt;p&gt;总体来说，对于任何检测任务来说，延迟和误报存在如下图的关系：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/v2-2c0d4736cf5fde58e2b58530d7643006_1440w.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://nicehuster.github.io/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://nicehuster.github.io/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>解决PIL读取图片出现自动旋转的解决方案</title>
    <link href="https://nicehuster.github.io/2020/08/06/0062-PILrotate/"/>
    <id>https://nicehuster.github.io/2020/08/06/0062-PILrotate/</id>
    <published>2020-08-06T11:13:39.000Z</published>
    <updated>2020-09-18T02:40:56.930Z</updated>
    
    <content type="html"><![CDATA[<p>最近项目中，使用手机采集了数据，交给标注组进行标注时，发现返回来的标注文件与图片存在不匹配问题，部分标注存在旋转情况。从网上了解到电子设备在拍摄照片时，如手机、相机等，由于手持朝向的不同，拍摄的照片可能会出现旋转 0、90、180、270 角度的情况，其 EXIF 信息中会保留相应的方位信息.有些情况下，电脑上打开显示照片是正常的，但在用 PIL 或 OpenCV 读取图片后，图片出现旋转，且读取的图片尺寸也可能与直接在电脑上打开的尺寸不同的问题.</p><p>对此，需要在读取图片时，同时解析图片的 EXIF 中的方位信息，将图片转正，再进行后续的其他操作.实例如下：</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ExifTags</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">IsRotate</span><span class="params">(img)</span>:</span> <span class="comment"># 返回false表示存在旋转情况</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">for</span> orientation <span class="keyword">in</span> ExifTags.TAGS.keys() :</span><br><span class="line">            <span class="keyword">if</span> ExifTags.TAGS[orientation]==<span class="string">'Orientation'</span> :</span><br><span class="line">                img2 = img.rotate(<span class="number">0</span>, expand = <span class="keyword">True</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        exif=dict(img._getexif().items())</span><br><span class="line">        <span class="keyword">if</span>  exif[orientation] == <span class="number">3</span> :</span><br><span class="line">            img2=img.rotate(<span class="number">180</span>, expand = <span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">elif</span> exif[orientation] == <span class="number">6</span> :</span><br><span class="line">            img2=img.rotate(<span class="number">270</span>, expand = <span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">elif</span> exif[orientation] == <span class="number">8</span> :</span><br><span class="line">            img2=img.rotate(<span class="number">90</span>, expand = <span class="keyword">True</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> img.size == img2.size</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br></pre></td></tr></table></figure><p>更多可参考：<a href="https://stackoverflow.com/questions/4228530/pil-thumbnail-is-rotating-my-image" target="_blank" rel="noopener">Stackoverflow - PIL thumbnail is rotating my image?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近项目中，使用手机采集了数据，交给标注组进行标注时，发现返回来的标注文件与图片存在不匹配问题，部分标注存在旋转情况。从网上了解到电子设备在拍摄照片时，如手机、相机等，由于手持朝向的不同，拍摄的照片可能会出现旋转 0、90、180、270 角度的情况，其 EXIF 信息中会保留相应的方位信息.有些情况下，电脑上打开显示照片是正常的，但在用 PIL 或 OpenCV 读取图片后，图片出现旋转，且读取的图片尺寸也可能与直接在电脑上打开的尺寸不同的问题.&lt;/p&gt;
&lt;p&gt;对此，需要在读取图片时，同时解析图片的 EXIF 中的方位信息，将图片转正，再进行后续的其他操作.实例如下：&lt;/p&gt;
    
    </summary>
    
      <category term="project" scheme="https://nicehuster.github.io/categories/project/"/>
    
    
      <category term="opencv" scheme="https://nicehuster.github.io/tags/opencv/"/>
    
      <category term="python" scheme="https://nicehuster.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python计算任意多边形的面积</title>
    <link href="https://nicehuster.github.io/2020/06/05/0064-polygonArea/"/>
    <id>https://nicehuster.github.io/2020/06/05/0064-polygonArea/</id>
    <published>2020-06-05T11:13:39.000Z</published>
    <updated>2020-09-16T02:30:44.354Z</updated>
    
    <content type="html"><![CDATA[<p>在网上发现一个很有意思且很有用的多边形面积计算公式-鞋带公式</p><p>鞋带公式的表达式为：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{A} &=\frac{1}{2}\left|\sum_{i=1}^{n-1} x_{i} y_{i+1}+x_{n} y_{1}-\sum_{i=1}^{n-1} x_{i+1} y_{i}-x_{1} y_{n}\right| \\&=\frac{1}{2}\left|x_{1} y_{2}+x_{2} y_{3}+\cdots+x_{n-1} y_{n}+x_{n} y_{1}-x_{2} y_{1}-x_{3} y_{2}-\cdots-x_{n} y_{n-1}-x_{1} y_{n}\right|\end{aligned}</script><a id="more"></a><p><img src="/img/b1f95fc8a46e6d66.jpg" alt="img"></p><p>where<br>$\bullet A$ is the area of the polygon,<br>$\bullet n$ is the number of sides of the polygon, and $\cdot\left(x_{i}, y_{i}\right), i=1,2, \ldots, n$ are the ordered vertices (or “corners”) of the polygon.</p><blockquote><p>参考wiki:<a href="https://en.wikipedia.org/wiki/Shoelace_formula" target="_blank" rel="noopener">Shoelace formula</a></p></blockquote><p>可以理解为，是把每个顶点向x轴做垂线，每个边和坐标轴构成的梯形面积矢量和就是多边形的面积。</p><h3 id="1-鞋带公式实现"><a href="#1-鞋带公式实现" class="headerlink" title="1. 鞋带公式实现"></a>1. 鞋带公式实现</h3><blockquote><p>From: stackoverflow - <a href="https://stackoverflow.com/questions/24467972/calculate-area-of-polygon-given-x-y-coordinates" target="_blank" rel="noopener">calculate-area-of-polygon-given-x-y-coordinates</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">polygon_area</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span>*np.abs(np.dot(x,np.roll(y,<span class="number">1</span>))-np.dot(y,np.roll(x,<span class="number">1</span>)))</span><br></pre></td></tr></table></figure><h3 id="2-示例1-计算曲线与坐标轴的面积"><a href="#2-示例1-计算曲线与坐标轴的面积" class="headerlink" title="2. 示例1 - 计算曲线与坐标轴的面积"></a>2. 示例1 - 计算曲线与坐标轴的面积</h3><blockquote><p>[计算曲线与坐标轴的面积</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">1</span>,<span class="number">0.001</span>)</span><br><span class="line">y = np.sqrt(<span class="number">1</span>-x**<span class="number">2</span>)</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.show()</span><br><span class="line">area_value = polygon_area(np.append(x, <span class="number">0</span>), np.append(y, <span class="number">0</span>))</span><br></pre></td></tr></table></figure><h3 id="3-示例2-detectron2-mask-面积"><a href="#3-示例2-detectron2-mask-面积" class="headerlink" title="3. 示例2 - detectron2 mask 面积"></a>3. 示例2 - detectron2 mask 面积</h3><blockquote><p><a href="https://github.com/facebookresearch/detectron2/blob/b0b6ccfef5e00255de22857eca0e2dfa1d1144b1/detectron2/structures/masks.py" target="_blank" rel="noopener">detectron2/structures/masks.py</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">area</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes area of the mask.</span></span><br><span class="line"><span class="string">    Only works with Polygons, using the shoelace formula</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Tensor: a vector, area for each instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    area = []</span><br><span class="line">    <span class="keyword">for</span> polygons_per_instance <span class="keyword">in</span> self.polygons:</span><br><span class="line">        area_per_instance = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> polygons_per_instance:</span><br><span class="line">            area_per_instance += polygon_area(p[<span class="number">0</span>::<span class="number">2</span>], p[<span class="number">1</span>::<span class="number">2</span>])</span><br><span class="line">        area.append(area_per_instance)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> torch.tensor(area)</span><br></pre></td></tr></table></figure><h3 id="在线多变形面积计算工具"><a href="#在线多变形面积计算工具" class="headerlink" title="在线多变形面积计算工具"></a>在线多变形面积计算工具</h3><blockquote><p><a href="https://www.mathsisfun.com/geometry/area-polygon-drawing.html" target="_blank" rel="noopener">多边形面积计算工具</a></p></blockquote><p><img src="/img/image-20200916102553183.png" alt="image-20200916102553183"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在网上发现一个很有意思且很有用的多边形面积计算公式-鞋带公式&lt;/p&gt;
&lt;p&gt;鞋带公式的表达式为：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{aligned}
\mathbf{A} &amp;=\frac{1}{2}\left|\sum_{i=1}^{n-1} x_{i} y_{i+1}+x_{n} y_{1}-\sum_{i=1}^{n-1} x_{i+1} y_{i}-x_{1} y_{n}\right| \\
&amp;=\frac{1}{2}\left|x_{1} y_{2}+x_{2} y_{3}+\cdots+x_{n-1} y_{n}+x_{n} y_{1}-x_{2} y_{1}-x_{3} y_{2}-\cdots-x_{n} y_{n-1}-x_{1} y_{n}\right|
\end{aligned}&lt;/script&gt;
    
    </summary>
    
      <category term="python" scheme="https://nicehuster.github.io/categories/python/"/>
    
    
      <category term="python" scheme="https://nicehuster.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>ffmpeg常用指令笔记</title>
    <link href="https://nicehuster.github.io/2020/03/26/0061-ffmpeg/"/>
    <id>https://nicehuster.github.io/2020/03/26/0061-ffmpeg/</id>
    <published>2020-03-26T11:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>平时使用ffmpeg对视频解码成图片比较多，就稍微简单的了解一些ffmpeg常用的相关指令。下面是一些相关指令的介绍笔记。</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>Debian/Ubuntu/Linux Mint 下安装 ffmpeg 很简单：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-<span class="builtin-name">get</span> install ffmpeg</span><br></pre></td></tr></table></figure><p>其他操作系统安装方法，参考<a href="https://www.ffmpeg.org/download.html" target="_blank" rel="noopener">官网</a></p><p>如果想要手工编译 ffmpeg 可以参考官方 <a href="https://trac.ffmpeg.org/wiki#CompilingFFmpeg" target="_blank" rel="noopener">wiki</a>。 Ubuntu/Debian/Mint 系手工编译 ffmpeg 参考 <a href="https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu" target="_blank" rel="noopener">wiki</a>。</p><h3 id="常用指令"><a href="#常用指令" class="headerlink" title="常用指令"></a>常用指令</h3><h4 id="1-显示文件信息"><a href="#1-显示文件信息" class="headerlink" title="1.显示文件信息"></a>1.显示文件信息</h4><p>显示视频信息</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span>.avi</span><br></pre></td></tr></table></figure><h4 id="2-将视频拆分图片-批量截图"><a href="#2-将视频拆分图片-批量截图" class="headerlink" title="2.将视频拆分图片 批量截图"></a>2.将视频拆分图片 批量截图</h4><a id="more"></a><p>将视频拆分多张图片，每一帧图片，保存到 frames 文件夹下，命名 frame001.png 这种。可以加上 -r 参数以用来限制每秒的帧数，<code>-r 10</code> 就表示每秒 10 帧。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="built_in">i</span> input.mp4 frames/frame<span class="comment">%03d.png</span></span><br></pre></td></tr></table></figure><h4 id="3-图片合成视频"><a href="#3-图片合成视频" class="headerlink" title="3.图片合成视频"></a>3.图片合成视频</h4><p>将多张图片合成视频</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="built_in">i</span> frames/frame<span class="comment">%3d.png output.mp4</span></span><br></pre></td></tr></table></figure><h4 id="4-转换格式"><a href="#4-转换格式" class="headerlink" title="4.转换格式"></a>4.转换格式</h4><p>格式之间转换 大部分的情况下直接运行一下即可</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> output.avi</span><br></pre></td></tr></table></figure><p>将 flv 转码 MP4</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.flv</span> -vcodec copy -acodec copy out.mp4</span><br></pre></td></tr></table></figure><p><code>-vcodec copy</code> 和 <code>-acodec copy</code> 表示所使用的视频和音频编码格式，为原样拷贝。</p><h4 id="5-视频切片操作"><a href="#5-视频切片操作" class="headerlink" title="5.视频切片操作"></a>5.视频切片操作</h4><p>对视频切片操作,比如需要从视频第 1 分 45 秒地方，剪 10 秒画面，-ss 表示开始位置，-t 表示延长时间</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> -ss <span class="number">00</span>:<span class="number">01</span>:<span class="number">45</span> -t <span class="number">10</span> output.mp4</span><br></pre></td></tr></table></figure><h4 id="6-加速减速视频"><a href="#6-加速减速视频" class="headerlink" title="6.加速减速视频"></a>6.加速减速视频</h4><p>加速视频</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> -vf “setpts=<span class="number">0.5</span>*PTS” output.mp4</span><br></pre></td></tr></table></figure><p>同理减速视频</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> -vf “setpts=<span class="number">2.0</span>*PTS” output.mp4</span><br></pre></td></tr></table></figure><p>此操作对音频无影响</p><h4 id="7-视频截图"><a href="#7-视频截图" class="headerlink" title="7.视频截图"></a>7.视频截图</h4><p>视频 10 秒的地方 (<code>-ss</code> 参数）截取一张 1920x1080 尺寸大小的，格式为 jpg 的图片  <code>-ss</code>后跟的时间单位为秒</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> input_video<span class="selector-class">.mp4</span> -y -f image2 -t <span class="number">0.001</span> -ss <span class="number">10</span> -s <span class="number">1920</span>x1080 output.jpg</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">ffmpeg</span> <span class="selector-tag">-i</span> <span class="selector-tag">input_video</span><span class="selector-class">.mp4</span> <span class="selector-tag">-ss</span> 00<span class="selector-pseudo">:00</span><span class="selector-pseudo">:06.000</span> <span class="selector-tag">-vframes</span> 1 <span class="selector-tag">output</span><span class="selector-class">.png</span></span><br></pre></td></tr></table></figure><h4 id="8-合成gif"><a href="#8-合成gif" class="headerlink" title="8.合成gif"></a>8.合成gif</h4><p>把视频的前 30 帧转换成一个 Gif</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> input_video<span class="selector-class">.mp4</span> -vframes <span class="number">30</span> -y -f gif output.gif</span><br></pre></td></tr></table></figure><p>将视频转成 gif</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">ffmpeg</span> <span class="selector-tag">-ss</span> 00<span class="selector-pseudo">:00</span><span class="selector-pseudo">:00.000</span> <span class="selector-tag">-i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> <span class="selector-tag">-pix_fmt</span> <span class="selector-tag">rgb24</span> <span class="selector-tag">-r</span> 10 <span class="selector-tag">-s</span> 320<span class="selector-tag">x240</span> <span class="selector-tag">-t</span> 00<span class="selector-pseudo">:00</span><span class="selector-pseudo">:10.000</span> <span class="selector-tag">output</span><span class="selector-class">.gif</span></span><br></pre></td></tr></table></figure><h4 id="9-更换视频的分辨率"><a href="#9-更换视频的分辨率" class="headerlink" title="9.更换视频的分辨率"></a>9.更换视频的分辨率</h4><p>可以使用如下命令更换视频的分辨率</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">    ffmpeg -i <span class="built_in">input</span>.mp4 -<span class="built_in">filter</span>:v scale=<span class="number">1280</span>:<span class="number">720</span> -<span class="keyword">c</span>:<span class="keyword">a</span> <span class="keyword">copy</span> output.mp4</span><br><span class="line"><span class="built_in">or</span></span><br><span class="line">    ffmpeg -i <span class="built_in">input</span>.mp4 -s <span class="number">1280</span>x720 -<span class="keyword">c</span>:<span class="keyword">a</span> <span class="keyword">copy</span> output.mp4</span><br></pre></td></tr></table></figure><h4 id="10-设置视频的宽高比"><a href="#10-设置视频的宽高比" class="headerlink" title="10.设置视频的宽高比"></a>10.设置视频的宽高比</h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> -aspect <span class="number">16</span>:<span class="number">9</span> output.mp4</span><br></pre></td></tr></table></figure><p>常见的宽高比：<code>16:9、4:3、16:10、5:4</code></p><h4 id="11-利用ffmpeg屏幕录制"><a href="#11-利用ffmpeg屏幕录制" class="headerlink" title="11.利用ffmpeg屏幕录制"></a>11.利用ffmpeg屏幕录制</h4><p>参考：<a href="https://trac.ffmpeg.org/wiki/Capture/Desktop" target="_blank" rel="noopener">https://trac.ffmpeg.org/wiki/Capture/Desktop</a></p><h4 id="12-添加水印"><a href="#12-添加水印" class="headerlink" title="12.添加水印"></a>12.添加水印</h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> -<span class="selector-tag">i</span> picture<span class="selector-class">.png</span> -filter_complex overlay=<span class="string">"(main_w/2)-(overlay_w/2):(main_h/2)-(overlay_h)/2"</span> output.mp4</span><br></pre></td></tr></table></figure><p><code>picture.png</code> 为水印图片， <code>overlay</code> 为水印位置</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;平时使用ffmpeg对视频解码成图片比较多，就稍微简单的了解一些ffmpeg常用的相关指令。下面是一些相关指令的介绍笔记。&lt;/p&gt;
&lt;h3 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h3&gt;&lt;p&gt;Debian/Ubuntu/Linux Mint 下安装 ffmpeg 很简单：&lt;/p&gt;
&lt;figure class=&quot;highlight routeros&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;apt-&lt;span class=&quot;builtin-name&quot;&gt;get&lt;/span&gt; install ffmpeg&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;其他操作系统安装方法，参考&lt;a href=&quot;https://www.ffmpeg.org/download.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官网&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;如果想要手工编译 ffmpeg 可以参考官方 &lt;a href=&quot;https://trac.ffmpeg.org/wiki#CompilingFFmpeg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;wiki&lt;/a&gt;。 Ubuntu/Debian/Mint 系手工编译 ffmpeg 参考 &lt;a href=&quot;https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;wiki&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id=&quot;常用指令&quot;&gt;&lt;a href=&quot;#常用指令&quot; class=&quot;headerlink&quot; title=&quot;常用指令&quot;&gt;&lt;/a&gt;常用指令&lt;/h3&gt;&lt;h4 id=&quot;1-显示文件信息&quot;&gt;&lt;a href=&quot;#1-显示文件信息&quot; class=&quot;headerlink&quot; title=&quot;1.显示文件信息&quot;&gt;&lt;/a&gt;1.显示文件信息&lt;/h4&gt;&lt;p&gt;显示视频信息&lt;/p&gt;
&lt;figure class=&quot;highlight stylus&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;ffmpeg -&lt;span class=&quot;selector-tag&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;selector-tag&quot;&gt;input&lt;/span&gt;.avi&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;2-将视频拆分图片-批量截图&quot;&gt;&lt;a href=&quot;#2-将视频拆分图片-批量截图&quot; class=&quot;headerlink&quot; title=&quot;2.将视频拆分图片 批量截图&quot;&gt;&lt;/a&gt;2.将视频拆分图片 批量截图&lt;/h4&gt;
    
    </summary>
    
      <category term="linux" scheme="https://nicehuster.github.io/categories/linux/"/>
    
    
      <category term="ffmpeg" scheme="https://nicehuster.github.io/tags/ffmpeg/"/>
    
  </entry>
  
  <entry>
    <title>bash命令并行</title>
    <link href="https://nicehuster.github.io/2019/10/18/0060-bash-parallel/"/>
    <id>https://nicehuster.github.io/2019/10/18/0060-bash-parallel/</id>
    <published>2019-10-18T11:13:39.000Z</published>
    <updated>2020-09-18T02:40:45.271Z</updated>
    
    <content type="html"><![CDATA[<p>&#160; &#160; &#160; &#160;在bash中，使用后台任务来实现任务的“多进程化”。在不加控制的模式下，不管有多少任务，全部都后台执行。也就是说，在这种情况下，有多少任务就有多少“进程”在同时执行。<br><a id="more"></a><br><strong>实例一：正常情况脚本</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ((i=0;i&lt;5;i++));<span class="keyword">do</span></span><br><span class="line">        &#123;</span><br><span class="line">                sleep 3;<span class="built_in">echo</span> 1&gt;&gt;aa &amp;&amp; <span class="built_in">echo</span> <span class="string">"done!"</span></span><br><span class="line">        &#125; </span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">wait</span></span><br><span class="line">cat aa|wc -l</span><br><span class="line">rm aa</span><br></pre></td></tr></table></figure></p><p>&#160; &#160; &#160; &#160;这种情况下，程序顺序执行，每个循环3s，共需15s左右。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ time bash test.sh </span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">5</span><br><span class="line"></span><br><span class="line">real    0m15.030s</span><br><span class="line">user    0m0.002s</span><br><span class="line">sys     0m0.003s</span><br></pre></td></tr></table></figure></p><p><strong>实例二：“多进程”实现</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ((i=0;i&lt;5;i++));<span class="keyword">do</span></span><br><span class="line">        &#123;</span><br><span class="line">                sleep 3;<span class="built_in">echo</span> 1&gt;&gt;aa &amp;&amp; <span class="built_in">echo</span> <span class="string">"done!"</span></span><br><span class="line">        &#125;&amp;</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">wait</span></span><br><span class="line">cat aa|wc -l</span><br><span class="line">rm aa</span><br></pre></td></tr></table></figure></p><p>&#160; &#160; &#160; &#160;这个实例实际上就在上面基础上多加了一个后台执行&amp;符号，此时应该是5个循环任务并发执行，最后需要3s左右时间。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ time bash test.sh </span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">5</span><br><span class="line"></span><br><span class="line">real    0m3.109s</span><br><span class="line">user    0m0.008s</span><br><span class="line">sys     0m0.100s</span><br></pre></td></tr></table></figure></p><p>&#160; &#160; &#160; &#160;效果非常明显。这里需要说明一下wait的左右。wait是等待前面的后台任务全部完成才往下执行，否则程序本身是不会等待的，这样对后面依赖前面任务结果的命令来说就可能出错。</p><p>&#160; &#160; &#160; &#160;以上所讲的实例都是进程数目不可控制的情况，下面描述如何准确控制并发的进程数目。</p><p> **实例三：“多进程可控”实现<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">tmp_fifofile=<span class="string">"/tmp/$$.fifo"</span></span><br><span class="line">mkfifo <span class="variable">$tmp_fifofile</span>      <span class="comment"># 新建一个fifo类型的文件</span></span><br><span class="line"><span class="built_in">exec</span> 6&lt;&gt;<span class="variable">$tmp_fifofile</span>      <span class="comment"># 将fd6指向fifo类型</span></span><br><span class="line">rm <span class="variable">$tmp_fifofile</span></span><br><span class="line">thread=15 <span class="comment"># 此处定义线程数</span></span><br><span class="line"><span class="keyword">for</span> ((i=0;i&lt;<span class="variable">$thread</span>;i++));<span class="keyword">do</span> </span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"><span class="keyword">done</span> &gt;&amp;6 <span class="comment"># 事实上就是在fd6中放置了$thread个回车符</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ((i=0;i&lt;50;i++));<span class="keyword">do</span> <span class="comment"># 50次循环</span></span><br><span class="line"><span class="built_in">read</span> -u6 </span><br><span class="line"><span class="comment"># 一个read -u6命令执行一次，就从fd6中减去一个回车符，然后向下执行，</span></span><br><span class="line"><span class="comment"># fd6中没有回车符的时候，就停在这了，从而实现了线程数量控制</span></span><br><span class="line"></span><br><span class="line">&#123; <span class="comment"># 此处子进程开始执行，被放到后台</span></span><br><span class="line">      &#123;sleep 3&#125; &amp;&amp; &#123; <span class="built_in">echo</span> <span class="string">"a_sub is finished"</span>&#125;</span><br><span class="line"></span><br><span class="line">     <span class="built_in">echo</span> &gt;&amp;6 <span class="comment"># 当进程结束以后，再向fd6中加上一个回车符，即补上了read -u6减去的那个</span></span><br><span class="line">&#125; &amp;</span><br><span class="line"></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">wait</span> <span class="comment"># 等待所有的后台子进程结束</span></span><br><span class="line"><span class="built_in">exec</span> 6&gt;&amp;- <span class="comment"># 关闭df6</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span> 0</span><br></pre></td></tr></table></figure></p><p>sleep 3s，线程数为15，一共循环50次，所以，此脚本一共的执行时间大约为12秒</p><p>即：<br>15x3=45, 所以 3x3s=9s<br>(50-45=5)&lt;15, 所以1x3s=3s<br>所以9s+3s = 12s</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ time bash multithread.sh </span><br><span class="line"></span><br><span class="line">real        0m12.025s</span><br><span class="line">user        0m0.020s</span><br><span class="line">sys         0m0.064s</span><br></pre></td></tr></table></figure><p>而当不使用多线程技巧的时候，执行时间为：50 x 3s = 150s。</p><p>注：此文转载自<a href="http://www.cnitblog.com/sysop/archive/2008/11/03/50974.aspx" target="_blank" rel="noopener">这里</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;#160; &amp;#160; &amp;#160; &amp;#160;在bash中，使用后台任务来实现任务的“多进程化”。在不加控制的模式下，不管有多少任务，全部都后台执行。也就是说，在这种情况下，有多少任务就有多少“进程”在同时执行。&lt;br&gt;
    
    </summary>
    
      <category term="linux" scheme="https://nicehuster.github.io/categories/linux/"/>
    
    
      <category term="-bash" scheme="https://nicehuster.github.io/tags/bash/"/>
    
  </entry>
  
  <entry>
    <title>mean-shift算法详解</title>
    <link href="https://nicehuster.github.io/2019/08/05/mean-shift/"/>
    <id>https://nicehuster.github.io/2019/08/05/mean-shift/</id>
    <published>2019-08-05T11:13:39.000Z</published>
    <updated>2020-09-18T06:42:42.507Z</updated>
    
    <content type="html"><![CDATA[<p>MeanShift最初由Fukunaga和Hostetler在1975年提出，但是一直到2000左右这篇PAMI的论文Mean Shift: A Robust Approach Toward Feature Space Analysis，将它的原理和收敛性等重新整理阐述，并应用于计算机视觉和图像处理领域之后，才逐渐为人熟知。在了解mean-shift算法之前，先了解一下概率密度估计的概念。</p><a id="more"></a><h3 id="概率密度估计"><a href="#概率密度估计" class="headerlink" title="概率密度估计"></a>概率密度估计</h3><p>密度估计是指有给定样本集和求解随机变量的分布密度函数，解决这一问题的方法包括：参数估计和非参数估计。</p><p><strong>参数估计</strong>：在我们已经知道观测数据符合某些模型的情况下，我们可以利用参数估计的方法来确定这些参数值，然后得出概率密度模型。前提是观测数据服从一个已知概率密度函数。</p><p><strong>非参数估计</strong>：无需任何先验知识完全依靠特征空间中样本点计算其密度估计值.可以处理任意概率分布，不必假设服从已知分布；常用的无参数密度估计方法有：直方图法、最近邻域法和核密度估计法。<strong>MeanShift算法正属于核密度估计法</strong>。无需任何先验知识完全依靠特征空间中样本点计算其密度估计值。</p><h4 id="核密度估计"><a href="#核密度估计" class="headerlink" title="核密度估计"></a>核密度估计</h4><p>mean shift算法使用核函数估计样本密度，假设对于大小为$n$,维度为$d$ 的数据集，$D=\left\{x_{1}, x_{2}, x_{3}, \ldots x_{n}\right\}, D \in R^{d}$ ，核函数K的带宽为h，则该函数的核密度估计为：</p><script type="math/tex; mode=display">f(x)=\frac{1}{n h^{d}} \sum_{i=1}^{n} K\left(\frac{x-x_{i}}{h}\right)</script><p>定义满足核函数条件为：</p><script type="math/tex; mode=display">K(x)=c_{k, d} k\left(\|x\|^{2}\right), \quad \int c_{k, d} \cdot K(x) d x=1</script><p>其中，$c_{k,d}$ 系数是归一化常数，使得$K(x)$ 的积分为1.</p><p>常见的核函数有高斯核函数，其形式如下：</p><script type="math/tex; mode=display">N(x)=\frac{1}{\sqrt{2 \pi h}} e^{-\frac{x^{2}}{2 h^{2}}}</script><p>其中，h称为带宽(bandwidth)，不同带宽的核函数如下图所示：</p><p><img src="/img/gaussian.png" alt="img"></p><p>从高斯函数的图像可以看出，当带宽h一定时，样本点之间的距离越近，其核函数的值越大，当样本点之间的距离相等时，随着高斯函数的带宽h的增加，核函数的值在减小。高斯核的python实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian_kernel</span><span class="params">(distance, bandwidth)</span>:</span></span><br><span class="line">    <span class="string">''' 高斯核函数</span></span><br><span class="line"><span class="string">    :param distance: 欧氏距离计算函数</span></span><br><span class="line"><span class="string">    :param bandwidth: 核函数的带宽</span></span><br><span class="line"><span class="string">    :return: 高斯函数值</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    m = np.shape(distance)[<span class="number">0</span>]  <span class="comment"># 样本个数</span></span><br><span class="line">    right = np.mat(np.zeros((m, <span class="number">1</span>)))  <span class="comment"># m * 1 矩阵</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        right[i, <span class="number">0</span>] = (<span class="number">-0.5</span> * distance[i] * distance[i].T) / (bandwidth * bandwidth)</span><br><span class="line">        right[i, <span class="number">0</span>] = np.exp(right[i, <span class="number">0</span>])</span><br><span class="line">    left = <span class="number">1</span> / (bandwidth * math.sqrt(<span class="number">2</span> * math.pi))</span><br><span class="line">    gaussian_val = left * right</span><br><span class="line">    <span class="keyword">return</span> gaussian_val</span><br></pre></td></tr></table></figure><p>以高斯核估计一维数据集的密度为例，每个样本点都设置以该样本为中心的高斯分布，累加所有高斯分布，就得到该数据集的密度。</p><p><img src="/img/image-20200918115326176.png" alt="image-20200918115326176"></p><p>其中虚线表示每个样本点的高斯核，实现表示累加后所有样本高斯核后的数据集密度。</p><h3 id="mean-shift-算法理论"><a href="#mean-shift-算法理论" class="headerlink" title="mean-shift 算法理论"></a>mean-shift 算法理论</h3><h4 id="朴素mean-shift向量形式"><a href="#朴素mean-shift向量形式" class="headerlink" title="朴素mean-shift向量形式"></a>朴素mean-shift向量形式</h4><p>对于给定的d维度空间中的n个样本点$\left\{x_{1}, x_{2}, x_{3}, \ldots x_{n}\right\}$ ，则对于x点，其mean-shift向量的基本形式为：</p><script type="math/tex; mode=display">M_{h}(x)=\frac{1}{k} \sum_{x_{i} \in S_{h}}\left(x_{i}-x\right)</script><p><img src="/img/image-20200918115606176.png" alt="image-20200918115606176"></p><p>其中$S_h$指的是一个半径为h的高维球区域，如上图中的圆形区域。$S_h$的定义为：</p><script type="math/tex; mode=display">S_{h}(x)=\left(y \mid(y-x)(y-x)^{T} \leq h^{2}\right)</script><p>里面所有点与圆心为起点形成的向量相加的结果就是Mean shift向量。下图黄色箭头就是 $M_h$ (mean-shift 向量)。</p><p><img src="/img/image-20200918115822021.png" alt="image-20200918115822021"></p><p>对于Mean Shift算法，是一个迭代的步骤，即先算出当前点的偏移均值，将该点移动到此偏移均值，然后以此为新的起始点，继续移动，直到满足最终的条件。</p><p><img src="/img/wpsicYkqP.jpg" alt="img"> </p><h4 id="引入核函数的Mean-Shift向量形式"><a href="#引入核函数的Mean-Shift向量形式" class="headerlink" title="引入核函数的Mean Shift向量形式"></a>引入核函数的Mean Shift向量形式</h4><p>Mean Shift算法的基本目标是将样本点向局部密度增加的方向移动，我们常常所说的均值漂移向量就是指局部密度增加最快的方向。上节通过引入高斯核可以知道数据集的密度，梯度是函数增加最快的方向，因此，数据集密度的梯度方向就是密度增加最快的方向。</p><script type="math/tex; mode=display">f(x)=\frac{1}{n h^{d}} \sum_{i=1}^{n} K\left(\frac{x-x_{i}}{h}\right)</script><p>高斯核：$K(x)=c_{k, d} k\left(|x|^{2}\right)$</p><script type="math/tex; mode=display">\begin{aligned}\nabla f(x) &=\frac{2 c_{k, d}}{n h^{d+2}} \sum_{i=1}^{n}\left(x_{i}-x\right) g\left(\left\|\frac{x-x_{i}}{h}\right\|^{2}\right) \\&\left.=\frac{2 c_{k, d}}{n h^{d+2}}\left[\sum_{i=1}^{n} g\left(\left\|\frac{x-x_{i}}{h}\right\|\right)\right]^{2}\right)\left[\frac{\sum_{i=1}^{n} x_{i} g\left(\left\|\frac{x-x_{i}}{h}\right\|^{2}\right)}{\sum_{i=1}^{n} g\left(\left\|\frac{x-x_{i}}{h}\right\|^{2}\right)}-x\right]\end{aligned}</script><p>其中$g(s)=-k^{\prime}(s)$ ，上式的第一项为实数值。</p><p>因此第二项的向量方向与梯度方向一致，第二项的表达式为：</p><script type="math/tex; mode=display">m_{h}(x)=\frac{\sum_{i=1}^{n} x_{i} g\left(\mid \frac{x-x_{i}}{h} \|^{2}\right)}{\sum_{i=1}^{n} g\left(\left\|\frac{x-x_{i}}{h}\right\|^{2}\right)}-x</script><p>上式的含义就是本篇文章的主题：<strong>均值漂移</strong>。由上式推导可知：<strong>均值漂移向量所指的方向是密度增加最大的方向</strong>。</p><p><img src="/img/image-20200918120319712.png" alt="image-20200918120319712"></p><p>要使$\nabla f(x)=0$ ，当且仅当$m_{h}(\mathrm{x})=0$ ，可以得出新坐标：</p><script type="math/tex; mode=display">x=\frac{\sum_{i=1}^{n} x_{i} g\left(\left\|\frac{x-x_{i}}{h}\right\|^{2}\right)}{\sum_{i=1}^{n} g\left(\left\|\frac{x-x_{i}}{h}\right\|^{2}\right)}</script><p>因此，Mean Shift算法流程为：</p><blockquote><p>（1）计算每个样本的均值漂移向量 $m_{h}(\mathrm{x})$ ;</p><p>（2）对每个样本点以 $m_{h}(\mathrm{x})$ 进行平移，即：$x=x+m_{h}(x)$ ；</p><p>（3）重复（1）（2），直到样本点收敛，即：$m_{h}(\mathrm{x})&lt;\varepsilon$ (人工设定)或者迭代次数小于设定值；</p><p>（4）收敛到相同点的样本被认为是同一簇类的成员；</p></blockquote><h3 id="mean-shift应用"><a href="#mean-shift应用" class="headerlink" title="mean-shift应用"></a>mean-shift应用</h3><h4 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h4><p>Mean-Shift聚类就是对于集合中的每一个元素，对它执行下面的操作：把该元素移动到它邻域中所有元素的特征值的均值的位置，不断重复直到收敛。准确的说，不是真正移动元素，而是把该元素与它的收敛位置的元素标记为同一类。在实际中，为了加速，初始化的时候往往会初始化多个窗口，然后再进行聚类。</p><p>对应python的实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"> </span><br><span class="line">MIN_DISTANCE = <span class="number">0.00001</span>  <span class="comment"># 最小误差</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">euclidean_dist</span><span class="params">(pointA, pointB)</span>:</span></span><br><span class="line">    <span class="comment"># 计算pointA和pointB之间的欧式距离</span></span><br><span class="line">    total = (pointA - pointB) * (pointA - pointB).T</span><br><span class="line">    <span class="keyword">return</span> math.sqrt(total)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian_kernel</span><span class="params">(distance, bandwidth)</span>:</span></span><br><span class="line">    <span class="string">''' 高斯核函数</span></span><br><span class="line"><span class="string">    :param distance: 欧氏距离计算函数</span></span><br><span class="line"><span class="string">    :param bandwidth: 核函数的带宽</span></span><br><span class="line"><span class="string">    :return: 高斯函数值</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    m = np.shape(distance)[<span class="number">0</span>]  <span class="comment"># 样本个数</span></span><br><span class="line">    right = np.mat(np.zeros((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        right[i, <span class="number">0</span>] = (<span class="number">-0.5</span> * distance[i] * distance[i].T) / (bandwidth * bandwidth)</span><br><span class="line">        right[i, <span class="number">0</span>] = np.exp(right[i, <span class="number">0</span>])</span><br><span class="line">    left = <span class="number">1</span> / (bandwidth * math.sqrt(<span class="number">2</span> * math.pi))</span><br><span class="line">    gaussian_val = left * right</span><br><span class="line">    <span class="keyword">return</span> gaussian_val</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shift_point</span><span class="params">(point, points, kernel_bandwidth)</span>:</span></span><br><span class="line">    <span class="string">'''计算均值漂移点</span></span><br><span class="line"><span class="string">    :param point: 需要计算的点</span></span><br><span class="line"><span class="string">    :param points: 所有的样本点</span></span><br><span class="line"><span class="string">    :param kernel_bandwidth: 核函数的带宽</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        point_shifted：漂移后的点</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    points = np.mat(points)</span><br><span class="line">    m = np.shape(points)[<span class="number">0</span>]  <span class="comment"># 样本个数</span></span><br><span class="line">    <span class="comment"># 计算距离</span></span><br><span class="line">    point_distances = np.mat(np.zeros((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        point_distances[i, <span class="number">0</span>] = euclidean_dist(point, points[i])</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 计算高斯核</span></span><br><span class="line">    point_weights = gaussian_kernel(point_distances, kernel_bandwidth)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 计算分母</span></span><br><span class="line">    all = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        all += point_weights[i, <span class="number">0</span>]</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 均值偏移</span></span><br><span class="line">    point_shifted = point_weights.T * points / all</span><br><span class="line">    <span class="keyword">return</span> point_shifted</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_points</span><span class="params">(mean_shift_points)</span>:</span></span><br><span class="line">    <span class="string">'''计算所属的类别</span></span><br><span class="line"><span class="string">    :param mean_shift_points:漂移向量</span></span><br><span class="line"><span class="string">    :return: group_assignment：所属类别</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    group_assignment = []</span><br><span class="line">    m, n = np.shape(mean_shift_points)</span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    index_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        item = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            item.append(str((<span class="string">"%5.2f"</span> % mean_shift_points[i, j])))</span><br><span class="line"> </span><br><span class="line">        item_1 = <span class="string">"_"</span>.join(item)</span><br><span class="line">        <span class="keyword">if</span> item_1 <span class="keyword">not</span> <span class="keyword">in</span> index_dict:</span><br><span class="line">            index_dict[item_1] = index</span><br><span class="line">            index += <span class="number">1</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        item = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            item.append(str((<span class="string">"%5.2f"</span> % mean_shift_points[i, j])))</span><br><span class="line"> </span><br><span class="line">        item_1 = <span class="string">"_"</span>.join(item)</span><br><span class="line">        group_assignment.append(index_dict[item_1])</span><br><span class="line">    <span class="keyword">return</span> group_assignment</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_mean_shift</span><span class="params">(points, kernel_bandwidth=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="string">'''训练Mean Shift模型</span></span><br><span class="line"><span class="string">    :param points: 特征数据</span></span><br><span class="line"><span class="string">    :param kernel_bandwidth: 核函数带宽</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        points：特征点</span></span><br><span class="line"><span class="string">        mean_shift_points：均值漂移点</span></span><br><span class="line"><span class="string">        group：类别</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    mean_shift_points = np.mat(points)</span><br><span class="line">    max_min_dist = <span class="number">1</span></span><br><span class="line">    iteration = <span class="number">0</span></span><br><span class="line">    m = np.shape(mean_shift_points)[<span class="number">0</span>]  <span class="comment"># 样本的个数</span></span><br><span class="line">    need_shift = [<span class="keyword">True</span>] * m  <span class="comment"># 标记是否需要漂移</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 计算均值漂移向量</span></span><br><span class="line">    <span class="keyword">while</span> max_min_dist &gt; MIN_DISTANCE:</span><br><span class="line">        max_min_dist = <span class="number">0</span></span><br><span class="line">        iteration += <span class="number">1</span></span><br><span class="line">        print(<span class="string">"iteration : "</span> + str(iteration))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, m):</span><br><span class="line">            <span class="comment"># 判断每一个样本点是否需要计算偏置均值</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> need_shift[i]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            p_new = mean_shift_points[i]</span><br><span class="line">            p_new_start = p_new</span><br><span class="line">            p_new = shift_point(p_new, points, kernel_bandwidth)  <span class="comment"># 对样本点进行偏移</span></span><br><span class="line">            dist = euclidean_dist(p_new, p_new_start)  <span class="comment"># 计算该点与漂移后的点之间的距离</span></span><br><span class="line"> </span><br><span class="line">            <span class="keyword">if</span> dist &gt; max_min_dist:  <span class="comment"># 记录是有点的最大距离</span></span><br><span class="line">                max_min_dist = dist</span><br><span class="line">            <span class="keyword">if</span> dist &lt; MIN_DISTANCE:  <span class="comment"># 不需要移动</span></span><br><span class="line">                need_shift[i] = <span class="keyword">False</span></span><br><span class="line"> </span><br><span class="line">            mean_shift_points[i] = p_new</span><br><span class="line">    <span class="comment"># 计算最终的group</span></span><br><span class="line">    group = group_points(mean_shift_points)  <span class="comment"># 计算所属的类别</span></span><br><span class="line">    <span class="keyword">return</span> np.mat(points), mean_shift_points, group</span><br></pre></td></tr></table></figure><p>以上代码实现了基本的流程，但是执行效率很慢，正式使用时建议使用scikit-learn库中的<a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html#sphx-glr-auto-examples-cluster-plot-mean-shift-py" target="_blank" rel="noopener">MeanShift</a>。scikit-learn 中mean-shift的使用方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> MeanShift, estimate_bandwidth</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Generate sample data</span></span><br><span class="line">centers = [[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">-1</span>, <span class="number">-1</span>], [<span class="number">1</span>, <span class="number">-1</span>]]</span><br><span class="line">X, _ = make_blobs(n_samples=<span class="number">10000</span>, centers=centers, cluster_std=<span class="number">0.6</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Compute clustering with MeanShift</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The following bandwidth can be automatically detected using</span></span><br><span class="line">bandwidth = estimate_bandwidth(X, quantile=<span class="number">0.2</span>, n_samples=<span class="number">500</span>)</span><br><span class="line"></span><br><span class="line">ms = MeanShift(bandwidth=bandwidth, bin_seeding=<span class="keyword">True</span>)</span><br><span class="line">ms.fit(X)</span><br><span class="line">labels = ms.labels_</span><br><span class="line">cluster_centers = ms.cluster_centers_</span><br><span class="line"></span><br><span class="line">labels_unique = np.unique(labels)</span><br><span class="line">n_clusters_ = len(labels_unique)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"number of estimated clusters : %d"</span> % n_clusters_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Plot result</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> cycle</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line">colors = cycle(<span class="string">'bgrcmykbgrcmykbgrcmykbgrcmyk'</span>)</span><br><span class="line"><span class="keyword">for</span> k, col <span class="keyword">in</span> zip(range(n_clusters_), colors):</span><br><span class="line">    my_members = labels == k</span><br><span class="line">    cluster_center = cluster_centers[k]</span><br><span class="line">    plt.plot(X[my_members, <span class="number">0</span>], X[my_members, <span class="number">1</span>], col + <span class="string">'.'</span>)</span><br><span class="line">    plt.plot(cluster_center[<span class="number">0</span>], cluster_center[<span class="number">1</span>], <span class="string">'o'</span>, markerfacecolor=col,</span><br><span class="line">             markeredgecolor=<span class="string">'k'</span>, markersize=<span class="number">14</span>)</span><br><span class="line">plt.title(<span class="string">'Estimated number of clusters: %d'</span> % n_clusters_)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><p><code>number of estimated clusters : 3</code></p><p><img src="/img/sphx_glr_plot_mean_shift_001.png" alt="Estimated number of clusters: 3"></p><h4 id="图像分割"><a href="#图像分割" class="headerlink" title="图像分割"></a>图像分割</h4><p>对于图像分割，最简单直接的方法就是对图像上每个点的像素值进行聚类。我们对下图的像素点映射为RGB三维空间：</p><p><img src="/img/image-20200918143152873.png" alt="image-20200918143152873"></p><p>每个样本点最终会移动到核概率密度的峰值，移动到相同峰值的样本点属于同一种颜色，下图给出图像分割结果：</p><p><img src="/img/image-20200918143444607.png" alt="image-20200918143444607"></p><h4 id="目标跟踪"><a href="#目标跟踪" class="headerlink" title="目标跟踪"></a>目标跟踪</h4><p> 基于meanshift的目标跟踪算法通过分别计算目标区域和候选区域内像素的特征值概率得到关于目标模型和候选模型的描述，然后利用相似函数度量初始帧目标模型和当前帧的候选模版的相似性，选择使相似函数最大的候选模型并得到关于目标模型的Meanshift向量，这个向量正是目标由初始位置向正确位置移动的向量。由于均值漂移算法的快速收敛性，通过不断迭代计算Meanshift向量，算法最终将收敛到目标的真实位置，达到跟踪的目的。</p><p><img src="/img/image-20200918143502507.png" alt="image-20200918143502507"></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><blockquote><p><a href="http://www.360doc.com/content/19/0623/22/99071_844418459.shtml" target="_blank" rel="noopener">深入剖析meanshift聚类算法原理</a></p><p><a href="https://www.biaodianfu.com/mean-shift.html" target="_blank" rel="noopener">聚类算法之Mean Shift</a></p><p><a href="https://spin.atomicobject.com/2015/05/26/mean-shift-clustering/" target="_blank" rel="noopener">Mean Shift Clustering</a></p><p><a href="https://www.bogotobogo.com/python/OpenCV_Python/python_opencv3_mean_shift_tracking_segmentation.php" target="_blank" rel="noopener">Mean shift tracking</a> </p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MeanShift最初由Fukunaga和Hostetler在1975年提出，但是一直到2000左右这篇PAMI的论文Mean Shift: A Robust Approach Toward Feature Space Analysis，将它的原理和收敛性等重新整理阐述，并应用于计算机视觉和图像处理领域之后，才逐渐为人熟知。在了解mean-shift算法之前，先了解一下概率密度估计的概念。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://nicehuster.github.io/categories/paper-reading/"/>
    
    
      <category term="mean shift" scheme="https://nicehuster.github.io/tags/mean-shift/"/>
    
  </entry>
  
  <entry>
    <title>图像分类算法优化技巧</title>
    <link href="https://nicehuster.github.io/2019/06/13/0059-cnn_tricks/"/>
    <id>https://nicehuster.github.io/2019/06/13/0059-cnn_tricks/</id>
    <published>2019-06-13T11:13:39.000Z</published>
    <updated>2020-09-18T02:40:04.736Z</updated>
    
    <content type="html"><![CDATA[<p>上午看了一下<a href="https://arxiv.org/pdf/1812.01187.pdf" target="_blank" rel="noopener">Bag of Tricks for Image Classification with Convolutional Neural Networks</a>这篇文章，该文提到了许多关于如何提升CNN性能的trick。trick在CNN中起着非常重要的作用，然而很少有文章详细地介绍这些trick的使用。而这篇文章则对一些常用的trick做了详尽的介绍以及使用trick前后带来的对比，可以说是满满的干货，对应的代码实现，在<a href="https://github.com/dmlc/gluon-cv" target="_blank" rel="noopener">这里</a>。文章提到的trick包括：模型结构调整，数据增强，学习率调整，训练速度优化等等。<br><a id="more"></a></p><h3 id="加快模型训练"><a href="#加快模型训练" class="headerlink" title="加快模型训练"></a>加快模型训练</h3><p>目前加快模型训练有效的两种方式是：（1）使用大的batch size；（2）采用低精度训练；</p><p>（1）<strong>使用大的batch size训练</strong><br>使用较大的 batch size 时，如果还是使用同样的 epochs 数量进行运算，则准确度往往低于 batch size 较小的场景。为了确保使用较大的 batch size 训练能够在相同 epochs 前提下获得与较小的 batch size 相近的测试准确度。这部分具体可以参考<a href="https://arxiv.org/pdf/1706.02677.pdf" target="_blank" rel="noopener">Accurate, Large Minibatch SGD</a>这篇文章。作者总结了如下几种解决方案：</p><p><strong>A.增大学习率</strong>：如果我们将 batch size 由 B 增加至 kB，我们亦需要将学习率由η增加至 kη（其中 k 为倍数）。</p><p><strong>B.warm up</strong>：先用一个小的学习率先训几个epoch（warmup），因为网络的参数是随机初始化的，假如一开始就采用较大的学习率容易出现数值不稳定，这是使用warmup的原因。等到训练过程基本稳定了就可以使用原先设定的初始学习率进行训练了。论文中提到了warmup采用线性增加策略实现。举例而言，假设，初始学习率为η,选择前m个batch用于warm up，则在第i个batch时, 1 ≤ i ≤ m，设置学习率为 iη/m。</p><p><strong>C.每个残差模块最后的BN层中γ设置为0</strong>：BN层的γ、β参数是用来对标准化后的输入做线性变换的，也就是γx^+β，一般γ参数都会初始化为1，β参数会初始化为0，作者认为初始化为0更有利于模型的训练。</p><p><strong>D.不对bias执行weight decay操作</strong>：weight decay通常用于所有网络层的可学习参数（包括weight和bias）。其作用等效于对所有参数做L2正则化，以达到减少模型过拟合的作用。作者推荐仅仅只对weight进行weight decay操作。</p><p>（2）<strong>使用低精度训练</strong><br>采用低精度比如FP16训练可以在数值层面上对网络训练加速。通过大多数网络训练都是采用FP32精度训练，是因为网络的输入，网络参数以及网络输出都是采用FP32。如果能使用16位浮点型参数进行训练，就可以大大加快模型的训练速度。下表是使用大的batch size和半精度FP16进行训练的前后对比，其中baseline 使用BS=256,FP32，efficient使用BS=1024,FP16：<br><img src="/img/cnn_trick.png" alt><br>从实验结果可以看出，相比baseline,训练速度得到明显提升，而且准确率上也得到了一定程度提升。更加详细的对比实验如下：<br><img src="/img/cnn_trick2.png" alt></p><h3 id="模型结构调整"><a href="#模型结构调整" class="headerlink" title="模型结构调整"></a>模型结构调整</h3><p>这种模型结构调整是在不显著增加计算量的情况下对模型性能进行进一步提升。作者以ResNet为例进行优化。ResNet通常包括一个input stem，4个stage和1个output。下图展示的是原始resnet50结构。<br><img src="/img/cnn_trick_resnet.png" alt><br>作者在downsampling 部分，input stem以及Path B部分做了一些小的调整，对应如下图(a),(b),(c)<br><img src="/img/cnn_trick_resnet1.png" alt><br>实验结果对比如下：<br><img src="/img/cnn_trick_resnet-res.png" alt><br>从实验结果可以看出，这些结构上微小的调整，并没有对计算量带来较大的变化，但对accuracy带来不小的提升。</p><h3 id="模型训练调优"><a href="#模型训练调优" class="headerlink" title="模型训练调优"></a>模型训练调优</h3><p>作者在这一部分提到了四种调优技巧：</p><p><strong>采用cosine学习率衰减策略</strong>：实验对比结果如下，相比于常用的step decay,cosine decay在起始阶段开始衰减学习率，在step decay的学习率下降了10x时，cosine依然可以保持较大的学习率，这潜在的提高了训练速度。<br><img src="/img/cosine.png" alt></p><p><strong>知识蒸馏</strong>：使用一个效果更好的teacher model训练student model，使得student model在模型结构不改变的情况下提升效果。作者采用ResNet-152作为teacher model，用ResNet-50作为student model。代码上通过在ResNet网络后添加一个蒸馏损失函数实现，这个损失函数用来评价teacher model输出和student model输出的差异，因此整体的损失函数原损失函数和蒸馏损失函数的结合：<br><img src="/img/distill.png" alt><br>其中p表示真实标签，z表示student model的全连接层输出，r表示teacher model的全连接层输出，T是超参数，用来平滑softmax函数的输出。</p><p><strong>mixup</strong>：mixup也是一种数据增强操作，其大致操作是，每次读取2张输入图像，假设用（xi，yi）和（xj，yj）表示，那么通过如下公式就可以合成得到一张新的图像（x，y），然后用这张新图像进行训练：<br><img src="/img/mixup.png" alt><br>其中λ属于[0,1]服从Beta分布。实验结果如下<br><img src="/img/cnn_trick_res.png" alt></p><p>此外，作者将这些trick应用于其他图像任务中，比如目标检测，图像分割上同样有效，可以带来2-3个点的提升。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上午看了一下&lt;a href=&quot;https://arxiv.org/pdf/1812.01187.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Bag of Tricks for Image Classification with Convolutional Neural Networks&lt;/a&gt;这篇文章，该文提到了许多关于如何提升CNN性能的trick。trick在CNN中起着非常重要的作用，然而很少有文章详细地介绍这些trick的使用。而这篇文章则对一些常用的trick做了详尽的介绍以及使用trick前后带来的对比，可以说是满满的干货，对应的代码实现，在&lt;a href=&quot;https://github.com/dmlc/gluon-cv&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;。文章提到的trick包括：模型结构调整，数据增强，学习率调整，训练速度优化等等。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://nicehuster.github.io/categories/paper-reading/"/>
    
    
      <category term="tricks" scheme="https://nicehuster.github.io/tags/tricks/"/>
    
  </entry>
  
  <entry>
    <title>细粒度图像识别</title>
    <link href="https://nicehuster.github.io/2019/06/12/0058-fine-grain/"/>
    <id>https://nicehuster.github.io/2019/06/12/0058-fine-grain/</id>
    <published>2019-06-12T11:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>一般而言，图像识别分为两种：传统图像识别和细粒度图像识别。前者指的是对一些大的类别比如汽车、动物、植物等大的类别进行分类，这是属于粗粒度的图像识别。而后者则是在某个类别下做进一步分类。比如在狗的类别下区分狗的品种是哈士奇、柯基、萨摩还是阿拉斯加等等，这是属于细粒度图像识别。<br><img src="/img/fine-grain.png" alt><br><a id="more"></a></p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>在细粒度图像识别领域，经典的基准数据集包括：</p><ul><li>鸟类数据集CUB200-2011，11788张图像，200个细粒度分类</li><li>狗类数据集Stanford Dogs，20580张图像，120个细粒度分类</li><li>花类数据集Oxford Flowers，8189张图像，102个细粒度分类</li><li>飞机数据集Aircrafts，10200张图像，100个细粒度分类</li><li>汽车数据集Stanford Cars，16185张图像，196个细粒度分类</li></ul><p>细粒度图像分类作为一个热门的研究方向，每年的计算机视觉顶会都会举办一些workshop和挑战赛，比如Workshop on Fine-Grained Visual Categorization和iFood Classification Challenge。</p><h3 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h3><p><img src="/img/fine-grain-challenge.png" alt><br>上图展示的是CUB20鸟类数据集的部分图片。不同行表示的不同的鸟类别。很明显，这些鸟类数据集在同一类别上存在巨大差异，比如上图中每一行所展示的一样，这些差异包括姿态、背景等差异。但在不同类别的鸟类上却又存在着差异性小的问题，比如上图展示的第一列，第一列虽然分别属于不同类别，但却又十分相似。</p><p>因此可以看出，细粒度图像识别普遍存在类内差异性大（large intra-class variance）和类间差异性小（small inter-class variance）的特点。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>细粒度图像识别同样是作为图像分类任务，因此也可以直接使用通用图像识别中一些算法来做，比如直接使用resnet,vgg等网络模型直接训练识别，通常在数据集上，比如CUB200上就可以达到75%的准确率，但这种方法离目前的SOTA方法的精度至少差了10个点。</p><p>目前细粒度图像识别方法大致可以分为两类：</p><p>1.<strong>基于强监督学习方法</strong>：这里指的强监督信息是指bounding box或者landmark，举个例子，针对某一种鸟类，他和其他的类别的差异一般在于它的嘴巴、腿部，羽毛颜色等<br><img src="/img/v2-f04c284bb40f1fc3da258bb6764d4728_hd.jpg" alt><br>主流的方法像Part-based R-CNN，Pose Normalized CNN,Part-Stacked CNN等。</p><p>2.<strong>基于弱监督学习方法</strong>：什么是弱监督信息呢？就是说没有bounding box或者landmark信息，只有类别信息，开山之作应该属于2015年Bilinear CNN，这个模型当时在CUB200上是state of the art，即使和强监督学习方法相比也只是差1个点左右。</p><p>关于前几年细粒度图像分析的综述，可以参考<a href="https://zhuanlan.zhihu.com/p/24738319" target="_blank" rel="noopener">这里</a>。由于强监督学习方法中对于大规模数据集来说，bounding box和landmark标注成本较高，因此，现在主流的研究方法都是是基于弱监督学习方法。</p><p>下面是我要介绍的近1/2年来比较有代表性的顶会paper，这些paper都是基于弱监督信息，自主去挖掘Discriminative Region。</p><h3 id="Look-Closer-to-See-Better-Recurrent-Attention-Convolutional-Neural-Network-for-Fine-grained-Image-Recognition"><a href="#Look-Closer-to-See-Better-Recurrent-Attention-Convolutional-Neural-Network-for-Fine-grained-Image-Recognition" class="headerlink" title="Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-grained Image Recognition"></a><center>Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-grained Image Recognition</center></h3><p>代码链接：<a href="https://github.com/Jianlong-Fu/Recurrent-Attention-CNN" target="_blank" rel="noopener">https://github.com/Jianlong-Fu/Recurrent-Attention-CNN</a><br>这篇文章是CVPR2017的一篇oral paper。细粒度图像识别的挑战主要包括两个方面：判别力区域定位以及从判别力区域学习精细化特征。RA-CNN以一种相互强化的方式递归地学习判别力区域attention和基于区域的特征表示。具体模型结构如下：<br><img src="/img/ra-cnn.png" alt></p><h4 id="主要思路"><a href="#主要思路" class="headerlink" title="主要思路"></a>主要思路</h4><p>如上图，每一行表示一个普通的CNN网络，<br>（1）图片a1进入b1（堆叠多个卷积层）之后，分成两个部分，一个部分到c1连接fc+softmax进行普通的分类；另一个部分进入d1，Attention Proposal Network得到一个region proposal。<br>（2）在原图上利用d1提出的region proposal，在原图上crop出一个更有判别性的小区域，插值之后得到a2,同样的道理得到a3。<br>可以看出特征区域经过两个APN之后不断放大和精细化，为了使得APN选取的特征区域是图像中最具有判别性的区域，作者引入了一个Ranking loss：即强迫a1、a2、a3区域的分类confidence score越来越高(图片最后一列的对应Pt概率越来越大)。这样以来，联合普通的分类损失，使网络不断细化discriminative attention region。</p><h4 id="部分细节"><a href="#部分细节" class="headerlink" title="部分细节"></a>部分细节</h4><p><strong>attention 定位和放大</strong><br>作者使用二维boxcar函数作为attention mask与原图相乘得到候选区域位置。这样做的目的在于实现APN的端对端训练。因为普通的crop操作不可导。<br><strong>损失函数</strong><br>该模型的损失函数包含两个部分，一部分是每一路经过fc和softmax之后的一个分类误差；一部分是Ranking loss使得越精细化的区域得到了置信度分数越高。<br><img src="/img/ra-cnn-loss.png" alt><br>对于ranking loss，<br><img src="/img/rank-loss.png" alt><br>在训练的过程中，迫使$p^{(s+1)}_t &gt;p^{(s)}_t$。</p><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p>在CUB-200-2011数据集上<br><img src="/img/ra-cnn-res.png" alt></p><h3 id="Pairwise-Confusion-for-Fine-Grained-Visual-Classification"><a href="#Pairwise-Confusion-for-Fine-Grained-Visual-Classification" class="headerlink" title="Pairwise Confusion for Fine-Grained Visual Classification"></a><center>Pairwise Confusion for Fine-Grained Visual Classification</center></h3><p>代码链接：<a href="https://github.com/abhimanyudubey/confusion" target="_blank" rel="noopener">https://github.com/abhimanyudubey/confusion</a><br>这是ECCV2018的一篇文章，这篇文章提出了一种Pairwise Confusion正则化方法，主要用于解决在细粒度图像分类问题上类间相似性和样本少导致过拟合的问题。在通用图像分类问题上，由于数据集一般较大，直接使用交叉熵损失函数就可以迫使网络学习类间差异性。然而对于细粒度图像分类问题而言，数据集小，且普遍存在类间差异较小，类内差异较大的特点。假如对于两张鸟类图像样本，内容相似却有着不同的标签，直接最小化交叉熵损失将会迫使网络去学习图像本身的差异比如一些差异性较大的背景，而不能很好的挖掘不同鸟类的细粒度区别。<br><img src="/img/pc.png" alt><br>因此，作者提出了Pairwise Confusion方法 ，网络结构如上图所示。网络采用Siamese结构共享权值，对于一个Batch的图片会分成两部分，组成很多“图片对”，如果这些图片对属于相同的label，那么就把两张图片分别求Cross entropy loss；如果有一对图片属于不同的label，那么在分别对他们求Cross Entropy Loss的同时还要附加一个Euclidean Confusion作为惩罚项</p><p>论文的主要出发点还是制约不同类别图片表示的特征向量之间的距离。作为一个涨点的trick。可以加在任何细粒度识别算法中。下面是一些实验对比结果，可以看出，添加PC之后在每个数据集都能带来1-2个点。<br><img src="/img/pc-res.png" alt></p><h3 id="Learning-to-Navigate-for-Fine-grained-Classification"><a href="#Learning-to-Navigate-for-Fine-grained-Classification" class="headerlink" title="Learning to Navigate for Fine-grained Classification"></a><center>Learning to Navigate for Fine-grained Classification</center></h3><p>代码链接：<a href="https://github.com/yangze0930/NTS-Net" target="_blank" rel="noopener">https://github.com/yangze0930/NTS-Net</a><br>这也是ECCV2018的文章，这篇文章借鉴了RPN的思路，通过在原图上生成anchors，利用rank loss选出信息量最大的一些proposal，然后crop出这些区域，和原图一起提取特征然后进行决策判断。这是这篇文章方法的一个大致结构。<br><img src="/img/ntsnet.png" alt></p><h4 id="Navigator"><a href="#Navigator" class="headerlink" title="Navigator"></a>Navigator</h4><p>这个结构和FPN类似，在三个不同尺度的feature map上生成候选框，Navigator就是给每一个候选区域的“信息量”打分,信息量大的区域分数高。</p><h4 id="Teacher"><a href="#Teacher" class="headerlink" title="Teacher"></a>Teacher</h4><p>这个就是对topN分数的候选区域进行Feature Extractor + FC + softmax，判断这些候选区域属于目标的概率；</p><h4 id="Scrutinizer"><a href="#Scrutinizer" class="headerlink" title="Scrutinizer"></a>Scrutinizer</h4><p>这个是把所有候选区域part_feats和原图的raw_feats提取出来然后concat在一起，经过fc输出200个对应类别。</p><h4 id="实现流程"><a href="#实现流程" class="headerlink" title="实现流程"></a>实现流程</h4><p>下面是作者给出的源码一个具体实现流程：<br>（1）输入大小448x448尺寸原图输入feature extractor（resnet50），得到（14x14x2048）feature maps，经过Global Pooling之后2048维的Feature以及经过Global Pooling+ FC分类之后的200维的raw_logits;<br>（2）预设的RPN在14x14,7x7,4x4这三种尺度的feature map上根据不同的scale和ratio生成对应的Anchors 一共1614个（14x14x6+7x7x6+4x4x9=1614）;<br>（3）用步骤（1）得到的14x14x2048大小的feature map经过navigator对每个anchors进行打分，使用NMS进行处理，只保留topN（N=6）个proposal。<br>（4）把topN个proposal使用bilinear到224x224，输入feature extractor，得到这些局部区域的part_features,part_features经过全连接层可以到part_logits;<br>（5）把步骤（1）得到的全局Feature和步骤（4）得到的局部的part_features拼接在一起，经过全连接层得到concat_logits，用于最终的推断决策;</p><h4 id="监督训练"><a href="#监督训练" class="headerlink" title="监督训练"></a>监督训练</h4><p>（1）交叉熵损失：步骤（1）中的<strong>raw_logits</strong>, 步骤（4）中的<strong>part_logits</strong>,步骤5中的<strong>concat_logits</strong>都是直接使用交叉熵损失函数监督。<br>（2）<strong>ranking loss</strong>：步骤（3）中的信息量打分需要用步骤（4）中的part分类概率进行监督，即对于步骤（4）中的判断的属于目标Label概率高的局部区域，必须在步骤（3）中判断的信息量也高。<br><img src="/img/navigate.png" alt><br>值得注意的是，在原文中，作者提到的级联训练损失函数只由rank_loss，concat_logits以及part_logits三个部分组成，但在代码实现上加入了raw_logits部分，每个部分的权重都为1。在CUB200数据集上实验结果如下：<br><img src="/img/ntsnet-res.png" alt></p><p>从上面看到的几篇paper都可以看到ranking loss的影子，ranking loss是Learning to Rank提出的一种机器学习算法，通过训练模型来解决排序问题，具体介绍可以看<a href="https://www.cnblogs.com/bentuwuying/p/6681943.html" target="_blank" rel="noopener">这里</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一般而言，图像识别分为两种：传统图像识别和细粒度图像识别。前者指的是对一些大的类别比如汽车、动物、植物等大的类别进行分类，这是属于粗粒度的图像识别。而后者则是在某个类别下做进一步分类。比如在狗的类别下区分狗的品种是哈士奇、柯基、萨摩还是阿拉斯加等等，这是属于细粒度图像识别。&lt;br&gt;&lt;img src=&quot;/img/fine-grain.png&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://nicehuster.github.io/categories/paper-reading/"/>
    
    
      <category term="fine-grain" scheme="https://nicehuster.github.io/tags/fine-grain/"/>
    
  </entry>
  
  <entry>
    <title>了解零和博弈/贸易顺差逆差</title>
    <link href="https://nicehuster.github.io/2019/05/16/0057-zero-game/"/>
    <id>https://nicehuster.github.io/2019/05/16/0057-zero-game/</id>
    <published>2019-05-16T13:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近几天比较热的话题莫过于中美贸易谈判，看懂官方（人民日报，央视新闻等）给出的评论以及一些国际锐评，就不可避免地需要了解零和博弈和贸易逆差等关键字。</p><h3 id="零和博弈"><a href="#零和博弈" class="headerlink" title="零和博弈"></a>零和博弈</h3><p>零和博弈（zero-sum game），又称零和游戏，与非零和博弈相对，是博弈论的一个概念，属非合作博弈。指参与博弈的各方，在严格竞争下，一方的收益必然意味着另一方的损失，博弈各方的收益和损失相加总和永远为“零”，双方不存在合作的可能。</p><a id="more"></a><p>零和博弈的结果是一方吃掉另一方，一方的所得正是另一方的所失，整个社会的利益并不会因此而增加一分。也可以说：自己的幸福是建立在他人的痛苦之上的，二者的大小完全相等，因而双方都想尽一切办法以实现“损人利己”。与“零和”对应的“双赢”的基本理论就是“利己”不“损人”，通过谈判、合作达到皆大欢喜的结果。</p><p>零和博弈比较常见的例子就是打扑克/麻将，打扑克的人的钱总数是不变，变的只是钱从一个口袋转移至另一个口袋而已。</p><h3 id="非零和博弈"><a href="#非零和博弈" class="headerlink" title="非零和博弈"></a>非零和博弈</h3><p>非零和博弈是一种合作下的博弈，博弈中各方的收益或损失的总和不是零值，它区别于零和博弈。在经济学研究中比较有用。 在这种状况时，自己的所得并不与他人的损失的大小相等，连自己的幸福也未必建立在他人的痛苦之上，即使伤害他人也可能“损人不利己”，所以博弈双方存在 “双赢”的可能，进而达成合作。</p><p>非零和博弈既有可能是<strong>正和博弈</strong>，也有可能是<strong>负和博弈</strong>。<br><strong>正和博弈</strong>：指博弈双方的利益都有所增加，或者至少是一方的利益增加，而另一方的利益不受损害，因而整体的利益有所增加。<br><strong>负和博弈</strong>：指博弈双方都有损失，整体的利益有所减少。</p><p>非零和博弈比较具有代表性的例子就是“<strong>囚徒困境</strong>”。“囚徒困境”是1950年美国兰德公司提出的博弈论模型。两个共犯被关入监狱，在不能互相沟通情况。如果两个人都不揭发对方，则由于证据不确定，每个人都坐牢一年；若一人揭发，而另一人保持沉默，则揭发者因为立功而立即获释，沉默者因不合作而入狱五年；若互相揭发，则因证据确实，二者都判刑五年。由于囚徒无法信任对方，因此倾向于互相揭发，而不是同守沉默。<br>我们可以用图表来表示上述情况，如果立即获释，得0分，每人获刑1年，则-1分；如果获刑两年则-2分，如果获刑五年则-5分。结果如下：<br><img src="/img/no-zero-game.png" alt><br>囚徒困境是博弈论的非零和博弈中具代表性的例子，反映<strong>个人最佳选择并非团体最佳选择</strong>。</p><p>目前中美贸易谈判失败的一个原因在于两国的立足点从本质上就矛盾的。中国认为，目前经济全球化下，中美合作就是双赢，可以互相推动经济发展，一起赚钱。而美国则认为，中美合作就是零和博弈，全球资源总数是不变的，蛋糕就这么大，你吃了一点，那我吃的就少一点，况且，我本来就强，为什么要和你一起合作吃大蛋糕呢。这就是典型的霸权主义。近年来，中国发展迅速，赚的钱远比美国多。所以美国贸易谈判不肯合作，是说的过去的，不愧是作为生意人出身的特朗普，很有生意头脑。其实，中国在对待南海问题上也是这个态度，搁置争议，共同开发。毕竟，当前实现中华民族伟大复兴是最重要的。</p><h3 id="贸易顺差逆差"><a href="#贸易顺差逆差" class="headerlink" title="贸易顺差逆差"></a>贸易顺差逆差</h3><p><strong>贸易顺差逆差</strong>是指在一定时间内，国家的出口贸易总额大于进口贸易总额，又称“出超”，反之，则是贸易逆差，又称“入超”或者“贸易赤字”。</p><p>贸易顺差逆差会影响一国货币的汇率变化。以中国为例，贸易顺差大则就意味着商品出口增加，人民币面临升值的压力，反之当贸易逆差较大时，人民币就容易贬值，当一个国家出现贸易顺差时，说明该国对外贸易中是净赚的，对国民经济来说，在推动就业发展，增加就业，推进出口增长，增加外汇储备等方面有着积极的作用，但贸易顺差过大则意味着对国际市场依赖性较强，一旦国际市场不买账了，国内经济就会受到影响，另外为了应对本国货币的升值压力，国家通常会增加货币发行量，造成通货膨胀，除此以外，长期过大的贸易顺差还容易导致与贸易伙伴国之间的摩擦，毕竟谁愿意跟一个总是赚自己钱的人做生意呢。</p><p>而当一个国家出现贸易逆差时，说明该国在对外贸易中处于不利地位，为了支付进口产生的债务，国民收入就会外流，经济表现转弱，但贸易逆差又并非一无是处，适当的逆差有利于降低国内的通货膨胀压力，对缓解短期贸易纠纷，促进长期稳定增长都有着积极的作用。</p><p>总的来说，贸易顺差和逆差并没有孰优孰劣，对一个国家来说，平衡才是最好的状态。目前，中美贸易谈判失败的另一个原因在于，美国认为，美中两国之间存在巨额的贸易逆差。个人认为，美国的巨额贸易逆差并非因中国而生，也不会因中国而终。产生的根本原因很多，比如财政赤字，过度消费等等。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近几天比较热的话题莫过于中美贸易谈判，看懂官方（人民日报，央视新闻等）给出的评论以及一些国际锐评，就不可避免地需要了解零和博弈和贸易逆差等关键字。&lt;/p&gt;
&lt;h3 id=&quot;零和博弈&quot;&gt;&lt;a href=&quot;#零和博弈&quot; class=&quot;headerlink&quot; title=&quot;零和博弈&quot;&gt;&lt;/a&gt;零和博弈&lt;/h3&gt;&lt;p&gt;零和博弈（zero-sum game），又称零和游戏，与非零和博弈相对，是博弈论的一个概念，属非合作博弈。指参与博弈的各方，在严格竞争下，一方的收益必然意味着另一方的损失，博弈各方的收益和损失相加总和永远为“零”，双方不存在合作的可能。&lt;/p&gt;
    
    </summary>
    
      <category term="杂谈" scheme="https://nicehuster.github.io/categories/%E6%9D%82%E8%B0%88/"/>
    
    
      <category term="摘记" scheme="https://nicehuster.github.io/tags/%E6%91%98%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>HRNet详解</title>
    <link href="https://nicehuster.github.io/2019/05/15/0056-HRNet/"/>
    <id>https://nicehuster.github.io/2019/05/15/0056-HRNet/</id>
    <published>2019-05-15T13:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> Ke Sun,USTC &amp; MSRA, CVPR2019<br><strong>代码链接：</strong> <a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch" target="_blank" rel="noopener">https://github.com/leoxiaobin/deep-high-resolution-net.pytorch</a><br><strong>整体信息：</strong> HRNet（High Resolution Network）由一个中科大的学生在MSRA实习时的一篇工作。这篇工作设计了一种新的人体姿态估计模型。在姿态估计中，图像feature map的分辨率大小至关重要，以往的姿态估计方法都是采用串行的高—&gt;低—&gt;高的方式来获得语义信息强的高分辨feature map。而HRNet不同的是，其自始至终都保持的高分辨率。并且这非常有效。刷新 COCO keypoint detection数据集和the MPII Human Pose数据集。<br><img src="/img/other-hrnet.png" alt><br><a id="more"></a></p><p>上图表示的姿态估计方法中几种经典结构，（a）是Hourglass中的结构，（b）是CPN中的结构，（c）是simpleBaseline，（d）则是DeepCut结构，可以看这几种经典的结构中，图像的feature map都经历了high-to-low和low-to-high的两个变化过程，前者的目的在于生成低分辨率（low-resolution）以及高层表示（high-level representation），后者目的则是生成高分辨率表示（high-resolution representation），feature map大小变化是串行的。而且，从高到低和从低到高是分开的。</p><p>其实这些网络的设计模式不外乎：1）对称的high-to-low和low-to-high过程，比如hourglass；2）Heavy high-to-low 和 light low-to-high过程；后者这类方法high-to-low过程一般采用分类网络（比如ResNet,VGG）是一个heavy部分，而low-to-high过程则是简单地堆叠 bilinear-upsampling层或者或者transpose卷积层。</p><h3 id="HRNet"><a href="#HRNet" class="headerlink" title="HRNet"></a>HRNet</h3><p><img src="/img/HRNet.png" alt><br>HRNet的网络结构如上图所示，非常清晰易懂。一共有三个不同feature map大小的branch。每个branch在前向传播过程中，feature map大小不发生变化。三个branch虽然有着不同大小feature map但是每个branch之间会存在信息的交流。例如在前向的过程中，上面branch会把自己的feature map大小减半，然后传入到下面的branch中，而下面的branch也会通过upsample把变大后的feature送给上面的branch，两个操作可以在同一个阶段同时进行。</p><p>可以看出，不同于其他方法，HRNet通过并行的方式连接高-低分辨率特征图，因此可以直接在高分辨率的特征图上预测姿态，而不需要通过降采样再升采样来预测姿态，而且在整个过程中，不停地融合各种不同尺度的表征，实现了多尺度融合，提高了高分辨率的语义信息。</p><p>实验结果，在两种输入分辨率上，大模型HRNet-W48和小模型HRNet-W32，都刷新了COCO纪录。其中，大模型在384x288的输入分辨率上，拿到了76.3的mAP。<br><img src="/img/HRNet-res.png" alt></p><h3 id="姿态估计评估指标"><a href="#姿态估计评估指标" class="headerlink" title="姿态估计评估指标"></a>姿态估计评估指标</h3><p>最后提一下姿态估计中一些常用的评估指标。不得不提的两个指标就是OKS和PCK。</p><h4 id="OKS（Object-Keypoint-Similarity）"><a href="#OKS（Object-Keypoint-Similarity）" class="headerlink" title="OKS（Object Keypoint Similarity）"></a>OKS（Object Keypoint Similarity）</h4><p>这个是coco姿态估计挑战赛提出的一个评估指标，基于对象关键点相似度的mAP,常用于coco姿态估计中。<br><img src="/img/oks.png" alt><br>其中，di表示预测的关键点与ground truth之间的欧式距离。vi是ground truth的可见性标志，s是目标尺度，此值等于该人在ground truth中的面积的平方根,ki控制衰减的每个关键点常量。</p><p>简而言之，OKS扮演的角色与IoU在目标检测中扮演的角色相同。它是根据人的尺度标准化的预测点和标准真值点之间的距离计算出来的。在coco<a href="http://cocodataset.org/#keypoints-leaderboard" target="_blank" rel="noopener">官网</a>可以看到和目标检测一样的评估指标AP[0.5:0.95]和AR[0.5:95]。从官网目前给出的leaderboard可以看到mAP最高是0.764。</p><h4 id="PCK（Probability-of-Correct-Keypoint）"><a href="#PCK（Probability-of-Correct-Keypoint）" class="headerlink" title="PCK（Probability of Correct Keypoint）"></a>PCK（Probability of Correct Keypoint）</h4><p>预测的关键点与其对应的 groundtruth 之间的归一化距离小于设定阈值的比例。如果预测关节与真实关节之间的距离在特定阈值内，则检测到的关节被认为是正确的。阈值可以是：</p><ul><li>PCK@0.2表示以躯干（torso size）直径最为归一化参考，如果归一化后的距离大于阈值0.2，则认为改点预测正确。FLIC数据集评估指标采用的就是PCK@0.2。</li><li>PCKh@0.5表示以头部长度（head length）作为归一化参考，如果归一化后的距离大于阈值0.5，则认为改点预测正确。MPII数据集的评估指标采用的就是PCKh@0.5，目前MPII数据集PCKh最高为92.5；</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; Ke Sun,USTC &amp;amp; MSRA, CVPR2019&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt; &lt;a href=&quot;https://github.com/leoxiaobin/deep-high-resolution-net.pytorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/leoxiaobin/deep-high-resolution-net.pytorch&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt; HRNet（High Resolution Network）由一个中科大的学生在MSRA实习时的一篇工作。这篇工作设计了一种新的人体姿态估计模型。在姿态估计中，图像feature map的分辨率大小至关重要，以往的姿态估计方法都是采用串行的高—&amp;gt;低—&amp;gt;高的方式来获得语义信息强的高分辨feature map。而HRNet不同的是，其自始至终都保持的高分辨率。并且这非常有效。刷新 COCO keypoint detection数据集和the MPII Human Pose数据集。&lt;br&gt;&lt;img src=&quot;/img/other-hrnet.png&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://nicehuster.github.io/categories/paper-reading/"/>
    
    
      <category term="pose estimation" scheme="https://nicehuster.github.io/tags/pose-estimation/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch常用API解析</title>
    <link href="https://nicehuster.github.io/2019/05/14/0055-pytorch-api/"/>
    <id>https://nicehuster.github.io/2019/05/14/0055-pytorch-api/</id>
    <published>2019-05-14T11:13:39.000Z</published>
    <updated>2020-09-18T02:39:22.802Z</updated>
    
    <content type="html"><![CDATA[<p>好记性不如烂笔头，主要是纪录一些Pytorch中常用函数用法以及自定义数据读取collate_fn()介绍。</p><h3 id="常用数学类操作符"><a href="#常用数学类操作符" class="headerlink" title="常用数学类操作符"></a>常用数学类操作符</h3><h4 id="torchTensor-transpose-amp-Tensor-permute"><a href="#torchTensor-transpose-amp-Tensor-permute" class="headerlink" title="torchTensor.transpose() &amp; Tensor.permute()"></a>torchTensor.transpose() &amp; Tensor.permute()</h4><p>transpose只能操作矩阵的两个维度。只接受两个维度参数。若要对多个维度进行操作，使用permute更加灵活方便。<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a=torch.rand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)  #torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">b=a.transpose(<span class="number">0</span>,<span class="number">1</span>)   #torch.Size([<span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">c=a.transpose(<span class="number">0</span>,<span class="number">1</span>).transpose(<span class="number">1</span>,<span class="number">2</span>)  #torch.Size([<span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>])</span><br><span class="line">d=a.permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)                 #torch.Size([<span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure></p><p>同样是对三个维度进行变换，transpose需要操作三次，而是用permute只需要操作一次，因此对于高维度的矩阵变化，permute更加方便。<br><a id="more"></a></p><h4 id="toch-cat-amp-torch-stack"><a href="#toch-cat-amp-torch-stack" class="headerlink" title="toch.cat() &amp; torch.stack()"></a>toch.cat() &amp; torch.stack()</h4><p>cat是对数据沿着某一维度进行拼接。cat后数据的总维数不变。二维的矩阵拼接后依旧是二维的矩阵。如下面代码对两个2维tensor（分别为2<em>3,4</em>3）进行拼接，拼接完后变为3*3还是2维的tensor。<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=torch.rand(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">y=torch.rand(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">z=torch.cat((x,y),<span class="number">0</span>)  #torch.Size([<span class="number">6</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure></p><p><strong>注意</strong>：对维度进行拼接是必须满足维度一致的要求，除了指定拼接的维度除外，其他维度必须都相等。否则就会出现维度不匹配问题。<br>stack是增加新的维度进行堆叠。这个比较常见，比如在pytorch中对数据进行加载组成一个batch时常用到。比较好理解。<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a=torch.rand(<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">b=torch.rand(<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">c=torch.stack((a,b),<span class="number">0</span>)  # torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>])</span><br><span class="line">d=torch.stack((a,b),<span class="number">3</span>)  # torch.Size([<span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure></p><p>注意cat()和stack()两者区别就在于后者会增加维度。</p><h4 id="torch-squeeze-amp-torch-unsqueeze"><a href="#torch-squeeze-amp-torch-unsqueeze" class="headerlink" title="torch.squeeze() &amp; torch.unsqueeze()"></a>torch.squeeze() &amp; torch.unsqueeze()</h4><p>squeeze(dim_n)压缩，即去掉元素数量为1的dim_n维度。同理unsqueeze(dim_n)，增加dim_n维度，元素数量为1。<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#减少维度</span><br><span class="line">a=torch.rand(<span class="number">2</span>,<span class="number">1</span>,<span class="number">4</span>)  #torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">a_=a.squeeze()       #torch.Size([<span class="number">2</span>, <span class="number">4</span>])，不加参数，去掉所有为元素个数为<span class="number">1</span>的维度</span><br><span class="line">a_=a.squeeze(<span class="number">0</span>)      #torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>])加上参数，去掉第一维的元素为<span class="number">1</span>，不起作用，因为第一维有<span class="number">2</span>个元素</span><br><span class="line">a_=a.squeeze(<span class="number">1</span>)      #torch.Size([<span class="number">2</span>, <span class="number">4</span>])，这样就可以</span><br><span class="line"></span><br><span class="line">#增加维度</span><br><span class="line">a_=a.unsqueeze(<span class="number">0</span>)    #torch.Size([<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure></p><h4 id="Tensor-expand-as"><a href="#Tensor-expand-as" class="headerlink" title="Tensor.expand_as()"></a>Tensor.expand_as()</h4><p>expand_as()这是tensor变量的一个内置方法，如果使用b.expand_as(a)就是将b进行扩充，扩充到a的维度，需要说明的是a的低维度需要比b大，例如b的shape是3<em>1，如果a的shape是3</em>2不会出错，但是是2*2就会报错了<br><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a=torch.Tensor(<span class="string">[[1],[2],[3]]</span>)</span><br><span class="line">b=torch.Tensor(<span class="string">[[4]]</span>)</span><br><span class="line">c=b.expand_as(a)</span><br><span class="line">#tensor(<span class="string">[[4.],</span></span><br><span class="line"><span class="string">        [4.],</span></span><br><span class="line"><span class="string">        [4.]]</span>)</span><br></pre></td></tr></table></figure></p><h4 id="torch-contiguous"><a href="#torch-contiguous" class="headerlink" title="torch.contiguous()"></a>torch.contiguous()</h4><p>contiguous：view只能用在contiguous的variable上。如果在view之前用了transpose, permute等，需要用contiguous()来返回一个contiguous copy。 因为view需要tensor的内存是整块的。有些tensor并不是占用一整块内存，而是由不同的数据块组成，而tensor的view()操作依赖于内存是整块的，这时只需要执行contiguous()这个函数，把tensor变成在内存中连续分布的形式。判断是否contiguous用torch.Tensor.is_contiguous()函数<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=torch.ones(<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line">x.is_contiguous()  #<span class="literal">True</span></span><br><span class="line">x.transpose(<span class="number">0</span>,<span class="number">1</span>).is_contiguous()  #<span class="literal">False</span></span><br><span class="line">x.transpose(<span class="number">0</span>, <span class="number">1</span>).contiguous().is_contiguous()  #<span class="literal">True</span></span><br></pre></td></tr></table></figure></p><p><strong>因此，在调用view之前最好先contiguous一下，x.contiguous().view()</strong> 。</p><h3 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h3><p>在pytorch中一个现有的数据读取方法就是torchvision.datasets.ImageFolder()，这个api主要用于做分类问题。将每一类数据放到同一个文件夹中，比如有10个类别，那么就在一个大的文件夹下面建立10个子文件夹，每个子文件夹里面放的是同一类的数据。从api的实现可以发现ImageFolder该类继承自torch.utils.data.Dataset。</p><h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h4><p>Dataset的定义如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""An abstract class representing a Dataset.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    All other datasets should subclass it. All subclasses should override</span></span><br><span class="line"><span class="string">    ``__len__``, that provides the size of the dataset, and ``__getitem__``,</span></span><br><span class="line"><span class="string">    supporting integer indexing in range from 0 to len(self) exclusive.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__add__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> ConcatDataset([self, other])</span><br></pre></td></tr></table></figure></p><p>从上面的注释知道，这个是代表数据集的一个抽象类。有关于数据集的类都可以定义为其子类，只需要重写<strong>getitem</strong>和<strong>len</strong>就可以了。<br>那么定义好了数据集我们不可能将所有的数据集都放到内存，这样内存肯定就爆了，我们需要定义一个迭代器，每一步产生一个batch，这里PyTorch已经为我们实现好了，就是下面的torch.utils.data.DataLoader。</p><h4 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h4><p>DataLoader能够为我们自动生成一个多线程的迭代器。DataLoader的函数定义如下：DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0, collate_fn=default_collate, pin_memory=False, drop_last=False)</p><ul><li>dataset：加载的数据集(Dataset对象)</li><li>batch_size：batch size</li><li>shuffle:：是否将数据打乱</li><li>sampler： 样本抽样，后续会详细介绍</li><li>num_workers：使用多进程加载的进程数，0代表不使用多进程</li><li>collate_fn： 如何将多个样本数据拼接成一个batch，一般使用默认的拼接方式即可</li><li>pin_memory：是否将数据保存在pin memory区，pin memory中的数据转到GPU会快一些</li><li>drop_last：dataset中的数据个数可能不是batch_size的整数倍，drop_last为True会将多出来不足一个batch的数据丢弃</li></ul><h4 id="collate-fn"><a href="#collate-fn" class="headerlink" title="collate_fn()"></a>collate_fn()</h4><p>到这里，我们可以发现pytorch数据载入很简单，基本上对于简单地任务都不会出啥问题，然而数据载入容易出错的地方往往在于collate_fn函数。尤其是对于batch大小不一致的情况，比如目标检测，需要检测出图片中所有的目标。如下：<br><img src="/img/yolo2_result.png" alt><br>问题就是，输入的是一张张图片，大小不同可以resize成一样大小组成一个batch，但是它的label是每个目标的boxes和categories。而且每张图片中的目标个数也不一样，怎么组成一个batch。这个时候我们就需要自定义实现一个collate_fn函数。这里可以使用任何名字，只要在DataLoader里面传入就可以了。<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">def collate_fn(self, batch):</span><br><span class="line">    <span class="string">""</span><span class="comment">"</span></span><br><span class="line">    Since each image may have <span class="keyword">a</span> different <span class="keyword">number</span> of objects, we need <span class="keyword">a</span> collate <span class="function"><span class="keyword">function</span> <span class="params">(to be passed to the DataLoader)</span>.</span></span><br><span class="line"></span><br><span class="line">    This describes how <span class="keyword">to</span> combine these tensors of different sizes. We use lists.</span><br><span class="line"></span><br><span class="line">    Note: this need not <span class="keyword">be</span> defined in this Class, can <span class="keyword">be</span> standalone.</span><br><span class="line"></span><br><span class="line">    :param batch: <span class="keyword">an</span> iterable of <span class="keyword">N</span> sets from __getitem__()</span><br><span class="line">    :<span class="keyword">return</span>: <span class="keyword">a</span> tensor of images, lists of varying-size tensors of bounding boxes, labels, <span class="built_in">and</span> difficulties</span><br><span class="line">    <span class="string">""</span><span class="comment">"</span></span><br><span class="line">    </span><br><span class="line">    images = <span class="keyword">list</span>()</span><br><span class="line">    boxes = <span class="keyword">list</span>()</span><br><span class="line">    labels = <span class="keyword">list</span>()</span><br><span class="line">    difficulties = <span class="keyword">list</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> <span class="keyword">b</span> in batch:</span><br><span class="line">        images.<span class="keyword">append</span>(<span class="keyword">b</span>[<span class="number">0</span>])</span><br><span class="line">        boxes.<span class="keyword">append</span>(<span class="keyword">b</span>[<span class="number">1</span>])</span><br><span class="line">        labels.<span class="keyword">append</span>(<span class="keyword">b</span>[<span class="number">2</span>])</span><br><span class="line">        difficulties.<span class="keyword">append</span>(<span class="keyword">b</span>[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    images = torch.stack(images, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> images, boxes, labels, difficulties  # tensor (<span class="keyword">N</span>, <span class="number">3</span>, <span class="number">300</span>, <span class="number">300</span>), <span class="number">3</span> lists of <span class="keyword">N</span> tensors each</span><br></pre></td></tr></table></figure></p><h4 id="pin-memory"><a href="#pin-memory" class="headerlink" title="pin_memory()"></a>pin_memory()</h4><p>上面提到了pin_memory参数，pin_memory就是锁页内存，创建DataLoader时，设置pin_memory=True，则意味着生成的Tensor数据最开始是属于内存中的锁页内存，这样将内存的Tensor转义到GPU的显存就会更快一些。</p><p>主机中的内存，有两种存在方式，一是锁页，二是不锁页，锁页内存存放的内容在任何情况下都不会与主机的虚拟内存进行交换（注：虚拟内存就是硬盘），而不锁页内存在主机内存不足时，数据会存放在虚拟内存中。</p><p>而显卡中的显存全部是锁页内存！</p><p>当计算机的内存充足的时候，可以设置pin_memory=True。当系统卡住，或者交换内存使用过多的时候，设置pin_memory=False。因为pin_memory与电脑硬件性能有关，pytorch开发者不能确保每一个炼丹玩家都有高端设备，因此pin_memory默认为False。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;好记性不如烂笔头，主要是纪录一些Pytorch中常用函数用法以及自定义数据读取collate_fn()介绍。&lt;/p&gt;
&lt;h3 id=&quot;常用数学类操作符&quot;&gt;&lt;a href=&quot;#常用数学类操作符&quot; class=&quot;headerlink&quot; title=&quot;常用数学类操作符&quot;&gt;&lt;/a&gt;常用数学类操作符&lt;/h3&gt;&lt;h4 id=&quot;torchTensor-transpose-amp-Tensor-permute&quot;&gt;&lt;a href=&quot;#torchTensor-transpose-amp-Tensor-permute&quot; class=&quot;headerlink&quot; title=&quot;torchTensor.transpose() &amp;amp; Tensor.permute()&quot;&gt;&lt;/a&gt;torchTensor.transpose() &amp;amp; Tensor.permute()&lt;/h4&gt;&lt;p&gt;transpose只能操作矩阵的两个维度。只接受两个维度参数。若要对多个维度进行操作，使用permute更加灵活方便。&lt;br&gt;&lt;figure class=&quot;highlight lsl&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;a=torch.rand(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)  #torch.Size([&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b=a.transpose(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)   #torch.Size([&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;c=a.transpose(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;).transpose(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)  #torch.Size([&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;d=a.permute(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)                 #torch.Size([&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;同样是对三个维度进行变换，transpose需要操作三次，而是用permute只需要操作一次，因此对于高维度的矩阵变化，permute更加方便。&lt;br&gt;
    
    </summary>
    
      <category term="project" scheme="https://nicehuster.github.io/categories/project/"/>
    
    
      <category term="pytorch" scheme="https://nicehuster.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>SSD中的数据增强细节</title>
    <link href="https://nicehuster.github.io/2019/05/11/0054-ssd-dataaug/"/>
    <id>https://nicehuster.github.io/2019/05/11/0054-ssd-dataaug/</id>
    <published>2019-05-11T11:13:39.000Z</published>
    <updated>2020-09-18T02:40:13.917Z</updated>
    
    <content type="html"><![CDATA[<p>在SSD中数据增强起着至关重要的作用，从原文的实验结果可以看出，数据增强可以为提高8.8%mAP。如下表所示：<br><img src="/img/img_5b340d37f2563.png" alt><br>数据增强对于提高小目标的检测精度尤为重要，因为它在分类器可以看到更多对象结构的图像中创建放大图像。数据增强还可用于处理包含被遮挡对象的图像，通过将裁剪的图像包括在训练数据中，在训练数据中只能看到对象的一部分。在SSD中数据增强包括如下步骤：<br><a id="more"></a></p><ul><li>光度畸变（Photometric Distortions）<ul><li>随机亮度（Random Brightness）</li><li>随机对比度，色调和饱和度（Random Contrast, Hue, Saturation）</li><li>随机光噪声（RandomLightingNoise）</li></ul></li><li>几何畸变（Geometric Distortions）<ul><li>随机扩展（RandomExpand）</li><li>随机裁剪（RandomCrop）</li><li>随机翻转（RandomMirror）</li></ul></li></ul><p><img src="/img/img_5b345c68e6487-768x489.png" alt></p><h3 id="Photometric-Distortions"><a href="#Photometric-Distortions" class="headerlink" title="Photometric Distortions"></a><strong>Photometric Distortions</strong></h3><h4 id="Random-Brightness"><a href="#Random-Brightness" class="headerlink" title="Random Brightness"></a><strong>Random Brightness</strong></h4><p>这个是从$[-\delta,\delta]$中随机选择一个值添加到图像的所有像素中，默认值设置为32。<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def __call__(self, <span class="built_in">image</span>, boxes=None, <span class="built_in">labels</span>=None):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">random</span>.randint(<span class="number">2</span>):</span><br><span class="line">        <span class="built_in">delta</span> = <span class="built_in">random</span>.uniform(-self.<span class="built_in">delta</span>, self.<span class="built_in">delta</span>)</span><br><span class="line">        <span class="built_in">image</span> += <span class="built_in">delta</span></span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">image</span>, boxes, <span class="built_in">labels</span></span><br></pre></td></tr></table></figure></p><p><img src="/img/img_5b345fd789997.png" alt></p><h4 id="Random-Contrast-Hue-Saturation"><a href="#Random-Contrast-Hue-Saturation" class="headerlink" title="Random Contrast, Hue, Saturation"></a><strong>Random Contrast, Hue, Saturation</strong></h4><p>对于Contrast, Hue, Saturation的应用，有两种选择：Contrast + Hue + Saturation和Hue + Saturation + Contrast。每种选择概率为0.5.<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">self<span class="selector-class">.pd</span> = [</span><br><span class="line">            RandomContrast(),</span><br><span class="line">            ConvertColor(<span class="attribute">transform</span>=<span class="string">'HSV'</span>),</span><br><span class="line">            RandomSaturation(),</span><br><span class="line">            RandomHue(),</span><br><span class="line">            ConvertColor(current=<span class="string">'HSV'</span>, <span class="attribute">transform</span>=<span class="string">'BGR'</span>),</span><br><span class="line">            RandomContrast()</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line"> im, boxes, labels = self.rand_brightness(im, boxes, labels)</span><br><span class="line">        <span class="keyword">if</span> random.randint(<span class="number">2</span>):</span><br><span class="line">            distort = Compose(self<span class="selector-class">.pd</span>[:-<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            distort = Compose(self<span class="selector-class">.pd</span>[<span class="number">1</span>:])</span><br><span class="line">        im, boxes, labels = distort(im, boxes, labels)</span><br></pre></td></tr></table></figure></p><p>需要注意的是，Contrast应用于RGB空间，而Hue, Saturation则是应用HSV空间。因此，在应用每个操作之前，必须执行适当的颜色空间转换。应用随机contrast之后，随机hue and 随机saturation操作与随机Brightness类似。随机saturation如下：<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def __call__(self, <span class="built_in">image</span>, boxes=None, <span class="built_in">labels</span>=None):</span><br><span class="line">       <span class="keyword">if</span> <span class="built_in">random</span>.randint(<span class="number">2</span>):</span><br><span class="line">           <span class="built_in">image</span>[:, :, <span class="number">1</span>] *= <span class="built_in">random</span>.uniform(self.lower, self.upper)</span><br><span class="line"></span><br><span class="line">       <span class="built_in">return</span> <span class="built_in">image</span>, boxes, <span class="built_in">labels</span></span><br></pre></td></tr></table></figure></p><p><img src="/img/img_5b3463b4d93e0.png" alt></p><h4 id="RandomLightingNoise"><a href="#RandomLightingNoise" class="headerlink" title="RandomLightingNoise"></a><strong>RandomLightingNoise</strong></h4><p>最后一个 光度畸变是随机光噪声，这种操作主要是颜色通道的随机交换。三个通道有六种jiaohua<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.perms = ((<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>), (<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">                      (<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">                      (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>))</span><br></pre></td></tr></table></figure></p><p>随机交换后如下：<br><img src="/img/img_5b34650f2861b.png" alt></p><h3 id="Geometric-Distortions"><a href="#Geometric-Distortions" class="headerlink" title="Geometric Distortions"></a><strong>Geometric Distortions</strong></h3><h4 id="RandomExpand"><a href="#RandomExpand" class="headerlink" title="RandomExpand"></a><strong>RandomExpand</strong></h4><p>随机扩展的发生概率为0.5，随机扩展的步骤如下<br><img src="/img/img_5b35382c10419.png" alt></p><h4 id="RandomCrop"><a href="#RandomCrop" class="headerlink" title="RandomCrop"></a><strong>RandomCrop</strong></h4><p>这一步主要是在上一步随机扩展的基础上，对目标进行随机裁剪，有利于缓解目标遮挡问题。随机生成裁剪框，如果框与目标的iou不满足要求则删除，如果iou满足要求，而目标中心点不包含在随机裁剪框也删除。<br><img src="/img/img_5b3544b2a975a.png" alt><br>部分例子如下：<br><img src="/img/img_5b35476dbeb90.png" alt><br>其中，红色表示ground truth，绿色表示随机裁剪的框。</p><h4 id="RandomMirror"><a href="#RandomMirror" class="headerlink" title="RandomMirror"></a><strong>RandomMirror</strong></h4><p>随机裁剪主要是镜像翻转。很简单。</p><p>最后，对经过数据增强的图像resize到（300,300），然后减均值。<br>代码参考：  <a href="https://github.com/amdegroot/ssd.pytorch/" target="_blank" rel="noopener">https://github.com/amdegroot/ssd.pytorch/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在SSD中数据增强起着至关重要的作用，从原文的实验结果可以看出，数据增强可以为提高8.8%mAP。如下表所示：&lt;br&gt;&lt;img src=&quot;/img/img_5b340d37f2563.png&quot; alt&gt;&lt;br&gt;数据增强对于提高小目标的检测精度尤为重要，因为它在分类器可以看到更多对象结构的图像中创建放大图像。数据增强还可用于处理包含被遮挡对象的图像，通过将裁剪的图像包括在训练数据中，在训练数据中只能看到对象的一部分。在SSD中数据增强包括如下步骤：&lt;br&gt;
    
    </summary>
    
      <category term="project" scheme="https://nicehuster.github.io/categories/project/"/>
    
    
      <category term="tricks" scheme="https://nicehuster.github.io/tags/tricks/"/>
    
  </entry>
  
  <entry>
    <title>SSD详解以及代码实现</title>
    <link href="https://nicehuster.github.io/2019/05/09/0053-ssd-explained%20with%20code/"/>
    <id>https://nicehuster.github.io/2019/05/09/0053-ssd-explained with code/</id>
    <published>2019-05-09T11:13:39.000Z</published>
    <updated>2020-09-18T02:39:05.305Z</updated>
    
    <content type="html"><![CDATA[<p>&#160; &#160; &#160; &#160;SSD作为one-stage目标检测算法中比较经典且具有代表性的一种方法。其精度可以与Faster R-CNN相匹敌，而速度达到了惊人的59FPS，速度上完爆 Faster R-CNN。相比于Faster R-CNN,其速度快的根本原因在于移除了region proposals的步骤以及后续的像素采样或特征采样步骤。论文连接：<a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noopener">SSD: Single Shot MultiBox Detector</a>，作者开源的代码连接：<a href="https://github.com/weiliu89/caffe/tree/ssd" target="_blank" rel="noopener">code</a>。由于作者开源代码使用caffe实现，这里以pytorch源码的方式实现。作者在论文中实现了两种不同输入大小的SSD模型：SSD300（输入图像大小统一为300x300）以及SSD512（输入图像大小统一为512x512）.这里主要针对SSD300,下面主要分四个部分介绍SSD300以及代码具体实现。<br><img src="/img/ssd.png" alt><br><a id="more"></a></p><h3 id="Base-Convolutions"><a href="#Base-Convolutions" class="headerlink" title="Base Convolutions"></a>Base Convolutions</h3><p>如上图所示，在SSD算法中基础网络采用的是VGG-16结构作为其基础网络。与VGG-16不同的是，把fc6和fc7替换成了卷积层。替换后的backbone结构如下图所示<br><img src="/img/modifiedvgg.png" alt><br>对应代码实现如下：<br><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VGGBase</span>(<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    VGG base convolutions to produce lower-level feature maps.</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span></span>(<span class="keyword">self</span>):</span><br><span class="line">        <span class="keyword">super</span>(VGGBase, <span class="keyword">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Standard convolutional layers in VGG16</span></span><br><span class="line">        <span class="keyword">self</span>.conv1_1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)  <span class="comment"># stride = 1, by default</span></span><br><span class="line">        <span class="keyword">self</span>.conv1_2 = nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.pool1 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.conv2_1 = nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv2_2 = nn.Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.pool2 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.conv3_1 = nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv3_2 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv3_3 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.pool3 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, ceil_mode=True)  <span class="comment"># ceiling (not floor) here for even dims</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.conv4_1 = nn.Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv4_2 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv4_3 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.pool4 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.conv5_1 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv5_2 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv5_3 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.pool5 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)  <span class="comment"># retains size because stride is 1 (and padding)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Replacements for FC6 and FC7 in VGG16</span></span><br><span class="line">        <span class="keyword">self</span>.conv6 = nn.Conv2d(<span class="number">512</span>, <span class="number">1024</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">6</span>, dilation=<span class="number">6</span>)  <span class="comment"># atrous convolution</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.conv7 = nn.Conv2d(<span class="number">1024</span>, <span class="number">1024</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Load pretrained layers</span></span><br><span class="line">        <span class="keyword">self</span>.load_pretrained_layers()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span></span>(<span class="keyword">self</span>, image):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        Forward propagation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param image: images, a tensor of dimensions (N, 3, 300, 300)</span></span><br><span class="line"><span class="string">        :return: lower-level feature maps conv4_3 and conv7</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv1_1(image))  <span class="comment"># (N, 64, 300, 300)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv1_2(<span class="keyword">out</span>))  <span class="comment"># (N, 64, 300, 300)</span></span><br><span class="line">        <span class="keyword">out</span> = <span class="keyword">self</span>.pool1(<span class="keyword">out</span>)  <span class="comment"># (N, 64, 150, 150)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv2_1(<span class="keyword">out</span>))  <span class="comment"># (N, 128, 150, 150)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv2_2(<span class="keyword">out</span>))  <span class="comment"># (N, 128, 150, 150)</span></span><br><span class="line">        <span class="keyword">out</span> = <span class="keyword">self</span>.pool2(<span class="keyword">out</span>)  <span class="comment"># (N, 128, 75, 75)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv3_1(<span class="keyword">out</span>))  <span class="comment"># (N, 256, 75, 75)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv3_2(<span class="keyword">out</span>))  <span class="comment"># (N, 256, 75, 75)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv3_3(<span class="keyword">out</span>))  <span class="comment"># (N, 256, 75, 75)</span></span><br><span class="line">        <span class="keyword">out</span> = <span class="keyword">self</span>.pool3(<span class="keyword">out</span>)  <span class="comment"># (N, 256, 38, 38), it would have been 37 if not for ceil_mode = True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv4_1(<span class="keyword">out</span>))  <span class="comment"># (N, 512, 38, 38)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv4_2(<span class="keyword">out</span>))  <span class="comment"># (N, 512, 38, 38)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv4_3(<span class="keyword">out</span>))  <span class="comment"># (N, 512, 38, 38)</span></span><br><span class="line">        conv4_3_feats = <span class="keyword">out</span>  <span class="comment"># (N, 512, 38, 38)</span></span><br><span class="line">        <span class="keyword">out</span> = <span class="keyword">self</span>.pool4(<span class="keyword">out</span>)  <span class="comment"># (N, 512, 19, 19)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv5_1(<span class="keyword">out</span>))  <span class="comment"># (N, 512, 19, 19)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv5_2(<span class="keyword">out</span>))  <span class="comment"># (N, 512, 19, 19)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv5_3(<span class="keyword">out</span>))  <span class="comment"># (N, 512, 19, 19)</span></span><br><span class="line">        <span class="keyword">out</span> = <span class="keyword">self</span>.pool5(<span class="keyword">out</span>)  <span class="comment"># (N, 512, 19, 19), pool5 does not reduce dimensions</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv6(<span class="keyword">out</span>))  <span class="comment"># (N, 1024, 19, 19)</span></span><br><span class="line"></span><br><span class="line">        conv7_feats = F.relu(<span class="keyword">self</span>.conv7(<span class="keyword">out</span>))  <span class="comment"># (N, 1024, 19, 19)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Lower-level feature maps</span></span><br><span class="line">        <span class="keyword">return</span> conv4_3_feats, conv7_feats</span><br></pre></td></tr></table></figure></p><p>值得注意的是：</p><ul><li>pool3采用的是向上取整操作，ceil(75/2)=38；</li><li>将原始pool5（2x2,stride 2）修改为pool5（3x3,stride 1），这样改的结果是，pool5操作不改变特征图大小；</li><li>将VGG16中的fc6,fc7替换成卷积层conv6,和conv7，此外，conv6使用了atrous卷积；</li><li>去掉了所有的dropout层和fc8层；</li><li>VGGBase输出的是conv4_3和conv_7的feature map；</li></ul><p>Atrous/dilated 卷积使用说明：<br>通常卷积过程中为了使特征图尺寸特征图尺寸保持不变，通过会在边缘打padding，但人为加入的padding值会引入噪声，因此，使用atrous卷积能够在保持感受野不变的条件下，减少padding噪声。</p><h3 id="Auxiliary-Convolutions"><a href="#Auxiliary-Convolutions" class="headerlink" title="Auxiliary Convolutions"></a>Auxiliary Convolutions</h3><p>为了能够充分利用不同大小的feature map用于目标检测，作者在Base Convolutions的基础上，在conv_7后面又堆叠了8层卷积用于生成不同大小的特征图。如下图所示<br><img src="/img/auxconv.jpg" alt><br>对应代码实现：<br><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AuxiliaryConvolutions</span>(<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    Additional convolutions to produce higher-level feature maps.</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span></span>(<span class="keyword">self</span>):</span><br><span class="line">        <span class="keyword">super</span>(AuxiliaryConvolutions, <span class="keyword">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Auxiliary/additional convolutions on top of the VGG base</span></span><br><span class="line">        <span class="keyword">self</span>.conv8_1 = nn.Conv2d(<span class="number">1024</span>, <span class="number">256</span>, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>)  <span class="comment"># stride = 1, by default</span></span><br><span class="line">        <span class="keyword">self</span>.conv8_2 = nn.Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)  <span class="comment"># dim. reduction because stride &gt; 1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.conv9_1 = nn.Conv2d(<span class="number">512</span>, <span class="number">128</span>, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv9_2 = nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)  <span class="comment"># dim. reduction because stride &gt; 1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.conv10_1 = nn.Conv2d(<span class="number">256</span>, <span class="number">128</span>, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv10_2 = nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">0</span>)  <span class="comment"># dim. reduction because padding = 0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.conv11_1 = nn.Conv2d(<span class="number">256</span>, <span class="number">128</span>, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv11_2 = nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">0</span>)  <span class="comment"># dim. reduction because padding = 0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize convolutions' parameters</span></span><br><span class="line">        <span class="keyword">self</span>.init_conv2d()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_conv2d</span></span>(<span class="keyword">self</span>):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        Initialize convolution parameters.</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        <span class="keyword">for</span> c in <span class="keyword">self</span>.children():</span><br><span class="line">            <span class="keyword">if</span> isinstance(c, nn.Conv2d):</span><br><span class="line">                nn.init.xavier_uniform_(c.weight)</span><br><span class="line">                nn.init.constant_(c.bias, <span class="number">0</span>.)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span></span>(<span class="keyword">self</span>, conv7_feats):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        Forward propagation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param conv7_feats: lower-level conv7 feature map, a tensor of dimensions (N, 1024, 19, 19)</span></span><br><span class="line"><span class="string">        :return: higher-level feature maps conv8_2, conv9_2, conv10_2, and conv11_2</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv8_1(conv7_feats))  <span class="comment"># (N, 256, 19, 19)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv8_2(<span class="keyword">out</span>))  <span class="comment"># (N, 512, 10, 10)</span></span><br><span class="line">        conv8_2_feats = <span class="keyword">out</span>  <span class="comment"># (N, 512, 10, 10)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv9_1(<span class="keyword">out</span>))  <span class="comment"># (N, 128, 10, 10)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv9_2(<span class="keyword">out</span>))  <span class="comment"># (N, 256, 5, 5)</span></span><br><span class="line">        conv9_2_feats = <span class="keyword">out</span>  <span class="comment"># (N, 256, 5, 5)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv10_1(<span class="keyword">out</span>))  <span class="comment"># (N, 128, 5, 5)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv10_2(<span class="keyword">out</span>))  <span class="comment"># (N, 256, 3, 3)</span></span><br><span class="line">        conv10_2_feats = <span class="keyword">out</span>  <span class="comment"># (N, 256, 3, 3)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv11_1(<span class="keyword">out</span>))  <span class="comment"># (N, 128, 3, 3)</span></span><br><span class="line">        conv11_2_feats = F.relu(<span class="keyword">self</span>.conv11_2(<span class="keyword">out</span>))  <span class="comment"># (N, 256, 1, 1)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Higher-level feature maps</span></span><br><span class="line">        <span class="keyword">return</span> conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats</span><br></pre></td></tr></table></figure></p><p>如SSD论文在section 3.1提到的一样，新添加的卷积层的初始化采用的xavier初始化方法。AuxiliaryConvolutions输出的是不同大小的feature map：conv8_2, conv9_2, conv10_2, and conv11_2。对于SSD512，作者在conv11_2后面来额外添加了两层conv12。<br>SSD算法中使用到了conv4_3,conv_7，conv8_2,conv7_2,conv8_2,conv9_2,conv10_2,conv11_2这些大小不同的feature maps，其<strong>目的是为了能够准确的检测到不同尺度的物体</strong>，因为在低层的feature map,感受野比较小，高层的感受野比较大，在不同的feature map进行卷积，可以达到多尺度的目的。</p><h3 id="Priors"><a href="#Priors" class="headerlink" title="Priors"></a>Priors</h3><p>在继续进行预测卷积之前，我们必须首先了解我们预测的是什么。当然，这是物体和它们的位置，但是以什么形式？在这里，我们必须了解priors以及它们在SSD中所起的关键作用。priors和anchor的概念类似，也是预先在feature map上定义一些不同大小形状的box，然后再这个基础上进行回归预测分类目标。如下图所示<br><img src="/img/defaultbox.png" alt><br>如上图所示，在特征图的每个位置预测K个box，对于每一个box，预测C个类别得分，以及相对于prior box的4个偏移量值，这样总共需要（C+4）<em> K个预测器，则在m</em>n的特征图上面将会产生（C+4）<em> K </em> m * n个预测值。在SSD300中，分别在6个不同尺度的feature map上生成prior box。对应prior box如下：          </p><div class="table-container"><table><thead><tr><th style="text-align:center">Feature Map From</th><th style="text-align:center">Feature Map Dimensions</th><th style="text-align:center">Prior Scale</th><th style="text-align:center">Aspect Ratios</th><th style="text-align:center">Number of Priors per Position</th><th style="text-align:center">Total Number of Priors on this Feature Map</th></tr></thead><tbody><tr><td style="text-align:center"><code>conv4_3</code></td><td style="text-align:center">38, 38</td><td style="text-align:center">0.1</td><td style="text-align:center">1:1, 2:1, 1:2 + an extra prior</td><td style="text-align:center">4</td><td style="text-align:center">38x38x4=5776</td></tr><tr><td style="text-align:center"><code>conv7</code></td><td style="text-align:center">19, 19</td><td style="text-align:center">0.2</td><td style="text-align:center">1:1, 2:1, 1:2, 3:1, 1:3 + an extra prior</td><td style="text-align:center">6</td><td style="text-align:center">19x19x6=2166</td></tr><tr><td style="text-align:center"><code>conv8_2</code></td><td style="text-align:center">10, 10</td><td style="text-align:center">0.375</td><td style="text-align:center">1:1, 2:1, 1:2, 3:1, 1:3 + an extra prior</td><td style="text-align:center">6</td><td style="text-align:center">10x10x6=600</td></tr><tr><td style="text-align:center"><code>conv9_2</code></td><td style="text-align:center">5, 5</td><td style="text-align:center">0.55</td><td style="text-align:center">1:1, 2:1, 1:2, 3:1, 1:3 + an extra prior</td><td style="text-align:center">6</td><td style="text-align:center">5x5x6=150</td></tr><tr><td style="text-align:center"><code>conv10_2</code></td><td style="text-align:center">3,  3</td><td style="text-align:center">0.725</td><td style="text-align:center">1:1, 2:1, 1:2 + an extra prior</td><td style="text-align:center">4</td><td style="text-align:center">3x3x4=36</td></tr><tr><td style="text-align:center"><code>conv11_2</code></td><td style="text-align:center">1, 1</td><td style="text-align:center">0.9</td><td style="text-align:center">1:1, 2:1, 1:2 + an extra prior</td><td style="text-align:center">4</td><td style="text-align:center">1x1x4=4</td></tr><tr><td style="text-align:center"><strong>Grand Total</strong></td><td style="text-align:center">–</td><td style="text-align:center">–</td><td style="text-align:center">–</td><td style="text-align:center">–</td><td style="text-align:center"><strong>8732 priors</strong></td></tr></tbody></table></div><p>Prior Scale计算公式如下：<br><img src="/img/ssd_scale.png" alt><br>其中，$s_{min}=0.2,s_{max}=0.9$，conv4_3设置为0.1。<br>值得注意的是，对于aspect ratio为1的时候，作者额外添加了一个extra prior，对应的additional_scale：</p><script type="math/tex; mode=display">\acute{s}_k=\sqrt{s_k\times s_{k+1}}</script><p>因此在SSD300中总共有9732个prior boxes ！<br>对于每一个priorbox，对应宽度w和高度h的计算如下：<br><img src="/img/wh1.jpg" alt><br>求解w和h可得：<br><img src="/img/wh2.jpg" alt><br>然后以每个feature map上的每个点的中心点为中心，就可以生成一系列不同scale和aspect ratio的prior box。对应代码实现如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_prior_boxes</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Create the 8732 prior (default) boxes for the SSD300, as defined in the paper.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return: prior boxes in center-size coordinates, a tensor of dimensions (8732, 4)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    fmap_dims = &#123;<span class="string">'conv4_3'</span>: <span class="number">38</span>,</span><br><span class="line">                 <span class="string">'conv7'</span>: <span class="number">19</span>,</span><br><span class="line">                 <span class="string">'conv8_2'</span>: <span class="number">10</span>,</span><br><span class="line">                 <span class="string">'conv9_2'</span>: <span class="number">5</span>,</span><br><span class="line">                 <span class="string">'conv10_2'</span>: <span class="number">3</span>,</span><br><span class="line">                 <span class="string">'conv11_2'</span>: <span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">    obj_scales = &#123;<span class="string">'conv4_3'</span>: <span class="number">0.1</span>,</span><br><span class="line">                      <span class="string">'conv7'</span>: <span class="number">0.2</span>,</span><br><span class="line">                      <span class="string">'conv8_2'</span>: <span class="number">0.375</span>,</span><br><span class="line">                      <span class="string">'conv9_2'</span>: <span class="number">0.55</span>,</span><br><span class="line">                      <span class="string">'conv10_2'</span>: <span class="number">0.725</span>,</span><br><span class="line">                      <span class="string">'conv11_2'</span>: <span class="number">0.9</span>&#125;</span><br><span class="line"></span><br><span class="line">    aspect_ratios = &#123;<span class="string">'conv4_3'</span>: [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">0.5</span>],</span><br><span class="line">                         <span class="string">'conv7'</span>: [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">0.5</span>, <span class="number">.333</span>],</span><br><span class="line">                         <span class="string">'conv8_2'</span>: [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">0.5</span>, <span class="number">.333</span>],</span><br><span class="line">                         <span class="string">'conv9_2'</span>: [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">0.5</span>, <span class="number">.333</span>],</span><br><span class="line">                         <span class="string">'conv10_2'</span>: [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">0.5</span>],</span><br><span class="line">                         <span class="string">'conv11_2'</span>: [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">0.5</span>]&#125;</span><br><span class="line"></span><br><span class="line">    fmaps = list(fmap_dims.keys())</span><br><span class="line"></span><br><span class="line">    prior_boxes = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> k, fmap <span class="keyword">in</span> enumerate(fmaps):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(fmap_dims[fmap]):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(fmap_dims[fmap]):</span><br><span class="line">                cx = (j + <span class="number">0.5</span>) / fmap_dims[fmap]</span><br><span class="line">                cy = (i + <span class="number">0.5</span>) / fmap_dims[fmap]</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> ratio <span class="keyword">in</span> aspect_ratios[fmap]:</span><br><span class="line">                    prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] / sqrt(ratio)])</span><br><span class="line"></span><br><span class="line">                        <span class="comment"># For an aspect ratio of 1, use an additional prior whose scale is the geometric mean of the</span></span><br><span class="line">                        <span class="comment"># scale of the current feature map and the scale of the next feature map</span></span><br><span class="line">                    <span class="keyword">if</span> ratio == <span class="number">1.</span>:</span><br><span class="line">                        <span class="keyword">try</span>:</span><br><span class="line">                            additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k + <span class="number">1</span>]])</span><br><span class="line">                            <span class="comment"># For the last feature map, there is no "next" feature map</span></span><br><span class="line">                        <span class="keyword">except</span> IndexError:</span><br><span class="line">                            additional_scale = <span class="number">1.</span></span><br><span class="line">                        prior_boxes.append([cx, cy, additional_scale, additional_scale])</span><br><span class="line"></span><br><span class="line">    prior_boxes = torch.FloatTensor(prior_boxes).to(device)  <span class="comment"># (8732, 4)</span></span><br><span class="line">    prior_boxes.clamp_(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># (8732, 4)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> prior_boxes</span><br></pre></td></tr></table></figure></p><h3 id="Prediction-convolutions"><a href="#Prediction-convolutions" class="headerlink" title="Prediction convolutions"></a>Prediction convolutions</h3><p>前面提及我们使用回归预测来找到目标的bounding box，当然，priors并不能表示我们最终的预测boxes。不过可以priors作为回归的起始框，然后找出需要调整多少才能获得更精确的边界框预测。而我们的目标就是预测这个偏差offsets,如下图示：<br><img src="/img/ecs2.png" alt><br>通过预测这四个偏差(g_c_x, g_c_y, g_w, g_h)来回归出bounding box的坐标位置。除了预测offsets之外，我们还需要输出对应目标是哪一类的分数scores。以conv9_2为例，对应输出为：<br><img src="/img/predconv2.jpg" alt><br>对应6个不同尺度feature map上的输出同样也输出对应的offset和scores，具体代码实现如下：<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">PredictionConvolutions</span>(<span class="title">nn</span>.<span class="type">Module</span>):</span></span><br><span class="line"><span class="class">    """</span></span><br><span class="line"><span class="class">    <span class="type">Convolutions</span> to predict <span class="keyword">class</span> scores and bounding boxes using lower and higher-level feature maps.</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    <span class="type">The</span> bounding boxes (<span class="title">locations</span>) are predicted as encoded offsets w.r.t each of the 8732 prior (<span class="title">default</span>) boxes.</span></span><br><span class="line"><span class="class">    <span class="type">See</span> 'cxcy_to_gcxgcy' in utils.py for the encoding definition.</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    <span class="type">The</span> <span class="keyword">class</span> scores represent the scores of each object <span class="keyword">class</span> in each of the 8732 bounding boxes located.</span></span><br><span class="line"><span class="class">    <span class="type">A</span> high score for 'background' = no object.</span></span><br><span class="line"><span class="class">    """</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>, <span class="title">n_classes</span>):</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        :param n_classes: number of different types of objects</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        super(<span class="type">PredictionConvolutions</span>, <span class="title">self</span>).__init__()</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        self.n_classes = n_classes</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        # <span class="type">Number</span> of prior-boxes we are considering per position in each feature map</span></span><br><span class="line"><span class="class">        n_boxes = &#123;'conv4_3': 4,</span></span><br><span class="line"><span class="class">                   'conv7': 6,</span></span><br><span class="line"><span class="class">                   'conv8_2': 6,</span></span><br><span class="line"><span class="class">                   'conv9_2': 6,</span></span><br><span class="line"><span class="class">                   'conv10_2': 4,</span></span><br><span class="line"><span class="class">                   'conv11_2': 4&#125;</span></span><br><span class="line"><span class="class">        # 4 prior-boxes implies we use 4 different aspect ratios, etc.</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        # <span class="type">Localization</span> prediction convolutions (<span class="title">predict</span> <span class="title">offsets</span> <span class="title">w</span>.<span class="title">r</span>.<span class="title">t</span> <span class="title">prior</span>-<span class="title">boxes</span>)</span></span><br><span class="line"><span class="class">        self.loc_conv4_3 = nn.<span class="type">Conv2d</span>(512, <span class="title">n_boxes</span>['<span class="title">conv4_3'</span>] * 4, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.loc_conv7 = nn.<span class="type">Conv2d</span>(1024, <span class="title">n_boxes</span>['<span class="title">conv7'</span>] * 4, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.loc_conv8_2 = nn.<span class="type">Conv2d</span>(512, <span class="title">n_boxes</span>['<span class="title">conv8_2'</span>] * 4, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.loc_conv9_2 = nn.<span class="type">Conv2d</span>(256, <span class="title">n_boxes</span>['<span class="title">conv9_2'</span>] * 4, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.loc_conv10_2 = nn.<span class="type">Conv2d</span>(256, <span class="title">n_boxes</span>['<span class="title">conv10_2'</span>] * 4, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.loc_conv11_2 = nn.<span class="type">Conv2d</span>(256, <span class="title">n_boxes</span>['<span class="title">conv11_2'</span>] * 4, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        # <span class="type">Class</span> prediction convolutions (<span class="title">predict</span> <span class="title">classes</span> <span class="title">in</span> <span class="title">localization</span> <span class="title">boxes</span>)</span></span><br><span class="line"><span class="class">        self.cl_conv4_3 = nn.<span class="type">Conv2d</span>(512, <span class="title">n_boxes</span>['<span class="title">conv4_3'</span>] * <span class="title">n_classes</span>, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.cl_conv7 = nn.<span class="type">Conv2d</span>(1024, <span class="title">n_boxes</span>['<span class="title">conv7'</span>] * <span class="title">n_classes</span>, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.cl_conv8_2 = nn.<span class="type">Conv2d</span>(512, <span class="title">n_boxes</span>['<span class="title">conv8_2'</span>] * <span class="title">n_classes</span>, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.cl_conv9_2 = nn.<span class="type">Conv2d</span>(256, <span class="title">n_boxes</span>['<span class="title">conv9_2'</span>] * <span class="title">n_classes</span>, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.cl_conv10_2 = nn.<span class="type">Conv2d</span>(256, <span class="title">n_boxes</span>['<span class="title">conv10_2'</span>] * <span class="title">n_classes</span>, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.cl_conv11_2 = nn.<span class="type">Conv2d</span>(256, <span class="title">n_boxes</span>['<span class="title">conv11_2'</span>] * <span class="title">n_classes</span>, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        # <span class="type">Initialize</span> convolutions' parameters</span></span><br><span class="line"><span class="class">        self.init_conv2d()</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def init_conv2d(<span class="title">self</span>):</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        <span class="type">Initialize</span> convolution parameters.</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        for c in self.children():</span></span><br><span class="line"><span class="class">            if isinstance(<span class="title">c</span>, <span class="title">nn</span>.<span class="type">Conv2d</span>):</span></span><br><span class="line"><span class="class">                nn.init.xavier_uniform_(<span class="title">c</span>.<span class="title">weight</span>)</span></span><br><span class="line"><span class="class">                nn.init.constant_(<span class="title">c</span>.<span class="title">bias</span>, 0.)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>, <span class="title">conv4_3_feats</span>, <span class="title">conv7_feats</span>, <span class="title">conv8_2_feats</span>, <span class="title">conv9_2_feats</span>, <span class="title">conv10_2_feats</span>, <span class="title">conv11_2_feats</span>):</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        <span class="type">Forward</span> propagation.</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        :param conv4_3_feats: conv4_3 feature map, a tensor of dimensions (<span class="type">N</span>, 512, 38, 38)</span></span><br><span class="line"><span class="class">        :param conv7_feats: conv7 feature map, a tensor of dimensions (<span class="type">N</span>, 1024, 19, 19)</span></span><br><span class="line"><span class="class">        :param conv8_2_feats: conv8_2 feature map, a tensor of dimensions (<span class="type">N</span>, 512, 10, 10)</span></span><br><span class="line"><span class="class">        :param conv9_2_feats: conv9_2 feature map, a tensor of dimensions (<span class="type">N</span>, 256, 5, 5)</span></span><br><span class="line"><span class="class">        :param conv10_2_feats: conv10_2 feature map, a tensor of dimensions (<span class="type">N</span>, 256, 3, 3)</span></span><br><span class="line"><span class="class">        :param conv11_2_feats: conv11_2 feature map, a tensor of dimensions (<span class="type">N</span>, 256, 1, 1)</span></span><br><span class="line"><span class="class">        :return: 8732 locations and <span class="keyword">class</span> scores (<span class="title">i</span>.<span class="title">e</span>. <span class="title">w</span>.<span class="title">r</span>.<span class="title">t</span> <span class="title">each</span> <span class="title">prior</span> <span class="title">box</span>) for each image</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        batch_size = conv4_3_feats.size(0)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        # <span class="type">Predict</span> localization boxes' bounds (<span class="title">as</span> <span class="title">offsets</span> <span class="title">w</span>.<span class="title">r</span>.<span class="title">t</span> <span class="title">prior</span>-<span class="title">boxes</span>)</span></span><br><span class="line"><span class="class">        l_conv4_3 = self.loc_conv4_3(<span class="title">conv4_3_feats</span>)  # (<span class="type">N</span>, 16, 38, 38)</span></span><br><span class="line"><span class="class">        l_conv4_3 = l_conv4_3.permute(0, 2, 3,</span></span><br><span class="line"><span class="class">                                      1).contiguous()  # (<span class="type">N</span>, 38, 38, 16), to match prior-box order (<span class="title">after</span> .<span class="title">view</span>())</span></span><br><span class="line"><span class="class">        # (.<span class="title">contiguous</span>() ensures it is stored in a contiguous chunk of memory, needed for .view() below)</span></span><br><span class="line"><span class="class">        l_conv4_3 = l_conv4_3.view(<span class="title">batch_size</span>, -1, 4)  # (<span class="type">N</span>, 5776, 4), there are a total 5776 boxes on this feature map</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        l_conv7 = self.loc_conv7(<span class="title">conv7_feats</span>)  # (<span class="type">N</span>, 24, 19, 19)</span></span><br><span class="line"><span class="class">        l_conv7 = l_conv7.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 19, 19, 24)</span></span><br><span class="line"><span class="class">        l_conv7 = l_conv7.view(<span class="title">batch_size</span>, -1, 4)  # (<span class="type">N</span>, 2166, 4), there are a total 2116 boxes on this feature map</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        l_conv8_2 = self.loc_conv8_2(<span class="title">conv8_2_feats</span>)  # (<span class="type">N</span>, 24, 10, 10)</span></span><br><span class="line"><span class="class">        l_conv8_2 = l_conv8_2.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 10, 10, 24)</span></span><br><span class="line"><span class="class">        l_conv8_2 = l_conv8_2.view(<span class="title">batch_size</span>, -1, 4)  # (<span class="type">N</span>, 600, 4)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        l_conv9_2 = self.loc_conv9_2(<span class="title">conv9_2_feats</span>)  # (<span class="type">N</span>, 24, 5, 5)</span></span><br><span class="line"><span class="class">        l_conv9_2 = l_conv9_2.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 5, 5, 24)</span></span><br><span class="line"><span class="class">        l_conv9_2 = l_conv9_2.view(<span class="title">batch_size</span>, -1, 4)  # (<span class="type">N</span>, 150, 4)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        l_conv10_2 = self.loc_conv10_2(<span class="title">conv10_2_feats</span>)  # (<span class="type">N</span>, 16, 3, 3)</span></span><br><span class="line"><span class="class">        l_conv10_2 = l_conv10_2.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 3, 3, 16)</span></span><br><span class="line"><span class="class">        l_conv10_2 = l_conv10_2.view(<span class="title">batch_size</span>, -1, 4)  # (<span class="type">N</span>, 36, 4)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        l_conv11_2 = self.loc_conv11_2(<span class="title">conv11_2_feats</span>)  # (<span class="type">N</span>, 16, 1, 1)</span></span><br><span class="line"><span class="class">        l_conv11_2 = l_conv11_2.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 1, 1, 16)</span></span><br><span class="line"><span class="class">        l_conv11_2 = l_conv11_2.view(<span class="title">batch_size</span>, -1, 4)  # (<span class="type">N</span>, 4, 4)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        # <span class="type">Predict</span> classes in localization boxes</span></span><br><span class="line"><span class="class">        c_conv4_3 = self.cl_conv4_3(<span class="title">conv4_3_feats</span>)  # (<span class="type">N</span>, 4 * <span class="title">n_classes</span>, 38, 38)</span></span><br><span class="line"><span class="class">        c_conv4_3 = c_conv4_3.permute(0, 2, 3,</span></span><br><span class="line"><span class="class">                                      1).contiguous()  # (<span class="type">N</span>, 38, 38, 4 * <span class="title">n_classes</span>), to match prior-box order (<span class="title">after</span> .<span class="title">view</span>())</span></span><br><span class="line"><span class="class">        c_conv4_3 = c_conv4_3.view(<span class="title">batch_size</span>, -1,</span></span><br><span class="line"><span class="class">                                   <span class="title">self</span>.<span class="title">n_classes</span>)  # (<span class="type">N</span>, 5776, <span class="title">n_classes</span>), there are a total 5776 boxes on this feature map</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        c_conv7 = self.cl_conv7(<span class="title">conv7_feats</span>)  # (<span class="type">N</span>, 6 * <span class="title">n_classes</span>, 19, 19)</span></span><br><span class="line"><span class="class">        c_conv7 = c_conv7.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 19, 19, 6 * <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class">        c_conv7 = c_conv7.view(<span class="title">batch_size</span>, -1,</span></span><br><span class="line"><span class="class">                               <span class="title">self</span>.<span class="title">n_classes</span>)  # (<span class="type">N</span>, 2166, <span class="title">n_classes</span>), there are a total 2116 boxes on this feature map</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        c_conv8_2 = self.cl_conv8_2(<span class="title">conv8_2_feats</span>)  # (<span class="type">N</span>, 6 * <span class="title">n_classes</span>, 10, 10)</span></span><br><span class="line"><span class="class">        c_conv8_2 = c_conv8_2.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 10, 10, 6 * <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class">        c_conv8_2 = c_conv8_2.view(<span class="title">batch_size</span>, -1, <span class="title">self</span>.<span class="title">n_classes</span>)  # (<span class="type">N</span>, 600, <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        c_conv9_2 = self.cl_conv9_2(<span class="title">conv9_2_feats</span>)  # (<span class="type">N</span>, 6 * <span class="title">n_classes</span>, 5, 5)</span></span><br><span class="line"><span class="class">        c_conv9_2 = c_conv9_2.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 5, 5, 6 * <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class">        c_conv9_2 = c_conv9_2.view(<span class="title">batch_size</span>, -1, <span class="title">self</span>.<span class="title">n_classes</span>)  # (<span class="type">N</span>, 150, <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        c_conv10_2 = self.cl_conv10_2(<span class="title">conv10_2_feats</span>)  # (<span class="type">N</span>, 4 * <span class="title">n_classes</span>, 3, 3)</span></span><br><span class="line"><span class="class">        c_conv10_2 = c_conv10_2.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 3, 3, 4 * <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class">        c_conv10_2 = c_conv10_2.view(<span class="title">batch_size</span>, -1, <span class="title">self</span>.<span class="title">n_classes</span>)  # (<span class="type">N</span>, 36, <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        c_conv11_2 = self.cl_conv11_2(<span class="title">conv11_2_feats</span>)  # (<span class="type">N</span>, 4 * <span class="title">n_classes</span>, 1, 1)</span></span><br><span class="line"><span class="class">        c_conv11_2 = c_conv11_2.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 1, 1, 4 * <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class">        c_conv11_2 = c_conv11_2.view(<span class="title">batch_size</span>, -1, <span class="title">self</span>.<span class="title">n_classes</span>)  # (<span class="type">N</span>, 4, <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        # <span class="type">A</span> total of 8732 boxes</span></span><br><span class="line"><span class="class">        # <span class="type">Concatenate</span> in this specific order (<span class="title">i</span>.<span class="title">e</span>. <span class="title">must</span> <span class="title">match</span> <span class="title">the</span> <span class="title">order</span> <span class="title">of</span> <span class="title">the</span> <span class="title">prior</span>-<span class="title">boxes</span>)</span></span><br><span class="line"><span class="class">        locs = torch.cat([<span class="title">l_conv4_3</span>, <span class="title">l_conv7</span>, <span class="title">l_conv8_2</span>, <span class="title">l_conv9_2</span>, <span class="title">l_conv10_2</span>, <span class="title">l_conv11_2</span>], <span class="title">dim</span>=1)  # (<span class="type">N</span>, 8732, 4)</span></span><br><span class="line"><span class="class">        classes_scores = torch.cat([<span class="title">c_conv4_3</span>, <span class="title">c_conv7</span>, <span class="title">c_conv8_2</span>, <span class="title">c_conv9_2</span>, <span class="title">c_conv10_2</span>, <span class="title">c_conv11_2</span>],</span></span><br><span class="line"><span class="class">                                   <span class="title">dim</span>=1)  # (<span class="type">N</span>, 8732, <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        return locs, classes_scores</span></span><br></pre></td></tr></table></figure></p><h3 id="Multibox-loss"><a href="#Multibox-loss" class="headerlink" title="Multibox loss"></a>Multibox loss</h3><p>与常见的 Object Detection模型的目标函数相同，SSD算法的目标函数分为两部分：计算相应的prior box与目标类别的confidence loss以及相应的位置回归Localization loss。<br><img src="/img/multibox_loss.png" alt><br>loss的计算过程如下：</p><h4 id="Localization-loss"><a href="#Localization-loss" class="headerlink" title="Localization loss"></a>Localization loss</h4><ul><li>计算8732个prior box与groud truth的IOU重叠值，保留重合度大于0.5的所有priors，对应的prior也成为positive prior；在论文中换了个名词Jaccard overlaps，其实和IOU是一个意思;</li><li>由于网络输出的是相对于priors的预测偏差(g_c_x, g_c_y, g_w, g_h)，因此我们需要将positive prior转换为绝对坐标位置，然后与groud truth计算smooth l1 loss;</li><li>smooth l1 loss损失计算如下：<br><img src="/img/locloss.jpg" alt></li></ul><h4 id="Confidence-loss"><a href="#Confidence-loss" class="headerlink" title="Confidence loss"></a>Confidence loss</h4><p>在计算Localization loss中，只计算了positive prior的损失，没有计算negative prior的损失是因为标签中只有positive prior对应的gt。而在计算Confidence loss的不一样，每一次预测，不管正样本和是负样本都存在一个与之对应的标签。注意，这里的正负样本都是基于positive prior，iou&gt;0.5的prior box，正确分类为目标类别的为正样本，否则为负样本。在SSD中使用了hard negative mining。这是因为在positive prior中，分类正样本和负样本存在大量不平衡问题，因此使用hard negative mining来缓解这个问题。在这里，正样本与负样本的数量比例为1:3，负样本选择分类置信度靠前的。Confidence loss使用的交叉熵损失函数，如下：<br><img src="/img/confloss.jpg" alt></p><p>在SSD中，最后整个算法模型的损失函数为：<br><img src="/img/totalloss.jpg" alt><br>具体代码实现如下：<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">class MultiBoxLoss(nn.Module):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    The MultiBox loss, a loss function for object detection.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This is a combination of:</span></span><br><span class="line"><span class="string">    (1) a localization loss for the predicted locations of the boxes, and</span></span><br><span class="line"><span class="string">    (2) a confidence loss for the predicted class scores.</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line">    def __init__(self, priors_cxcy, <span class="attr">threshold=0.5,</span> <span class="attr">neg_pos_ratio=3,</span> <span class="attr">alpha=1.):</span></span><br><span class="line">        super(MultiBoxLoss, self).__init__()</span><br><span class="line">        self.<span class="attr">priors_cxcy</span> = priors_cxcy  <span class="comment">#priors_cxcy即上面create_prior_boxes（）生成的8732 priorbox</span></span><br><span class="line">        print(self.priors_cxcy[<span class="number">0</span>])</span><br><span class="line">        self.<span class="attr">priors_xy</span> = cxcy_to_xy(priors_cxcy)</span><br><span class="line">        self.<span class="attr">threshold</span> = threshold</span><br><span class="line">        self.<span class="attr">neg_pos_ratio</span> = neg_pos_ratio</span><br><span class="line">        self.<span class="attr">alpha</span> = alpha</span><br><span class="line"></span><br><span class="line">        self.<span class="attr">smooth_l1</span> = nn.L1Loss()</span><br><span class="line">        self.<span class="attr">cross_entropy</span> = nn.CrossEntropyLoss(<span class="attr">reduce=False)</span></span><br><span class="line"></span><br><span class="line">    def forward(self, predicted_locs, predicted_scores, boxes, labels):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        Forward propagation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)</span></span><br><span class="line"><span class="string">        :param predicted_scores: class scores for each of the encoded locations/boxes, a tensor of dimensions (N, 8732, n_classes)</span></span><br><span class="line"><span class="string">        :param boxes: true  object bounding boxes in boundary coordinates, a list of N tensors</span></span><br><span class="line"><span class="string">        :param labels: true object labels, a list of N tensors</span></span><br><span class="line"><span class="string">        :return: multibox loss, a scalar</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        <span class="attr">batch_size</span> = predicted_locs.size(<span class="number">0</span>)</span><br><span class="line">        <span class="attr">n_priors</span> = self.priors_cxcy.size(<span class="number">0</span>)</span><br><span class="line">        print (self.priors_xy[<span class="number">0</span>:<span class="number">2</span>])</span><br><span class="line">        <span class="attr">n_classes</span> = predicted_scores.size(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> <span class="attr">n_priors</span> == predicted_locs.size(<span class="number">1</span>) == predicted_scores.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="attr">true_locs</span> = torch.zeros((batch_size, n_priors, <span class="number">4</span>), <span class="attr">dtype=torch.float).to(device)</span>  <span class="comment"># (N, 8732, 4)</span></span><br><span class="line">        <span class="attr">true_classes</span> = torch.zeros((batch_size, n_priors), <span class="attr">dtype=torch.long).to(device)</span>  <span class="comment"># (N, 8732)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># For each image</span></span><br><span class="line">        for i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">            <span class="attr">n_objects</span> = boxes[i].size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            <span class="attr">overlap</span> = find_jaccard_overlap(boxes[i],</span><br><span class="line">                                           self.priors_xy)  <span class="comment"># (n_objects, 8732)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># For each prior, find the object that has the maximum overlap</span></span><br><span class="line">            overlap_for_each_prior, <span class="attr">object_for_each_prior</span> = overlap.max(<span class="attr">dim=0)</span>  <span class="comment"># (8732)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># We don't want a situation where an object is not represented in our positive (non-background) priors -</span></span><br><span class="line">            <span class="comment"># 1. An object might not be the best object for all priors, and is therefore not in object_for_each_prior.</span></span><br><span class="line">            <span class="comment"># 2. All priors with the object may be assigned as background based on the threshold (0.5).</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># To remedy this -</span></span><br><span class="line">            <span class="comment"># First, find the prior that has the maximum overlap for each object.</span></span><br><span class="line">            _, <span class="attr">prior_for_each_object</span> = overlap.max(<span class="attr">dim=1)</span>  <span class="comment"># (N_o)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Then, assign each object to the corresponding maximum-overlap-prior. (This fixes 1.)</span></span><br><span class="line">            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># To ensure these priors qualify, artificially give them an overlap of greater than 0.5. (This fixes 2.)</span></span><br><span class="line">            overlap_for_each_prior[prior_for_each_object] = <span class="number">1</span>.</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Labels for each prior</span></span><br><span class="line">            <span class="attr">label_for_each_prior</span> = labels[i][object_for_each_prior]  <span class="comment"># (8732)</span></span><br><span class="line">            <span class="comment"># Set priors whose overlaps with objects are less than the threshold to be background (no object)</span></span><br><span class="line">            label_for_each_prior[overlap_for_each_prior &lt; self.threshold] = <span class="number">0</span>  <span class="comment"># (8732)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Store</span></span><br><span class="line">            true_classes[i] = label_for_each_prior</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Encode center-size object coordinates into the form we regressed predicted boxes to</span></span><br><span class="line">            true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy)  <span class="comment"># (8732, 4)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Identify priors that are positive (object/non-background)</span></span><br><span class="line">        <span class="attr">positive_priors</span> = true_classes != <span class="number">0</span>  <span class="comment"># (N, 8732)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># LOCALIZATION LOSS</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Localization loss is computed only over positive (non-background) priors</span></span><br><span class="line">        <span class="attr">loc_loss</span> = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])  <span class="comment"># (), scalar</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># <span class="doctag">Note:</span> indexing with a torch.uint8 (byte) tensor flattens the tensor when indexing is across multiple dimensions (N &amp; 8732)</span></span><br><span class="line">        <span class="comment"># So, if predicted_locs has the shape (N, 8732, 4), predicted_locs[positive_priors] will have (total positives, 4)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># CONFIDENCE LOSS</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Confidence loss is computed over positive priors and the most difficult (hardest) negative priors in each image</span></span><br><span class="line">        <span class="comment"># That is, FOR EACH IMAGE,</span></span><br><span class="line">        <span class="comment"># we will take the hardest (neg_pos_ratio * n_positives) negative priors, i.e where there is maximum loss</span></span><br><span class="line">        <span class="comment"># This is called Hard Negative Mining - it concentrates on hardest negatives in each image, and also minimizes pos/neg imbalance</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Number of positive and hard-negative priors per image</span></span><br><span class="line">        <span class="attr">n_positives</span> = positive_priors.sum(<span class="attr">dim=1)</span>  <span class="comment"># (N)</span></span><br><span class="line">        <span class="attr">n_hard_negatives</span> = self.neg_pos_ratio * n_positives  <span class="comment"># (N)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># First, find the loss for all priors</span></span><br><span class="line">        <span class="attr">conf_loss_all</span> = self.cross_entropy(predicted_scores.view(-<span class="number">1</span>, n_classes), true_classes.view(-<span class="number">1</span>))  <span class="comment"># (N * 8732)</span></span><br><span class="line">        <span class="attr">conf_loss_all</span> = conf_loss_all.view(batch_size, n_priors)  <span class="comment"># (N, 8732)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># We already know which priors are positive</span></span><br><span class="line">        <span class="attr">conf_loss_pos</span> = conf_loss_all[positive_priors]  <span class="comment"># (sum(n_positives))</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Next, find which priors are hard-negative</span></span><br><span class="line">        <span class="comment"># To do this, sort ONLY negative priors in each image in order of decreasing loss and take top n_hard_negatives</span></span><br><span class="line">        <span class="attr">conf_loss_neg</span> = conf_loss_all.clone()  <span class="comment"># (N, 8732)</span></span><br><span class="line">        conf_loss_neg[positive_priors] = <span class="number">0</span>.  <span class="comment"># (N, 8732), positive priors are ignored (never in top n_hard_negatives)</span></span><br><span class="line">        conf_loss_neg, <span class="attr">_</span> = conf_loss_neg.sort(<span class="attr">dim=1,</span> <span class="attr">descending=True)</span>  <span class="comment"># (N, 8732), sorted by decreasing hardness</span></span><br><span class="line">        <span class="attr">hardness_ranks</span> = torch.LongTensor(range(n_priors)).unsqueeze(<span class="number">0</span>).expand_as(conf_loss_neg).to(device)  <span class="comment"># (N, 8732)</span></span><br><span class="line">        <span class="attr">hard_negatives</span> = hardness_ranks &lt; n_hard_negatives.unsqueeze(<span class="number">1</span>)  <span class="comment"># (N, 8732)</span></span><br><span class="line">        <span class="attr">conf_loss_hard_neg</span> = conf_loss_neg[hard_negatives]  <span class="comment"># (sum(n_hard_negatives))</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># As in the paper, averaged over positive priors only, although computed over both positive and hard-negative priors</span></span><br><span class="line">        <span class="attr">conf_loss</span> = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()  <span class="comment"># (), scalar</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># TOTAL LOSS</span></span><br><span class="line"></span><br><span class="line">        return conf_loss + self.alpha * loc_loss</span><br></pre></td></tr></table></figure></p><h3 id="Processing-predictions"><a href="#Processing-predictions" class="headerlink" title="Processing predictions"></a>Processing predictions</h3><p>对模型进行训练后，我们可以将其应用于图像目标检测。然而，模型预测输出的是包含8732个priors的偏移量和类别分数。因此需要对这些进行后处理，以获得最终的目标检测的边界框。大致过程如下：<br>（1）在获得了8732个priors的预测偏移量(g_c_x, g_c_y, g_w, g_h)之后，通过如下公式进行逆运算转换成绝对坐标：<br><img src="/img/ecs2.png" alt><br>（2）通过NMS（非最大值抑制）过滤掉背景和得分不是很高的框（这个是为了避免重复预测），得到最终的预测。<br>具体代码实现如下：<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line">def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    Decipher the 8732 locations and class scores (output of ths SSD300) to detect objects.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For each class, perform Non-Maximum Suppression (NMS) on boxes that are above a minimum threshold.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)</span></span><br><span class="line"><span class="string">    :param predicted_scores: class scores for each of the encoded locations/boxes, a tensor of dimensions (N, 8732, n_classes)</span></span><br><span class="line"><span class="string">    :param min_score: minimum threshold for a box to be considered a match for a certain class</span></span><br><span class="line"><span class="string">    :param max_overlap: maximum overlap two boxes can have so that the one with the lower score is not suppressed via NMS</span></span><br><span class="line"><span class="string">    :param top_k: if there are a lot of resulting detection across all classes, keep only the top 'k'</span></span><br><span class="line"><span class="string">    :return: detections (boxes, labels, and scores), lists of length batch_size</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    <span class="attr">batch_size</span> = predicted_locs.size(<span class="number">0</span>)</span><br><span class="line">    <span class="attr">n_priors</span> = self.priors_cxcy.size(<span class="number">0</span>)</span><br><span class="line">    <span class="attr">predicted_scores</span> = F.softmax(predicted_scores, <span class="attr">dim=2)</span>  <span class="comment"># (N, 8732, n_classes)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Lists to store final predicted boxes, labels, and scores for all images</span></span><br><span class="line">    <span class="attr">all_images_boxes</span> = list()</span><br><span class="line">    <span class="attr">all_images_labels</span> = list()</span><br><span class="line">    <span class="attr">all_images_scores</span> = list()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> <span class="attr">n_priors</span> == predicted_locs.size(<span class="number">1</span>) == predicted_scores.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    for i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">        <span class="comment"># Decode object coordinates from the form we regressed predicted boxes to</span></span><br><span class="line">        <span class="attr">decoded_locs</span> = cxcy_to_xy(</span><br><span class="line">            gcxgcy_to_cxcy(predicted_locs[i], self.priors_cxcy))  <span class="comment"># (8732, 4), these are fractional pt. coordinates</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Lists to store boxes and scores for this image</span></span><br><span class="line">        <span class="attr">image_boxes</span> = list()</span><br><span class="line">        <span class="attr">image_labels</span> = list()</span><br><span class="line">        <span class="attr">image_scores</span> = list()</span><br><span class="line"></span><br><span class="line">        max_scores, <span class="attr">best_label</span> = predicted_scores[i].max(<span class="attr">dim=1)</span>  <span class="comment"># (8732)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check for each class</span></span><br><span class="line">        for c <span class="keyword">in</span> range(<span class="number">1</span>, self.n_classes):</span><br><span class="line">            <span class="comment"># Keep only predicted boxes and scores where scores for this class are above the minimum score</span></span><br><span class="line">            <span class="attr">class_scores</span> = predicted_scores[i][:, c]  <span class="comment"># (8732)</span></span><br><span class="line">            <span class="attr">score_above_min_score</span> = class_scores &gt; min_score  <span class="comment"># torch.uint8 (byte) tensor, for indexing</span></span><br><span class="line">            <span class="attr">n_above_min_score</span> = score_above_min_score.sum().item()</span><br><span class="line">            <span class="keyword">if</span> <span class="attr">n_above_min_score</span> == <span class="number">0</span>:</span><br><span class="line">                continue</span><br><span class="line">            <span class="attr">class_scores</span> = class_scores[score_above_min_score]  <span class="comment"># (n_qualified), n_min_score &lt;= 8732</span></span><br><span class="line">            <span class="attr">class_decoded_locs</span> = decoded_locs[score_above_min_score]  <span class="comment"># (n_qualified, 4)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Sort predicted boxes and scores by scores</span></span><br><span class="line">            class_scores, <span class="attr">sort_ind</span> = class_scores.sort(<span class="attr">dim=0,</span> <span class="attr">descending=True)</span>  <span class="comment"># (n_qualified), (n_min_score)</span></span><br><span class="line">            <span class="attr">class_decoded_locs</span> = class_decoded_locs[sort_ind]  <span class="comment"># (n_min_score, 4)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Find the overlap between predicted boxes</span></span><br><span class="line">            <span class="attr">overlap</span> = find_jaccard_overlap(class_decoded_locs, class_decoded_locs)  <span class="comment"># (n_qualified, n_min_score)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Non-Maximum Suppression (NMS)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># A torch.uint8 (byte) tensor to keep track of which predicted boxes to suppress</span></span><br><span class="line">            <span class="comment"># 1 implies suppress, 0 implies don't suppress</span></span><br><span class="line">            <span class="attr">suppress</span> = torch.zeros((n_above_min_score), <span class="attr">dtype=torch.uint8).to(device)</span>  <span class="comment"># (n_qualified)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Consider each box in order of decreasing scores</span></span><br><span class="line">            for box <span class="keyword">in</span> range(class_decoded_locs.size(<span class="number">0</span>)):</span><br><span class="line">                <span class="comment"># If this box is already marked for suppression</span></span><br><span class="line">                <span class="keyword">if</span> suppress[box] == <span class="number">1</span>:</span><br><span class="line">                    continue</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Suppress boxes whose overlaps (with this box) are greater than maximum overlap</span></span><br><span class="line">                <span class="comment"># Find such boxes and update suppress indices</span></span><br><span class="line">                <span class="attr">suppress</span> = torch.max(suppress, overlap[box] &gt; max_overlap)</span><br><span class="line">                <span class="comment"># The max operation retains previously suppressed boxes, like an 'OR' operation</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># Don't suppress this box, even though it has an overlap of 1 with itself</span></span><br><span class="line">                suppress[box] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Store only unsuppressed boxes for this class</span></span><br><span class="line">            image_boxes.append(class_decoded_locs[<span class="number">1</span> - suppress])</span><br><span class="line">            image_labels.append(torch.LongTensor((<span class="number">1</span> - suppress).sum().item() * [c]).to(device))</span><br><span class="line">            image_scores.append(class_scores[<span class="number">1</span> - suppress])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If no object in any class is found, store a placeholder for 'background'</span></span><br><span class="line">        <span class="keyword">if</span> len(image_boxes) == <span class="number">0</span>:</span><br><span class="line">            image_boxes.append(torch.FloatTensor([[<span class="number">0</span>., <span class="number">0</span>., <span class="number">1</span>., <span class="number">1</span>.]]).to(device))</span><br><span class="line">            image_labels.append(torch.LongTensor([<span class="number">0</span>]).to(device))</span><br><span class="line">            image_scores.append(torch.FloatTensor([<span class="number">0</span>.]).to(device))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Concatenate into single tensors</span></span><br><span class="line">        <span class="attr">image_boxes</span> = torch.cat(image_boxes, <span class="attr">dim=0)</span>  <span class="comment"># (n_objects, 4)</span></span><br><span class="line">        <span class="attr">image_labels</span> = torch.cat(image_labels, <span class="attr">dim=0)</span>  <span class="comment"># (n_objects)</span></span><br><span class="line">        <span class="attr">image_scores</span> = torch.cat(image_scores, <span class="attr">dim=0)</span>  <span class="comment"># (n_objects)</span></span><br><span class="line">        <span class="attr">n_objects</span> = image_scores.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep only the top k objects</span></span><br><span class="line">        <span class="keyword">if</span> n_objects &gt; top_k:</span><br><span class="line">            image_scores, <span class="attr">sort_ind</span> = image_scores.sort(<span class="attr">dim=0,</span> <span class="attr">descending=True)</span></span><br><span class="line">            <span class="attr">image_scores</span> = image_scores[:top_k]  <span class="comment"># (top_k)</span></span><br><span class="line">            <span class="attr">image_boxes</span> = image_boxes[sort_ind][:top_k]  <span class="comment"># (top_k, 4)</span></span><br><span class="line">            <span class="attr">image_labels</span> = image_labels[sort_ind][:top_k]  <span class="comment"># (top_k)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Append to lists that store predicted boxes and scores for all images</span></span><br><span class="line">        all_images_boxes.append(image_boxes)</span><br><span class="line">        all_images_labels.append(image_labels)</span><br><span class="line">        all_images_scores.append(image_scores)</span><br><span class="line"></span><br><span class="line">    return all_images_boxes, all_images_labels, all_images_scores  <span class="comment"># lists of length batch_size</span></span><br></pre></td></tr></table></figure></p><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>在SSD中作者使用VOC2007和VOC2012数据集进行实验验证，两个数据集是互斥，不相容的。在SSD论文中针对 VOC2007和VOC2012 的具体用法有以下几种：</p><blockquote><ul><li><strong>07</strong>： VOC2007 trainval ，做训练数据；</li><li><strong>07+12</strong>： VOC2007 trainval + VOC200712 trainval，两者组合作为训练数据；</li><li><strong>07++12</strong>： VOC2007 trainval + VOC200712 trainval + VOC2007 test， 三者组合作为训练数据；</li></ul></blockquote><p>此外，还有先在coco进行预训练，然后再VOC上进行finetune的用法：</p><blockquote><ul><li><strong>07+12+coco</strong>： VOC2007 trainval + VOC200712 trainval + coco trainval135k, 表示先在coco trainval135k上训练，然后使用07+12进行finetune；</li><li><strong>07++12+coco</strong>： VOC2007 trainval + VOC2007 test + VOC200712 trainval + coco trainval135k, 表示先在coco trainval135k上训练，然后使用07++12进行finetune；</li></ul></blockquote><h3 id="code"><a href="#code" class="headerlink" title="code"></a>code</h3><p>SSD300实现的完整代码：<a href="https://github.com/nicehuster/ssd.pytorch" target="_blank" rel="noopener">https://github.com/nicehuster/ssd.pytorch</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;#160; &amp;#160; &amp;#160; &amp;#160;SSD作为one-stage目标检测算法中比较经典且具有代表性的一种方法。其精度可以与Faster R-CNN相匹敌，而速度达到了惊人的59FPS，速度上完爆 Faster R-CNN。相比于Faster R-CNN,其速度快的根本原因在于移除了region proposals的步骤以及后续的像素采样或特征采样步骤。论文连接：&lt;a href=&quot;https://arxiv.org/abs/1512.02325&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SSD: Single Shot MultiBox Detector&lt;/a&gt;，作者开源的代码连接：&lt;a href=&quot;https://github.com/weiliu89/caffe/tree/ssd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;code&lt;/a&gt;。由于作者开源代码使用caffe实现，这里以pytorch源码的方式实现。作者在论文中实现了两种不同输入大小的SSD模型：SSD300（输入图像大小统一为300x300）以及SSD512（输入图像大小统一为512x512）.这里主要针对SSD300,下面主要分四个部分介绍SSD300以及代码具体实现。&lt;br&gt;&lt;img src=&quot;/img/ssd.png&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="project" scheme="https://nicehuster.github.io/categories/project/"/>
    
    
      <category term="detection" scheme="https://nicehuster.github.io/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>Faster rcnn Explained with code</title>
    <link href="https://nicehuster.github.io/2019/05/08/0052-Faster-rcnn%20Explained%20with%20code/"/>
    <id>https://nicehuster.github.io/2019/05/08/0052-Faster-rcnn Explained with code/</id>
    <published>2019-05-08T11:13:39.000Z</published>
    <updated>2020-09-18T02:38:57.121Z</updated>
    
    <content type="html"><![CDATA[<p>&#160; &#160; &#160; &#160;最近在看mmdetection源码，对于有些代码实现原理不是很清楚。由于mmdetection中实现的算法大多是two-stage方法，Faster rcnn作为这类方法的鼻祖，十分经典，详细地了解其实现具体原理十分重要。下面主要是推荐两篇关于Faster RCNN的blog，这两篇blog都是基于同一个代码实现来讲解的。<br><strong>代码地址</strong>：<a href="https://github.com/jwyang/faster-rcnn.pytorch" target="_blank" rel="noopener">https://github.com/jwyang/faster-rcnn.pytorch</a><br><strong>blog 1</strong> :<a href="http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/" target="_blank" rel="noopener">Object Detection and Classification using R-CNNs</a><br><strong>blog 2</strong>：<a href="https://towardsdatascience.com/fasterrcnn-explained-part-1-with-code-599c16568cff" target="_blank" rel="noopener">FasterRCNN Explained with Code</a>  </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;#160; &amp;#160; &amp;#160; &amp;#160;最近在看mmdetection源码，对于有些代码实现原理不是很清楚。由于mmdetection中实现的算法大多是two-stage方法，Faster rcnn作为这类方法的鼻祖，十分经典，详细地了解其实现具体原理十分重要
      
    
    </summary>
    
      <category term="project" scheme="https://nicehuster.github.io/categories/project/"/>
    
    
      <category term="detection" scheme="https://nicehuster.github.io/tags/detection/"/>
    
  </entry>
  
</feed>
