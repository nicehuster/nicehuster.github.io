<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一起打怪升级呀</title>
  <icon>https://www.gravatar.com/avatar/2555127dc0de830d31ceeb98d8565ac8</icon>
  <subtitle>别整太大鸭力,多鸡立自己qaq</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://blog.nicehuster.cn/"/>
  <updated>2021-02-24T08:09:44.536Z</updated>
  <id>https://blog.nicehuster.cn/</id>
  
  <author>
    <name>nicehuster</name>
    <email>nicehuster@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ATSS自适应选择正负样本的目标检测方法</title>
    <link href="https://blog.nicehuster.cn/2021/02/23/atss/"/>
    <id>https://blog.nicehuster.cn/2021/02/23/atss/</id>
    <published>2021-02-23T11:13:39.000Z</published>
    <updated>2021-02-24T08:09:44.536Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/abs/1912.02424" target="_blank" rel="noopener">Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</a><br><strong>代码链接：</strong><a href="https://github.com/sfzhang15/ATSS" target="_blank" rel="noopener">https://github.com/sfzhang15/ATSS</a><br><strong>整体信息：</strong>这篇论文是中科院张士峰发表在CVPR2020上的一篇文章，论文通过大量实验证明指出anchor-based方法和anchor-free方法的性能差异主要来自于正负样本的选取上，基于此提出ATSS(Adaptive Training Sample Selection)方法，该方法通过自动统计标注GT上的相关统计特征自适应的选择合适的anchor box作为正负样本，在不带来额外计算量和参数的情况下，能够大幅提升模型的性能。</p><a id="more"></a><h3 id="1-差异分析"><a href="#1-差异分析" class="headerlink" title="1.差异分析"></a>1.差异分析</h3><p>在不考虑各种trick的情况下，anchor-based方法RetinaNet和anchor-free方法FCOS主要存在两个差异：1）正负样本选取；2）回归起始状态；为公平比较，降低两者方法的差异性，在实验中将RetinaNet的anchor数改为1降低差异性，方便与FCOS比较，作者后续在实验中也验证不同anchor数对模型性能的影响。</p><h4 id="1-Inconsistency-Removal"><a href="#1-Inconsistency-Removal" class="headerlink" title="1).Inconsistency Removal"></a>1).Inconsistency Removal</h4><p><img src="/img/atss_Inconsistency%20Removal.png" alt="atss_Inconsistency Removal"></p><p>由于FCOS加入了很多trick，为了与RetinaNet进行公平比较，对FCOS上的trick进行对齐，包括GroupNorm、GIoU loss、限制正样本必须在GT内、Centerness branch以及添加可学习的标量控制FPN的各层的尺寸。具体结果如上面表格所示，最终的RetinaNet与FCOS相差无几，但仍然存在0.8个点的差异。</p><h4 id="2-Essential-Difference"><a href="#2-Essential-Difference" class="headerlink" title="2).Essential Difference"></a>2).Essential Difference</h4><p>在经过上述实验对齐之后，RetinaNet和FCOS仅存在两个差异：1）正负样本选取；2）回归起始状态；针对这两个差异，论文通过实验做进一步分析。</p><p><img src="/img/image-20210224115350117.png" alt="image-20210224115350117"></p><ul><li><p><strong>Classification</strong></p><p>在RetinaNet中，通过划分IOU阈值区间来区分正负样本，$\mathrm{IoU}&gt;\theta_{p}$ 置为正样本，$\mathrm{IoU}&lt;\theta_{n}$ 置为负样本，至于区间内所有样本忽略；在FCOS使用空间尺寸和尺寸限制来区分正负anchor point，正样本首先必须在GT box内，其次需要是GT尺寸对应的层，其余均为负样本；</p></li></ul><p><img src="/img/image-20210224115743063.png" alt="image-20210224115743063"></p><ul><li><p><strong>Regression</strong></p><p>在RetinaNet中，模型预测的是相对anchor box的4个偏移量，通过4个偏移量对anchor box进行调整；而在FCOS则预测的是相对于anchor point的4个偏移量，基于anchor point和4个偏移量组成bbox；</p><p><img src="/img/image-20210224143241390.png" alt="image-20210224143241390"></p></li><li><p><strong>Conclusion</strong></p><p>基于上述两个差异，论文做了实验进行交叉对比，实验结果如上表格2所示，在相同正负样本定义下的RetinaNet和FCOS性能几乎一样，不同的定义方法性能差异较大，具体可以看下表格的纵向对比结果；而从横向对比结果上来看，无论是基于box或基于Point的形式，指标基本不发生变化，这也印证了回归初始状态的不同对模型性能影响不大。所以，基本可以确定正负样本的选取是影响模型性能的关键；</p></li></ul><h3 id="2-自适应样本选取（Adaptive-Training-Sample-Selection）"><a href="#2-自适应样本选取（Adaptive-Training-Sample-Selection）" class="headerlink" title="2. 自适应样本选取（Adaptive Training Sample Selection）"></a>2. 自适应样本选取（Adaptive Training Sample Selection）</h3><p><img src="/img/image-20210224151209904.png" alt="image-20210224151209904"></p><h4 id="1-算法流程"><a href="#1-算法流程" class="headerlink" title="1).算法流程"></a>1).算法流程</h4><p>论文提出基于统计特征自适应样本选取的方法具体流程如上图所示，具体来说，对于每个标注gt框g，首先针对每个FPN层级基于L2距离找到与目标中心点最近的k个anchor box，计算anchor box与gt box的IOU值，计算所有iou值的均值$m_{g}$和标准差$v_{g}$，基于均值和阈值的计算结果，得到IOU阈值 $t_{g}=m_{g}+v_{g}$ ，最后选择阈值大于等于$t_{g}$ 且中心点位于gt box内的box作为最后的输出。如果anchor box对应多个gt，则选择IoU最大的gt。</p><h4 id="2-算法思想"><a href="#2-算法思想" class="headerlink" title="2).算法思想"></a>2).算法思想</h4><p>从思想上来看，自适应样本选取的方法遵循如下几点：</p><blockquote><ul><li>基于目标中心距离远近程度来选择样本；在RetinaNet中，anchor box与gt中心点越近一般IoU越高，而在FCOS中，中心点越近一般预测的质量越高；</li><li>使用使用iou的统计量均值和方差之和作为iou阈值；均反映的是预设的anchor box与gt的匹配程度，均值高则应当提高阈值来调整正样本，均值低则应当降低阈值来调整正样本。标准差反映的是适合GT的FPN层数，标准差高则表示高质量的anchor box集中在一个层中，应将阈值加上标准差来过滤其他层的anchor box，低则表示多个层都适合该gt，将阈值加上标准差来选择合适的层的anchor box，均值和标准差结合作为IoU阈值能够很好地自动选择对应的特征层上合适的anchor box；</li><li>正样本的中心必须位于目标内；若anchor box的中心点不在gt区域内，则其会使用非gt区域的背景特征进行预测，影响模型性能，这种情况应予以剔除；</li><li>几乎无超参数；从上面的算法流程可以看出，唯一的超参数就是k，论文后面也通过实验验证ATSS算法对于k值的选取不明感；</li></ul></blockquote><h4 id="3-ATSS实验对比"><a href="#3-ATSS实验对比" class="headerlink" title="3).ATSS实验对比"></a>3).ATSS实验对比</h4><p><img src="/img/image-20210224154804455.png" alt="image-20210224154804455"></p><p>将ATSS应用到RetinaNet和FCOS上测试效果：</p><ul><li>将RetinaNet中的正负样本替换为ATSS，AP提升了2.3%；</li><li>在FCOS上的应用主要用两种：lite版本采用ATSS的思想，从选取GT内的anchor point改为选取每层离GT最近的topk个候选anchor point，提升了0.8%AP；full版本将FCOS的anchor point改为长宽为8S的anchor box来根据ATSS选择正负样本，但仍然使用原始的回归方法，提升了1.4%AP。两种方法找到的anchor point在空间位置上大致相同，但是在FPN层上的选择不太一样。从结果来看，自适应的选择方法比固定的方法更有效。至于与其他SOTA方法的对比，可以直接参考论文。</li></ul><h3 id="3-Discussion"><a href="#3-Discussion" class="headerlink" title="3.Discussion"></a>3.Discussion</h3><p>上面的实验对比可以看出RetinaNet实验中没有涉及任何关于anchor宽高比ratio和尺度scale相关的参数，而且在实验中仅使用了一个anchor,在原始RetinaNet中，一个目标选择的是9个预定义的不同尺度和宽高比的anchor box，因此论文补充实验测试了在不同尺度、不同宽高比以及不同anchors数量下的实验结果。具体实验结果，可以直接参考原文，从论文po出来的实验结果来看，ATSS对于anchor尺度和宽高比并不敏感，而且在每个位置设定多个anchor box是无用的操作，关键在于选择合适的正样本。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/1912.02424&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/sfzhang15/ATSS&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/sfzhang15/ATSS&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这篇论文是中科院张士峰发表在CVPR2020上的一篇文章，论文通过大量实验证明指出anchor-based方法和anchor-free方法的性能差异主要来自于正负样本的选取上，基于此提出ATSS(Adaptive Training Sample Selection)方法，该方法通过自动统计标注GT上的相关统计特征自适应的选择合适的anchor box作为正负样本，在不带来额外计算量和参数的情况下，能够大幅提升模型的性能。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://blog.nicehuster.cn/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读： A Simple and Strong Anchor-freeObject Detector(FCOS_imprv)</title>
    <link href="https://blog.nicehuster.cn/2021/02/22/FCOS_improv/"/>
    <id>https://blog.nicehuster.cn/2021/02/22/FCOS_improv/</id>
    <published>2021-02-22T11:13:39.000Z</published>
    <updated>2021-02-23T02:46:30.600Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> Zhi Tian,Adelaide<br><strong>代码链接：</strong><a href="https://github.com/aim-uofa/AdelaiDet" target="_blank" rel="noopener">https://github.com/aim-uofa/AdelaiDet</a><br><strong>整体框架：</strong> 这篇论文是fcos原作者重新修改后发表在PAMI上面的一篇文章，fcos_imprv和fcos整体思想是一致的，只是在原作上做了部分修改，是网络性能得到进一步提升，在backbone为ResNet-101-FPN上面的性能从41.0提升到43.2，提升明显。本文主要是记录一下fcos_imprv相比原方法的一些改进和提升方法。<br><a id="more"></a></p><h3 id="改进点"><a href="#改进点" class="headerlink" title="改进点"></a>改进点</h3><p>对于原始fcos方法的原理介绍，可以参考<a href="https://nicehuster.github.io/2019/04/15/FCOS/" target="_blank" rel="noopener">这里</a> ，这里详细说一下改进的地方：</p><h4 id="1-正负样本"><a href="#1-正负样本" class="headerlink" title="1.正负样本"></a>1.正负样本</h4><p>作者对正负样本的指定做了修改，在原始fcos中，对于目标被的所有特征点均指定为正样本，以及该特征点到目标边界的距离满足所处的FPN层的约束；而在FCOS_imprv中则要求指定只有在目标中心区域的特征点才为正样本；目标中心区域的大小为：</p><script type="math/tex; mode=display">\left(c_{x}-r s, c_{y}-r s, c_{x}+r s, c_{y}+r s\right)</script><p>其中 $\left(c_{x}, c_{y}\right)$ 为目标中心，s为当前层的stride，r 为超参数，在论文中r设置为1.5。这个代码的实现可以看一下mmdetection中FCOS的实现：<a href="mmdet/models/dense_heads/fcos_head.py">fcos_head.py</a> 其中center_sample_radius参数默认设置为1.5.</p><h4 id="2-回归目标修改"><a href="#2-回归目标修改" class="headerlink" title="2.回归目标修改"></a>2.回归目标修改</h4><p>Fcos的回归目标直接是特征点到目标边界的距离，由于Head是共用的，所以在预测时为每个level预设一个可学习的scale因子，而fcos_imprv则加入stride，变得更适应FPN的尺寸，可学习的scale因子依然使用，具体形式如下：</p><script type="math/tex; mode=display">\begin{array}{l}l^{*}=\left(x-x_{0}^{(i)}\right) / s, \quad t^{*}=\left(y-y_{0}^{(i)}\right) / s \\r^{*}=\left(x_{1}^{(i)}-x\right) / s, \quad b^{*}=\left(y_{1}^{(i)}-y\right) / s\end{array}</script><p>这一部分的代码实现，可以看一下这个地方：<a href="https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/dense_heads/fcos_head.py#L147" target="_blank" rel="noopener">fcos_head.py#L147</a> 其中scale为可学习参数。</p><h4 id="3-centerness-修改"><a href="#3-centerness-修改" class="headerlink" title="3.centerness 修改"></a>3.centerness 修改</h4><p>在原始fcos中，centerness是和分类分支放在一起预测，而在fcos_imprv中，将该分支移至回归分支，以便更好的利用位置相关信息；</p><p><img src="/img/fcos_imprv-3999865.png" alt="fcos_imprv"></p><h4 id="4-回归损失函数"><a href="#4-回归损失函数" class="headerlink" title="4.回归损失函数"></a>4.回归损失函数</h4><p>在fcos方法中，训练损失函数如下：</p><script type="math/tex; mode=display">\begin{aligned}L\left(\left\{\boldsymbol{p}_{x, y}\right\},\left\{\boldsymbol{t}_{x, y}\right\}\right) &=\frac{1}{N_{\mathrm{pos}}} \sum_{x, y} L_{\mathrm{cls}}\left(\boldsymbol{p}_{x, y}, c_{x, y}^{*}\right) \\&+\frac{\lambda}{N_{\mathrm{pos}}} \sum_{x, y} \mathbb{1}_{\left\{c_{x, y}^{*}>0\right\}} L_{\mathrm{reg}}\left(\boldsymbol{t}_{x, y}, \boldsymbol{t}_{x, y}^{*}\right)\end{aligned}</script><p>第一项是分类分支的损失函数，使用的是的focal loss，第二项是回归分支的损失函数。在原始fcos中，使用的IOU loss，而在fcos_imprv中，使用的GIOU loss。这里顺便介绍一下IOU loss和GIOU loss的区别。</p><blockquote><ul><li>IOU loss，原始smooth l1 loss在计算目标检测的 bbox loss时，都是独立的求出4个点的 loss，然后相加得到最终的 bbox loss。这种做法的默认4个点是相互独立的，而目标检测评价的是IOU指标，与实际不符。IOU loss计算如下：</li></ul><script type="math/tex; mode=display">\text { IoU loss }=-\ln \operatorname{IoU}\left(bbox_\text {gt }, b b o x_{\text {pred }}\right)</script><ul><li><p>GIOU loss，iou loss计算的是预测框与gt框存在重叠情况下的损失函数，而对于不重叠情况下，损失函数不可导，无法优化两个框不相交的情况。此外，如果iou是确定的，其iou值是无法确定两个框是如何相交的。GIOU 计算如下：</p><script type="math/tex; mode=display">G I o U=I o U-\frac{|C \backslash(A \cup B)|}{|C|}</script><p>GIoU 的实现方式如上式，其中 C 为 A 和 B 的外接矩形。用 C 减去 A 和 B 的并集除以 C 得到一个数值，然后再用 A 和 B 的 IoU 减去这个数值即可得到 GIoU 的值。可以看出：1）GIoU 取值范围为 [-1, 1]，在两框重合时取最大值1，在两框无限远的时候取最小值-1；2）与 IoU 只关注重叠区域不同，GIoU不仅关注重叠区域，还关注其他的非重合区域，能更好的反映两者的重合度。</p><p>GIOU loss计算如下：</p><script type="math/tex; mode=display">\mathcal{L}_{G I o U}= 1-GIOU</script></li></ul></blockquote><h4 id="5-分数计算"><a href="#5-分数计算" class="headerlink" title="5.分数计算"></a>5.分数计算</h4><p>最终分数的计算，fcos采用分类分数以及center-ness之积，fcos_imprv则采用分类分数以及center-ness之积的平方根：</p><script type="math/tex; mode=display">\boldsymbol{s}_{x, y}=\sqrt{\boldsymbol{p}_{x, y} \times o_{x, y}}</script><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/img/fcos_exp.png" alt="fcos_exp"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; Zhi Tian,Adelaide&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/aim-uofa/AdelaiDet&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/aim-uofa/AdelaiDet&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体框架：&lt;/strong&gt; 这篇论文是fcos原作者重新修改后发表在PAMI上面的一篇文章，fcos_imprv和fcos整体思想是一致的，只是在原作上做了部分修改，是网络性能得到进一步提升，在backbone为ResNet-101-FPN上面的性能从41.0提升到43.2，提升明显。本文主要是记录一下fcos_imprv相比原方法的一些改进和提升方法。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://blog.nicehuster.cn/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>DETR 基于transformer的目标检测方法</title>
    <link href="https://blog.nicehuster.cn/2020/10/27/detr/"/>
    <id>https://blog.nicehuster.cn/2020/10/27/detr/</id>
    <published>2020-10-27T11:13:39.000Z</published>
    <updated>2020-10-28T08:49:02.649Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/pdf/2005.12872.pdf" target="_blank" rel="noopener">End to End Object Detection with Transformers</a><br><strong>代码链接：</strong> <a href="https://github.com/facebookresearch/detr" target="_blank" rel="noopener">https://github.com/facebookresearch/detr</a><br><strong>整体信息：</strong> 这是FAIR最近新出了一篇用transformer做检测的文章。相比于以往所有的检测方法不同的是，没有使用先验知识比如anchor设计，以及后处理步骤比如nms等操作。而是使用transformer预测集合的形式，直接输出目标位置及类别信息。在object detection上DETR准确率和运行时间上和Faster RCNN相当；将模型generalize到panoptic segmentation任务上，DETR表现甚至还超过了其他的baseline.</p><a id="more"></a><h3 id="1-概览"><a href="#1-概览" class="headerlink" title="1.概览"></a>1.概览</h3><p><img src="/img/detr.png" alt="detr"></p><p>上图展示的是DETR的整体结构图。如上图所示，可以看出两个关键的部分：</p><blockquote><ul><li>使用transformer 的encoder-decoder结构生成N个 box predictions；其中N为事先设计的、远大于图像中object数的一个整数；</li><li>设计了bipartite matching loss，基于预测的box和gt box的二分图匹配计算loss，从而使得预测的box更接近gt box；</li></ul></blockquote><p>这篇文章的撰写风格比较诡异，不是按照正常的网络结构的先后顺序来介绍各个部分。而是先介绍了bipartite matching loss，再介绍transformer的encoder和decoder。为了便于理解，我这里还是按照网络结构的先后顺序来介绍各个模块。</p><h3 id="2-Transformer"><a href="#2-Transformer" class="headerlink" title="2.Transformer"></a>2.Transformer</h3><p><img src="/img/detr-transformer.png" alt="detr-transformer"></p><p>上图展示的是transformer的结构，其中包括encoder、decoder以及FFN三部分。</p><h4 id="2-1-Transformer-Encoder"><a href="#2-1-Transformer-Encoder" class="headerlink" title="2.1 Transformer Encoder"></a>2.1 Transformer Encoder</h4><p>如上图左侧所示，在DETR中，首先输入图片 $x_{\mathrm{img}} \in \mathbb{R}^{3 \times H_{0} \times W_{0}}$ 经过CNN backbone处理后，输出feature map$f \in \mathbb{R}^{C \times H \times W}$ ，一般 $C=2048,H, W=\frac{H_{0}}{32}, \frac{W_{0}}{32}$ 。然后将backbone输出的feature map和position encoding相加，输入Transformer Encoder中处理，得到用于输入到Transformer Decoder的image embedding。由于Transformer的输入为序列化数据，因此会对backbone输出的feature map做进一步处理转化为序列化数据，具体处理包括如下：</p><blockquote><ul><li><strong>维度压缩</strong>：使用1x1卷积将feature map 维度从C压缩为d,生成新的feature map $z_{0} \in \mathbb{R}^{d \times H \times W}$ ；</li><li><strong>转化为序列化数据：</strong>将空间的维度（高和宽）压缩为一个维度，即把上一步得到的$d \times H \times W$维的feature map通过reshape成$d \times H W$维的feature map；</li><li><strong>加上positoin encoding:</strong> 由于transformer模型是permutation-invariant（转置不变性，也可以理解为和位置无关），而显然图像中目标是具有空间信息的，与原图的位置有关，所以需要加上position encoding反映位置信息。具体生成的方法后续补充讲解。</li></ul></blockquote><h4 id="2-2-Transformer-Decoder"><a href="#2-2-Transformer-Decoder" class="headerlink" title="2.2 Transformer Decoder"></a>2.2 Transformer Decoder</h4><p>这部分的话，有两个输入，一个是Transformer Encoder的输出，另一个是object queries。这里讲一下object queries，object queries有N个，N是一个事先设定的、比远远大于image中object个数的一个整数），输入Transformer Decoder后分别得到N个 output embedding，然后经过FFN处理之后，输出N个box的位置和类别信息。object queries具体是什么，论文阐述的很模糊。</p><h4 id="2-3-FFN"><a href="#2-3-FFN" class="headerlink" title="2.3 FFN"></a>2.3 FFN</h4><p>这个是网络结构最后的输出部分，在DETR中，其实有两种FFN：一种预测bounding box的中心位置、高和宽，第二种预测class标签。下图是更细节的DETR Transformer结构，大家感兴趣可以仔细看一下。</p><p><img src="/img/detr-ffn.png" alt="detr-ffn"></p><h3 id="3-Loss设计"><a href="#3-Loss设计" class="headerlink" title="3. Loss设计"></a>3. Loss设计</h3><p>前面讲到transformer经过FFN之后会预测输出N个prediction boxes,其中，N是事先设计好的一个远大于image objects的正整数。通过得到这N个prediction boxes和image objects的最优二分图匹配，通过计算配对的box pair的loss来对模型进行优化。</p><h4 id="3-1-最优二分图匹配"><a href="#3-1-最优二分图匹配" class="headerlink" title="3.1 最优二分图匹配"></a>3.1 最优二分图匹配</h4><p>假设对于一张图来说，image objects的个数为m，由于N是事先设计好的一个远大于m的正整数，所以 N&gt;&gt;m，即生成的prediction boxes的数量会远远大于image objects 数量。这样怎么做匹配？</p><p>为了解决这个问题，作者人为构造了一个新的物体类别 $\varnothing$ (表示没有物体)并加入image objects中，上面所说到的多出来的N-m个个prediction embedding就会和类别 $\varnothing$ 配对。这样就可以将prediction boxes和image objects的配对看作两个等容量的集合的二分图匹配了。</p><p>设计好匹配的cost，就可以使用<a href="https://www.cxyxiaowu.com/874.html" target="_blank" rel="noopener">匈牙利算法</a>快速地找到使cost最小的二分图匹配方案了。cost计算如下：</p><script type="math/tex; mode=display">\hat{\sigma}=\underset{\sigma \in \widetilde{S}_{N}}{\arg \min } \sum_{i}^{N} \mathcal{L}_{\operatorname{match}}\left(y_{i}, \hat{y}_{\sigma(i)}\right)</script><p>对应单个prediction box和image object匹配cost $\mathcal{L}_{\operatorname{match}}\left(y_{i}, \hat{y}_{\sigma(i)}\right)$计算如下：</p><script type="math/tex; mode=display">-\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \hat{p}_{\sigma(i)}\left(c_{i}\right)+\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \mathcal{L}_{\mathrm{box}}\left(b_{i}, \hat{b}_{\sigma(i)}\right)</script><p>其中，</p><p>$c_i$ 第i个image object的class标签，$\sigma(i)$ 表示与第i个image object匹配的prediction box的index;</p><p>$\mathbb{1}_{c_{i} \neq \varnothing}$ 是一个函数，当 $c_{i} \neq \phi$ 时为1，否则为0；</p><p>$\hat{p}_{\sigma(i)}\left(c_{i}\right)$ 表示Transformer预测第$\sigma(i)$ 个prediction box为类别$c_i$ 的概率；</p><p>$b_{i}, \hat{b}_{\sigma(i)}$ 分别为第i个image object和第$\sigma(i)$ 个prediction box的位置向量；</p><p>$\mathcal{L}_{\mathrm{box}}\left(b_{i}, \hat{b}_{\sigma(i)}\right)$ 计算的是ground truth box和prediction box之间的距离；具体计算方式论文有提及：</p><script type="math/tex; mode=display">\lambda_{\text {iou }} \mathcal{L}_{\text {iou }}\left(b_{i}, \hat{b}_{\sigma(i)}\right)+\lambda_{\mathrm{L} 1}\left\|b_{i}-\hat{b}_{\sigma(i)}\right\|_{1}</script><p>计算box的距离实验的IOU loss以及L1 loss。</p><p>这样，我们就完全定义好了每对prediction box和 image object 配对时的cost。再利用匈牙利算法即可得到二分图最优匹配。</p><h4 id="3-2-计算set-prediction-loss"><a href="#3-2-计算set-prediction-loss" class="headerlink" title="3.2 计算set prediction loss"></a>3.2 计算set prediction loss</h4><p>上面我们得到了prediction boxes和image objects之间的最优匹配。这里我们基于这个最优匹配，来计算set prediction loss，即评价Transformer生成这些prediction boxes的效果好坏。 set prediction loss的计算如下：</p><script type="math/tex; mode=display">\mathcal{L}_{\text {Hungarian }}(y, \hat{y})=\sum^{N}\left[-\log \hat{p}_{\hat{\sigma}(i)}\left(c_{i}\right)+\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \mathcal{L}_{\text {box }}\left(b_{i}, \hat{b}_{\hat{\sigma}}(i)\right)\right]</script><p>其中，$\hat{\sigma}$ 为最优匹配。将第 i个image object匹配到第$\hat{\sigma}(i)$ 个prediction boxes。这里值得注意的是，和上面计算cost不太一样的地方是这里计算分类概率使用的是log对数的形式；将匹配到类别 $\varnothing$ 的概率考虑进去了，而前面cost的计算中则直接将其置为0了；</p><h3 id="4-实验"><a href="#4-实验" class="headerlink" title="4.实验"></a>4.实验</h3><p>论文中实验细节较多，而且实验结果也比较solid，这里不细讲，有兴趣的可以直接看原文，这里贴一张在coco上实验结果对比：</p><p><img src="/img/detr-exp.png" alt="detr-exp"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2005.12872.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;End to End Object Detection with Transformers&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt; &lt;a href=&quot;https://github.com/facebookresearch/detr&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/facebookresearch/detr&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt; 这是FAIR最近新出了一篇用transformer做检测的文章。相比于以往所有的检测方法不同的是，没有使用先验知识比如anchor设计，以及后处理步骤比如nms等操作。而是使用transformer预测集合的形式，直接输出目标位置及类别信息。在object detection上DETR准确率和运行时间上和Faster RCNN相当；将模型generalize到panoptic segmentation任务上，DETR表现甚至还超过了其他的baseline.&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://blog.nicehuster.cn/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>抠图神器U^2-Net</title>
    <link href="https://blog.nicehuster.cn/2020/09/14/U-2-Net/"/>
    <id>https://blog.nicehuster.cn/2020/09/14/U-2-Net/</id>
    <published>2020-09-14T11:13:39.000Z</published>
    <updated>2020-09-18T07:17:55.130Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> Going Deeper with Nested U-Structure for Salient Object Detection<br><strong>代码链接：</strong><a href="https://github.com/NathanUA/U-2-Net" target="_blank" rel="noopener">https://github.com/NathanUA/U-2-Net</a><br><strong>整体信息：</strong> 这是发表在PR2020上的一篇关于显著性检测的文章，作者是秦雪彬。从标题上可以看到本文的一个idea是设计了一个deeper的U型结构的网络解决显著性目标检测问题。作者认为，目前显著性目标检测有两种主流思路，一为多层次深层特征集成multi-level deep feature integration，一为多尺度特征提取Multi-scale feature extraction。多层次深层特征集成方法主要集中在开发更好的多层次特征聚合策略上。而多尺度特征提取这一类方法旨在设计更新的模块，从主干网获取的特征中同时提取局部和全局信息。而几乎所有上述方法，都是为了更好地利用现有的图像分类的Backbones生成的特征映射。而作者另辟蹊径，提出了一种新颖而简单的结构，它直接逐级提取多尺度特征，用于显著目标检测，而不是利用这些主干的特征来开发和添加更复杂的模块和策略。下图是该方法与其他方法的一个比较：</p><a id="more"></a><p><img src="/img/image-20200918105536819.png" alt="image-20200918105536819"></p><h3 id="显著性检测"><a href="#显著性检测" class="headerlink" title="显著性检测"></a>显著性检测</h3><p>在讲这篇文章之前，有必要先了解显著性检测这个任务。</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/115002897" target="_blank" rel="noopener">显著性检测</a></p></blockquote><p><strong>显著性检测</strong>，就是使用图像处理技术和计算机视觉算法来定位图片中最“显著”的区域。显著区域就是指图片中引人注目的区域或比较重要的区域，例如人眼在观看一幅图片时会首先关注的区域。例如下图，我们人眼一眼看过去首先注意到的不是草坪，而是躺在草坪上的内马尔，内马尔所在的区域就是显著性区域。这种自动定位图像或场景重要区域的过程称为<strong>显着性检测</strong>。</p><p><img src="/img/v2-422783ab5d5d66fd0dbf5653166ddbd8_720w.jpg" alt></p><h4 id="显著性检测和图像分割区别"><a href="#显著性检测和图像分割区别" class="headerlink" title="显著性检测和图像分割区别"></a>显著性检测和图像分割区别</h4><p>这个任务和图像分割十分类似，区别在于：</p><blockquote><ul><li>1.目标数量：显著性目标检测一般只检一个目标，一般目标检测不会限制数量；</li><li>目标类别： 显著性目标检测不关心目标类别，只关心显著性强的目标，一般目标检测会得到目标位置和类别；</li><li>问题建模不同：显著性目标检测，很早期的时候是通过对一些共性的特征建模，比如该目标一般在图像中心，一般是什么样的颜色分布等等。一般目标检测问题，则是希望特征能够更好的把目标表达出来，越细节越好，特征越丰富约好，因为要区分类别</li><li>groundtruth定义不同：两者互有交集，也有不同；前者，往往定义的是一些显著性较强的目标，比如行人，动物等等，而且前者是不输出类别标签信息；后者，gt标签定义是明确的，可以严格的输出对应类别标签；</li></ul></blockquote><p>其实，说白了，显著性检测等同于是一个二分类的语义分割模型，此外，显著的区域 不一定就是 目标，目标很可能不是显著的；</p><h3 id="方法设计"><a href="#方法设计" class="headerlink" title="方法设计"></a>方法设计</h3><h4 id="Residual-U-blocks"><a href="#Residual-U-blocks" class="headerlink" title="Residual U-blocks"></a>Residual U-blocks</h4><p>了解了显著性检测这个任务之后，来具体了解一下这篇文章的具体方法设计。作者设计一种Residual U-blocks，用于捕获 intra-stage 的 multi-scales 特征。</p><p><img src="/img/image-20200917201033485.png" alt="image-20200917201033485"></p><p>上图为普通卷积block，Res-like block，Inception-like block，Dense-like block和Residual U-blocks的对比图，明显可以看出Residual U-blocks是受了U-Net的启发。</p><p>Residual U-blocks有以下三部分组成：</p><blockquote><ul><li>一个输入卷积层，它将输入的feature map x (H × W × $C_{in}$)转换成中间feature map $F_1(x)$，$F_1(x)$通道数为$C_{out}$。这是一个用于局部特征提取的普通卷积层。</li><li>一个U-like的对称的encoder-decoder结构，高度为L，以中间feature map $F_1(x)$为输入，去学习提取和编码多尺度文本信息$U(F_1(x))$,U表示类U-Net结构。更大L会得到更深层的U-block（RSU），更多的池操作，更大的感受野和更丰富的局部和全局特征。配置此参数允许从具有任意空间分辨率的输入特征图中提取多尺度特征。从逐渐降采样特征映射中提取多尺度特征，并通过渐进上采样、合并和卷积等方法将其编码到高分辨率的特征图中。这一过程减少了大尺度直接上采样造成的细节损失。</li><li>一种残差连接，它通过求和来融合局部特征和多尺度特征：$F_1(x) + U(F_1(x))$。</li></ul></blockquote><p><img src="/img/image-20200917201132725.png" alt="image-20200917201132725"></p><p>RSU与Res block的主要设计区别在于RSU用U-Net结构代替了普通的单流卷积，用一个权重层(weight layer)形成的局部特征来代替原始特征。这种设计的变更使网络能够从多个尺度直接从每个残差块提取特征。更值得注意的是，U结构的计算开销很小，因为大多数操作都是在下采样的特征映射上进行的。</p><h4 id="U-2-Net的结构"><a href="#U-2-Net的结构" class="headerlink" title="U^2-Net的结构"></a>U^2-Net的结构</h4><p>U^2-Net的网络结构如下：</p><p><img src="/img/image-20200917201255253.png" alt="image-20200917201255253"></p><p>与U-Net的网络结构做一个对比：</p><p><img src="/img/1*TXfEPqTbFBPCbXYh2bstlA.png" alt="Learn How to Train U-Net On Your Dataset | by Sukriti Paul | Coinmonks |  Medium"></p><p>直观上可以发现，U^2-Net的每一个Block都是一个U-Net结构的模块，即上述Residual U-blocks。当然，你也可以继续Going Deeper, 每个Block里面的U-Net的子Block仍然可以是一个U-Net结构，命名为U^3-Net。</p><h3 id="损失函数设计"><a href="#损失函数设计" class="headerlink" title="损失函数设计"></a>损失函数设计</h3><p>类似于HED算法的deep supervision方式，作者设计了如下函数：</p><script type="math/tex; mode=display">\mathcal{L}=\sum_{m=1}^{M} w_{\text {side}}^{(m)} \ell_{\text {side}}^{(m)}+w_{\text {fuse}} \ell_{\text {fuse}}</script><p>其中，M=6, 为U2Net 的 Sup1, Sup2, …, Sup6 stage.$w_{\text {side}}^{(m)}$  $l_{\text {side}}^{(m)}$ 为对应的损失函数输出和权重；$w_{f u s e} \ell_{f u s e}$ 为融合的损失函数和权重;对于每一个$l$使用的都是标准的BCE Loss：</p><script type="math/tex; mode=display">\ell=-\sum_{(r, c)}^{(H, W)}\left[P_{G(r, c)} \log P_{S(r, c)}+\left(1-P_{G(r, c)}\right) \log \left(1-P_{S(r, c)}\right)\right]</script><h3 id="实验可视化"><a href="#实验可视化" class="headerlink" title="实验可视化"></a>实验可视化</h3><p>所提出的模型是使用DUTS-TR数据集进行训练，该数据集包含大约10000个样本图像，并使用标准数据增强技术进行扩充。研究人员在6个用于突出目标检测的基准数据集上评估了该模型：DUT-OMRON、DUTS-TE、HKU-IS、ECSSD、PASCAL-S和SOD。评价结果表明，在这6个基准点上，新模型与现有方法具有相当好的性能。</p><p><img src="/img/image-20200918110749081.png" alt="image-20200918110749081"></p><p>U^2-Net的实现是开源的，并提供了两种不同的预训练模型：U2Net(176.3M的较大模型，在GTX 1080Ti GPU上为30 FPS)，以及U2NetP(4.7M小模型，最高可达到40 FPS)。代码和预训练模型都可以在<a href="https://github.com/NathanUA/U-2-Net" target="_blank" rel="noopener">Github</a>。下面是我直接用作者开源的模型跑出来的结果，抠图效果很好，精细到发丝的那种。</p><p><img src="/img/image-20200917201523395.png" alt="image-20200917201523395"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>该论文的优势在于：</p><blockquote><ul><li>提出RSU模块，融合不同尺度感受野的特征，来捕捉不同尺度的上下文信息；</li><li>基于 RSU 模块的 池化(pooling) 操作，在不显著增加计算成本的前提下，增加了整个网络结构的深度(depth).</li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; Going Deeper with Nested U-Structure for Salient Object Detection&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/NathanUA/U-2-Net&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/NathanUA/U-2-Net&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt; 这是发表在PR2020上的一篇关于显著性检测的文章，作者是秦雪彬。从标题上可以看到本文的一个idea是设计了一个deeper的U型结构的网络解决显著性目标检测问题。作者认为，目前显著性目标检测有两种主流思路，一为多层次深层特征集成multi-level deep feature integration，一为多尺度特征提取Multi-scale feature extraction。多层次深层特征集成方法主要集中在开发更好的多层次特征聚合策略上。而多尺度特征提取这一类方法旨在设计更新的模块，从主干网获取的特征中同时提取局部和全局信息。而几乎所有上述方法，都是为了更好地利用现有的图像分类的Backbones生成的特征映射。而作者另辟蹊径，提出了一种新颖而简单的结构，它直接逐级提取多尺度特征，用于显著目标检测，而不是利用这些主干的特征来开发和添加更复杂的模块和策略。下图是该方法与其他方法的一个比较：&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="saliency detection" scheme="https://blog.nicehuster.cn/tags/saliency-detection/"/>
    
  </entry>
  
  <entry>
    <title>mmdetection详解指北 (四)</title>
    <link href="https://blog.nicehuster.cn/2020/09/08/mmdetection-4/"/>
    <id>https://blog.nicehuster.cn/2020/09/08/mmdetection-4/</id>
    <published>2020-09-08T11:16:39.000Z</published>
    <updated>2020-09-17T12:44:41.312Z</updated>
    
    <content type="html"><![CDATA[<h3 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h3><p>训练流程的包装过程大致如下:tools/train.py-&gt;apis/train.py-&gt;mmcv/runner.py-&gt;mmcv/hook.py(后面是分散的), 其中 runner 维护了数据信息,优化器, 日志系统, 训练 loop 中的各节点信息, 模型保存, 学习率等. 另外补充一点, 以上包装过程, 在 mmdet 中无处不在, 包括 mmcv 的代码也是对日常频繁使用的函数进行了统一封装。</p><a id="more"></a><h4 id="训练逻辑"><a href="#训练逻辑" class="headerlink" title="训练逻辑"></a>训练逻辑</h4><p><img src="/img/image-20200917203908378.png" alt="image-20200917203908378"></p><p>图见2, 注意它的四个层级. 代码上, 主要查看 apis/train.py, mmcv 中的runner 相关文件. 核心围绕 Runner,Hook 两个类. Runner 将模型, 批处理函数 batch_pro cessor, 优化器作为基本属性, 训练过程中与训练状态, 各节点相关的信息被记录在mode,_hooks,_epoch,_iter,_inner_iter,_max_epochs, _max_iters中，这些信息维护了训练过程中插入不同 hook 的操作方式. 理清训练流程只需看 Runner 的成员函数 run. 在 run 里会根据 mode 按配置中 workflow 的 epoch 循环调用 train 和 val 函数, 跑完所有的 epoch. 比如train:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EpochBasedRunner</span>(<span class="title">BaseRunner</span>):</span></span><br><span class="line">    <span class="string">""</span><span class="string">"Epoch-based Runner.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This runner train models epoch by epoch.</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(<span class="keyword">self</span>, data_loader, **kwargs)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.model.train()</span><br><span class="line">        <span class="keyword">self</span>.mode = <span class="string">'train'</span></span><br><span class="line">        <span class="keyword">self</span>.data_loader = data_loader</span><br><span class="line">        <span class="keyword">self</span>._max_iters = <span class="keyword">self</span>._max_epochs * len(<span class="keyword">self</span>.data_loader) <span class="comment"># 最大迭代次数</span></span><br><span class="line">        <span class="keyword">self</span>.call_hook(<span class="string">'before_train_epoch'</span>)<span class="comment"># 根据名字获取hook对象函数</span></span><br><span class="line">        time.sleep(<span class="number">2</span>)  <span class="comment"># Prevent possible deadlock during epoch transition</span></span><br><span class="line">        <span class="keyword">for</span> i, data_batch <span class="keyword">in</span> enumerate(<span class="keyword">self</span>.data_loader)<span class="symbol">:</span></span><br><span class="line">            <span class="keyword">self</span>._inner_iter = i <span class="comment"># 记录当前训练迭代次数</span></span><br><span class="line">            <span class="keyword">self</span>.call_hook(<span class="string">'before_train_iter'</span>) <span class="comment">#一个batch 前向操作开始</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">self</span>.batch_processor is <span class="symbol">None:</span></span><br><span class="line">                outputs = <span class="keyword">self</span>.model.train_step(data_batch, <span class="keyword">self</span>.optimizer,</span><br><span class="line">                                                **kwargs)</span><br><span class="line">            <span class="symbol">else:</span></span><br><span class="line">                outputs = <span class="keyword">self</span>.batch_processor(</span><br><span class="line">                    <span class="keyword">self</span>.model, data_batch, train_mode=True, **kwargs) </span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> isinstance(outputs, dict)<span class="symbol">:</span></span><br><span class="line">                raise TypeError(<span class="string">'"batch_processor()" or "model.train_step()"'</span></span><br><span class="line">                                <span class="string">' must return a dict'</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'log_vars'</span> <span class="keyword">in</span> <span class="symbol">outputs:</span></span><br><span class="line">                <span class="keyword">self</span>.log_buffer.update(outputs[<span class="string">'log_vars'</span>],</span><br><span class="line">                                       outputs[<span class="string">'num_samples'</span>])</span><br><span class="line">            <span class="keyword">self</span>.outputs = outputs</span><br><span class="line">            <span class="keyword">self</span>.call_hook(<span class="string">'after_train_iter'</span>)<span class="comment">#一个batch 前向操作结束</span></span><br><span class="line">            <span class="keyword">self</span>._iter += <span class="number">1</span> <span class="comment"># 方便resume，知道从哪次开始优化</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.call_hook(<span class="string">'after_train_epoch'</span>) <span class="comment">#一个epoch结束</span></span><br><span class="line">        <span class="keyword">self</span>._epoch += <span class="number">1</span> <span class="comment">#记录训练epoch状态，方便resume</span></span><br></pre></td></tr></table></figure><p>上面需要说明的是自定义 hook 类, 自定义 hook 类需继承 mmcv 的Hook 类, 其默认了 6+8+4 个成员函数, 也即2所示的 6 个层级节点, 外加 2*4 个区分 train 和 val 的节点记录函数, 以及 4 个边界检查函数. 从train.py 中容易看出, 在训练之前, 已经将需要的 hook 函数注册到 Runner的 self._hook 中了, 包括从配置文件解析的优化器, 学习率调整函数, 模型保存, 一个 batch 的时间记录等 (注册 hook 算子在 self._hook 中按优先级升序排列). 这里的 call_hook 函数定义如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseRunner</span><span class="params">(metaclass=ABCMeta)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call_hook</span><span class="params">(self, fn_name)</span>:</span></span><br><span class="line">        <span class="string">"""Call all hooks.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            fn_name (str): The function name in each hook to be called, such as</span></span><br><span class="line"><span class="string">                "before_train_epoch".</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> hook <span class="keyword">in</span> self._hooks:</span><br><span class="line">            getattr(hook, fn_name)(self)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>容易看出, 在训练的不同节点, 将从注册列表中调用实现了该节点函数的类成员函数. 比如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@HOOKS.register_module()</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OptimizerHook</span><span class="params">(Hook)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, grad_clip=None)</span>:</span></span><br><span class="line">        self.grad_clip = grad_clip</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">clip_grads</span><span class="params">(self, params)</span>:</span></span><br><span class="line">        params = list(</span><br><span class="line">            filter(<span class="keyword">lambda</span> p: p.requires_grad <span class="keyword">and</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, params))</span><br><span class="line">        <span class="keyword">if</span> len(params) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> clip_grad.clip_grad_norm_(params, **self.grad_clip)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">after_train_iter</span><span class="params">(self, runner)</span>:</span></span><br><span class="line">        runner.optimizer.zero_grad()</span><br><span class="line">        runner.outputs[<span class="string">'loss'</span>].backward()</span><br><span class="line">        <span class="keyword">if</span> self.grad_clip <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            grad_norm = self.clip_grads(runner.model.parameters())</span><br><span class="line">            <span class="keyword">if</span> grad_norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="comment"># Add grad norm to the logger</span></span><br><span class="line">                runner.log_buffer.update(&#123;<span class="string">'grad_norm'</span>: float(grad_norm)&#125;,</span><br><span class="line">                                         runner.outputs[<span class="string">'num_samples'</span>])</span><br><span class="line">        runner.optimizer.step()</span><br></pre></td></tr></table></figure><p>将在每个 train_iter 后实现反向传播和参数更新。学习率优化相对复杂一点, 其基类 LrUpdaterHook, 实现了 before_run,before_train_epoch, before_train_iter 三个 hook 函数, 意义自明. . 这里选一个余弦式变化, 稍作说明:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">annealing_cos</span><span class="params">(start, end, factor, weight=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Calculate annealing cos learning rate.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Cosine anneal from `weight * start + (1 - weight) * end` to `end` as</span></span><br><span class="line"><span class="string">    percentage goes from 0.0 to 1.0.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        start (float): The starting learning rate of the cosine annealing.</span></span><br><span class="line"><span class="string">        end (float): The ending learing rate of the cosine annealing.</span></span><br><span class="line"><span class="string">        factor (float): The coefficient of `pi` when calculating the current</span></span><br><span class="line"><span class="string">            percentage. Range from 0.0 to 1.0.</span></span><br><span class="line"><span class="string">        weight (float, optional): The combination factor of `start` and `end`</span></span><br><span class="line"><span class="string">            when calculating the actual starting learning rate. Default to 1.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    cos_out = cos(pi * factor) + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> end + <span class="number">0.5</span> * weight * (start - end) * cos_out</span><br><span class="line"></span><br><span class="line"><span class="meta">@HOOKS.register_module()</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CosineAnnealingLrUpdaterHook</span><span class="params">(LrUpdaterHook)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, min_lr=None, min_lr_ratio=None, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> (min_lr <span class="keyword">is</span> <span class="keyword">None</span>) ^ (min_lr_ratio <span class="keyword">is</span> <span class="keyword">None</span>)</span><br><span class="line">        self.min_lr = min_lr</span><br><span class="line">        self.min_lr_ratio = min_lr_ratio</span><br><span class="line">        super(CosineAnnealingLrUpdaterHook, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_lr</span><span class="params">(self, runner, base_lr)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.by_epoch:</span><br><span class="line">            progress = runner.epoch</span><br><span class="line">            max_progress = runner.max_epochs</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            progress = runner.iter</span><br><span class="line">            max_progress = runner.max_iters</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.min_lr_ratio <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            target_lr = base_lr * self.min_lr_ratio</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            target_lr = self.min_lr</span><br><span class="line">        <span class="keyword">return</span> annealing_cos(base_lr, target_lr, progress / max_progress)</span><br></pre></td></tr></table></figure><p>从 get_lr 可以看到, 学习率变换周期有两种,epoch-&gt;max_epoch, 或者更大的 iter-&gt;max_iter, 后者表明一个 epoch 内不同 batch 的学习率可以不同, 因为没有什么理论, 所有这两种方式都行. 其中 base_lr 为初始学习率,target_lr 为学习率衰减的上界, 而当前学习率即为返回值.</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;训练流程&quot;&gt;&lt;a href=&quot;#训练流程&quot; class=&quot;headerlink&quot; title=&quot;训练流程&quot;&gt;&lt;/a&gt;训练流程&lt;/h3&gt;&lt;p&gt;训练流程的包装过程大致如下:tools/train.py-&amp;gt;apis/train.py-&amp;gt;mmcv/runner.py-&amp;gt;mmcv/hook.py(后面是分散的), 其中 runner 维护了数据信息,优化器, 日志系统, 训练 loop 中的各节点信息, 模型保存, 学习率等. 另外补充一点, 以上包装过程, 在 mmdet 中无处不在, 包括 mmcv 的代码也是对日常频繁使用的函数进行了统一封装。&lt;/p&gt;
    
    </summary>
    
      <category term="mmdetection" scheme="https://blog.nicehuster.cn/categories/mmdetection/"/>
    
    
      <category term="mmdetection" scheme="https://blog.nicehuster.cn/tags/mmdetection/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：Delving Deeper into Anti-aliasing in ConvNets</title>
    <link href="https://blog.nicehuster.cn/2020/09/06/Adaptive-anti-Aliasing/"/>
    <id>https://blog.nicehuster.cn/2020/09/06/Adaptive-anti-Aliasing/</id>
    <published>2020-09-06T11:13:39.000Z</published>
    <updated>2020-09-16T09:01:33.472Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> Xueyan Zou,UC Davis,NVIDIA<br><strong>代码链接：</strong><a href="https://github.com/MaureenZOU/" target="_blank" rel="noopener">https://github.com/MaureenZOU/</a><br><strong>整体框架：</strong>这篇文章是前几天BMVC2020获得best paper award 的一篇文章，这篇文章提出了一个plugin  module ，用来提高CNN的鲁棒性。在图像分类、图像分割、目标检测等任务上都能带来1+个点的提升。对于cnn，众所周知存在一个比较明显的缺陷：即使图像移动几个pixel都可能导致分类识别任务结果发生改变，作者发现其中重要原因在于网络中被大量用来降低参数量的下采样层导致混叠(aliasing)问题，即高频信号在采样后退化为完全不同的部分现象。为此提出了这样一个content-aware anti-aliasing的模块，用于缓解下采样过程中带来的高频信息的退化问题。</p><h3 id="下采样的问题"><a href="#下采样的问题" class="headerlink" title="下采样的问题"></a>下采样的问题</h3><p>以一维信号的的降采样为例：</p><script type="math/tex; mode=display">\begin{array}{l}001100110011 \frac{\mathrm{k}=2, \text { stride }=2}{\operatorname{maxpool}} 010101 \\011001100110 \frac{\mathrm{k}=2, \text { stride }=2}{\operatorname{maxpool}} 111111\end{array}</script><a id="more"></a><p>对于k=2,stride=2的maxpool操作而言，对输入信号移动一位数字，经过maxpool之后的输出结果是截然不同的。而在CNN中大量使用降采样来降低参数量，这种aliasing问题更为明显，标准解决方案是在下采样之前应用低通滤波器（例如，高斯模糊）。但是，在整个内容上应用相同的过滤器可能不是最佳选择，因为特征的频率可能会在<strong>空间位置</strong>和<strong>特征通道</strong>之间发生变化。可以看下作者在论文中给出的实验结果：</p><p><img src="/img/image-20200916161719818.png" alt="image-20200916161719818" style="zoom:50%;"></p><p>上图(a)输入图片；(b)直接4x下采样；(c)应用经过调整以匹配噪声频率的单个高斯滤波器后的下采样结果;(d)应用多个空间自适应高斯滤波后的下采样结果（具有很强的背景模糊和边界弱化能力）</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p><img src="/img/image-20200916161851654.png" alt="image-20200916161851654" style="zoom:50%;"></p><p>作者针对空间和通道分别生成低通滤波器用于缓解aliasing问题。基于空间自适应的低通滤波器就是一个简单的分组卷积操作：</p><script type="math/tex; mode=display">Y_{i, j}=\sum_{p, q \in \Omega} w_{i, j}^{p, q} \cdot X_{i+p, j+q}</script><p>在论文中也有提及，为了避免权重为负数，作者取了一个softmax操作。</p><p>之后对上面生成的权重进行分组再和输入进行一次卷积操作，最终输出Y：</p><script type="math/tex; mode=display">Y_{i, j}^{g}=\sum_{p, q \in \Omega} w_{i, j, g}^{p, q} \cdot X_{i+p, j+q}^{c}</script><p>具体可以看下作者放出来的代码：</p><blockquote><p><a href="https://github.com/MaureenZOU/Adaptive-anti-Aliasing/blob/master/models_lpf/layers/pasa.py" target="_blank" rel="noopener">Adaptive-anti-Aliasing/models_lpf/layers/pasa.py</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Downsample_PASA_group_softmax</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, kernel_size, stride=<span class="number">1</span>, pad_type=<span class="string">'reflect'</span>, group=<span class="number">2</span>)</span>:</span></span><br><span class="line">        super(Downsample_PASA_group_softmax, self).__init__()</span><br><span class="line">        self.pad = get_pad_layer(pad_type)(kernel_size//<span class="number">2</span>)</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.kernel_size = kernel_size</span><br><span class="line">        self.group = group</span><br><span class="line"></span><br><span class="line">        self.conv = nn.Conv2d(in_channels, group*kernel_size*kernel_size, kernel_size=kernel_size, stride=<span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.bn = nn.BatchNorm2d(group*kernel_size*kernel_size)</span><br><span class="line">        self.softmax = nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">        nn.init.kaiming_normal_(self.conv.weight, mode=<span class="string">'fan_out'</span>, nonlinearity=<span class="string">'relu'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        sigma = self.conv(self.pad(x))</span><br><span class="line">        sigma = self.bn(sigma)</span><br><span class="line">        sigma = self.softmax(sigma)</span><br><span class="line"></span><br><span class="line">        n,c,h,w = sigma.shape</span><br><span class="line"></span><br><span class="line">        sigma = sigma.reshape(n,<span class="number">1</span>,c,h*w)</span><br><span class="line"></span><br><span class="line">        n,c,h,w = x.shape</span><br><span class="line">        x = F.unfold(self.pad(x), kernel_size=self.kernel_size).reshape((n,c,self.kernel_size*self.kernel_size,h*w))</span><br><span class="line"></span><br><span class="line">        n,c1,p,q = x.shape</span><br><span class="line">        x = x.permute(<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>).reshape(self.group, c1//self.group, n, p, q).permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        n,c2,p,q = sigma.shape</span><br><span class="line">        sigma = sigma.permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>).reshape((p//(self.kernel_size*self.kernel_size), self.kernel_size*self.kernel_size,n,c2,q)).permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        x = torch.sum(x*sigma, dim=<span class="number">3</span>).reshape(n,c1,h,w)</span><br><span class="line">        <span class="keyword">return</span> x[:,:,torch.arange(h)%self.stride==<span class="number">0</span>,:][:,:,:,torch.arange(w)%self.stride==<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>作者分别在分类，检测，分割任务上均有验证其有效性，均能提高1个点左右。具体实验结果可以看原论文，这里就不贴上来了。</p><h3 id="一致性指标"><a href="#一致性指标" class="headerlink" title="一致性指标"></a>一致性指标</h3><p>作者为了验证该方法的有效性，针对不同任务提出了一系列的一致性指标。比如针对分类任务的一致性指标计算如下：</p><script type="math/tex; mode=display">\text { Consistency }=\mathbb{E}_{X, h_{1}, w_{1}, h_{2}, w_{2}} \mathbb{I}\left\{F\left(X_{h_{1}, w_{1}}\right)=F\left(X_{h_{2}, w_{2}}\right)\right\}</script><p>X表示输入图像，$h_{1}, w_{1}, h_{2}, w_{2}$ 表示偏移量。$F(\cdot)$ 表示模型输出的top1的类别标签。具体代码实现可以看下面：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">output0 = model(input[:,:,off0[<span class="number">0</span>]:off0[<span class="number">0</span>]+args.size,off0[<span class="number">1</span>]:off0[<span class="number">1</span>]+args.size])</span><br><span class="line">output1 = model(input[:,:,off1[<span class="number">0</span>]:off1[<span class="number">0</span>]+args.size,off1[<span class="number">1</span>]:off1[<span class="number">1</span>]+args.size])</span><br><span class="line">cur_agree = agreement_correct(output0, output1, target).type(torch.FloatTensor).to(output0.device)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agreement_correct</span><span class="params">(output0, output1, target)</span>:</span></span><br><span class="line">    pred0 = output0.argmax(dim=<span class="number">1</span>, keepdim=<span class="keyword">False</span>)</span><br><span class="line">    pred1 = output1.argmax(dim=<span class="number">1</span>, keepdim=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    agree = pred0.eq(pred1)</span><br><span class="line">    agree_target_pred0 = pred0.eq(target)</span><br><span class="line">    agree_target_pred1 = pred1.eq(target)</span><br><span class="line"></span><br><span class="line">    correct_or = (agree_target_pred0 + agree_target_pred1) &gt; <span class="number">0</span></span><br><span class="line">    agree = agree * correct_or</span><br><span class="line"></span><br><span class="line">    agree = <span class="number">100.</span>*(torch.sum(agree).float() / (torch.sum(correct_or).float() + <span class="number">1e-10</span>)).to(output0.device)</span><br><span class="line">    <span class="keyword">return</span> agree</span><br></pre></td></tr></table></figure><p><code>agreement_correct</code> 是统计一致性指标的函数，具体可看出来统计方法是分别对输入图像随机偏移<code>off0</code>和<code>off1</code> 然后统计在该偏移量下，两者输出一致且和gt一致的比例。对于检测分割任务，作者也提出了相应的一致性指标计算方法：mAISC和mAISC这两个指标。具体计算如下图。</p><p><img src="/img/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_16002437758073.png" alt="企业微信截图_16002437758073"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; Xueyan Zou,UC Davis,NVIDIA&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/MaureenZOU/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/MaureenZOU/&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体框架：&lt;/strong&gt;这篇文章是前几天BMVC2020获得best paper award 的一篇文章，这篇文章提出了一个plugin  module ，用来提高CNN的鲁棒性。在图像分类、图像分割、目标检测等任务上都能带来1+个点的提升。对于cnn，众所周知存在一个比较明显的缺陷：即使图像移动几个pixel都可能导致分类识别任务结果发生改变，作者发现其中重要原因在于网络中被大量用来降低参数量的下采样层导致混叠(aliasing)问题，即高频信号在采样后退化为完全不同的部分现象。为此提出了这样一个content-aware anti-aliasing的模块，用于缓解下采样过程中带来的高频信息的退化问题。&lt;/p&gt;
&lt;h3 id=&quot;下采样的问题&quot;&gt;&lt;a href=&quot;#下采样的问题&quot; class=&quot;headerlink&quot; title=&quot;下采样的问题&quot;&gt;&lt;/a&gt;下采样的问题&lt;/h3&gt;&lt;p&gt;以一维信号的的降采样为例：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{array}{l}
001100110011 \frac{\mathrm{k}=2, \text { stride }=2}{\operatorname{maxpool}} 010101 \\
011001100110 \frac{\mathrm{k}=2, \text { stride }=2}{\operatorname{maxpool}} 111111
\end{array}&lt;/script&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="classification" scheme="https://blog.nicehuster.cn/tags/classification/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：Minimum Delay Object Detection from Video</title>
    <link href="https://blog.nicehuster.cn/2020/09/05/minidelay-detection/"/>
    <id>https://blog.nicehuster.cn/2020/09/05/minidelay-detection/</id>
    <published>2020-09-05T11:13:39.000Z</published>
    <updated>2020-09-15T06:28:52.040Z</updated>
    
    <content type="html"><![CDATA[<p>最近逛知乎上了解到<strong>低延迟目标检测</strong>这个方向，这个方向解决的是视觉任务工程化过程中存在一个痛点问题，如何解决延迟和误报的情况。拿视频中的目标检测问题来讲，我们通常会使用一个单帧检测器来检测视频中每一帧存在目标情况，静态上看可能会存在一些误检漏检情况，动态上表现为检测到的目标一闪一闪的情况，导致视觉效果很不友好。而低延迟目标检测的任务就是通过贝叶斯概率建模的方式融合多帧信息来解决这样的问题。</p><p>总体来说，对于任何检测任务来说，延迟和误报存在如下图的关系：</p><p><img src="/img/v2-2c0d4736cf5fde58e2b58530d7643006_1440w.jpg" alt="img"></p><a id="more"></a><p>这个关系不难理解，不只是视觉问题，世间万物，更长的决策过程（delay）往往能带来更高的准确度，但是这个更长的决策过程也会带来更大的延迟。两者之间的平衡，对很多需要在线决策（online process）的系统来说非常重要。例如生物视觉，假定一个动物检测到掠食者就需要逃跑，如果追求低误报率，就要承担高延迟带来的风险，有可能检测到掠食者时为时已晚无法逃脱；如果追求低延迟，虽然相对安全，但是误报率高，有一点风吹草动就犹如惊弓之鸟。</p><h3 id="低延迟检测思路"><a href="#低延迟检测思路" class="headerlink" title="低延迟检测思路"></a>低延迟检测思路</h3><p>在视频物体检测中，如果使用上一帧的检测结果作为先验，将下一帧的检测结果输入贝叶斯框架，输出后验，那么总体来说，这个后验结果融合了两帧的信息，会比单帧更准。在这个思路下，理论上来说使用的帧数越多，检测越准。如以下单帧和多帧的对比：</p><p><img src="/img/v2-fcf08710643bcb172cb661c687f36fe5_b.webp" alt="img"></p><p>但是同时更多的帧数会造成更长的延迟（延迟 := 物体被检测到的时刻 - 物体出现的时刻）。如何在保证物体检测精度的情况下，尽量降低延迟呢？我们参照QD理论进行如下建模：</p><p>假设一个物体在时刻$t_s$出现在视频中，在 $t_e$离开视频，则这个物体的移动轨迹可以用时序上的一组检测框$b_{t_s,t_e}=(b_{t_s},b_{t_s+1},…,b_{t_e})$表示。这样的一组检测框检测框序列，在目标追踪（Data association / tracking）领域被一些人称作tracklet。简单说，我们的算法目标是以低延迟判断检测框序列内是否含有物体。因此，我们称这样一组检测框序列为一个candidate。在quickest change detection框架下， 可以用如下似然比检验判断$t$时刻物体是否出现在该candidate内：</p><script type="math/tex; mode=display">\begin{aligned}\Lambda_{t}\left(b_{1, t}\right) &=\max _{i} \frac{\mathrm{p}\left(\Gamma_{0, i}<t \mid D_{1, t}, b_{1, t}\right)}{\mathrm{p}\left(\Gamma_{0, i} \geq t \mid D_{1, t}, b_{1, t}\right)} \\&=\max _{i} \max _{t_{c} \geq 1} \frac{\mathrm{p}_{i}\left(D_{t_{c}, t} \mid b_{t_{c}, t}\right)}{\mathrm{p}_{0}\left(D_{t_{c}, t} \mid b_{t_{c}, t}\right)}\end{aligned}</script><p>其中$D_t$代表一个单帧检测器在$I_t$上的检测结果，$T_{0,i}$代表candidate中的内容从背景变为第类$i$物体（如行人）这一事件发生的时刻,$p_i(\bullet )=p(\bullet |l=l_i)$ 代表给定类别$i$的时候，$\bullet$ 事件发生的概率。由条件概率的独立性与,$p_i(\bullet |b_t)$类别$i$和检测框独$b_t$立，继而时序上各时刻检测结果的联合概率变成各时刻概率的连乘：</p><script type="math/tex; mode=display">\Lambda_{t}\left(b_{1, t}\right)=\max _{i} \max _{t_{c} \geq 1} \prod_{j=t_{c}}^{t} \frac{\mathrm{p}_{i}\left(D_{j} \mid b_{j}\right)}{\mathrm{p}_{0}\left(D_{j} \mid b_{j}\right)}</script><p>这里需要明确一下，上边公式中的条件概率并非简单的检测器输出的结果，具体如何计算$p$需要一套比较复杂的建模。由于这里只介绍低延迟检测的整体思路，关于$p$的建模待我有空时会附在文末，有兴趣的朋友可以直接去论文查阅。总之，我们可以对这个似然比取阈值，进行检测。阈值越高，结果越准，但是延迟越大，反之同理。由QD理论中递归算法（CuSum算法），我们可以对上述似然比取log，记为W。最终的检测流程可以参照如下框图。</p><p><img src="/img/v2-bd434876b196828b2db7d14ea5eb8c52_1440w.jpg" alt></p><p>整体流程为：</p><blockquote><ul><li>（1）将已有但似然比未超过阈值的candidate做tracking进入下一帧；</li><li>（2）在下一帧进行单帧检测，生成新的检测框，与前一帧tracking后的检测框合并到一起；</li><li>（3）对这些candidate进行似然比检验，W超出阈值则输出检测结果，W小于零则去除该检测框，W大于零小于阈值则回到（1），进入下一帧。</li></ul></blockquote><p>这样一个检测框架，可以与<strong>任何</strong>单帧检测器结合。</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>以上算法的具体实现过程,作者开源了一个简单的python实现，git地址在<a href="https://github.com/donglao/mindelay/blob/master/detection.py" target="_blank" rel="noopener">这里</a>。这里主要看下核心的代码：</p><p>detection.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">result = toolbox.initialize_result(num_cat) <span class="comment"># 第一步，外循环，根据类别初始化result</span></span><br><span class="line"></span><br><span class="line">result = association.update(result, result_det) <span class="comment">#使用当前帧信息更新历史轨迹，result保存的是历史帧的检测信息，result_det是当前帧的检测信息</span></span><br><span class="line"></span><br><span class="line">result = toolbox.combine_result(result, result_det, <span class="number">0.5</span>) <span class="comment">#对结合的轨迹信息和当前帧信息就行结合，输出最终需要alarm的检测结果；</span></span><br></pre></td></tr></table></figure><p>其中association.update的具体代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> mindelay.toolbox.IoU <span class="keyword">as</span> IoU</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_traj</span><span class="params">(old, det)</span>:</span></span><br><span class="line">    prior = <span class="number">0.5</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(old.shape[<span class="number">0</span>]):</span><br><span class="line">        a = old[i, <span class="number">0</span>:<span class="number">4</span>] * prior</span><br><span class="line">        b = prior</span><br><span class="line">        l = prior * <span class="number">0.5</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(det.shape[<span class="number">0</span>]):</span><br><span class="line">            weight = (IoU(old[i, <span class="number">0</span>:<span class="number">4</span>], det[j, <span class="number">0</span>:<span class="number">4</span>])&gt;<span class="number">0.5</span>)*IoU(old[i, <span class="number">0</span>:<span class="number">4</span>], det[j, <span class="number">0</span>:<span class="number">4</span>])</span><br><span class="line">        <span class="comment"># The bounding box update is here. I've tried different methods in the original Matlab code. </span></span><br><span class="line">        <span class="comment"># But here I just use the previous frame as the guidance of current frame and leave it as-is.</span></span><br><span class="line">        <span class="comment"># I will combine it with trackers (some work to be done!) and update in later version of the code. </span></span><br><span class="line">            a = a + weight * det[j, <span class="number">0</span>:<span class="number">4</span>]</span><br><span class="line">            b = b + weight</span><br><span class="line">            l = l + weight * det[j, <span class="number">4</span>]</span><br><span class="line">        old[i, <span class="number">0</span>:<span class="number">4</span>] = a / b</span><br><span class="line">        l = l / b</span><br><span class="line"></span><br><span class="line">        temp_lr = np.log(l + <span class="number">0.25</span>) - np.log((<span class="number">1</span> - l) + <span class="number">0.25</span>) + old[i, <span class="number">4</span>] - <span class="number">2.5</span>/b</span><br><span class="line">        <span class="comment"># the +0.25 is making the output smoother. If the output of the detector is not ideal you may want to tune it.</span></span><br><span class="line">        <span class="comment"># the 2.5/b is a prior. Instead of setting a fixed prior I am trying to make it adaptive. Feel free to play with it! </span></span><br><span class="line">    </span><br><span class="line">        old[i, <span class="number">4</span>] = max(temp_lr, <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> old</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(result, result_det)</span>:</span></span><br><span class="line">    n = result.__len__()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        result[i] = update_traj(np.array(result[i]), np.array(result_det[i]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br></pre></td></tr></table></figure><p>其中，toolbox.combine_result的实现比较简单，对经过更新的轨迹信息和当前帧信息进行combine后经nms处理一下，代码如下：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def combine_result(result, result_det, thre_nms):</span><br><span class="line">    n = result.__len__()</span><br><span class="line">    output = np.empty((n,), dtype = np.object)</span><br><span class="line">    for i in range(n):</span><br><span class="line">        if result_det<span class="string">[i]</span>.shape<span class="string">[0]</span> &gt; <span class="number">0</span>:</span><br><span class="line">            temp_lr = np.log(result_det<span class="string">[i]</span><span class="string">[:,4]</span>+<span class="number">0</span>.<span class="number">25</span>) - np.log(<span class="number">1</span> - result_det<span class="string">[i]</span><span class="string">[:,4]</span>+<span class="number">0</span>.<span class="number">25</span>)</span><br><span class="line">            temp_lr<span class="string">[temp_lr &lt; 0]</span> = <span class="number">0</span></span><br><span class="line">            result_det<span class="string">[i]</span><span class="string">[:, 4]</span> = temp_lr</span><br><span class="line">        tmp = np.vstack((result<span class="string">[i]</span>, result_det<span class="string">[i]</span>))</span><br><span class="line">        keep = py_cpu_nms(tmp,thre_nms)</span><br><span class="line">        #print(keep,thre_nms,tmp<span class="string">[keep]</span>)</span><br><span class="line">        output<span class="string">[i]</span> = tmp<span class="string">[keep]</span></span><br><span class="line"></span><br><span class="line">    return output</span><br></pre></td></tr></table></figure><p>以上理论内容转自：<a href="https://zhuanlan.zhihu.com/p/212842916" target="_blank" rel="noopener">计算机视觉中低延迟检测的相关理论和应用</a>；</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近逛知乎上了解到&lt;strong&gt;低延迟目标检测&lt;/strong&gt;这个方向，这个方向解决的是视觉任务工程化过程中存在一个痛点问题，如何解决延迟和误报的情况。拿视频中的目标检测问题来讲，我们通常会使用一个单帧检测器来检测视频中每一帧存在目标情况，静态上看可能会存在一些误检漏检情况，动态上表现为检测到的目标一闪一闪的情况，导致视觉效果很不友好。而低延迟目标检测的任务就是通过贝叶斯概率建模的方式融合多帧信息来解决这样的问题。&lt;/p&gt;
&lt;p&gt;总体来说，对于任何检测任务来说，延迟和误报存在如下图的关系：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/v2-2c0d4736cf5fde58e2b58530d7643006_1440w.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://blog.nicehuster.cn/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>mmdetection详解指北 (三)</title>
    <link href="https://blog.nicehuster.cn/2020/09/04/mmdetection-3/"/>
    <id>https://blog.nicehuster.cn/2020/09/04/mmdetection-3/</id>
    <published>2020-09-04T11:15:39.000Z</published>
    <updated>2020-09-18T07:19:41.114Z</updated>
    
    <content type="html"><![CDATA[<h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>数据处理可能是炼丹师接触最为密集的了，因为通常情况，除了数据的离线处理，写个数据类，就可以炼丹了。但本节主要涉及数据的在线处理，更进一步应该是检测分割数据的 pytorch 处理方式。虽然 mmdet 将常用的数据都实现了，而且也实现了中间通用数据格式，但，这和模型，损失函数，性能评估的实现也相关，比如你想把官网的 centernet 完整的改成 mmdet风格，就能看到 (看起来没必要)。</p><a id="more"></a><h4 id="CustomDataset"><a href="#CustomDataset" class="headerlink" title="CustomDataset"></a>CustomDataset</h4><p>看看配置文件，数据相关的有 data dict，里面包含了 train,val,test 的路径信息，用于数据类初始化, 有 pipeline，将各个函数及对应参数以字典形式放到列表里，是对 pytorch 原装的 transforms+compose，在检测，分割相关数据上的一次封装，使得形式更加统一。</p><p>从 builder.py 中 build_dataset 函数能看到，构建数据有三种方式，ConcatDataset，RepeatDataset 和从注册器中提取。其中 dataset_wrappers.py中 ConcatDataset 和 RepeatDataset 意义自明，前者继承自 pytorch 原始的ConcatDataset，将多个数据集整合到一起，具体为把不同序列（ 可参考<a href="https://docs.python.org/zh-cn/3/library/collections.abc.html" target="_blank" rel="noopener">容器的抽象基类</a>) 的长度相加<strong>getitem</strong> 函数对应 index 替换一下。后者就是单个数据类 (序列) 的多次重复。就功能来说，前者提高数据丰富度，后者可解决数据太少使得 loading 时间长的问题 (见代码注释)。而被注册的数据类在 datasets 下一些熟知的数据名文件中。其中，基类为 custom.py 中的 CustomDataset，coco 继承自它，cityscapes 继承自 coco，xml_style 的XMLDataset 继承 CustomDataset，然后 wider_face，voc 均继承自 XMLDataset。因此这里先分析一下CustomDataset。</p><p>CustomDataset 记录数据路径等信息，解析标注文件，将每一张图的所有信息以字典作为数据结构存在 results 中，然后进入pipeline: 数据增强相关操作，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@DATASETS.register_module()</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(...)</span>:</span></span><br><span class="line">        ...</span><br><span class="line">        self.pipeline = Compose(pipeline)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pre_pipeline</span><span class="params">(self, results)</span>:</span></span><br><span class="line">        <span class="string">"""Prepare results dict for pipeline."""</span></span><br><span class="line">        results[<span class="string">'img_prefix'</span>] = self.img_prefix</span><br><span class="line">        results[<span class="string">'seg_prefix'</span>] = self.seg_prefix</span><br><span class="line">        results[<span class="string">'proposal_file'</span>] = self.proposal_file</span><br><span class="line">        results[<span class="string">'bbox_fields'</span>] = []</span><br><span class="line">        results[<span class="string">'mask_fields'</span>] = []</span><br><span class="line">        results[<span class="string">'seg_fields'</span>] = []</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare_train_img</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="string">"""Get training data and annotations after pipeline.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            idx (int): Index of data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            dict: Training data and annotation after pipeline with new keys \</span></span><br><span class="line"><span class="string">                introduced by pipeline.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        img_info = self.data_infos[idx]</span><br><span class="line">        ann_info = self.get_ann_info(idx)</span><br><span class="line">        <span class="comment"># 基本信息，初始化字典</span></span><br><span class="line">        results = dict(img_info=img_info, ann_info=ann_info)</span><br><span class="line">        <span class="keyword">if</span> self.proposals <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            results[<span class="string">'proposals'</span>] = self.proposals[idx]</span><br><span class="line">        self.pre_pipeline(results)</span><br><span class="line">        <span class="keyword">return</span> self.pipeline(results) <span class="comment"># 数据增强等</span></span><br></pre></td></tr></table></figure><p>这里数据结构的选取需要注意一下，字典结构，在数据增强库 albu 中也是如此处理，因此可以快速替换为 albu 中的算法。另外每个数据类增加了各自的 evaluate 函数。evaluate 基础函数在 mmdet.core.evaluation 中，后做补充。</p><p>mmdet 的数<strong>据处理，字典结构，pipeline，evaluate</strong> 是三个关键部分。其他所有类的文件解析部分，数据筛选等，看看即可。因为我们知道，pytorch读取数据，是将序列转化为迭代器后进行 io 操作，所以在 dataset 下除了pipelines 外还有 loader 文件夹，里面实现了分组，分布式分组采样方法，以及调用了 mmcv 中的 collate 函数 (此处为 1.x 版本，2.0 版本将 loader移植到了 builder.py 中)，且 build_dataloader 封装的 DataLoader 最后在train_detector 中被调用，这部分将在后面补充，这里说说 pipelines。</p><h4 id="data-config"><a href="#data-config" class="headerlink" title="data_config"></a>data_config</h4><p>返回 maskrcnn 的配置文件 (1.x,2.0 看 base config)，可以看到训练和测试的不同之处：LoadAnnotations，MultiScaleFlipAug，DefaultFormatBundle 和 Collect。额外提示，虽然测试没有 LoadAnnotations，根据 CustomDataset 可知，它仍需标注文件，这和 inference 的 pipeline 不同，也即这里的 test 实为 evaluate。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 序列中的dict可以随意删减，增加，属于数据增强调参内容</span></span><br><span class="line">train_pipeline = [</span><br><span class="line">    dict(type=<span class="string">'LoadImageFromFile'</span>),</span><br><span class="line">    dict(type=<span class="string">'LoadAnnotations'</span>, with_bbox=<span class="keyword">True</span>),</span><br><span class="line">    dict(type=<span class="string">'Resize'</span>, img_scale=(<span class="number">1333</span>, <span class="number">800</span>), keep_ratio=<span class="keyword">True</span>),</span><br><span class="line">    dict(type=<span class="string">'RandomFlip'</span>, flip_ratio=<span class="number">0.5</span>),</span><br><span class="line">    dict(type=<span class="string">'Normalize'</span>, **img_norm_cfg),</span><br><span class="line">    dict(type=<span class="string">'Pad'</span>, size_divisor=<span class="number">32</span>),</span><br><span class="line">    dict(type=<span class="string">'DefaultFormatBundle'</span>),</span><br><span class="line">    dict(type=<span class="string">'Collect'</span>, keys=[<span class="string">'img'</span>, <span class="string">'gt_bboxes'</span>, <span class="string">'gt_labels'</span>]),</span><br><span class="line">]</span><br><span class="line">test_pipeline = [</span><br><span class="line">    dict(type=<span class="string">'LoadImageFromFile'</span>),</span><br><span class="line">    dict(</span><br><span class="line">        type=<span class="string">'MultiScaleFlipAug'</span>,</span><br><span class="line">        img_scale=(<span class="number">1333</span>, <span class="number">800</span>),</span><br><span class="line">        flip=<span class="keyword">False</span>,</span><br><span class="line">        transforms=[</span><br><span class="line">            dict(type=<span class="string">'Resize'</span>, keep_ratio=<span class="keyword">True</span>),</span><br><span class="line">            dict(type=<span class="string">'RandomFlip'</span>),</span><br><span class="line">            dict(type=<span class="string">'Normalize'</span>, **img_norm_cfg),</span><br><span class="line">            dict(type=<span class="string">'Pad'</span>, size_divisor=<span class="number">32</span>),</span><br><span class="line">            dict(type=<span class="string">'ImageToTensor'</span>, keys=[<span class="string">'img'</span>]),</span><br><span class="line">            dict(type=<span class="string">'Collect'</span>, keys=[<span class="string">'img'</span>]),</span><br><span class="line">        ])</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>最后这些所有操作被 Compose 串联起来，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@PIPELINES.register_module()</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Compose</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, transforms)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(transforms, collections.abc.Sequence) <span class="comment">#列表是序列结构</span></span><br><span class="line">        self.transforms = []</span><br><span class="line">        <span class="keyword">for</span> transform <span class="keyword">in</span> transforms:</span><br><span class="line">            <span class="keyword">if</span> isinstance(transform, dict):</span><br><span class="line">                transform = build_from_cfg(transform, PIPELINES)</span><br><span class="line">                self.transforms.append(transform)</span><br><span class="line">            <span class="keyword">elif</span> callable(transform):</span><br><span class="line">                self.transforms.append(transform)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> TypeError(<span class="string">'transform must be callable or a dict'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> self.transforms:</span><br><span class="line">            data = t(data)</span><br><span class="line">            <span class="keyword">if</span> data <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        format_string = self.__class__.__name__ + <span class="string">'('</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> self.transforms:</span><br><span class="line">            format_string += <span class="string">'\n'</span></span><br><span class="line">            format_string += <span class="string">f'    <span class="subst">&#123;t&#125;</span>'</span></span><br><span class="line">        format_string += <span class="string">'\n)'</span></span><br><span class="line">        <span class="keyword">return</span> format_string</span><br></pre></td></tr></table></figure><p>上面代码能看到，配置文件中 pipeline 中的字典传入 build_from_cfg 函数，逐一实现了各个增强类 (方法)。扩展的增强类均需实现 <strong>call</strong> 方法，这和 pytorch 原始方法是一致的。有了以上认识，重新梳理一下 pipelines 的逻辑，由三部分组成，load，transforms，和 format。load 相关的 LoadImageFromFile，LoadAnnotations都是字典 results 进去，字典 results 出来。具体代码看下便知，LoadImageFromFile 增加了’filename’，’img’，’img_shape’，’ori_shape’,’pad_shape’,’scale_factor’,’img_norm_cfg’ 字段。其中 img 是 numpy 格式。LoadAnnotations 从 results[’ann_info’] 中解析出 bboxs,masks,labels 等信息。注意 coco 格式的原始解析来自 pycocotools，包括其评估方法，这里关键是字典结构 (这个和模型损失函数，评估等相关，统一结构，使得代码统一)。transforms 中的类作用于字典的 values，也即数据增强。format 中的 DefaultFormatBundle 是将数据转成 mmcv 扩展的容器类格式 DataContainer。另外 Collect 会根据不同任务的不同配置，从 results 中选取只含 keys 的信息生成新的字典，具体看下该类帮助文档。</p><h4 id="DataContainer"><a href="#DataContainer" class="headerlink" title="DataContainer"></a>DataContainer</h4><p>那么 DataContainer 是什么呢？它是对 tensor 的封装，将 results 中的 tensor 转成 DataContainer 格式，实际上只是增加了几个 property 函数，cpu_only，stack，padding_value，pad_dims，其含义自明，以及 size，dim用来获取数据的维度，形状信息。考虑到序列数据在进入 DataLoader 时，需要以 batch 方式进入模型，那么通常的 collate_fn 会要求 tensor 数据的形状一致。但是这样不是很方便，于是有了 DataContainer。它可以做到载入 GPU 的数据可以保持统一 shape，并被 stack，也可以不 stack，也可以保持原样，或者在非 batch 维度上做 pad。当然这个也要对 default_collate进行改造，mmcv 在 parallel.collate 中实现了这个。</p><p>collate_fn 是 DataLoader 中将序列 dataset 组织成 batch 大小的函数，这里帖三个普通例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn_1</span><span class="params">(batch)</span> :</span></span><br><span class="line">    <span class="comment"># 这 是 默 认 的， 明 显batch中 包 含 相 同 形 状 的img\_tensor和 label</span></span><br><span class="line">    <span class="keyword">return</span> tuple ( zip(*batch))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coco_collate_2</span><span class="params">(batch)</span> :</span></span><br><span class="line">    <span class="comment"># 传 入 的batch数 据 是 被albu增 强 后 的(字 典 结 构)</span></span><br><span class="line">    imgs = [s [ ’image’ ] <span class="keyword">for</span> s <span class="keyword">in</span> batch] </span><br><span class="line">    annots = [s [ ’bboxes’ ] <span class="keyword">for</span> s <span class="keyword">in</span> batch]</span><br><span class="line">    labels = [s [ ’category_id’ ] <span class="keyword">for</span> s <span class="keyword">in</span> batch]</span><br><span class="line">    <span class="comment"># 以 当 前batch中 图 片annot数 量 的 最 大 值 作 为 标 记 数 据 的 第 二 维 度 值， 空 出 的 就补−1</span></span><br><span class="line">    max_num_annots = max( len (annot) <span class="keyword">for</span> annot <span class="keyword">in</span> annots)</span><br><span class="line">    annot_padded = np. ones (( len (annots) , max_num_annots, <span class="number">5</span>))*−<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> max_num_annots &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">for</span> idx , (annot , lab) <span class="keyword">in</span> enumerate( zip (annots , labels )) :</span><br><span class="line">            <span class="keyword">if</span> len (annot) &gt; <span class="number">0</span>:</span><br><span class="line">                annot_padded[idx,:len(annot),:<span class="number">4</span>] = annot</span><br><span class="line">                annot_padded[idx,:len(annot),<span class="number">2</span>] += annot_padded[idx,:len(annot),<span class="number">0</span>]</span><br><span class="line">                annot_padded[idx,:len(annot),<span class="number">3</span>] += annot_padded[idx,:len(annot),<span class="number">1</span>]</span><br><span class="line">                annot_padded[idx,:len(annot),:] /= <span class="number">640</span></span><br><span class="line">    annot_padded[idx,:len(annot),<span class="number">4</span>] = lab</span><br><span class="line">   <span class="keyword">return</span> torch.stack(imgs,<span class="number">0</span>), torch.FloatTensor(annot_padded)   </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detection_collate_3</span><span class="params">(batch)</span> :</span></span><br><span class="line">    targets = []</span><br><span class="line">    imgs = [ ]</span><br><span class="line">    <span class="keyword">for</span> _, sample <span class="keyword">in</span> enumerate(batch) :</span><br><span class="line">        <span class="keyword">for</span> _, img_anno <span class="keyword">in</span> enumerate(sample) :</span><br><span class="line">            <span class="keyword">if</span> torch.is_tensor(img_anno) :</span><br><span class="line">                imgs.append(img_anno)</span><br><span class="line">           <span class="keyword">elif</span> isinstance(img_anno, np.ndarray) :  </span><br><span class="line">                annos = torch.from_numpy(img_anno).float()</span><br><span class="line">                targets.append(annos)</span><br><span class="line">   <span class="keyword">return</span> torch.stack(imgs,<span class="number">0</span>), targets <span class="comment"># 做了stack，DataContainer可以不做 stack</span></span><br></pre></td></tr></table></figure><p>以上就是数据处理的相关内容。最后再用 DataLoader 封装拆成迭代器，其相关细节，sampler 等暂略。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data_loader = DataLoader(</span><br><span class="line">        dataset,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        sampler=sampler,</span><br><span class="line">        num_workers=num_workers,</span><br><span class="line">        collate_fn=partial(collate, samples_per_gpu=samples_per_gpu),</span><br><span class="line">        pin_memory=<span class="keyword">False</span>,</span><br><span class="line">        worker_init_fn=init_fn,</span><br><span class="line">        **kwargs)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;数据处理&quot;&gt;&lt;a href=&quot;#数据处理&quot; class=&quot;headerlink&quot; title=&quot;数据处理&quot;&gt;&lt;/a&gt;数据处理&lt;/h3&gt;&lt;p&gt;数据处理可能是炼丹师接触最为密集的了，因为通常情况，除了数据的离线处理，写个数据类，就可以炼丹了。但本节主要涉及数据的在线处理，更进一步应该是检测分割数据的 pytorch 处理方式。虽然 mmdet 将常用的数据都实现了，而且也实现了中间通用数据格式，但，这和模型，损失函数，性能评估的实现也相关，比如你想把官网的 centernet 完整的改成 mmdet风格，就能看到 (看起来没必要)。&lt;/p&gt;
    
    </summary>
    
      <category term="mmdetection" scheme="https://blog.nicehuster.cn/categories/mmdetection/"/>
    
    
      <category term="mmdetection" scheme="https://blog.nicehuster.cn/tags/mmdetection/"/>
    
  </entry>
  
  <entry>
    <title>mmdetection详解指北 (二)</title>
    <link href="https://blog.nicehuster.cn/2020/09/03/mmdetection-2/"/>
    <id>https://blog.nicehuster.cn/2020/09/03/mmdetection-2/</id>
    <published>2020-09-03T11:14:39.000Z</published>
    <updated>2020-09-18T07:19:34.051Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇博客主要介绍到mmdetection这个检测框架的一些结构设计以及代码的总体逻辑。这篇就主要介绍一下在mmdetection被大量使用的配置和注册。</p><h3 id="配置类"><a href="#配置类" class="headerlink" title="配置类"></a>配置类</h3><p>配置方式支持 python/json/yaml, 从 mmcv 的 Config 解析, 其功能同 maskrcnn-benchmark 的 yacs 类似, 将字典的取值方式属性化. 这里帖部分代码，以供学习。</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Config</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""A facility for config and config files.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It supports common file formats as configs: python/json/yaml. The interface</span></span><br><span class="line"><span class="string">    is the same as a dict object and also allows access config values as</span></span><br><span class="line"><span class="string">    attributes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg = Config(dict(a=1, b=dict(b1=[0, 1])))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg.a</span></span><br><span class="line"><span class="string">        1</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg.b</span></span><br><span class="line"><span class="string">        &#123;'b1': [0, 1]&#125;</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg.b.b1</span></span><br><span class="line"><span class="string">        [0, 1]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg = Config.fromfile('tests/data/config/a.py')</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg.filename</span></span><br><span class="line"><span class="string">        "/home/kchen/projects/mmcv/tests/data/config/a.py"</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg.item4</span></span><br><span class="line"><span class="string">        'test'</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg</span></span><br><span class="line"><span class="string">        "Config [path: /home/kchen/projects/mmcv/tests/data/config/a.py]: "</span></span><br><span class="line"><span class="string">        "&#123;'item1': [1, 2], 'item2': &#123;'a': 0&#125;, 'item3': True, 'item4': 'test'&#125;"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fromfile</span><span class="params">(filename)</span>:</span></span><br><span class="line">        filename = osp.abspath(osp.expanduser(filename))</span><br><span class="line">        check_file_exist(filename)</span><br><span class="line">        <span class="keyword">if</span> filename.endswith(<span class="string">'.py'</span>):</span><br><span class="line">            module_name = osp.basename(filename)[:<span class="number">-3</span>]</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'.'</span> <span class="keyword">in</span> module_name:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">'Dots are not allowed in config file path.'</span>)</span><br><span class="line">            config_dir = osp.dirname(filename)</span><br><span class="line">            sys.path.insert(<span class="number">0</span>, config_dir)</span><br><span class="line">            mod = import_module(module_name)</span><br><span class="line">            sys.path.pop(<span class="number">0</span>)</span><br><span class="line">            cfg_dict = &#123;</span><br><span class="line">                name: value</span><br><span class="line">                <span class="keyword">for</span> name, value <span class="keyword">in</span> mod.__dict__.items()</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> name.startswith(<span class="string">'__'</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">elif</span> filename.endswith((<span class="string">'.yml'</span>, <span class="string">'.yaml'</span>, <span class="string">'.json'</span>)):</span><br><span class="line">            <span class="keyword">import</span> mmcv</span><br><span class="line">            cfg_dict = mmcv.load(filename)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> IOError(<span class="string">'Only py/yml/yaml/json type are supported now!'</span>)</span><br><span class="line">        <span class="keyword">return</span> Config(cfg_dict, filename=filename)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">auto_argparser</span><span class="params">(description=None)</span>:</span></span><br><span class="line">        <span class="string">"""Generate argparser from config file automatically (experimental)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        partial_parser = ArgumentParser(description=description)</span><br><span class="line">        partial_parser.add_argument(<span class="string">'config'</span>, help=<span class="string">'config file path'</span>)</span><br><span class="line">        cfg_file = partial_parser.parse_known_args()[<span class="number">0</span>].config</span><br><span class="line">        cfg = Config.fromfile(cfg_file)</span><br><span class="line">        parser = ArgumentParser(description=description)</span><br><span class="line">        parser.add_argument(<span class="string">'config'</span>, help=<span class="string">'config file path'</span>)</span><br><span class="line">        add_args(parser, cfg)</span><br><span class="line">        <span class="keyword">return</span> parser, cfg</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, cfg_dict=None, filename=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> cfg_dict <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            cfg_dict = dict()</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> isinstance(cfg_dict, dict):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">'cfg_dict must be a dict, but got &#123;&#125;'</span>.format(</span><br><span class="line">                type(cfg_dict)))</span><br><span class="line"></span><br><span class="line">        super(Config, self).__setattr__(<span class="string">'_cfg_dict'</span>, ConfigDict(cfg_dict))</span><br><span class="line">        super(Config, self).__setattr__(<span class="string">'_filename'</span>, filename)</span><br><span class="line">        <span class="keyword">if</span> filename:</span><br><span class="line">            <span class="keyword">with</span> open(filename, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                super(Config, self).__setattr__(<span class="string">'_text'</span>, f.read())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            super(Config, self).__setattr__(<span class="string">'_text'</span>, <span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">filename</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._filename</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">text</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._text</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'Config (path: &#123;&#125;): &#123;&#125;'</span>.format(self.filename,</span><br><span class="line">                                              self._cfg_dict.__repr__())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self._cfg_dict)</span><br><span class="line">    <span class="comment"># 获取key值</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getattr__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> getattr(self._cfg_dict, name)</span><br><span class="line">    <span class="comment"># 序列化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._cfg_dict.__getitem__(name)</span><br><span class="line">    <span class="comment"># 序列化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setattr__</span><span class="params">(self, name, value)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(value, dict):</span><br><span class="line">            value = ConfigDict(value)</span><br><span class="line">        self._cfg_dict.__setattr__(name, value)</span><br><span class="line">    <span class="comment"># 更新key值</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setitem__</span><span class="params">(self, name, value)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(value, dict):</span><br><span class="line">            value = ConfigDict(value)</span><br><span class="line">        self._cfg_dict.__setitem__(name, value)</span><br><span class="line">    <span class="comment"># 迭代器</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> iter(self._cfg_dict)</span><br></pre></td></tr></table></figure><p>主要考虑点是自己怎么实现类似的东西，核心点就是 python 的基本魔法函数的应用，可同时参考 yacs。</p><h4 id="注册器"><a href="#注册器" class="headerlink" title="注册器"></a>注册器</h4><p>把基本对象放到一个继承了字典的对象中，实现了对象的灵活管理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Registry</span>:</span></span><br><span class="line">    <span class="string">"""A registry to map strings to classes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        name (str): Registry name.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self._name = name</span><br><span class="line">        self._module_dict = dict()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self._module_dict)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__contains__</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.get(key) <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        format_str = self.__class__.__name__ + \</span><br><span class="line">                     <span class="string">f'(name=<span class="subst">&#123;self._name&#125;</span>, '</span> \</span><br><span class="line">                     <span class="string">f'items=<span class="subst">&#123;self._module_dict&#125;</span>)'</span></span><br><span class="line">        <span class="keyword">return</span> format_str</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">name</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._name</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">module_dict</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._module_dict</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="string">"""Get the registry record.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            key (str): The class name in string format.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            class: The corresponding class.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self._module_dict.get(key, <span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_register_module</span><span class="params">(self, module_class, module_name=None, force=False)</span>:</span></span><br><span class="line">        <span class="comment"># 校验当前注册的module_class是否是类对象</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> inspect.isclass(module_class):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">'module must be a class, '</span></span><br><span class="line">                            <span class="string">f'but got <span class="subst">&#123;type(module_class)&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> module_name <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            module_name = module_class.__name__</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> force <span class="keyword">and</span> module_name <span class="keyword">in</span> self._module_dict:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(<span class="string">f'<span class="subst">&#123;module_name&#125;</span> is already registered '</span></span><br><span class="line">                           <span class="string">f'in <span class="subst">&#123;self.name&#125;</span>'</span>)</span><br><span class="line">        self._module_dict[module_name] = module_class  <span class="comment"># 类 名 : 类</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deprecated_register_module</span><span class="params">(self, cls=None, force=False)</span>:</span></span><br><span class="line">        warnings.warn(</span><br><span class="line">            <span class="string">'The old API of register_module(module, force=False) '</span></span><br><span class="line">            <span class="string">'is deprecated and will be removed, please use the new API '</span></span><br><span class="line">            <span class="string">'register_module(name=None, force=False, module=None) instead.'</span>)</span><br><span class="line">        <span class="keyword">if</span> cls <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> partial(self.deprecated_register_module, force=force)</span><br><span class="line">        self._register_module(cls, force=force)</span><br><span class="line">        <span class="keyword">return</span> cls</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">register_module</span><span class="params">(self, name=None, force=False, module=None)</span>:</span></span><br><span class="line">        <span class="comment"># 作 为 类 name 的 装 饰 器</span></span><br><span class="line">        <span class="string">"""Register a module.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        A record will be added to `self._module_dict`, whose key is the class</span></span><br><span class="line"><span class="string">        name or the specified name, and value is the class itself.</span></span><br><span class="line"><span class="string">        It can be used as a decorator or a normal function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Example:</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt; backbones = Registry('backbone')</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt; @backbones.register_module()</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt; class ResNet:</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt;     pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            name (str | None): The module name to be registered. If not</span></span><br><span class="line"><span class="string">                specified, the class name will be used.</span></span><br><span class="line"><span class="string">            force (bool, optional): Whether to override an existing class with</span></span><br><span class="line"><span class="string">                the same name. Default: False.</span></span><br><span class="line"><span class="string">            module (type): Module class to be registered.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(force, bool):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">f'force must be a boolean, but got <span class="subst">&#123;type(force)&#125;</span>'</span>)</span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> This is a walkaround to be compatible with the old api,</span></span><br><span class="line">        <span class="comment"># while it may introduce unexpected bugs.</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(name, type):</span><br><span class="line">            <span class="keyword">return</span> self.deprecated_register_module(name, force=force)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># use it as a normal method: x.register_module(module=SomeClass)</span></span><br><span class="line">        <span class="keyword">if</span> module <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self._register_module(</span><br><span class="line">                module_class=module, module_name=name, force=force)</span><br><span class="line">            <span class="keyword">return</span> module</span><br><span class="line"></span><br><span class="line">        <span class="comment"># raise the error ahead of time</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> (name <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">or</span> isinstance(name, str)):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">f'name must be a str, but got <span class="subst">&#123;type(name)&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># use it as a decorator: @x.register_module()</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_register</span><span class="params">(cls)</span>:</span></span><br><span class="line">            self._register_module(</span><br><span class="line">                module_class=cls, module_name=name, force=force)</span><br><span class="line">            <span class="keyword">return</span> cls</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> _register</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_from_cfg</span><span class="params">(cfg, registry, default_args=None)</span>:</span></span><br><span class="line">    <span class="string">"""Build a module from config dict.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        cfg (dict): Config dict. It should at least contain the key "type".</span></span><br><span class="line"><span class="string">        registry (:obj:`Registry`): The registry to search the type from.</span></span><br><span class="line"><span class="string">        default_args (dict, optional): Default initialization arguments.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        object: The constructed object.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(cfg, dict):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">f'cfg must be a dict, but got <span class="subst">&#123;type(cfg)&#125;</span>'</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'type'</span> <span class="keyword">not</span> <span class="keyword">in</span> cfg:</span><br><span class="line">        <span class="keyword">raise</span> KeyError(</span><br><span class="line">            <span class="string">f'the cfg dict must contain the key "type", but got <span class="subst">&#123;cfg&#125;</span>'</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(registry, Registry):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">'registry must be an mmcv.Registry object, '</span></span><br><span class="line">                        <span class="string">f'but got <span class="subst">&#123;type(registry)&#125;</span>'</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> (isinstance(default_args, dict) <span class="keyword">or</span> default_args <span class="keyword">is</span> <span class="keyword">None</span>):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">'default_args must be a dict or None, '</span></span><br><span class="line">                        <span class="string">f'but got <span class="subst">&#123;type(default_args)&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">    args = cfg.copy()</span><br><span class="line">    obj_type = args.pop(<span class="string">'type'</span>)</span><br><span class="line">    <span class="keyword">if</span> is_str(obj_type):</span><br><span class="line">        <span class="comment"># 从 注 册 类 中 拿 出obj_type类</span></span><br><span class="line">        obj_cls = registry.get(obj_type)</span><br><span class="line">        <span class="keyword">if</span> obj_cls <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(</span><br><span class="line">                <span class="string">f'<span class="subst">&#123;obj_type&#125;</span> is not in the <span class="subst">&#123;registry.name&#125;</span> registry'</span>)</span><br><span class="line">    <span class="keyword">elif</span> inspect.isclass(obj_type):</span><br><span class="line">        obj_cls = obj_type</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> TypeError(</span><br><span class="line">            <span class="string">f'type must be a str or valid type, but got <span class="subst">&#123;type(obj_type)&#125;</span>'</span>)</span><br><span class="line">    <span class="comment"># 增 加 一 些 新 的 参 数</span></span><br><span class="line">    <span class="keyword">if</span> default_args <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">for</span> name, value <span class="keyword">in</span> default_args.items():</span><br><span class="line">            args.setdefault(name, value)</span><br><span class="line">    <span class="keyword">return</span> obj_cls(**args)<span class="comment"># **args 是 将 字 典 解 析 成 位 置 参 数(k=v)。</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇博客主要介绍到mmdetection这个检测框架的一些结构设计以及代码的总体逻辑。这篇就主要介绍一下在mmdetection被大量使用的配置和注册。&lt;/p&gt;
&lt;h3 id=&quot;配置类&quot;&gt;&lt;a href=&quot;#配置类&quot; class=&quot;headerlink&quot; title=&quot;配置类&quot;&gt;&lt;/a&gt;配置类&lt;/h3&gt;&lt;p&gt;配置方式支持 python/json/yaml, 从 mmcv 的 Config 解析, 其功能同 maskrcnn-benchmark 的 yacs 类似, 将字典的取值方式属性化. 这里帖部分代码，以供学习。&lt;/p&gt;
    
    </summary>
    
      <category term="mmdetection" scheme="https://blog.nicehuster.cn/categories/mmdetection/"/>
    
    
      <category term="mmdetection" scheme="https://blog.nicehuster.cn/tags/mmdetection/"/>
    
  </entry>
  
  <entry>
    <title>mmdetection详解指北 (一)</title>
    <link href="https://blog.nicehuster.cn/2020/09/01/mmdetection-1/"/>
    <id>https://blog.nicehuster.cn/2020/09/01/mmdetection-1/</id>
    <published>2020-09-01T11:13:39.000Z</published>
    <updated>2020-09-18T07:19:24.317Z</updated>
    
    <content type="html"><![CDATA[<p>平时做的都些检测相关的项目，因此对于各类检测框架使用较多，以及一些不知名的repo都有用过，平时接触的都是业务项目，很少去认真的看一个repo中算法的框架设计，最近项目处于交付阶段，主要是客户和平台开发人员对界面的问题。趁有空mmdetection的代码重新看了一遍，顺便做了一些笔记。</p><h3 id="组件设计"><a href="#组件设计" class="headerlink" title="组件设计"></a>组件设计</h3><blockquote><ul><li>BackBone: 特征提取骨架网络,ResNet,ResneXt,ssd_vgg, hrnet 等。</li><li>Neck: 连接骨架和头部. 多层级特征融合,FPN,BFP,PAFPN 等。</li><li>DenseHead: 处理特征图上的密集框部分, 主要分 AnchorHead。AnchorFreeHead 两大类，分别有 RPNHead, SSDHead,RetinaHead 和 FCOSHead 等。</li><li>RoIHead (BBoxHead/MaskHead): 在特征图上对 roi 做类别分类或位置回归等 (1.x)。</li><li>ROIHead:bbox 或 mask 的 roi_extractor+head(2.0, 合并了 extractor 和 head)</li><li>SingleStage: BackBone + Neck + DenseHead</li><li>TwoStage: BackBone + Neck + DenseHead + RoIHead(2.0)</li></ul></blockquote><a id="more"></a><h3 id="结构设计"><a href="#结构设计" class="headerlink" title="结构设计"></a>结构设计</h3><h4 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h4><blockquote><ul><li><p>configs 网络组件结构等配置信息 </p></li><li><p>tools: 训练和测试的最终包装和一些实用脚本 </p></li><li><p>mmdet:</p><blockquote><ul><li>apis: 分布式环境设定 (1.x,2.0 移植到 mmcv), 推断, 测试, 训练基础代码;</li><li>core: anchor 生成,bbox,mask 编解码, 变换, 标签锚定, 采样等, 模型评估, 加速, 优化器，后处理;</li><li>datasets:coco,voc 等数据类, 数据 pipelines 的统一格式, 数据增强，数据采样;</li><li>models: 模型组件 (backbone,head,loss,neck)，采用注册和组合构建的形式完成模型搭建</li><li>ops: 优化加速代码, 包括 nms,roialign,dcn,masked_conv，focal_loss 等</li></ul></blockquote></li></ul></blockquote><p><img src="/img/image-20200917202833058.png" alt="image-20200917202833058"></p><h4 id="总体逻辑"><a href="#总体逻辑" class="headerlink" title="总体逻辑"></a>总体逻辑</h4><p>从 tools/train.py 中能看到整体可分如下 4 个步骤:</p><blockquote><ul><li><p>1.mmcv.Config.fromfile 从配置文件解析配置信息, 并做适当更新, 包括环境搜集，预加载模型文件, 分布式设置，日志记录等;</p></li><li><p>2.mmdet.models 中的 build_detector 根据配置信息构造模型 ;</p><blockquote><ul><li>2.1 build 系列函数调用 build_from_cfg 函数, 按 type 关键字从注册表中获取相应的对象, 对象的具名参数在注册文件中赋值;</li><li>2.2 registr.py 放置了模型的组件注册器。其中注册器的 register_module 成员函数是一个装饰器功能函数，在具体的类对象 <em>A</em> 头上装饰 @X.register _module，并同时在 <em>A</em> 对象所在包的初始化文件中调用 <em>A</em>，即可将 <em>A</em> 保存到 registry.module_dict 中, 完成注册;_</li><li>2.3 目前包含 BACKBONES,NECKS,ROI_EXTRACTORS,SHARED_ HEADS,HEADS,LOSSES,DETECTORS 七个模型相关注册器，另外还有数据类，优化器等注册器;</li></ul></blockquote></li></ul><ul><li><p>3.build_dataset 根据配置信息获取数据类;</p><blockquote><ul><li>3.1 coco，cityscapes，voc，deepfasion，lvis，wider_face 等数据 (数据类扩展见后续例子)。</li></ul></blockquote></li><li><p>4.train_detector 模型训练流程:</p><blockquote><ul><li>4.1数据 loader 化, 模型分布式化，优化器选取</li><li>4.2 进入 runner 训练流程 (来自 mmcv 库，采用 hook 方式，整合了 pytorch 训练流程)</li><li>4.3 训练 pipelines 具体细节见后续展开。</li></ul></blockquote></li></ul></blockquote><p>后续说说配置文件，注册机制和训练逻辑。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;平时做的都些检测相关的项目，因此对于各类检测框架使用较多，以及一些不知名的repo都有用过，平时接触的都是业务项目，很少去认真的看一个repo中算法的框架设计，最近项目处于交付阶段，主要是客户和平台开发人员对界面的问题。趁有空mmdetection的代码重新看了一遍，顺便做了一些笔记。&lt;/p&gt;
&lt;h3 id=&quot;组件设计&quot;&gt;&lt;a href=&quot;#组件设计&quot; class=&quot;headerlink&quot; title=&quot;组件设计&quot;&gt;&lt;/a&gt;组件设计&lt;/h3&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;BackBone: 特征提取骨架网络,ResNet,ResneXt,ssd_vgg, hrnet 等。&lt;/li&gt;
&lt;li&gt;Neck: 连接骨架和头部. 多层级特征融合,FPN,BFP,PAFPN 等。&lt;/li&gt;
&lt;li&gt;DenseHead: 处理特征图上的密集框部分, 主要分 AnchorHead。AnchorFreeHead 两大类，分别有 RPNHead, SSDHead,RetinaHead 和 FCOSHead 等。&lt;/li&gt;
&lt;li&gt;RoIHead (BBoxHead/MaskHead): 在特征图上对 roi 做类别分类或位置回归等 (1.x)。&lt;/li&gt;
&lt;li&gt;ROIHead:bbox 或 mask 的 roi_extractor+head(2.0, 合并了 extractor 和 head)&lt;/li&gt;
&lt;li&gt;SingleStage: BackBone + Neck + DenseHead&lt;/li&gt;
&lt;li&gt;TwoStage: BackBone + Neck + DenseHead + RoIHead(2.0)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="mmdetection" scheme="https://blog.nicehuster.cn/categories/mmdetection/"/>
    
    
      <category term="mmdetection" scheme="https://blog.nicehuster.cn/tags/mmdetection/"/>
    
  </entry>
  
  <entry>
    <title>解决PIL读取图片出现自动旋转的解决方案</title>
    <link href="https://blog.nicehuster.cn/2020/08/06/PILrotate/"/>
    <id>https://blog.nicehuster.cn/2020/08/06/PILrotate/</id>
    <published>2020-08-06T11:13:39.000Z</published>
    <updated>2020-09-18T02:40:56.930Z</updated>
    
    <content type="html"><![CDATA[<p>最近项目中，使用手机采集了数据，交给标注组进行标注时，发现返回来的标注文件与图片存在不匹配问题，部分标注存在旋转情况。从网上了解到电子设备在拍摄照片时，如手机、相机等，由于手持朝向的不同，拍摄的照片可能会出现旋转 0、90、180、270 角度的情况，其 EXIF 信息中会保留相应的方位信息.有些情况下，电脑上打开显示照片是正常的，但在用 PIL 或 OpenCV 读取图片后，图片出现旋转，且读取的图片尺寸也可能与直接在电脑上打开的尺寸不同的问题.</p><p>对此，需要在读取图片时，同时解析图片的 EXIF 中的方位信息，将图片转正，再进行后续的其他操作.实例如下：</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ExifTags</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">IsRotate</span><span class="params">(img)</span>:</span> <span class="comment"># 返回false表示存在旋转情况</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">for</span> orientation <span class="keyword">in</span> ExifTags.TAGS.keys() :</span><br><span class="line">            <span class="keyword">if</span> ExifTags.TAGS[orientation]==<span class="string">'Orientation'</span> :</span><br><span class="line">                img2 = img.rotate(<span class="number">0</span>, expand = <span class="keyword">True</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        exif=dict(img._getexif().items())</span><br><span class="line">        <span class="keyword">if</span>  exif[orientation] == <span class="number">3</span> :</span><br><span class="line">            img2=img.rotate(<span class="number">180</span>, expand = <span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">elif</span> exif[orientation] == <span class="number">6</span> :</span><br><span class="line">            img2=img.rotate(<span class="number">270</span>, expand = <span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">elif</span> exif[orientation] == <span class="number">8</span> :</span><br><span class="line">            img2=img.rotate(<span class="number">90</span>, expand = <span class="keyword">True</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> img.size == img2.size</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br></pre></td></tr></table></figure><p>更多可参考：<a href="https://stackoverflow.com/questions/4228530/pil-thumbnail-is-rotating-my-image" target="_blank" rel="noopener">Stackoverflow - PIL thumbnail is rotating my image?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近项目中，使用手机采集了数据，交给标注组进行标注时，发现返回来的标注文件与图片存在不匹配问题，部分标注存在旋转情况。从网上了解到电子设备在拍摄照片时，如手机、相机等，由于手持朝向的不同，拍摄的照片可能会出现旋转 0、90、180、270 角度的情况，其 EXIF 信息中会保留相应的方位信息.有些情况下，电脑上打开显示照片是正常的，但在用 PIL 或 OpenCV 读取图片后，图片出现旋转，且读取的图片尺寸也可能与直接在电脑上打开的尺寸不同的问题.&lt;/p&gt;
&lt;p&gt;对此，需要在读取图片时，同时解析图片的 EXIF 中的方位信息，将图片转正，再进行后续的其他操作.实例如下：&lt;/p&gt;
    
    </summary>
    
      <category term="project" scheme="https://blog.nicehuster.cn/categories/project/"/>
    
    
      <category term="opencv" scheme="https://blog.nicehuster.cn/tags/opencv/"/>
    
      <category term="python" scheme="https://blog.nicehuster.cn/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python计算任意多边形的面积</title>
    <link href="https://blog.nicehuster.cn/2020/06/05/polygonArea/"/>
    <id>https://blog.nicehuster.cn/2020/06/05/polygonArea/</id>
    <published>2020-06-05T11:13:39.000Z</published>
    <updated>2020-09-18T11:41:00.458Z</updated>
    
    <content type="html"><![CDATA[<p>在网上发现一个很有意思且很有用的多边形面积计算公式-鞋带公式</p><p>鞋带公式的表达式为：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{A} &=\frac{1}{2}\left|\sum_{i=1}^{n-1} x_{i} y_{i+1}+x_{n} y_{1}-\sum_{i=1}^{n-1} x_{i+1} y_{i}-x_{1} y_{n}\right| \\&=\frac{1}{2}\left|x_{1} y_{2}+x_{2} y_{3}+\cdots+x_{n-1} y_{n}+x_{n} y_{1}-x_{2} y_{1}-x_{3} y_{2}-\cdots-x_{n} y_{n-1}-x_{1} y_{n}\right|\end{aligned}</script><a id="more"></a><p><img src="/img/b1f95fc8a46e6d66.jpg" alt="img"></p><p>where<br>$\bullet A$ is the area of the polygon,<br>$\bullet n$ is the number of sides of the polygon, and $\cdot\left(x_{i}, y_{i}\right), i=1,2, \ldots, n$ are the ordered vertices (or “corners”) of the polygon.</p><blockquote><p>参考wiki:<a href="https://en.wikipedia.org/wiki/Shoelace_formula" target="_blank" rel="noopener">Shoelace formula</a></p></blockquote><p>可以理解为，是把每个顶点向x轴做垂线，每个边和坐标轴构成的梯形面积矢量和就是多边形的面积。</p><h3 id="1-鞋带公式实现"><a href="#1-鞋带公式实现" class="headerlink" title="1. 鞋带公式实现"></a>1. 鞋带公式实现</h3><blockquote><p>From: stackoverflow - <a href="https://stackoverflow.com/questions/24467972/calculate-area-of-polygon-given-x-y-coordinates" target="_blank" rel="noopener">calculate-area-of-polygon-given-x-y-coordinates</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">polygon_area</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span>*np.abs(np.dot(x,np.roll(y,<span class="number">1</span>))-np.dot(y,np.roll(x,<span class="number">1</span>)))</span><br></pre></td></tr></table></figure><h3 id="2-示例1-计算曲线与坐标轴的面积"><a href="#2-示例1-计算曲线与坐标轴的面积" class="headerlink" title="2. 示例1 - 计算曲线与坐标轴的面积"></a>2. 示例1 - 计算曲线与坐标轴的面积</h3><blockquote><p>[计算曲线与坐标轴的面积</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">1</span>,<span class="number">0.001</span>)</span><br><span class="line">y = np.sqrt(<span class="number">1</span>-x**<span class="number">2</span>)</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.show()</span><br><span class="line">area_value = polygon_area(np.append(x, <span class="number">0</span>), np.append(y, <span class="number">0</span>))</span><br></pre></td></tr></table></figure><h3 id="3-示例2-detectron2-mask-面积"><a href="#3-示例2-detectron2-mask-面积" class="headerlink" title="3. 示例2 - detectron2 mask 面积"></a>3. 示例2 - detectron2 mask 面积</h3><blockquote><p><a href="https://github.com/facebookresearch/detectron2/blob/b0b6ccfef5e00255de22857eca0e2dfa1d1144b1/detectron2/structures/masks.py" target="_blank" rel="noopener">detectron2/structures/masks.py</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">area</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes area of the mask.</span></span><br><span class="line"><span class="string">    Only works with Polygons, using the shoelace formula</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Tensor: a vector, area for each instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    area = []</span><br><span class="line">    <span class="keyword">for</span> polygons_per_instance <span class="keyword">in</span> self.polygons:</span><br><span class="line">        area_per_instance = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> polygons_per_instance:</span><br><span class="line">            area_per_instance += polygon_area(p[<span class="number">0</span>::<span class="number">2</span>], p[<span class="number">1</span>::<span class="number">2</span>])</span><br><span class="line">        area.append(area_per_instance)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> torch.tensor(area)</span><br></pre></td></tr></table></figure><h3 id="在线多变形面积计算工具"><a href="#在线多变形面积计算工具" class="headerlink" title="在线多变形面积计算工具"></a>在线多变形面积计算工具</h3><blockquote><p><a href="https://www.mathsisfun.com/geometry/area-polygon-drawing.html" target="_blank" rel="noopener">多边形面积计算工具</a></p></blockquote><p><img src="/img/image-20200916102553183.png" alt="image-20200916102553183"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在网上发现一个很有意思且很有用的多边形面积计算公式-鞋带公式&lt;/p&gt;
&lt;p&gt;鞋带公式的表达式为：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{aligned}
\mathbf{A} &amp;=\frac{1}{2}\left|\sum_{i=1}^{n-1} x_{i} y_{i+1}+x_{n} y_{1}-\sum_{i=1}^{n-1} x_{i+1} y_{i}-x_{1} y_{n}\right| \\
&amp;=\frac{1}{2}\left|x_{1} y_{2}+x_{2} y_{3}+\cdots+x_{n-1} y_{n}+x_{n} y_{1}-x_{2} y_{1}-x_{3} y_{2}-\cdots-x_{n} y_{n-1}-x_{1} y_{n}\right|
\end{aligned}&lt;/script&gt;
    
    </summary>
    
      <category term="project" scheme="https://blog.nicehuster.cn/categories/project/"/>
    
    
      <category term="python" scheme="https://blog.nicehuster.cn/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>ffmpeg常用指令笔记</title>
    <link href="https://blog.nicehuster.cn/2020/03/26/ffmpeg/"/>
    <id>https://blog.nicehuster.cn/2020/03/26/ffmpeg/</id>
    <published>2020-03-26T11:13:39.000Z</published>
    <updated>2020-09-18T11:41:27.855Z</updated>
    
    <content type="html"><![CDATA[<p>平时使用ffmpeg对视频解码成图片比较多，就稍微简单的了解一些ffmpeg常用的相关指令。下面是一些相关指令的介绍笔记。</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>Debian/Ubuntu/Linux Mint 下安装 ffmpeg 很简单：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-<span class="builtin-name">get</span> install ffmpeg</span><br></pre></td></tr></table></figure><p>其他操作系统安装方法，参考<a href="https://www.ffmpeg.org/download.html" target="_blank" rel="noopener">官网</a></p><p>如果想要手工编译 ffmpeg 可以参考官方 <a href="https://trac.ffmpeg.org/wiki#CompilingFFmpeg" target="_blank" rel="noopener">wiki</a>。 Ubuntu/Debian/Mint 系手工编译 ffmpeg 参考 <a href="https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu" target="_blank" rel="noopener">wiki</a>。</p><a id="more"></a><h3 id="常用指令"><a href="#常用指令" class="headerlink" title="常用指令"></a>常用指令</h3><h4 id="1-显示文件信息"><a href="#1-显示文件信息" class="headerlink" title="1.显示文件信息"></a>1.显示文件信息</h4><p>显示视频信息</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span>.avi</span><br></pre></td></tr></table></figure><h4 id="2-将视频拆分图片-批量截图"><a href="#2-将视频拆分图片-批量截图" class="headerlink" title="2.将视频拆分图片 批量截图"></a>2.将视频拆分图片 批量截图</h4><p>将视频拆分多张图片，每一帧图片，保存到 frames 文件夹下，命名 frame001.png 这种。可以加上 -r 参数以用来限制每秒的帧数，<code>-r 10</code> 就表示每秒 10 帧。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="built_in">i</span> input.mp4 frames/frame<span class="comment">%03d.png</span></span><br></pre></td></tr></table></figure><h4 id="3-图片合成视频"><a href="#3-图片合成视频" class="headerlink" title="3.图片合成视频"></a>3.图片合成视频</h4><p>将多张图片合成视频</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="built_in">i</span> frames/frame<span class="comment">%3d.png output.mp4</span></span><br></pre></td></tr></table></figure><h4 id="4-转换格式"><a href="#4-转换格式" class="headerlink" title="4.转换格式"></a>4.转换格式</h4><p>格式之间转换 大部分的情况下直接运行一下即可</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> output.avi</span><br></pre></td></tr></table></figure><p>将 flv 转码 MP4</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.flv</span> -vcodec copy -acodec copy out.mp4</span><br></pre></td></tr></table></figure><p><code>-vcodec copy</code> 和 <code>-acodec copy</code> 表示所使用的视频和音频编码格式，为原样拷贝。</p><h4 id="5-视频切片操作"><a href="#5-视频切片操作" class="headerlink" title="5.视频切片操作"></a>5.视频切片操作</h4><p>对视频切片操作,比如需要从视频第 1 分 45 秒地方，剪 10 秒画面，-ss 表示开始位置，-t 表示延长时间</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> -ss <span class="number">00</span>:<span class="number">01</span>:<span class="number">45</span> -t <span class="number">10</span> output.mp4</span><br></pre></td></tr></table></figure><h4 id="6-加速减速视频"><a href="#6-加速减速视频" class="headerlink" title="6.加速减速视频"></a>6.加速减速视频</h4><p>加速视频</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> -vf “setpts=<span class="number">0.5</span>*PTS” output.mp4</span><br></pre></td></tr></table></figure><p>同理减速视频</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> -vf “setpts=<span class="number">2.0</span>*PTS” output.mp4</span><br></pre></td></tr></table></figure><p>此操作对音频无影响</p><h4 id="7-视频截图"><a href="#7-视频截图" class="headerlink" title="7.视频截图"></a>7.视频截图</h4><p>视频 10 秒的地方 (<code>-ss</code> 参数）截取一张 1920x1080 尺寸大小的，格式为 jpg 的图片  <code>-ss</code>后跟的时间单位为秒</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> input_video<span class="selector-class">.mp4</span> -y -f image2 -t <span class="number">0.001</span> -ss <span class="number">10</span> -s <span class="number">1920</span>x1080 output.jpg</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">ffmpeg</span> <span class="selector-tag">-i</span> <span class="selector-tag">input_video</span><span class="selector-class">.mp4</span> <span class="selector-tag">-ss</span> 00<span class="selector-pseudo">:00</span><span class="selector-pseudo">:06.000</span> <span class="selector-tag">-vframes</span> 1 <span class="selector-tag">output</span><span class="selector-class">.png</span></span><br></pre></td></tr></table></figure><h4 id="8-合成gif"><a href="#8-合成gif" class="headerlink" title="8.合成gif"></a>8.合成gif</h4><p>把视频的前 30 帧转换成一个 Gif</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> input_video<span class="selector-class">.mp4</span> -vframes <span class="number">30</span> -y -f gif output.gif</span><br></pre></td></tr></table></figure><p>将视频转成 gif</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">ffmpeg</span> <span class="selector-tag">-ss</span> 00<span class="selector-pseudo">:00</span><span class="selector-pseudo">:00.000</span> <span class="selector-tag">-i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> <span class="selector-tag">-pix_fmt</span> <span class="selector-tag">rgb24</span> <span class="selector-tag">-r</span> 10 <span class="selector-tag">-s</span> 320<span class="selector-tag">x240</span> <span class="selector-tag">-t</span> 00<span class="selector-pseudo">:00</span><span class="selector-pseudo">:10.000</span> <span class="selector-tag">output</span><span class="selector-class">.gif</span></span><br></pre></td></tr></table></figure><h4 id="9-更换视频的分辨率"><a href="#9-更换视频的分辨率" class="headerlink" title="9.更换视频的分辨率"></a>9.更换视频的分辨率</h4><p>可以使用如下命令更换视频的分辨率</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">    ffmpeg -i <span class="built_in">input</span>.mp4 -<span class="built_in">filter</span>:v scale=<span class="number">1280</span>:<span class="number">720</span> -<span class="keyword">c</span>:<span class="keyword">a</span> <span class="keyword">copy</span> output.mp4</span><br><span class="line"><span class="built_in">or</span></span><br><span class="line">    ffmpeg -i <span class="built_in">input</span>.mp4 -s <span class="number">1280</span>x720 -<span class="keyword">c</span>:<span class="keyword">a</span> <span class="keyword">copy</span> output.mp4</span><br></pre></td></tr></table></figure><h4 id="10-设置视频的宽高比"><a href="#10-设置视频的宽高比" class="headerlink" title="10.设置视频的宽高比"></a>10.设置视频的宽高比</h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> -aspect <span class="number">16</span>:<span class="number">9</span> output.mp4</span><br></pre></td></tr></table></figure><p>常见的宽高比：<code>16:9、4:3、16:10、5:4</code></p><h4 id="11-利用ffmpeg屏幕录制"><a href="#11-利用ffmpeg屏幕录制" class="headerlink" title="11.利用ffmpeg屏幕录制"></a>11.利用ffmpeg屏幕录制</h4><p>参考：<a href="https://trac.ffmpeg.org/wiki/Capture/Desktop" target="_blank" rel="noopener">https://trac.ffmpeg.org/wiki/Capture/Desktop</a></p><h4 id="12-添加水印"><a href="#12-添加水印" class="headerlink" title="12.添加水印"></a>12.添加水印</h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> -<span class="selector-tag">i</span> picture<span class="selector-class">.png</span> -filter_complex overlay=<span class="string">"(main_w/2)-(overlay_w/2):(main_h/2)-(overlay_h)/2"</span> output.mp4</span><br></pre></td></tr></table></figure><p><code>picture.png</code> 为水印图片， <code>overlay</code> 为水印位置</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;平时使用ffmpeg对视频解码成图片比较多，就稍微简单的了解一些ffmpeg常用的相关指令。下面是一些相关指令的介绍笔记。&lt;/p&gt;
&lt;h3 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h3&gt;&lt;p&gt;Debian/Ubuntu/Linux Mint 下安装 ffmpeg 很简单：&lt;/p&gt;
&lt;figure class=&quot;highlight routeros&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;apt-&lt;span class=&quot;builtin-name&quot;&gt;get&lt;/span&gt; install ffmpeg&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;其他操作系统安装方法，参考&lt;a href=&quot;https://www.ffmpeg.org/download.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官网&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;如果想要手工编译 ffmpeg 可以参考官方 &lt;a href=&quot;https://trac.ffmpeg.org/wiki#CompilingFFmpeg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;wiki&lt;/a&gt;。 Ubuntu/Debian/Mint 系手工编译 ffmpeg 参考 &lt;a href=&quot;https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;wiki&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="project" scheme="https://blog.nicehuster.cn/categories/project/"/>
    
    
      <category term="ffmpeg" scheme="https://blog.nicehuster.cn/tags/ffmpeg/"/>
    
  </entry>
  
  <entry>
    <title>bash命令并行</title>
    <link href="https://blog.nicehuster.cn/2019/10/18/bash-parallel/"/>
    <id>https://blog.nicehuster.cn/2019/10/18/bash-parallel/</id>
    <published>2019-10-18T11:13:39.000Z</published>
    <updated>2020-09-18T11:41:40.100Z</updated>
    
    <content type="html"><![CDATA[<p>&#160; &#160; &#160; &#160;在bash中，使用后台任务来实现任务的“多进程化”。在不加控制的模式下，不管有多少任务，全部都后台执行。也就是说，在这种情况下，有多少任务就有多少“进程”在同时执行。<br><a id="more"></a><br><strong>实例一：正常情况脚本</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ((i=0;i&lt;5;i++));<span class="keyword">do</span></span><br><span class="line">        &#123;</span><br><span class="line">                sleep 3;<span class="built_in">echo</span> 1&gt;&gt;aa &amp;&amp; <span class="built_in">echo</span> <span class="string">"done!"</span></span><br><span class="line">        &#125; </span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">wait</span></span><br><span class="line">cat aa|wc -l</span><br><span class="line">rm aa</span><br></pre></td></tr></table></figure></p><p>&#160; &#160; &#160; &#160;这种情况下，程序顺序执行，每个循环3s，共需15s左右。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ time bash test.sh </span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">5</span><br><span class="line"></span><br><span class="line">real    0m15.030s</span><br><span class="line">user    0m0.002s</span><br><span class="line">sys     0m0.003s</span><br></pre></td></tr></table></figure></p><p><strong>实例二：“多进程”实现</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ((i=0;i&lt;5;i++));<span class="keyword">do</span></span><br><span class="line">        &#123;</span><br><span class="line">                sleep 3;<span class="built_in">echo</span> 1&gt;&gt;aa &amp;&amp; <span class="built_in">echo</span> <span class="string">"done!"</span></span><br><span class="line">        &#125;&amp;</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">wait</span></span><br><span class="line">cat aa|wc -l</span><br><span class="line">rm aa</span><br></pre></td></tr></table></figure></p><p>&#160; &#160; &#160; &#160;这个实例实际上就在上面基础上多加了一个后台执行&amp;符号，此时应该是5个循环任务并发执行，最后需要3s左右时间。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ time bash test.sh </span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">5</span><br><span class="line"></span><br><span class="line">real    0m3.109s</span><br><span class="line">user    0m0.008s</span><br><span class="line">sys     0m0.100s</span><br></pre></td></tr></table></figure></p><p>&#160; &#160; &#160; &#160;效果非常明显。这里需要说明一下wait的左右。wait是等待前面的后台任务全部完成才往下执行，否则程序本身是不会等待的，这样对后面依赖前面任务结果的命令来说就可能出错。</p><p>&#160; &#160; &#160; &#160;以上所讲的实例都是进程数目不可控制的情况，下面描述如何准确控制并发的进程数目。</p><p> **实例三：“多进程可控”实现<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">tmp_fifofile=<span class="string">"/tmp/$$.fifo"</span></span><br><span class="line">mkfifo <span class="variable">$tmp_fifofile</span>      <span class="comment"># 新建一个fifo类型的文件</span></span><br><span class="line"><span class="built_in">exec</span> 6&lt;&gt;<span class="variable">$tmp_fifofile</span>      <span class="comment"># 将fd6指向fifo类型</span></span><br><span class="line">rm <span class="variable">$tmp_fifofile</span></span><br><span class="line">thread=15 <span class="comment"># 此处定义线程数</span></span><br><span class="line"><span class="keyword">for</span> ((i=0;i&lt;<span class="variable">$thread</span>;i++));<span class="keyword">do</span> </span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"><span class="keyword">done</span> &gt;&amp;6 <span class="comment"># 事实上就是在fd6中放置了$thread个回车符</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ((i=0;i&lt;50;i++));<span class="keyword">do</span> <span class="comment"># 50次循环</span></span><br><span class="line"><span class="built_in">read</span> -u6 </span><br><span class="line"><span class="comment"># 一个read -u6命令执行一次，就从fd6中减去一个回车符，然后向下执行，</span></span><br><span class="line"><span class="comment"># fd6中没有回车符的时候，就停在这了，从而实现了线程数量控制</span></span><br><span class="line"></span><br><span class="line">&#123; <span class="comment"># 此处子进程开始执行，被放到后台</span></span><br><span class="line">      &#123;sleep 3&#125; &amp;&amp; &#123; <span class="built_in">echo</span> <span class="string">"a_sub is finished"</span>&#125;</span><br><span class="line"></span><br><span class="line">     <span class="built_in">echo</span> &gt;&amp;6 <span class="comment"># 当进程结束以后，再向fd6中加上一个回车符，即补上了read -u6减去的那个</span></span><br><span class="line">&#125; &amp;</span><br><span class="line"></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">wait</span> <span class="comment"># 等待所有的后台子进程结束</span></span><br><span class="line"><span class="built_in">exec</span> 6&gt;&amp;- <span class="comment"># 关闭df6</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span> 0</span><br></pre></td></tr></table></figure></p><p>sleep 3s，线程数为15，一共循环50次，所以，此脚本一共的执行时间大约为12秒</p><p>即：<br>15x3=45, 所以 3x3s=9s<br>(50-45=5)&lt;15, 所以1x3s=3s<br>所以9s+3s = 12s</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ time bash multithread.sh </span><br><span class="line"></span><br><span class="line">real        0m12.025s</span><br><span class="line">user        0m0.020s</span><br><span class="line">sys         0m0.064s</span><br></pre></td></tr></table></figure><p>而当不使用多线程技巧的时候，执行时间为：50 x 3s = 150s。</p><p>注：此文转载自<a href="http://www.cnitblog.com/sysop/archive/2008/11/03/50974.aspx" target="_blank" rel="noopener">这里</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;#160; &amp;#160; &amp;#160; &amp;#160;在bash中，使用后台任务来实现任务的“多进程化”。在不加控制的模式下，不管有多少任务，全部都后台执行。也就是说，在这种情况下，有多少任务就有多少“进程”在同时执行。&lt;br&gt;
    
    </summary>
    
      <category term="project" scheme="https://blog.nicehuster.cn/categories/project/"/>
    
    
      <category term="-bash" scheme="https://blog.nicehuster.cn/tags/bash/"/>
    
  </entry>
  
  <entry>
    <title>mean-shift颜色校正</title>
    <link href="https://blog.nicehuster.cn/2019/08/06/mean-shift-correction/"/>
    <id>https://blog.nicehuster.cn/2019/08/06/mean-shift-correction/</id>
    <published>2019-08-06T11:13:39.000Z</published>
    <updated>2020-09-18T07:08:22.461Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇文章主要介绍了mean-shift算法理论以及在cv领域的应用比如，聚类，分割，跟踪。这篇文章主要介绍我在实际项目中使用mean-shift算法解决颜色校正问题。</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇文章主要介绍了mean-shift算法理论以及在cv领域的应用比如，聚类，分割，跟踪。这篇文章主要介绍我在实际项目中使用mean-shift算法解决颜色校正问题。&lt;/p&gt;
    
    </summary>
    
      <category term="project" scheme="https://blog.nicehuster.cn/categories/project/"/>
    
    
      <category term="mean shift" scheme="https://blog.nicehuster.cn/tags/mean-shift/"/>
    
  </entry>
  
  <entry>
    <title>mean-shift算法详解</title>
    <link href="https://blog.nicehuster.cn/2019/08/05/shift/"/>
    <id>https://blog.nicehuster.cn/2019/08/05/shift/</id>
    <published>2019-08-05T11:13:39.000Z</published>
    <updated>2020-09-18T06:42:42.507Z</updated>
    
    <content type="html"><![CDATA[<p>MeanShift最初由Fukunaga和Hostetler在1975年提出，但是一直到2000左右这篇PAMI的论文Mean Shift: A Robust Approach Toward Feature Space Analysis，将它的原理和收敛性等重新整理阐述，并应用于计算机视觉和图像处理领域之后，才逐渐为人熟知。在了解mean-shift算法之前，先了解一下概率密度估计的概念。</p><a id="more"></a><h3 id="概率密度估计"><a href="#概率密度估计" class="headerlink" title="概率密度估计"></a>概率密度估计</h3><p>密度估计是指有给定样本集和求解随机变量的分布密度函数，解决这一问题的方法包括：参数估计和非参数估计。</p><p><strong>参数估计</strong>：在我们已经知道观测数据符合某些模型的情况下，我们可以利用参数估计的方法来确定这些参数值，然后得出概率密度模型。前提是观测数据服从一个已知概率密度函数。</p><p><strong>非参数估计</strong>：无需任何先验知识完全依靠特征空间中样本点计算其密度估计值.可以处理任意概率分布，不必假设服从已知分布；常用的无参数密度估计方法有：直方图法、最近邻域法和核密度估计法。<strong>MeanShift算法正属于核密度估计法</strong>。无需任何先验知识完全依靠特征空间中样本点计算其密度估计值。</p><h4 id="核密度估计"><a href="#核密度估计" class="headerlink" title="核密度估计"></a>核密度估计</h4><p>mean shift算法使用核函数估计样本密度，假设对于大小为$n$,维度为$d$ 的数据集，$D=\left\{x_{1}, x_{2}, x_{3}, \ldots x_{n}\right\}, D \in R^{d}$ ，核函数K的带宽为h，则该函数的核密度估计为：</p><script type="math/tex; mode=display">f(x)=\frac{1}{n h^{d}} \sum_{i=1}^{n} K\left(\frac{x-x_{i}}{h}\right)</script><p>定义满足核函数条件为：</p><script type="math/tex; mode=display">K(x)=c_{k, d} k\left(\|x\|^{2}\right), \quad \int c_{k, d} \cdot K(x) d x=1</script><p>其中，$c_{k,d}$ 系数是归一化常数，使得$K(x)$ 的积分为1.</p><p>常见的核函数有高斯核函数，其形式如下：</p><script type="math/tex; mode=display">N(x)=\frac{1}{\sqrt{2 \pi h}} e^{-\frac{x^{2}}{2 h^{2}}}</script><p>其中，h称为带宽(bandwidth)，不同带宽的核函数如下图所示：</p><p><img src="/img/gaussian.png" alt="img"></p><p>从高斯函数的图像可以看出，当带宽h一定时，样本点之间的距离越近，其核函数的值越大，当样本点之间的距离相等时，随着高斯函数的带宽h的增加，核函数的值在减小。高斯核的python实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian_kernel</span><span class="params">(distance, bandwidth)</span>:</span></span><br><span class="line">    <span class="string">''' 高斯核函数</span></span><br><span class="line"><span class="string">    :param distance: 欧氏距离计算函数</span></span><br><span class="line"><span class="string">    :param bandwidth: 核函数的带宽</span></span><br><span class="line"><span class="string">    :return: 高斯函数值</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    m = np.shape(distance)[<span class="number">0</span>]  <span class="comment"># 样本个数</span></span><br><span class="line">    right = np.mat(np.zeros((m, <span class="number">1</span>)))  <span class="comment"># m * 1 矩阵</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        right[i, <span class="number">0</span>] = (<span class="number">-0.5</span> * distance[i] * distance[i].T) / (bandwidth * bandwidth)</span><br><span class="line">        right[i, <span class="number">0</span>] = np.exp(right[i, <span class="number">0</span>])</span><br><span class="line">    left = <span class="number">1</span> / (bandwidth * math.sqrt(<span class="number">2</span> * math.pi))</span><br><span class="line">    gaussian_val = left * right</span><br><span class="line">    <span class="keyword">return</span> gaussian_val</span><br></pre></td></tr></table></figure><p>以高斯核估计一维数据集的密度为例，每个样本点都设置以该样本为中心的高斯分布，累加所有高斯分布，就得到该数据集的密度。</p><p><img src="/img/image-20200918115326176.png" alt="image-20200918115326176"></p><p>其中虚线表示每个样本点的高斯核，实现表示累加后所有样本高斯核后的数据集密度。</p><h3 id="mean-shift-算法理论"><a href="#mean-shift-算法理论" class="headerlink" title="mean-shift 算法理论"></a>mean-shift 算法理论</h3><h4 id="朴素mean-shift向量形式"><a href="#朴素mean-shift向量形式" class="headerlink" title="朴素mean-shift向量形式"></a>朴素mean-shift向量形式</h4><p>对于给定的d维度空间中的n个样本点$\left\{x_{1}, x_{2}, x_{3}, \ldots x_{n}\right\}$ ，则对于x点，其mean-shift向量的基本形式为：</p><script type="math/tex; mode=display">M_{h}(x)=\frac{1}{k} \sum_{x_{i} \in S_{h}}\left(x_{i}-x\right)</script><p><img src="/img/image-20200918115606176.png" alt="image-20200918115606176"></p><p>其中$S_h$指的是一个半径为h的高维球区域，如上图中的圆形区域。$S_h$的定义为：</p><script type="math/tex; mode=display">S_{h}(x)=\left(y \mid(y-x)(y-x)^{T} \leq h^{2}\right)</script><p>里面所有点与圆心为起点形成的向量相加的结果就是Mean shift向量。下图黄色箭头就是 $M_h$ (mean-shift 向量)。</p><p><img src="/img/image-20200918115822021.png" alt="image-20200918115822021"></p><p>对于Mean Shift算法，是一个迭代的步骤，即先算出当前点的偏移均值，将该点移动到此偏移均值，然后以此为新的起始点，继续移动，直到满足最终的条件。</p><p><img src="/img/wpsicYkqP.jpg" alt="img"> </p><h4 id="引入核函数的Mean-Shift向量形式"><a href="#引入核函数的Mean-Shift向量形式" class="headerlink" title="引入核函数的Mean Shift向量形式"></a>引入核函数的Mean Shift向量形式</h4><p>Mean Shift算法的基本目标是将样本点向局部密度增加的方向移动，我们常常所说的均值漂移向量就是指局部密度增加最快的方向。上节通过引入高斯核可以知道数据集的密度，梯度是函数增加最快的方向，因此，数据集密度的梯度方向就是密度增加最快的方向。</p><script type="math/tex; mode=display">f(x)=\frac{1}{n h^{d}} \sum_{i=1}^{n} K\left(\frac{x-x_{i}}{h}\right)</script><p>高斯核：$K(x)=c_{k, d} k\left(|x|^{2}\right)$</p><script type="math/tex; mode=display">\begin{aligned}\nabla f(x) &=\frac{2 c_{k, d}}{n h^{d+2}} \sum_{i=1}^{n}\left(x_{i}-x\right) g\left(\left\|\frac{x-x_{i}}{h}\right\|^{2}\right) \\&\left.=\frac{2 c_{k, d}}{n h^{d+2}}\left[\sum_{i=1}^{n} g\left(\left\|\frac{x-x_{i}}{h}\right\|\right)\right]^{2}\right)\left[\frac{\sum_{i=1}^{n} x_{i} g\left(\left\|\frac{x-x_{i}}{h}\right\|^{2}\right)}{\sum_{i=1}^{n} g\left(\left\|\frac{x-x_{i}}{h}\right\|^{2}\right)}-x\right]\end{aligned}</script><p>其中$g(s)=-k^{\prime}(s)$ ，上式的第一项为实数值。</p><p>因此第二项的向量方向与梯度方向一致，第二项的表达式为：</p><script type="math/tex; mode=display">m_{h}(x)=\frac{\sum_{i=1}^{n} x_{i} g\left(\mid \frac{x-x_{i}}{h} \|^{2}\right)}{\sum_{i=1}^{n} g\left(\left\|\frac{x-x_{i}}{h}\right\|^{2}\right)}-x</script><p>上式的含义就是本篇文章的主题：<strong>均值漂移</strong>。由上式推导可知：<strong>均值漂移向量所指的方向是密度增加最大的方向</strong>。</p><p><img src="/img/image-20200918120319712.png" alt="image-20200918120319712"></p><p>要使$\nabla f(x)=0$ ，当且仅当$m_{h}(\mathrm{x})=0$ ，可以得出新坐标：</p><script type="math/tex; mode=display">x=\frac{\sum_{i=1}^{n} x_{i} g\left(\left\|\frac{x-x_{i}}{h}\right\|^{2}\right)}{\sum_{i=1}^{n} g\left(\left\|\frac{x-x_{i}}{h}\right\|^{2}\right)}</script><p>因此，Mean Shift算法流程为：</p><blockquote><p>（1）计算每个样本的均值漂移向量 $m_{h}(\mathrm{x})$ ;</p><p>（2）对每个样本点以 $m_{h}(\mathrm{x})$ 进行平移，即：$x=x+m_{h}(x)$ ；</p><p>（3）重复（1）（2），直到样本点收敛，即：$m_{h}(\mathrm{x})&lt;\varepsilon$ (人工设定)或者迭代次数小于设定值；</p><p>（4）收敛到相同点的样本被认为是同一簇类的成员；</p></blockquote><h3 id="mean-shift应用"><a href="#mean-shift应用" class="headerlink" title="mean-shift应用"></a>mean-shift应用</h3><h4 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h4><p>Mean-Shift聚类就是对于集合中的每一个元素，对它执行下面的操作：把该元素移动到它邻域中所有元素的特征值的均值的位置，不断重复直到收敛。准确的说，不是真正移动元素，而是把该元素与它的收敛位置的元素标记为同一类。在实际中，为了加速，初始化的时候往往会初始化多个窗口，然后再进行聚类。</p><p>对应python的实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"> </span><br><span class="line">MIN_DISTANCE = <span class="number">0.00001</span>  <span class="comment"># 最小误差</span></span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">euclidean_dist</span><span class="params">(pointA, pointB)</span>:</span></span><br><span class="line">    <span class="comment"># 计算pointA和pointB之间的欧式距离</span></span><br><span class="line">    total = (pointA - pointB) * (pointA - pointB).T</span><br><span class="line">    <span class="keyword">return</span> math.sqrt(total)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gaussian_kernel</span><span class="params">(distance, bandwidth)</span>:</span></span><br><span class="line">    <span class="string">''' 高斯核函数</span></span><br><span class="line"><span class="string">    :param distance: 欧氏距离计算函数</span></span><br><span class="line"><span class="string">    :param bandwidth: 核函数的带宽</span></span><br><span class="line"><span class="string">    :return: 高斯函数值</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    m = np.shape(distance)[<span class="number">0</span>]  <span class="comment"># 样本个数</span></span><br><span class="line">    right = np.mat(np.zeros((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        right[i, <span class="number">0</span>] = (<span class="number">-0.5</span> * distance[i] * distance[i].T) / (bandwidth * bandwidth)</span><br><span class="line">        right[i, <span class="number">0</span>] = np.exp(right[i, <span class="number">0</span>])</span><br><span class="line">    left = <span class="number">1</span> / (bandwidth * math.sqrt(<span class="number">2</span> * math.pi))</span><br><span class="line">    gaussian_val = left * right</span><br><span class="line">    <span class="keyword">return</span> gaussian_val</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shift_point</span><span class="params">(point, points, kernel_bandwidth)</span>:</span></span><br><span class="line">    <span class="string">'''计算均值漂移点</span></span><br><span class="line"><span class="string">    :param point: 需要计算的点</span></span><br><span class="line"><span class="string">    :param points: 所有的样本点</span></span><br><span class="line"><span class="string">    :param kernel_bandwidth: 核函数的带宽</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        point_shifted：漂移后的点</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    points = np.mat(points)</span><br><span class="line">    m = np.shape(points)[<span class="number">0</span>]  <span class="comment"># 样本个数</span></span><br><span class="line">    <span class="comment"># 计算距离</span></span><br><span class="line">    point_distances = np.mat(np.zeros((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        point_distances[i, <span class="number">0</span>] = euclidean_dist(point, points[i])</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 计算高斯核</span></span><br><span class="line">    point_weights = gaussian_kernel(point_distances, kernel_bandwidth)</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 计算分母</span></span><br><span class="line">    all = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        all += point_weights[i, <span class="number">0</span>]</span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 均值偏移</span></span><br><span class="line">    point_shifted = point_weights.T * points / all</span><br><span class="line">    <span class="keyword">return</span> point_shifted</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_points</span><span class="params">(mean_shift_points)</span>:</span></span><br><span class="line">    <span class="string">'''计算所属的类别</span></span><br><span class="line"><span class="string">    :param mean_shift_points:漂移向量</span></span><br><span class="line"><span class="string">    :return: group_assignment：所属类别</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    group_assignment = []</span><br><span class="line">    m, n = np.shape(mean_shift_points)</span><br><span class="line">    index = <span class="number">0</span></span><br><span class="line">    index_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        item = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            item.append(str((<span class="string">"%5.2f"</span> % mean_shift_points[i, j])))</span><br><span class="line"> </span><br><span class="line">        item_1 = <span class="string">"_"</span>.join(item)</span><br><span class="line">        <span class="keyword">if</span> item_1 <span class="keyword">not</span> <span class="keyword">in</span> index_dict:</span><br><span class="line">            index_dict[item_1] = index</span><br><span class="line">            index += <span class="number">1</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        item = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            item.append(str((<span class="string">"%5.2f"</span> % mean_shift_points[i, j])))</span><br><span class="line"> </span><br><span class="line">        item_1 = <span class="string">"_"</span>.join(item)</span><br><span class="line">        group_assignment.append(index_dict[item_1])</span><br><span class="line">    <span class="keyword">return</span> group_assignment</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_mean_shift</span><span class="params">(points, kernel_bandwidth=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="string">'''训练Mean Shift模型</span></span><br><span class="line"><span class="string">    :param points: 特征数据</span></span><br><span class="line"><span class="string">    :param kernel_bandwidth: 核函数带宽</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">        points：特征点</span></span><br><span class="line"><span class="string">        mean_shift_points：均值漂移点</span></span><br><span class="line"><span class="string">        group：类别</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    mean_shift_points = np.mat(points)</span><br><span class="line">    max_min_dist = <span class="number">1</span></span><br><span class="line">    iteration = <span class="number">0</span></span><br><span class="line">    m = np.shape(mean_shift_points)[<span class="number">0</span>]  <span class="comment"># 样本的个数</span></span><br><span class="line">    need_shift = [<span class="keyword">True</span>] * m  <span class="comment"># 标记是否需要漂移</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 计算均值漂移向量</span></span><br><span class="line">    <span class="keyword">while</span> max_min_dist &gt; MIN_DISTANCE:</span><br><span class="line">        max_min_dist = <span class="number">0</span></span><br><span class="line">        iteration += <span class="number">1</span></span><br><span class="line">        print(<span class="string">"iteration : "</span> + str(iteration))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, m):</span><br><span class="line">            <span class="comment"># 判断每一个样本点是否需要计算偏置均值</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> need_shift[i]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            p_new = mean_shift_points[i]</span><br><span class="line">            p_new_start = p_new</span><br><span class="line">            p_new = shift_point(p_new, points, kernel_bandwidth)  <span class="comment"># 对样本点进行偏移</span></span><br><span class="line">            dist = euclidean_dist(p_new, p_new_start)  <span class="comment"># 计算该点与漂移后的点之间的距离</span></span><br><span class="line"> </span><br><span class="line">            <span class="keyword">if</span> dist &gt; max_min_dist:  <span class="comment"># 记录是有点的最大距离</span></span><br><span class="line">                max_min_dist = dist</span><br><span class="line">            <span class="keyword">if</span> dist &lt; MIN_DISTANCE:  <span class="comment"># 不需要移动</span></span><br><span class="line">                need_shift[i] = <span class="keyword">False</span></span><br><span class="line"> </span><br><span class="line">            mean_shift_points[i] = p_new</span><br><span class="line">    <span class="comment"># 计算最终的group</span></span><br><span class="line">    group = group_points(mean_shift_points)  <span class="comment"># 计算所属的类别</span></span><br><span class="line">    <span class="keyword">return</span> np.mat(points), mean_shift_points, group</span><br></pre></td></tr></table></figure><p>以上代码实现了基本的流程，但是执行效率很慢，正式使用时建议使用scikit-learn库中的<a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html#sphx-glr-auto-examples-cluster-plot-mean-shift-py" target="_blank" rel="noopener">MeanShift</a>。scikit-learn 中mean-shift的使用方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> MeanShift, estimate_bandwidth</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Generate sample data</span></span><br><span class="line">centers = [[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">-1</span>, <span class="number">-1</span>], [<span class="number">1</span>, <span class="number">-1</span>]]</span><br><span class="line">X, _ = make_blobs(n_samples=<span class="number">10000</span>, centers=centers, cluster_std=<span class="number">0.6</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Compute clustering with MeanShift</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The following bandwidth can be automatically detected using</span></span><br><span class="line">bandwidth = estimate_bandwidth(X, quantile=<span class="number">0.2</span>, n_samples=<span class="number">500</span>)</span><br><span class="line"></span><br><span class="line">ms = MeanShift(bandwidth=bandwidth, bin_seeding=<span class="keyword">True</span>)</span><br><span class="line">ms.fit(X)</span><br><span class="line">labels = ms.labels_</span><br><span class="line">cluster_centers = ms.cluster_centers_</span><br><span class="line"></span><br><span class="line">labels_unique = np.unique(labels)</span><br><span class="line">n_clusters_ = len(labels_unique)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"number of estimated clusters : %d"</span> % n_clusters_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># #############################################################################</span></span><br><span class="line"><span class="comment"># Plot result</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> cycle</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>)</span><br><span class="line">plt.clf()</span><br><span class="line"></span><br><span class="line">colors = cycle(<span class="string">'bgrcmykbgrcmykbgrcmykbgrcmyk'</span>)</span><br><span class="line"><span class="keyword">for</span> k, col <span class="keyword">in</span> zip(range(n_clusters_), colors):</span><br><span class="line">    my_members = labels == k</span><br><span class="line">    cluster_center = cluster_centers[k]</span><br><span class="line">    plt.plot(X[my_members, <span class="number">0</span>], X[my_members, <span class="number">1</span>], col + <span class="string">'.'</span>)</span><br><span class="line">    plt.plot(cluster_center[<span class="number">0</span>], cluster_center[<span class="number">1</span>], <span class="string">'o'</span>, markerfacecolor=col,</span><br><span class="line">             markeredgecolor=<span class="string">'k'</span>, markersize=<span class="number">14</span>)</span><br><span class="line">plt.title(<span class="string">'Estimated number of clusters: %d'</span> % n_clusters_)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><p><code>number of estimated clusters : 3</code></p><p><img src="/img/sphx_glr_plot_mean_shift_001.png" alt="Estimated number of clusters: 3"></p><h4 id="图像分割"><a href="#图像分割" class="headerlink" title="图像分割"></a>图像分割</h4><p>对于图像分割，最简单直接的方法就是对图像上每个点的像素值进行聚类。我们对下图的像素点映射为RGB三维空间：</p><p><img src="/img/image-20200918143152873.png" alt="image-20200918143152873"></p><p>每个样本点最终会移动到核概率密度的峰值，移动到相同峰值的样本点属于同一种颜色，下图给出图像分割结果：</p><p><img src="/img/image-20200918143444607.png" alt="image-20200918143444607"></p><h4 id="目标跟踪"><a href="#目标跟踪" class="headerlink" title="目标跟踪"></a>目标跟踪</h4><p> 基于meanshift的目标跟踪算法通过分别计算目标区域和候选区域内像素的特征值概率得到关于目标模型和候选模型的描述，然后利用相似函数度量初始帧目标模型和当前帧的候选模版的相似性，选择使相似函数最大的候选模型并得到关于目标模型的Meanshift向量，这个向量正是目标由初始位置向正确位置移动的向量。由于均值漂移算法的快速收敛性，通过不断迭代计算Meanshift向量，算法最终将收敛到目标的真实位置，达到跟踪的目的。</p><p><img src="/img/image-20200918143502507.png" alt="image-20200918143502507"></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><blockquote><p><a href="http://www.360doc.com/content/19/0623/22/99071_844418459.shtml" target="_blank" rel="noopener">深入剖析meanshift聚类算法原理</a></p><p><a href="https://www.biaodianfu.com/mean-shift.html" target="_blank" rel="noopener">聚类算法之Mean Shift</a></p><p><a href="https://spin.atomicobject.com/2015/05/26/mean-shift-clustering/" target="_blank" rel="noopener">Mean Shift Clustering</a></p><p><a href="https://www.bogotobogo.com/python/OpenCV_Python/python_opencv3_mean_shift_tracking_segmentation.php" target="_blank" rel="noopener">Mean shift tracking</a> </p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MeanShift最初由Fukunaga和Hostetler在1975年提出，但是一直到2000左右这篇PAMI的论文Mean Shift: A Robust Approach Toward Feature Space Analysis，将它的原理和收敛性等重新整理阐述，并应用于计算机视觉和图像处理领域之后，才逐渐为人熟知。在了解mean-shift算法之前，先了解一下概率密度估计的概念。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="mean shift" scheme="https://blog.nicehuster.cn/tags/mean-shift/"/>
    
  </entry>
  
  <entry>
    <title>图像分类算法优化技巧</title>
    <link href="https://blog.nicehuster.cn/2019/06/13/cnn_tricks/"/>
    <id>https://blog.nicehuster.cn/2019/06/13/cnn_tricks/</id>
    <published>2019-06-13T11:13:39.000Z</published>
    <updated>2020-09-18T02:40:04.736Z</updated>
    
    <content type="html"><![CDATA[<p>上午看了一下<a href="https://arxiv.org/pdf/1812.01187.pdf" target="_blank" rel="noopener">Bag of Tricks for Image Classification with Convolutional Neural Networks</a>这篇文章，该文提到了许多关于如何提升CNN性能的trick。trick在CNN中起着非常重要的作用，然而很少有文章详细地介绍这些trick的使用。而这篇文章则对一些常用的trick做了详尽的介绍以及使用trick前后带来的对比，可以说是满满的干货，对应的代码实现，在<a href="https://github.com/dmlc/gluon-cv" target="_blank" rel="noopener">这里</a>。文章提到的trick包括：模型结构调整，数据增强，学习率调整，训练速度优化等等。<br><a id="more"></a></p><h3 id="加快模型训练"><a href="#加快模型训练" class="headerlink" title="加快模型训练"></a>加快模型训练</h3><p>目前加快模型训练有效的两种方式是：（1）使用大的batch size；（2）采用低精度训练；</p><p>（1）<strong>使用大的batch size训练</strong><br>使用较大的 batch size 时，如果还是使用同样的 epochs 数量进行运算，则准确度往往低于 batch size 较小的场景。为了确保使用较大的 batch size 训练能够在相同 epochs 前提下获得与较小的 batch size 相近的测试准确度。这部分具体可以参考<a href="https://arxiv.org/pdf/1706.02677.pdf" target="_blank" rel="noopener">Accurate, Large Minibatch SGD</a>这篇文章。作者总结了如下几种解决方案：</p><p><strong>A.增大学习率</strong>：如果我们将 batch size 由 B 增加至 kB，我们亦需要将学习率由η增加至 kη（其中 k 为倍数）。</p><p><strong>B.warm up</strong>：先用一个小的学习率先训几个epoch（warmup），因为网络的参数是随机初始化的，假如一开始就采用较大的学习率容易出现数值不稳定，这是使用warmup的原因。等到训练过程基本稳定了就可以使用原先设定的初始学习率进行训练了。论文中提到了warmup采用线性增加策略实现。举例而言，假设，初始学习率为η,选择前m个batch用于warm up，则在第i个batch时, 1 ≤ i ≤ m，设置学习率为 iη/m。</p><p><strong>C.每个残差模块最后的BN层中γ设置为0</strong>：BN层的γ、β参数是用来对标准化后的输入做线性变换的，也就是γx^+β，一般γ参数都会初始化为1，β参数会初始化为0，作者认为初始化为0更有利于模型的训练。</p><p><strong>D.不对bias执行weight decay操作</strong>：weight decay通常用于所有网络层的可学习参数（包括weight和bias）。其作用等效于对所有参数做L2正则化，以达到减少模型过拟合的作用。作者推荐仅仅只对weight进行weight decay操作。</p><p>（2）<strong>使用低精度训练</strong><br>采用低精度比如FP16训练可以在数值层面上对网络训练加速。通过大多数网络训练都是采用FP32精度训练，是因为网络的输入，网络参数以及网络输出都是采用FP32。如果能使用16位浮点型参数进行训练，就可以大大加快模型的训练速度。下表是使用大的batch size和半精度FP16进行训练的前后对比，其中baseline 使用BS=256,FP32，efficient使用BS=1024,FP16：<br><img src="/img/cnn_trick.png" alt><br>从实验结果可以看出，相比baseline,训练速度得到明显提升，而且准确率上也得到了一定程度提升。更加详细的对比实验如下：<br><img src="/img/cnn_trick2.png" alt></p><h3 id="模型结构调整"><a href="#模型结构调整" class="headerlink" title="模型结构调整"></a>模型结构调整</h3><p>这种模型结构调整是在不显著增加计算量的情况下对模型性能进行进一步提升。作者以ResNet为例进行优化。ResNet通常包括一个input stem，4个stage和1个output。下图展示的是原始resnet50结构。<br><img src="/img/cnn_trick_resnet.png" alt><br>作者在downsampling 部分，input stem以及Path B部分做了一些小的调整，对应如下图(a),(b),(c)<br><img src="/img/cnn_trick_resnet1.png" alt><br>实验结果对比如下：<br><img src="/img/cnn_trick_resnet-res.png" alt><br>从实验结果可以看出，这些结构上微小的调整，并没有对计算量带来较大的变化，但对accuracy带来不小的提升。</p><h3 id="模型训练调优"><a href="#模型训练调优" class="headerlink" title="模型训练调优"></a>模型训练调优</h3><p>作者在这一部分提到了四种调优技巧：</p><p><strong>采用cosine学习率衰减策略</strong>：实验对比结果如下，相比于常用的step decay,cosine decay在起始阶段开始衰减学习率，在step decay的学习率下降了10x时，cosine依然可以保持较大的学习率，这潜在的提高了训练速度。<br><img src="/img/cosine.png" alt></p><p><strong>知识蒸馏</strong>：使用一个效果更好的teacher model训练student model，使得student model在模型结构不改变的情况下提升效果。作者采用ResNet-152作为teacher model，用ResNet-50作为student model。代码上通过在ResNet网络后添加一个蒸馏损失函数实现，这个损失函数用来评价teacher model输出和student model输出的差异，因此整体的损失函数原损失函数和蒸馏损失函数的结合：<br><img src="/img/distill.png" alt><br>其中p表示真实标签，z表示student model的全连接层输出，r表示teacher model的全连接层输出，T是超参数，用来平滑softmax函数的输出。</p><p><strong>mixup</strong>：mixup也是一种数据增强操作，其大致操作是，每次读取2张输入图像，假设用（xi，yi）和（xj，yj）表示，那么通过如下公式就可以合成得到一张新的图像（x，y），然后用这张新图像进行训练：<br><img src="/img/mixup.png" alt><br>其中λ属于[0,1]服从Beta分布。实验结果如下<br><img src="/img/cnn_trick_res.png" alt></p><p>此外，作者将这些trick应用于其他图像任务中，比如目标检测，图像分割上同样有效，可以带来2-3个点的提升。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上午看了一下&lt;a href=&quot;https://arxiv.org/pdf/1812.01187.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Bag of Tricks for Image Classification with Convolutional Neural Networks&lt;/a&gt;这篇文章，该文提到了许多关于如何提升CNN性能的trick。trick在CNN中起着非常重要的作用，然而很少有文章详细地介绍这些trick的使用。而这篇文章则对一些常用的trick做了详尽的介绍以及使用trick前后带来的对比，可以说是满满的干货，对应的代码实现，在&lt;a href=&quot;https://github.com/dmlc/gluon-cv&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;。文章提到的trick包括：模型结构调整，数据增强，学习率调整，训练速度优化等等。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="tricks" scheme="https://blog.nicehuster.cn/tags/tricks/"/>
    
  </entry>
  
  <entry>
    <title>细粒度图像识别</title>
    <link href="https://blog.nicehuster.cn/2019/06/12/fine-grain/"/>
    <id>https://blog.nicehuster.cn/2019/06/12/fine-grain/</id>
    <published>2019-06-12T11:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>一般而言，图像识别分为两种：传统图像识别和细粒度图像识别。前者指的是对一些大的类别比如汽车、动物、植物等大的类别进行分类，这是属于粗粒度的图像识别。而后者则是在某个类别下做进一步分类。比如在狗的类别下区分狗的品种是哈士奇、柯基、萨摩还是阿拉斯加等等，这是属于细粒度图像识别。<br><img src="/img/fine-grain.png" alt><br><a id="more"></a></p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>在细粒度图像识别领域，经典的基准数据集包括：</p><ul><li>鸟类数据集CUB200-2011，11788张图像，200个细粒度分类</li><li>狗类数据集Stanford Dogs，20580张图像，120个细粒度分类</li><li>花类数据集Oxford Flowers，8189张图像，102个细粒度分类</li><li>飞机数据集Aircrafts，10200张图像，100个细粒度分类</li><li>汽车数据集Stanford Cars，16185张图像，196个细粒度分类</li></ul><p>细粒度图像分类作为一个热门的研究方向，每年的计算机视觉顶会都会举办一些workshop和挑战赛，比如Workshop on Fine-Grained Visual Categorization和iFood Classification Challenge。</p><h3 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h3><p><img src="/img/fine-grain-challenge.png" alt><br>上图展示的是CUB20鸟类数据集的部分图片。不同行表示的不同的鸟类别。很明显，这些鸟类数据集在同一类别上存在巨大差异，比如上图中每一行所展示的一样，这些差异包括姿态、背景等差异。但在不同类别的鸟类上却又存在着差异性小的问题，比如上图展示的第一列，第一列虽然分别属于不同类别，但却又十分相似。</p><p>因此可以看出，细粒度图像识别普遍存在类内差异性大（large intra-class variance）和类间差异性小（small inter-class variance）的特点。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>细粒度图像识别同样是作为图像分类任务，因此也可以直接使用通用图像识别中一些算法来做，比如直接使用resnet,vgg等网络模型直接训练识别，通常在数据集上，比如CUB200上就可以达到75%的准确率，但这种方法离目前的SOTA方法的精度至少差了10个点。</p><p>目前细粒度图像识别方法大致可以分为两类：</p><p>1.<strong>基于强监督学习方法</strong>：这里指的强监督信息是指bounding box或者landmark，举个例子，针对某一种鸟类，他和其他的类别的差异一般在于它的嘴巴、腿部，羽毛颜色等<br><img src="/img/v2-f04c284bb40f1fc3da258bb6764d4728_hd.jpg" alt><br>主流的方法像Part-based R-CNN，Pose Normalized CNN,Part-Stacked CNN等。</p><p>2.<strong>基于弱监督学习方法</strong>：什么是弱监督信息呢？就是说没有bounding box或者landmark信息，只有类别信息，开山之作应该属于2015年Bilinear CNN，这个模型当时在CUB200上是state of the art，即使和强监督学习方法相比也只是差1个点左右。</p><p>关于前几年细粒度图像分析的综述，可以参考<a href="https://zhuanlan.zhihu.com/p/24738319" target="_blank" rel="noopener">这里</a>。由于强监督学习方法中对于大规模数据集来说，bounding box和landmark标注成本较高，因此，现在主流的研究方法都是是基于弱监督学习方法。</p><p>下面是我要介绍的近1/2年来比较有代表性的顶会paper，这些paper都是基于弱监督信息，自主去挖掘Discriminative Region。</p><h3 id="Look-Closer-to-See-Better-Recurrent-Attention-Convolutional-Neural-Network-for-Fine-grained-Image-Recognition"><a href="#Look-Closer-to-See-Better-Recurrent-Attention-Convolutional-Neural-Network-for-Fine-grained-Image-Recognition" class="headerlink" title="Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-grained Image Recognition"></a><center>Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-grained Image Recognition</center></h3><p>代码链接：<a href="https://github.com/Jianlong-Fu/Recurrent-Attention-CNN" target="_blank" rel="noopener">https://github.com/Jianlong-Fu/Recurrent-Attention-CNN</a><br>这篇文章是CVPR2017的一篇oral paper。细粒度图像识别的挑战主要包括两个方面：判别力区域定位以及从判别力区域学习精细化特征。RA-CNN以一种相互强化的方式递归地学习判别力区域attention和基于区域的特征表示。具体模型结构如下：<br><img src="/img/ra-cnn.png" alt></p><h4 id="主要思路"><a href="#主要思路" class="headerlink" title="主要思路"></a>主要思路</h4><p>如上图，每一行表示一个普通的CNN网络，<br>（1）图片a1进入b1（堆叠多个卷积层）之后，分成两个部分，一个部分到c1连接fc+softmax进行普通的分类；另一个部分进入d1，Attention Proposal Network得到一个region proposal。<br>（2）在原图上利用d1提出的region proposal，在原图上crop出一个更有判别性的小区域，插值之后得到a2,同样的道理得到a3。<br>可以看出特征区域经过两个APN之后不断放大和精细化，为了使得APN选取的特征区域是图像中最具有判别性的区域，作者引入了一个Ranking loss：即强迫a1、a2、a3区域的分类confidence score越来越高(图片最后一列的对应Pt概率越来越大)。这样以来，联合普通的分类损失，使网络不断细化discriminative attention region。</p><h4 id="部分细节"><a href="#部分细节" class="headerlink" title="部分细节"></a>部分细节</h4><p><strong>attention 定位和放大</strong><br>作者使用二维boxcar函数作为attention mask与原图相乘得到候选区域位置。这样做的目的在于实现APN的端对端训练。因为普通的crop操作不可导。<br><strong>损失函数</strong><br>该模型的损失函数包含两个部分，一部分是每一路经过fc和softmax之后的一个分类误差；一部分是Ranking loss使得越精细化的区域得到了置信度分数越高。<br><img src="/img/ra-cnn-loss.png" alt><br>对于ranking loss，<br><img src="/img/rank-loss.png" alt><br>在训练的过程中，迫使$p^{(s+1)}_t &gt;p^{(s)}_t$。</p><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p>在CUB-200-2011数据集上<br><img src="/img/ra-cnn-res.png" alt></p><h3 id="Pairwise-Confusion-for-Fine-Grained-Visual-Classification"><a href="#Pairwise-Confusion-for-Fine-Grained-Visual-Classification" class="headerlink" title="Pairwise Confusion for Fine-Grained Visual Classification"></a><center>Pairwise Confusion for Fine-Grained Visual Classification</center></h3><p>代码链接：<a href="https://github.com/abhimanyudubey/confusion" target="_blank" rel="noopener">https://github.com/abhimanyudubey/confusion</a><br>这是ECCV2018的一篇文章，这篇文章提出了一种Pairwise Confusion正则化方法，主要用于解决在细粒度图像分类问题上类间相似性和样本少导致过拟合的问题。在通用图像分类问题上，由于数据集一般较大，直接使用交叉熵损失函数就可以迫使网络学习类间差异性。然而对于细粒度图像分类问题而言，数据集小，且普遍存在类间差异较小，类内差异较大的特点。假如对于两张鸟类图像样本，内容相似却有着不同的标签，直接最小化交叉熵损失将会迫使网络去学习图像本身的差异比如一些差异性较大的背景，而不能很好的挖掘不同鸟类的细粒度区别。<br><img src="/img/pc.png" alt><br>因此，作者提出了Pairwise Confusion方法 ，网络结构如上图所示。网络采用Siamese结构共享权值，对于一个Batch的图片会分成两部分，组成很多“图片对”，如果这些图片对属于相同的label，那么就把两张图片分别求Cross entropy loss；如果有一对图片属于不同的label，那么在分别对他们求Cross Entropy Loss的同时还要附加一个Euclidean Confusion作为惩罚项</p><p>论文的主要出发点还是制约不同类别图片表示的特征向量之间的距离。作为一个涨点的trick。可以加在任何细粒度识别算法中。下面是一些实验对比结果，可以看出，添加PC之后在每个数据集都能带来1-2个点。<br><img src="/img/pc-res.png" alt></p><h3 id="Learning-to-Navigate-for-Fine-grained-Classification"><a href="#Learning-to-Navigate-for-Fine-grained-Classification" class="headerlink" title="Learning to Navigate for Fine-grained Classification"></a><center>Learning to Navigate for Fine-grained Classification</center></h3><p>代码链接：<a href="https://github.com/yangze0930/NTS-Net" target="_blank" rel="noopener">https://github.com/yangze0930/NTS-Net</a><br>这也是ECCV2018的文章，这篇文章借鉴了RPN的思路，通过在原图上生成anchors，利用rank loss选出信息量最大的一些proposal，然后crop出这些区域，和原图一起提取特征然后进行决策判断。这是这篇文章方法的一个大致结构。<br><img src="/img/ntsnet.png" alt></p><h4 id="Navigator"><a href="#Navigator" class="headerlink" title="Navigator"></a>Navigator</h4><p>这个结构和FPN类似，在三个不同尺度的feature map上生成候选框，Navigator就是给每一个候选区域的“信息量”打分,信息量大的区域分数高。</p><h4 id="Teacher"><a href="#Teacher" class="headerlink" title="Teacher"></a>Teacher</h4><p>这个就是对topN分数的候选区域进行Feature Extractor + FC + softmax，判断这些候选区域属于目标的概率；</p><h4 id="Scrutinizer"><a href="#Scrutinizer" class="headerlink" title="Scrutinizer"></a>Scrutinizer</h4><p>这个是把所有候选区域part_feats和原图的raw_feats提取出来然后concat在一起，经过fc输出200个对应类别。</p><h4 id="实现流程"><a href="#实现流程" class="headerlink" title="实现流程"></a>实现流程</h4><p>下面是作者给出的源码一个具体实现流程：<br>（1）输入大小448x448尺寸原图输入feature extractor（resnet50），得到（14x14x2048）feature maps，经过Global Pooling之后2048维的Feature以及经过Global Pooling+ FC分类之后的200维的raw_logits;<br>（2）预设的RPN在14x14,7x7,4x4这三种尺度的feature map上根据不同的scale和ratio生成对应的Anchors 一共1614个（14x14x6+7x7x6+4x4x9=1614）;<br>（3）用步骤（1）得到的14x14x2048大小的feature map经过navigator对每个anchors进行打分，使用NMS进行处理，只保留topN（N=6）个proposal。<br>（4）把topN个proposal使用bilinear到224x224，输入feature extractor，得到这些局部区域的part_features,part_features经过全连接层可以到part_logits;<br>（5）把步骤（1）得到的全局Feature和步骤（4）得到的局部的part_features拼接在一起，经过全连接层得到concat_logits，用于最终的推断决策;</p><h4 id="监督训练"><a href="#监督训练" class="headerlink" title="监督训练"></a>监督训练</h4><p>（1）交叉熵损失：步骤（1）中的<strong>raw_logits</strong>, 步骤（4）中的<strong>part_logits</strong>,步骤5中的<strong>concat_logits</strong>都是直接使用交叉熵损失函数监督。<br>（2）<strong>ranking loss</strong>：步骤（3）中的信息量打分需要用步骤（4）中的part分类概率进行监督，即对于步骤（4）中的判断的属于目标Label概率高的局部区域，必须在步骤（3）中判断的信息量也高。<br><img src="/img/navigate.png" alt><br>值得注意的是，在原文中，作者提到的级联训练损失函数只由rank_loss，concat_logits以及part_logits三个部分组成，但在代码实现上加入了raw_logits部分，每个部分的权重都为1。在CUB200数据集上实验结果如下：<br><img src="/img/ntsnet-res.png" alt></p><p>从上面看到的几篇paper都可以看到ranking loss的影子，ranking loss是Learning to Rank提出的一种机器学习算法，通过训练模型来解决排序问题，具体介绍可以看<a href="https://www.cnblogs.com/bentuwuying/p/6681943.html" target="_blank" rel="noopener">这里</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一般而言，图像识别分为两种：传统图像识别和细粒度图像识别。前者指的是对一些大的类别比如汽车、动物、植物等大的类别进行分类，这是属于粗粒度的图像识别。而后者则是在某个类别下做进一步分类。比如在狗的类别下区分狗的品种是哈士奇、柯基、萨摩还是阿拉斯加等等，这是属于细粒度图像识别。&lt;br&gt;&lt;img src=&quot;/img/fine-grain.png&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="fine-grain" scheme="https://blog.nicehuster.cn/tags/fine-grain/"/>
    
  </entry>
  
  <entry>
    <title>了解零和博弈/贸易顺差逆差</title>
    <link href="https://blog.nicehuster.cn/2019/05/16/zero-game/"/>
    <id>https://blog.nicehuster.cn/2019/05/16/zero-game/</id>
    <published>2019-05-16T13:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近几天比较热的话题莫过于中美贸易谈判，看懂官方（人民日报，央视新闻等）给出的评论以及一些国际锐评，就不可避免地需要了解零和博弈和贸易逆差等关键字。</p><h3 id="零和博弈"><a href="#零和博弈" class="headerlink" title="零和博弈"></a>零和博弈</h3><p>零和博弈（zero-sum game），又称零和游戏，与非零和博弈相对，是博弈论的一个概念，属非合作博弈。指参与博弈的各方，在严格竞争下，一方的收益必然意味着另一方的损失，博弈各方的收益和损失相加总和永远为“零”，双方不存在合作的可能。</p><a id="more"></a><p>零和博弈的结果是一方吃掉另一方，一方的所得正是另一方的所失，整个社会的利益并不会因此而增加一分。也可以说：自己的幸福是建立在他人的痛苦之上的，二者的大小完全相等，因而双方都想尽一切办法以实现“损人利己”。与“零和”对应的“双赢”的基本理论就是“利己”不“损人”，通过谈判、合作达到皆大欢喜的结果。</p><p>零和博弈比较常见的例子就是打扑克/麻将，打扑克的人的钱总数是不变，变的只是钱从一个口袋转移至另一个口袋而已。</p><h3 id="非零和博弈"><a href="#非零和博弈" class="headerlink" title="非零和博弈"></a>非零和博弈</h3><p>非零和博弈是一种合作下的博弈，博弈中各方的收益或损失的总和不是零值，它区别于零和博弈。在经济学研究中比较有用。 在这种状况时，自己的所得并不与他人的损失的大小相等，连自己的幸福也未必建立在他人的痛苦之上，即使伤害他人也可能“损人不利己”，所以博弈双方存在 “双赢”的可能，进而达成合作。</p><p>非零和博弈既有可能是<strong>正和博弈</strong>，也有可能是<strong>负和博弈</strong>。<br><strong>正和博弈</strong>：指博弈双方的利益都有所增加，或者至少是一方的利益增加，而另一方的利益不受损害，因而整体的利益有所增加。<br><strong>负和博弈</strong>：指博弈双方都有损失，整体的利益有所减少。</p><p>非零和博弈比较具有代表性的例子就是“<strong>囚徒困境</strong>”。“囚徒困境”是1950年美国兰德公司提出的博弈论模型。两个共犯被关入监狱，在不能互相沟通情况。如果两个人都不揭发对方，则由于证据不确定，每个人都坐牢一年；若一人揭发，而另一人保持沉默，则揭发者因为立功而立即获释，沉默者因不合作而入狱五年；若互相揭发，则因证据确实，二者都判刑五年。由于囚徒无法信任对方，因此倾向于互相揭发，而不是同守沉默。<br>我们可以用图表来表示上述情况，如果立即获释，得0分，每人获刑1年，则-1分；如果获刑两年则-2分，如果获刑五年则-5分。结果如下：<br><img src="/img/no-zero-game.png" alt><br>囚徒困境是博弈论的非零和博弈中具代表性的例子，反映<strong>个人最佳选择并非团体最佳选择</strong>。</p><p>目前中美贸易谈判失败的一个原因在于两国的立足点从本质上就矛盾的。中国认为，目前经济全球化下，中美合作就是双赢，可以互相推动经济发展，一起赚钱。而美国则认为，中美合作就是零和博弈，全球资源总数是不变的，蛋糕就这么大，你吃了一点，那我吃的就少一点，况且，我本来就强，为什么要和你一起合作吃大蛋糕呢。这就是典型的霸权主义。近年来，中国发展迅速，赚的钱远比美国多。所以美国贸易谈判不肯合作，是说的过去的，不愧是作为生意人出身的特朗普，很有生意头脑。其实，中国在对待南海问题上也是这个态度，搁置争议，共同开发。毕竟，当前实现中华民族伟大复兴是最重要的。</p><h3 id="贸易顺差逆差"><a href="#贸易顺差逆差" class="headerlink" title="贸易顺差逆差"></a>贸易顺差逆差</h3><p><strong>贸易顺差逆差</strong>是指在一定时间内，国家的出口贸易总额大于进口贸易总额，又称“出超”，反之，则是贸易逆差，又称“入超”或者“贸易赤字”。</p><p>贸易顺差逆差会影响一国货币的汇率变化。以中国为例，贸易顺差大则就意味着商品出口增加，人民币面临升值的压力，反之当贸易逆差较大时，人民币就容易贬值，当一个国家出现贸易顺差时，说明该国对外贸易中是净赚的，对国民经济来说，在推动就业发展，增加就业，推进出口增长，增加外汇储备等方面有着积极的作用，但贸易顺差过大则意味着对国际市场依赖性较强，一旦国际市场不买账了，国内经济就会受到影响，另外为了应对本国货币的升值压力，国家通常会增加货币发行量，造成通货膨胀，除此以外，长期过大的贸易顺差还容易导致与贸易伙伴国之间的摩擦，毕竟谁愿意跟一个总是赚自己钱的人做生意呢。</p><p>而当一个国家出现贸易逆差时，说明该国在对外贸易中处于不利地位，为了支付进口产生的债务，国民收入就会外流，经济表现转弱，但贸易逆差又并非一无是处，适当的逆差有利于降低国内的通货膨胀压力，对缓解短期贸易纠纷，促进长期稳定增长都有着积极的作用。</p><p>总的来说，贸易顺差和逆差并没有孰优孰劣，对一个国家来说，平衡才是最好的状态。目前，中美贸易谈判失败的另一个原因在于，美国认为，美中两国之间存在巨额的贸易逆差。个人认为，美国的巨额贸易逆差并非因中国而生，也不会因中国而终。产生的根本原因很多，比如财政赤字，过度消费等等。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近几天比较热的话题莫过于中美贸易谈判，看懂官方（人民日报，央视新闻等）给出的评论以及一些国际锐评，就不可避免地需要了解零和博弈和贸易逆差等关键字。&lt;/p&gt;
&lt;h3 id=&quot;零和博弈&quot;&gt;&lt;a href=&quot;#零和博弈&quot; class=&quot;headerlink&quot; title=&quot;零和博弈&quot;&gt;&lt;/a&gt;零和博弈&lt;/h3&gt;&lt;p&gt;零和博弈（zero-sum game），又称零和游戏，与非零和博弈相对，是博弈论的一个概念，属非合作博弈。指参与博弈的各方，在严格竞争下，一方的收益必然意味着另一方的损失，博弈各方的收益和损失相加总和永远为“零”，双方不存在合作的可能。&lt;/p&gt;
    
    </summary>
    
      <category term="杂谈" scheme="https://blog.nicehuster.cn/categories/%E6%9D%82%E8%B0%88/"/>
    
    
      <category term="摘记" scheme="https://blog.nicehuster.cn/tags/%E6%91%98%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>HRNet详解</title>
    <link href="https://blog.nicehuster.cn/2019/05/15/HRNet/"/>
    <id>https://blog.nicehuster.cn/2019/05/15/HRNet/</id>
    <published>2019-05-15T13:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> Ke Sun,USTC &amp; MSRA, CVPR2019<br><strong>代码链接：</strong> <a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch" target="_blank" rel="noopener">https://github.com/leoxiaobin/deep-high-resolution-net.pytorch</a><br><strong>整体信息：</strong> HRNet（High Resolution Network）由一个中科大的学生在MSRA实习时的一篇工作。这篇工作设计了一种新的人体姿态估计模型。在姿态估计中，图像feature map的分辨率大小至关重要，以往的姿态估计方法都是采用串行的高—&gt;低—&gt;高的方式来获得语义信息强的高分辨feature map。而HRNet不同的是，其自始至终都保持的高分辨率。并且这非常有效。刷新 COCO keypoint detection数据集和the MPII Human Pose数据集。<br><img src="/img/other-hrnet.png" alt><br><a id="more"></a></p><p>上图表示的姿态估计方法中几种经典结构，（a）是Hourglass中的结构，（b）是CPN中的结构，（c）是simpleBaseline，（d）则是DeepCut结构，可以看这几种经典的结构中，图像的feature map都经历了high-to-low和low-to-high的两个变化过程，前者的目的在于生成低分辨率（low-resolution）以及高层表示（high-level representation），后者目的则是生成高分辨率表示（high-resolution representation），feature map大小变化是串行的。而且，从高到低和从低到高是分开的。</p><p>其实这些网络的设计模式不外乎：1）对称的high-to-low和low-to-high过程，比如hourglass；2）Heavy high-to-low 和 light low-to-high过程；后者这类方法high-to-low过程一般采用分类网络（比如ResNet,VGG）是一个heavy部分，而low-to-high过程则是简单地堆叠 bilinear-upsampling层或者或者transpose卷积层。</p><h3 id="HRNet"><a href="#HRNet" class="headerlink" title="HRNet"></a>HRNet</h3><p><img src="/img/HRNet.png" alt><br>HRNet的网络结构如上图所示，非常清晰易懂。一共有三个不同feature map大小的branch。每个branch在前向传播过程中，feature map大小不发生变化。三个branch虽然有着不同大小feature map但是每个branch之间会存在信息的交流。例如在前向的过程中，上面branch会把自己的feature map大小减半，然后传入到下面的branch中，而下面的branch也会通过upsample把变大后的feature送给上面的branch，两个操作可以在同一个阶段同时进行。</p><p>可以看出，不同于其他方法，HRNet通过并行的方式连接高-低分辨率特征图，因此可以直接在高分辨率的特征图上预测姿态，而不需要通过降采样再升采样来预测姿态，而且在整个过程中，不停地融合各种不同尺度的表征，实现了多尺度融合，提高了高分辨率的语义信息。</p><p>实验结果，在两种输入分辨率上，大模型HRNet-W48和小模型HRNet-W32，都刷新了COCO纪录。其中，大模型在384x288的输入分辨率上，拿到了76.3的mAP。<br><img src="/img/HRNet-res.png" alt></p><h3 id="姿态估计评估指标"><a href="#姿态估计评估指标" class="headerlink" title="姿态估计评估指标"></a>姿态估计评估指标</h3><p>最后提一下姿态估计中一些常用的评估指标。不得不提的两个指标就是OKS和PCK。</p><h4 id="OKS（Object-Keypoint-Similarity）"><a href="#OKS（Object-Keypoint-Similarity）" class="headerlink" title="OKS（Object Keypoint Similarity）"></a>OKS（Object Keypoint Similarity）</h4><p>这个是coco姿态估计挑战赛提出的一个评估指标，基于对象关键点相似度的mAP,常用于coco姿态估计中。<br><img src="/img/oks.png" alt><br>其中，di表示预测的关键点与ground truth之间的欧式距离。vi是ground truth的可见性标志，s是目标尺度，此值等于该人在ground truth中的面积的平方根,ki控制衰减的每个关键点常量。</p><p>简而言之，OKS扮演的角色与IoU在目标检测中扮演的角色相同。它是根据人的尺度标准化的预测点和标准真值点之间的距离计算出来的。在coco<a href="http://cocodataset.org/#keypoints-leaderboard" target="_blank" rel="noopener">官网</a>可以看到和目标检测一样的评估指标AP[0.5:0.95]和AR[0.5:95]。从官网目前给出的leaderboard可以看到mAP最高是0.764。</p><h4 id="PCK（Probability-of-Correct-Keypoint）"><a href="#PCK（Probability-of-Correct-Keypoint）" class="headerlink" title="PCK（Probability of Correct Keypoint）"></a>PCK（Probability of Correct Keypoint）</h4><p>预测的关键点与其对应的 groundtruth 之间的归一化距离小于设定阈值的比例。如果预测关节与真实关节之间的距离在特定阈值内，则检测到的关节被认为是正确的。阈值可以是：</p><ul><li>PCK@0.2表示以躯干（torso size）直径最为归一化参考，如果归一化后的距离大于阈值0.2，则认为改点预测正确。FLIC数据集评估指标采用的就是PCK@0.2。</li><li>PCKh@0.5表示以头部长度（head length）作为归一化参考，如果归一化后的距离大于阈值0.5，则认为改点预测正确。MPII数据集的评估指标采用的就是PCKh@0.5，目前MPII数据集PCKh最高为92.5；</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; Ke Sun,USTC &amp;amp; MSRA, CVPR2019&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt; &lt;a href=&quot;https://github.com/leoxiaobin/deep-high-resolution-net.pytorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/leoxiaobin/deep-high-resolution-net.pytorch&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt; HRNet（High Resolution Network）由一个中科大的学生在MSRA实习时的一篇工作。这篇工作设计了一种新的人体姿态估计模型。在姿态估计中，图像feature map的分辨率大小至关重要，以往的姿态估计方法都是采用串行的高—&amp;gt;低—&amp;gt;高的方式来获得语义信息强的高分辨feature map。而HRNet不同的是，其自始至终都保持的高分辨率。并且这非常有效。刷新 COCO keypoint detection数据集和the MPII Human Pose数据集。&lt;br&gt;&lt;img src=&quot;/img/other-hrnet.png&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="pose estimation" scheme="https://blog.nicehuster.cn/tags/pose-estimation/"/>
    
  </entry>
  
</feed>
