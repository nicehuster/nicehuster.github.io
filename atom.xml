<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>nicehuster&#39;s blog</title>
  <icon>https://www.gravatar.com/avatar/2555127dc0de830d31ceeb98d8565ac8</icon>
  <subtitle>不积跬步，无以至千里</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://nicehuster.github.io/"/>
  <updated>2020-09-16T08:21:46.093Z</updated>
  <id>https://nicehuster.github.io/</id>
  
  <author>
    <name>nicehuster</name>
    <email>nicehuster@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文阅读：Delving Deeper into Anti-aliasing in ConvNets</title>
    <link href="https://nicehuster.github.io/2020/09/06/0065-Adaptive-anti-Aliasing/"/>
    <id>https://nicehuster.github.io/2020/09/06/0065-Adaptive-anti-Aliasing/</id>
    <published>2020-09-06T11:13:39.000Z</published>
    <updated>2020-09-16T08:21:46.093Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> Xueyan Zou,UC Davis,NVIDIA<br><strong>代码链接：</strong><a href="https://github.com/MaureenZOU/" target="_blank" rel="noopener">https://github.com/MaureenZOU/</a><br><strong>整体框架：</strong>这篇文章是前几天BMVC2020获得best paper award 的一篇文章，这篇文章提出了一个plugin  module ，用来提高CNN的鲁棒性。在图像分类、图像分割、目标检测等任务上都能带来1+个点的提升。对于cnn，众所周知存在一个比较明显的缺陷：即使图像移动几个pixel都可能导致分类识别任务结果发生改变，作者发现其中重要原因在于网络中被大量用来降低参数量的下采样层导致混叠(aliasing)问题，即高频信号在采样后退化为完全不同的部分现象。为此提出了这样一个content-aware anti-aliasing的模块，用于缓解下采样过程中带来的高频信息的退化问题。</p><h3 id="下采样的问题"><a href="#下采样的问题" class="headerlink" title="下采样的问题"></a>下采样的问题</h3><p>以一维信号的的降采样为例：</p><script type="math/tex; mode=display">\begin{array}{l}001100110011 \frac{\mathrm{k}=2, \text { stride }=2}{\operatorname{maxpool}} 010101 \\011001100110 \frac{\mathrm{k}=2, \text { stride }=2}{\operatorname{maxpool}} 111111\end{array}</script><a id="more"></a><p>对于k=2,stride=2的maxpool操作而言，对输入信号移动一位数字，经过maxpool之后的输出结果是截然不同的。而在CNN中大量使用降采样来降低参数量，这种aliasing问题更为明显，标准解决方案是在下采样之前应用低通滤波器（例如，高斯模糊）。但是，在整个内容上应用相同的过滤器可能不是最佳选择，因为特征的频率可能会在<strong>空间位置</strong>和<strong>特征通道</strong>之间发生变化。可以看下作者在论文中给出的实验结果：</p><p><img src="/img/image-20200916161719818.png" alt="image-20200916161719818" style="zoom:50%;"></p><p>上图(a)输入图片；(b)直接4x下采样；(c)应用经过调整以匹配噪声频率的单个高斯滤波器后的下采样结果;(d)应用多个空间自适应高斯滤波后的下采样结果（具有很强的背景模糊和边界弱化能力）</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p><img src="/img/image-20200916161851654.png" alt="image-20200916161851654" style="zoom:50%;"></p><p>作者针对空间和通道分别生成低通滤波器用于缓解aliasing问题。基于空间自适应的低通滤波器就是一个简单的分组卷积操作：</p><script type="math/tex; mode=display">Y_{i, j}=\sum_{p, q \in \Omega} w_{i, j}^{p, q} \cdot X_{i+p, j+q}</script><p>在论文中也有提及，为了避免权重为负数，作者取了一个softmax操作。</p><p>之后对上面生成的权重进行分组再和输入进行一次卷积操作，最终输出Y：</p><script type="math/tex; mode=display">Y_{i, j}^{g}=\sum_{p, q \in \Omega} w_{i, j, g}^{p, q} \cdot X_{i+p, j+q}^{c}</script><p>具体可以看下作者放出来的代码：</p><blockquote><p><a href="https://github.com/MaureenZOU/Adaptive-anti-Aliasing/blob/master/models_lpf/layers/pasa.py" target="_blank" rel="noopener">Adaptive-anti-Aliasing/models_lpf/layers/pasa.py</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Downsample_PASA_group_softmax</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, kernel_size, stride=<span class="number">1</span>, pad_type=<span class="string">'reflect'</span>, group=<span class="number">2</span>)</span>:</span></span><br><span class="line">        super(Downsample_PASA_group_softmax, self).__init__()</span><br><span class="line">        self.pad = get_pad_layer(pad_type)(kernel_size//<span class="number">2</span>)</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.kernel_size = kernel_size</span><br><span class="line">        self.group = group</span><br><span class="line"></span><br><span class="line">        self.conv = nn.Conv2d(in_channels, group*kernel_size*kernel_size, kernel_size=kernel_size, stride=<span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.bn = nn.BatchNorm2d(group*kernel_size*kernel_size)</span><br><span class="line">        self.softmax = nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">        nn.init.kaiming_normal_(self.conv.weight, mode=<span class="string">'fan_out'</span>, nonlinearity=<span class="string">'relu'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        sigma = self.conv(self.pad(x))</span><br><span class="line">        sigma = self.bn(sigma)</span><br><span class="line">        sigma = self.softmax(sigma)</span><br><span class="line"></span><br><span class="line">        n,c,h,w = sigma.shape</span><br><span class="line"></span><br><span class="line">        sigma = sigma.reshape(n,<span class="number">1</span>,c,h*w)</span><br><span class="line"></span><br><span class="line">        n,c,h,w = x.shape</span><br><span class="line">        x = F.unfold(self.pad(x), kernel_size=self.kernel_size).reshape((n,c,self.kernel_size*self.kernel_size,h*w))</span><br><span class="line"></span><br><span class="line">        n,c1,p,q = x.shape</span><br><span class="line">        x = x.permute(<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>).reshape(self.group, c1//self.group, n, p, q).permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        n,c2,p,q = sigma.shape</span><br><span class="line">        sigma = sigma.permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>).reshape((p//(self.kernel_size*self.kernel_size), self.kernel_size*self.kernel_size,n,c2,q)).permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        x = torch.sum(x*sigma, dim=<span class="number">3</span>).reshape(n,c1,h,w)</span><br><span class="line">        <span class="keyword">return</span> x[:,:,torch.arange(h)%self.stride==<span class="number">0</span>,:][:,:,:,torch.arange(w)%self.stride==<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>代码较好题解，就不详细解释。</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>作者分别在分类，检测，分割任务上均有验证其有效性，均能提高1个点左右。具体实验结果可以看原论文，这里就不贴上来了。</p><h3 id="一致性指标"><a href="#一致性指标" class="headerlink" title="一致性指标"></a>一致性指标</h3><p>作者为了验证该方法的有效性，针对不同任务提出了一系列的一致性指标。</p><h4 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h4><p>比如针对分类任务的一致性指标计算如下：</p><script type="math/tex; mode=display">\text { Consistency }=\mathbb{E}_{X, h_{1}, w_{1}, h_{2}, w_{2}} \mathbb{I}\left\{F\left(X_{h_{1}, w_{1}}\right)=F\left(X_{h_{2}, w_{2}}\right)\right\}</script><p>X表示输入图像，$h_{1}, w_{1}, h_{2}, w_{2}$ 表示偏移量。$F(\cdot)$ 表示模型输出的top1的类别标签。具体代码实现可以看下面：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">output0 = model(input[:,:,off0[<span class="number">0</span>]:off0[<span class="number">0</span>]+args.size,off0[<span class="number">1</span>]:off0[<span class="number">1</span>]+args.size])</span><br><span class="line">output1 = model(input[:,:,off1[<span class="number">0</span>]:off1[<span class="number">0</span>]+args.size,off1[<span class="number">1</span>]:off1[<span class="number">1</span>]+args.size])</span><br><span class="line">cur_agree = agreement_correct(output0, output1, target).type(torch.FloatTensor).to(output0.device)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agreement_correct</span><span class="params">(output0, output1, target)</span>:</span></span><br><span class="line">    pred0 = output0.argmax(dim=<span class="number">1</span>, keepdim=<span class="keyword">False</span>)</span><br><span class="line">    pred1 = output1.argmax(dim=<span class="number">1</span>, keepdim=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    agree = pred0.eq(pred1)</span><br><span class="line">    agree_target_pred0 = pred0.eq(target)</span><br><span class="line">    agree_target_pred1 = pred1.eq(target)</span><br><span class="line"></span><br><span class="line">    correct_or = (agree_target_pred0 + agree_target_pred1) &gt; <span class="number">0</span></span><br><span class="line">    agree = agree * correct_or</span><br><span class="line"></span><br><span class="line">    agree = <span class="number">100.</span>*(torch.sum(agree).float() / (torch.sum(correct_or).float() + <span class="number">1e-10</span>)).to(output0.device)</span><br><span class="line">    <span class="keyword">return</span> agree</span><br></pre></td></tr></table></figure><p><code>agreement_correct</code> 是统计一致性指标的函数，具体可看出来统计方法是分别对输入图像随机偏移<code>off0</code>和<code>off1</code> 然后统计在该偏移量下，两者输出一致且和gt一致的比例。</p><h4 id="检测-分割"><a href="#检测-分割" class="headerlink" title="检测/分割"></a>检测/分割</h4><p>对于分类分割任务，作者也提出了相应的一致性指标计算方法。mAISC和mAISC这两个指标。具体计算如下图。</p><p><img src="/img/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_16002437758073.png" alt="企业微信截图_16002437758073"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; Xueyan Zou,UC Davis,NVIDIA&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/MaureenZOU/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/MaureenZOU/&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体框架：&lt;/strong&gt;这篇文章是前几天BMVC2020获得best paper award 的一篇文章，这篇文章提出了一个plugin  module ，用来提高CNN的鲁棒性。在图像分类、图像分割、目标检测等任务上都能带来1+个点的提升。对于cnn，众所周知存在一个比较明显的缺陷：即使图像移动几个pixel都可能导致分类识别任务结果发生改变，作者发现其中重要原因在于网络中被大量用来降低参数量的下采样层导致混叠(aliasing)问题，即高频信号在采样后退化为完全不同的部分现象。为此提出了这样一个content-aware anti-aliasing的模块，用于缓解下采样过程中带来的高频信息的退化问题。&lt;/p&gt;
&lt;h3 id=&quot;下采样的问题&quot;&gt;&lt;a href=&quot;#下采样的问题&quot; class=&quot;headerlink&quot; title=&quot;下采样的问题&quot;&gt;&lt;/a&gt;下采样的问题&lt;/h3&gt;&lt;p&gt;以一维信号的的降采样为例：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{array}{l}
001100110011 \frac{\mathrm{k}=2, \text { stride }=2}{\operatorname{maxpool}} 010101 \\
011001100110 \frac{\mathrm{k}=2, \text { stride }=2}{\operatorname{maxpool}} 111111
\end{array}&lt;/script&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://nicehuster.github.io/categories/paper-reading/"/>
    
    
      <category term="classification" scheme="https://nicehuster.github.io/tags/classification/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：Minimum Delay Object Detection from Video</title>
    <link href="https://nicehuster.github.io/2020/09/05/0063-minidelay-detection/"/>
    <id>https://nicehuster.github.io/2020/09/05/0063-minidelay-detection/</id>
    <published>2020-09-05T11:13:39.000Z</published>
    <updated>2020-09-15T06:28:52.040Z</updated>
    
    <content type="html"><![CDATA[<p>最近逛知乎上了解到<strong>低延迟目标检测</strong>这个方向，这个方向解决的是视觉任务工程化过程中存在一个痛点问题，如何解决延迟和误报的情况。拿视频中的目标检测问题来讲，我们通常会使用一个单帧检测器来检测视频中每一帧存在目标情况，静态上看可能会存在一些误检漏检情况，动态上表现为检测到的目标一闪一闪的情况，导致视觉效果很不友好。而低延迟目标检测的任务就是通过贝叶斯概率建模的方式融合多帧信息来解决这样的问题。</p><p>总体来说，对于任何检测任务来说，延迟和误报存在如下图的关系：</p><p><img src="/img/v2-2c0d4736cf5fde58e2b58530d7643006_1440w.jpg" alt="img"></p><a id="more"></a><p>这个关系不难理解，不只是视觉问题，世间万物，更长的决策过程（delay）往往能带来更高的准确度，但是这个更长的决策过程也会带来更大的延迟。两者之间的平衡，对很多需要在线决策（online process）的系统来说非常重要。例如生物视觉，假定一个动物检测到掠食者就需要逃跑，如果追求低误报率，就要承担高延迟带来的风险，有可能检测到掠食者时为时已晚无法逃脱；如果追求低延迟，虽然相对安全，但是误报率高，有一点风吹草动就犹如惊弓之鸟。</p><h3 id="低延迟检测思路"><a href="#低延迟检测思路" class="headerlink" title="低延迟检测思路"></a>低延迟检测思路</h3><p>在视频物体检测中，如果使用上一帧的检测结果作为先验，将下一帧的检测结果输入贝叶斯框架，输出后验，那么总体来说，这个后验结果融合了两帧的信息，会比单帧更准。在这个思路下，理论上来说使用的帧数越多，检测越准。如以下单帧和多帧的对比：</p><p><img src="/img/v2-fcf08710643bcb172cb661c687f36fe5_b.webp" alt="img"></p><p>但是同时更多的帧数会造成更长的延迟（延迟 := 物体被检测到的时刻 - 物体出现的时刻）。如何在保证物体检测精度的情况下，尽量降低延迟呢？我们参照QD理论进行如下建模：</p><p>假设一个物体在时刻$t_s$出现在视频中，在 $t_e$离开视频，则这个物体的移动轨迹可以用时序上的一组检测框$b_{t_s,t_e}=(b_{t_s},b_{t_s+1},…,b_{t_e})$表示。这样的一组检测框检测框序列，在目标追踪（Data association / tracking）领域被一些人称作tracklet。简单说，我们的算法目标是以低延迟判断检测框序列内是否含有物体。因此，我们称这样一组检测框序列为一个candidate。在quickest change detection框架下， 可以用如下似然比检验判断$t$时刻物体是否出现在该candidate内：</p><script type="math/tex; mode=display">\begin{aligned}\Lambda_{t}\left(b_{1, t}\right) &=\max _{i} \frac{\mathrm{p}\left(\Gamma_{0, i}<t \mid D_{1, t}, b_{1, t}\right)}{\mathrm{p}\left(\Gamma_{0, i} \geq t \mid D_{1, t}, b_{1, t}\right)} \\&=\max _{i} \max _{t_{c} \geq 1} \frac{\mathrm{p}_{i}\left(D_{t_{c}, t} \mid b_{t_{c}, t}\right)}{\mathrm{p}_{0}\left(D_{t_{c}, t} \mid b_{t_{c}, t}\right)}\end{aligned}</script><p>其中$D_t$代表一个单帧检测器在$I_t$上的检测结果，$T_{0,i}$代表candidate中的内容从背景变为第类$i$物体（如行人）这一事件发生的时刻,$p_i(\bullet )=p(\bullet |l=l_i)$ 代表给定类别$i$的时候，$\bullet$ 事件发生的概率。由条件概率的独立性与,$p_i(\bullet |b_t)$类别$i$和检测框独$b_t$立，继而时序上各时刻检测结果的联合概率变成各时刻概率的连乘：</p><script type="math/tex; mode=display">\Lambda_{t}\left(b_{1, t}\right)=\max _{i} \max _{t_{c} \geq 1} \prod_{j=t_{c}}^{t} \frac{\mathrm{p}_{i}\left(D_{j} \mid b_{j}\right)}{\mathrm{p}_{0}\left(D_{j} \mid b_{j}\right)}</script><p>这里需要明确一下，上边公式中的条件概率并非简单的检测器输出的结果，具体如何计算$p$需要一套比较复杂的建模。由于这里只介绍低延迟检测的整体思路，关于$p$的建模待我有空时会附在文末，有兴趣的朋友可以直接去论文查阅。总之，我们可以对这个似然比取阈值，进行检测。阈值越高，结果越准，但是延迟越大，反之同理。由QD理论中递归算法（CuSum算法），我们可以对上述似然比取log，记为W。最终的检测流程可以参照如下框图。</p><p><img src="/img/v2-bd434876b196828b2db7d14ea5eb8c52_1440w.jpg" alt></p><p>整体流程为：</p><blockquote><ul><li>（1）将已有但似然比未超过阈值的candidate做tracking进入下一帧；</li><li>（2）在下一帧进行单帧检测，生成新的检测框，与前一帧tracking后的检测框合并到一起；</li><li>（3）对这些candidate进行似然比检验，W超出阈值则输出检测结果，W小于零则去除该检测框，W大于零小于阈值则回到（1），进入下一帧。</li></ul></blockquote><p>这样一个检测框架，可以与<strong>任何</strong>单帧检测器结合。</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>以上算法的具体实现过程,作者开源了一个简单的python实现，git地址在<a href="https://github.com/donglao/mindelay/blob/master/detection.py" target="_blank" rel="noopener">这里</a>。这里主要看下核心的代码：</p><p>detection.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">result = toolbox.initialize_result(num_cat) <span class="comment"># 第一步，外循环，根据类别初始化result</span></span><br><span class="line"></span><br><span class="line">result = association.update(result, result_det) <span class="comment">#使用当前帧信息更新历史轨迹，result保存的是历史帧的检测信息，result_det是当前帧的检测信息</span></span><br><span class="line"></span><br><span class="line">result = toolbox.combine_result(result, result_det, <span class="number">0.5</span>) <span class="comment">#对结合的轨迹信息和当前帧信息就行结合，输出最终需要alarm的检测结果；</span></span><br></pre></td></tr></table></figure><p>其中association.update的具体代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> mindelay.toolbox.IoU <span class="keyword">as</span> IoU</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_traj</span><span class="params">(old, det)</span>:</span></span><br><span class="line">    prior = <span class="number">0.5</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(old.shape[<span class="number">0</span>]):</span><br><span class="line">        a = old[i, <span class="number">0</span>:<span class="number">4</span>] * prior</span><br><span class="line">        b = prior</span><br><span class="line">        l = prior * <span class="number">0.5</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(det.shape[<span class="number">0</span>]):</span><br><span class="line">            weight = (IoU(old[i, <span class="number">0</span>:<span class="number">4</span>], det[j, <span class="number">0</span>:<span class="number">4</span>])&gt;<span class="number">0.5</span>)*IoU(old[i, <span class="number">0</span>:<span class="number">4</span>], det[j, <span class="number">0</span>:<span class="number">4</span>])</span><br><span class="line">        <span class="comment"># The bounding box update is here. I've tried different methods in the original Matlab code. </span></span><br><span class="line">        <span class="comment"># But here I just use the previous frame as the guidance of current frame and leave it as-is.</span></span><br><span class="line">        <span class="comment"># I will combine it with trackers (some work to be done!) and update in later version of the code. </span></span><br><span class="line">            a = a + weight * det[j, <span class="number">0</span>:<span class="number">4</span>]</span><br><span class="line">            b = b + weight</span><br><span class="line">            l = l + weight * det[j, <span class="number">4</span>]</span><br><span class="line">        old[i, <span class="number">0</span>:<span class="number">4</span>] = a / b</span><br><span class="line">        l = l / b</span><br><span class="line"></span><br><span class="line">        temp_lr = np.log(l + <span class="number">0.25</span>) - np.log((<span class="number">1</span> - l) + <span class="number">0.25</span>) + old[i, <span class="number">4</span>] - <span class="number">2.5</span>/b</span><br><span class="line">        <span class="comment"># the +0.25 is making the output smoother. If the output of the detector is not ideal you may want to tune it.</span></span><br><span class="line">        <span class="comment"># the 2.5/b is a prior. Instead of setting a fixed prior I am trying to make it adaptive. Feel free to play with it! </span></span><br><span class="line">    </span><br><span class="line">        old[i, <span class="number">4</span>] = max(temp_lr, <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> old</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(result, result_det)</span>:</span></span><br><span class="line">    n = result.__len__()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        result[i] = update_traj(np.array(result[i]), np.array(result_det[i]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br></pre></td></tr></table></figure><p>其中，toolbox.combine_result的实现比较简单，对经过更新的轨迹信息和当前帧信息进行combine后经nms处理一下，代码如下：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def combine_result(result, result_det, thre_nms):</span><br><span class="line">    n = result.__len__()</span><br><span class="line">    output = np.empty((n,), dtype = np.object)</span><br><span class="line">    for i in range(n):</span><br><span class="line">        if result_det<span class="string">[i]</span>.shape<span class="string">[0]</span> &gt; <span class="number">0</span>:</span><br><span class="line">            temp_lr = np.log(result_det<span class="string">[i]</span><span class="string">[:,4]</span>+<span class="number">0</span>.<span class="number">25</span>) - np.log(<span class="number">1</span> - result_det<span class="string">[i]</span><span class="string">[:,4]</span>+<span class="number">0</span>.<span class="number">25</span>)</span><br><span class="line">            temp_lr<span class="string">[temp_lr &lt; 0]</span> = <span class="number">0</span></span><br><span class="line">            result_det<span class="string">[i]</span><span class="string">[:, 4]</span> = temp_lr</span><br><span class="line">        tmp = np.vstack((result<span class="string">[i]</span>, result_det<span class="string">[i]</span>))</span><br><span class="line">        keep = py_cpu_nms(tmp,thre_nms)</span><br><span class="line">        #print(keep,thre_nms,tmp<span class="string">[keep]</span>)</span><br><span class="line">        output<span class="string">[i]</span> = tmp<span class="string">[keep]</span></span><br><span class="line"></span><br><span class="line">    return output</span><br></pre></td></tr></table></figure><p>以上理论内容转自：<a href="https://zhuanlan.zhihu.com/p/212842916" target="_blank" rel="noopener">计算机视觉中低延迟检测的相关理论和应用</a>；</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近逛知乎上了解到&lt;strong&gt;低延迟目标检测&lt;/strong&gt;这个方向，这个方向解决的是视觉任务工程化过程中存在一个痛点问题，如何解决延迟和误报的情况。拿视频中的目标检测问题来讲，我们通常会使用一个单帧检测器来检测视频中每一帧存在目标情况，静态上看可能会存在一些误检漏检情况，动态上表现为检测到的目标一闪一闪的情况，导致视觉效果很不友好。而低延迟目标检测的任务就是通过贝叶斯概率建模的方式融合多帧信息来解决这样的问题。&lt;/p&gt;
&lt;p&gt;总体来说，对于任何检测任务来说，延迟和误报存在如下图的关系：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/v2-2c0d4736cf5fde58e2b58530d7643006_1440w.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://nicehuster.github.io/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://nicehuster.github.io/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>解决PIL读取图片出现自动旋转的解决方案</title>
    <link href="https://nicehuster.github.io/2020/08/06/0062-PILrotate/"/>
    <id>https://nicehuster.github.io/2020/08/06/0062-PILrotate/</id>
    <published>2020-08-06T11:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近项目中，使用手机采集了数据，交给标注组进行标注时，发现返回来的标注文件与图片存在不匹配问题，部分标注存在旋转情况。从网上了解到电子设备在拍摄照片时，如手机、相机等，由于手持朝向的不同，拍摄的照片可能会出现旋转 0、90、180、270 角度的情况，其 EXIF 信息中会保留相应的方位信息.有些情况下，电脑上打开显示照片是正常的，但在用 PIL 或 OpenCV 读取图片后，图片出现旋转，且读取的图片尺寸也可能与直接在电脑上打开的尺寸不同的问题.</p><p>对此，需要在读取图片时，同时解析图片的 EXIF 中的方位信息，将图片转正，再进行后续的其他操作.实例如下：</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ExifTags</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">IsRotate</span><span class="params">(img)</span>:</span> <span class="comment"># 返回false表示存在旋转情况</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">for</span> orientation <span class="keyword">in</span> ExifTags.TAGS.keys() :</span><br><span class="line">            <span class="keyword">if</span> ExifTags.TAGS[orientation]==<span class="string">'Orientation'</span> :</span><br><span class="line">                img2 = img.rotate(<span class="number">0</span>, expand = <span class="keyword">True</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        exif=dict(img._getexif().items())</span><br><span class="line">        <span class="keyword">if</span>  exif[orientation] == <span class="number">3</span> :</span><br><span class="line">            img2=img.rotate(<span class="number">180</span>, expand = <span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">elif</span> exif[orientation] == <span class="number">6</span> :</span><br><span class="line">            img2=img.rotate(<span class="number">270</span>, expand = <span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">elif</span> exif[orientation] == <span class="number">8</span> :</span><br><span class="line">            img2=img.rotate(<span class="number">90</span>, expand = <span class="keyword">True</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> img.size == img2.size</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br></pre></td></tr></table></figure><p>更多可参考：<a href="https://stackoverflow.com/questions/4228530/pil-thumbnail-is-rotating-my-image" target="_blank" rel="noopener">Stackoverflow - PIL thumbnail is rotating my image?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近项目中，使用手机采集了数据，交给标注组进行标注时，发现返回来的标注文件与图片存在不匹配问题，部分标注存在旋转情况。从网上了解到电子设备在拍摄照片时，如手机、相机等，由于手持朝向的不同，拍摄的照片可能会出现旋转 0、90、180、270 角度的情况，其 EXIF 信息中会保留相应的方位信息.有些情况下，电脑上打开显示照片是正常的，但在用 PIL 或 OpenCV 读取图片后，图片出现旋转，且读取的图片尺寸也可能与直接在电脑上打开的尺寸不同的问题.&lt;/p&gt;
&lt;p&gt;对此，需要在读取图片时，同时解析图片的 EXIF 中的方位信息，将图片转正，再进行后续的其他操作.实例如下：&lt;/p&gt;
    
    </summary>
    
      <category term="linux" scheme="https://nicehuster.github.io/categories/linux/"/>
    
    
      <category term="opencv" scheme="https://nicehuster.github.io/tags/opencv/"/>
    
      <category term="python" scheme="https://nicehuster.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python计算任意多边形的面积</title>
    <link href="https://nicehuster.github.io/2020/06/05/0064-polygonArea/"/>
    <id>https://nicehuster.github.io/2020/06/05/0064-polygonArea/</id>
    <published>2020-06-05T11:13:39.000Z</published>
    <updated>2020-09-16T02:30:44.354Z</updated>
    
    <content type="html"><![CDATA[<p>在网上发现一个很有意思且很有用的多边形面积计算公式-鞋带公式</p><p>鞋带公式的表达式为：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{A} &=\frac{1}{2}\left|\sum_{i=1}^{n-1} x_{i} y_{i+1}+x_{n} y_{1}-\sum_{i=1}^{n-1} x_{i+1} y_{i}-x_{1} y_{n}\right| \\&=\frac{1}{2}\left|x_{1} y_{2}+x_{2} y_{3}+\cdots+x_{n-1} y_{n}+x_{n} y_{1}-x_{2} y_{1}-x_{3} y_{2}-\cdots-x_{n} y_{n-1}-x_{1} y_{n}\right|\end{aligned}</script><a id="more"></a><p><img src="/img/b1f95fc8a46e6d66.jpg" alt="img"></p><p>where<br>$\bullet A$ is the area of the polygon,<br>$\bullet n$ is the number of sides of the polygon, and $\cdot\left(x_{i}, y_{i}\right), i=1,2, \ldots, n$ are the ordered vertices (or “corners”) of the polygon.</p><blockquote><p>参考wiki:<a href="https://en.wikipedia.org/wiki/Shoelace_formula" target="_blank" rel="noopener">Shoelace formula</a></p></blockquote><p>可以理解为，是把每个顶点向x轴做垂线，每个边和坐标轴构成的梯形面积矢量和就是多边形的面积。</p><h3 id="1-鞋带公式实现"><a href="#1-鞋带公式实现" class="headerlink" title="1. 鞋带公式实现"></a>1. 鞋带公式实现</h3><blockquote><p>From: stackoverflow - <a href="https://stackoverflow.com/questions/24467972/calculate-area-of-polygon-given-x-y-coordinates" target="_blank" rel="noopener">calculate-area-of-polygon-given-x-y-coordinates</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">polygon_area</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span>*np.abs(np.dot(x,np.roll(y,<span class="number">1</span>))-np.dot(y,np.roll(x,<span class="number">1</span>)))</span><br></pre></td></tr></table></figure><h3 id="2-示例1-计算曲线与坐标轴的面积"><a href="#2-示例1-计算曲线与坐标轴的面积" class="headerlink" title="2. 示例1 - 计算曲线与坐标轴的面积"></a>2. 示例1 - 计算曲线与坐标轴的面积</h3><blockquote><p>[计算曲线与坐标轴的面积</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">1</span>,<span class="number">0.001</span>)</span><br><span class="line">y = np.sqrt(<span class="number">1</span>-x**<span class="number">2</span>)</span><br><span class="line">plt.plot(x, y)</span><br><span class="line">plt.show()</span><br><span class="line">area_value = polygon_area(np.append(x, <span class="number">0</span>), np.append(y, <span class="number">0</span>))</span><br></pre></td></tr></table></figure><h3 id="3-示例2-detectron2-mask-面积"><a href="#3-示例2-detectron2-mask-面积" class="headerlink" title="3. 示例2 - detectron2 mask 面积"></a>3. 示例2 - detectron2 mask 面积</h3><blockquote><p><a href="https://github.com/facebookresearch/detectron2/blob/b0b6ccfef5e00255de22857eca0e2dfa1d1144b1/detectron2/structures/masks.py" target="_blank" rel="noopener">detectron2/structures/masks.py</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">area</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes area of the mask.</span></span><br><span class="line"><span class="string">    Only works with Polygons, using the shoelace formula</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Tensor: a vector, area for each instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    area = []</span><br><span class="line">    <span class="keyword">for</span> polygons_per_instance <span class="keyword">in</span> self.polygons:</span><br><span class="line">        area_per_instance = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> polygons_per_instance:</span><br><span class="line">            area_per_instance += polygon_area(p[<span class="number">0</span>::<span class="number">2</span>], p[<span class="number">1</span>::<span class="number">2</span>])</span><br><span class="line">        area.append(area_per_instance)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> torch.tensor(area)</span><br></pre></td></tr></table></figure><h3 id="在线多变形面积计算工具"><a href="#在线多变形面积计算工具" class="headerlink" title="在线多变形面积计算工具"></a>在线多变形面积计算工具</h3><blockquote><p><a href="https://www.mathsisfun.com/geometry/area-polygon-drawing.html" target="_blank" rel="noopener">多边形面积计算工具</a></p></blockquote><p><img src="/img/image-20200916102553183.png" alt="image-20200916102553183"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在网上发现一个很有意思且很有用的多边形面积计算公式-鞋带公式&lt;/p&gt;
&lt;p&gt;鞋带公式的表达式为：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{aligned}
\mathbf{A} &amp;=\frac{1}{2}\left|\sum_{i=1}^{n-1} x_{i} y_{i+1}+x_{n} y_{1}-\sum_{i=1}^{n-1} x_{i+1} y_{i}-x_{1} y_{n}\right| \\
&amp;=\frac{1}{2}\left|x_{1} y_{2}+x_{2} y_{3}+\cdots+x_{n-1} y_{n}+x_{n} y_{1}-x_{2} y_{1}-x_{3} y_{2}-\cdots-x_{n} y_{n-1}-x_{1} y_{n}\right|
\end{aligned}&lt;/script&gt;
    
    </summary>
    
      <category term="python" scheme="https://nicehuster.github.io/categories/python/"/>
    
    
      <category term="python" scheme="https://nicehuster.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>ffmpeg常用指令笔记</title>
    <link href="https://nicehuster.github.io/2020/03/26/0061-ffmpeg/"/>
    <id>https://nicehuster.github.io/2020/03/26/0061-ffmpeg/</id>
    <published>2020-03-26T11:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>平时使用ffmpeg对视频解码成图片比较多，就稍微简单的了解一些ffmpeg常用的相关指令。下面是一些相关指令的介绍笔记。</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>Debian/Ubuntu/Linux Mint 下安装 ffmpeg 很简单：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-<span class="builtin-name">get</span> install ffmpeg</span><br></pre></td></tr></table></figure><p>其他操作系统安装方法，参考<a href="https://www.ffmpeg.org/download.html" target="_blank" rel="noopener">官网</a></p><p>如果想要手工编译 ffmpeg 可以参考官方 <a href="https://trac.ffmpeg.org/wiki#CompilingFFmpeg" target="_blank" rel="noopener">wiki</a>。 Ubuntu/Debian/Mint 系手工编译 ffmpeg 参考 <a href="https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu" target="_blank" rel="noopener">wiki</a>。</p><h3 id="常用指令"><a href="#常用指令" class="headerlink" title="常用指令"></a>常用指令</h3><h4 id="1-显示文件信息"><a href="#1-显示文件信息" class="headerlink" title="1.显示文件信息"></a>1.显示文件信息</h4><p>显示视频信息</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span>.avi</span><br></pre></td></tr></table></figure><h4 id="2-将视频拆分图片-批量截图"><a href="#2-将视频拆分图片-批量截图" class="headerlink" title="2.将视频拆分图片 批量截图"></a>2.将视频拆分图片 批量截图</h4><a id="more"></a><p>将视频拆分多张图片，每一帧图片，保存到 frames 文件夹下，命名 frame001.png 这种。可以加上 -r 参数以用来限制每秒的帧数，<code>-r 10</code> 就表示每秒 10 帧。</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="built_in">i</span> input.mp4 frames/frame<span class="comment">%03d.png</span></span><br></pre></td></tr></table></figure><h4 id="3-图片合成视频"><a href="#3-图片合成视频" class="headerlink" title="3.图片合成视频"></a>3.图片合成视频</h4><p>将多张图片合成视频</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="built_in">i</span> frames/frame<span class="comment">%3d.png output.mp4</span></span><br></pre></td></tr></table></figure><h4 id="4-转换格式"><a href="#4-转换格式" class="headerlink" title="4.转换格式"></a>4.转换格式</h4><p>格式之间转换 大部分的情况下直接运行一下即可</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> output.avi</span><br></pre></td></tr></table></figure><p>将 flv 转码 MP4</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.flv</span> -vcodec copy -acodec copy out.mp4</span><br></pre></td></tr></table></figure><p><code>-vcodec copy</code> 和 <code>-acodec copy</code> 表示所使用的视频和音频编码格式，为原样拷贝。</p><h4 id="5-视频切片操作"><a href="#5-视频切片操作" class="headerlink" title="5.视频切片操作"></a>5.视频切片操作</h4><p>对视频切片操作,比如需要从视频第 1 分 45 秒地方，剪 10 秒画面，-ss 表示开始位置，-t 表示延长时间</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> -ss <span class="number">00</span>:<span class="number">01</span>:<span class="number">45</span> -t <span class="number">10</span> output.mp4</span><br></pre></td></tr></table></figure><h4 id="6-加速减速视频"><a href="#6-加速减速视频" class="headerlink" title="6.加速减速视频"></a>6.加速减速视频</h4><p>加速视频</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> -vf “setpts=<span class="number">0.5</span>*PTS” output.mp4</span><br></pre></td></tr></table></figure><p>同理减速视频</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> -vf “setpts=<span class="number">2.0</span>*PTS” output.mp4</span><br></pre></td></tr></table></figure><p>此操作对音频无影响</p><h4 id="7-视频截图"><a href="#7-视频截图" class="headerlink" title="7.视频截图"></a>7.视频截图</h4><p>视频 10 秒的地方 (<code>-ss</code> 参数）截取一张 1920x1080 尺寸大小的，格式为 jpg 的图片  <code>-ss</code>后跟的时间单位为秒</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> input_video<span class="selector-class">.mp4</span> -y -f image2 -t <span class="number">0.001</span> -ss <span class="number">10</span> -s <span class="number">1920</span>x1080 output.jpg</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">ffmpeg</span> <span class="selector-tag">-i</span> <span class="selector-tag">input_video</span><span class="selector-class">.mp4</span> <span class="selector-tag">-ss</span> 00<span class="selector-pseudo">:00</span><span class="selector-pseudo">:06.000</span> <span class="selector-tag">-vframes</span> 1 <span class="selector-tag">output</span><span class="selector-class">.png</span></span><br></pre></td></tr></table></figure><h4 id="8-合成gif"><a href="#8-合成gif" class="headerlink" title="8.合成gif"></a>8.合成gif</h4><p>把视频的前 30 帧转换成一个 Gif</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> input_video<span class="selector-class">.mp4</span> -vframes <span class="number">30</span> -y -f gif output.gif</span><br></pre></td></tr></table></figure><p>将视频转成 gif</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">ffmpeg</span> <span class="selector-tag">-ss</span> 00<span class="selector-pseudo">:00</span><span class="selector-pseudo">:00.000</span> <span class="selector-tag">-i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> <span class="selector-tag">-pix_fmt</span> <span class="selector-tag">rgb24</span> <span class="selector-tag">-r</span> 10 <span class="selector-tag">-s</span> 320<span class="selector-tag">x240</span> <span class="selector-tag">-t</span> 00<span class="selector-pseudo">:00</span><span class="selector-pseudo">:10.000</span> <span class="selector-tag">output</span><span class="selector-class">.gif</span></span><br></pre></td></tr></table></figure><h4 id="9-更换视频的分辨率"><a href="#9-更换视频的分辨率" class="headerlink" title="9.更换视频的分辨率"></a>9.更换视频的分辨率</h4><p>可以使用如下命令更换视频的分辨率</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">    ffmpeg -i <span class="built_in">input</span>.mp4 -<span class="built_in">filter</span>:v scale=<span class="number">1280</span>:<span class="number">720</span> -<span class="keyword">c</span>:<span class="keyword">a</span> <span class="keyword">copy</span> output.mp4</span><br><span class="line"><span class="built_in">or</span></span><br><span class="line">    ffmpeg -i <span class="built_in">input</span>.mp4 -s <span class="number">1280</span>x720 -<span class="keyword">c</span>:<span class="keyword">a</span> <span class="keyword">copy</span> output.mp4</span><br></pre></td></tr></table></figure><h4 id="10-设置视频的宽高比"><a href="#10-设置视频的宽高比" class="headerlink" title="10.设置视频的宽高比"></a>10.设置视频的宽高比</h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> -aspect <span class="number">16</span>:<span class="number">9</span> output.mp4</span><br></pre></td></tr></table></figure><p>常见的宽高比：<code>16:9、4:3、16:10、5:4</code></p><h4 id="11-利用ffmpeg屏幕录制"><a href="#11-利用ffmpeg屏幕录制" class="headerlink" title="11.利用ffmpeg屏幕录制"></a>11.利用ffmpeg屏幕录制</h4><p>参考：<a href="https://trac.ffmpeg.org/wiki/Capture/Desktop" target="_blank" rel="noopener">https://trac.ffmpeg.org/wiki/Capture/Desktop</a></p><h4 id="12-添加水印"><a href="#12-添加水印" class="headerlink" title="12.添加水印"></a>12.添加水印</h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ffmpeg -<span class="selector-tag">i</span> <span class="selector-tag">input</span><span class="selector-class">.mp4</span> -<span class="selector-tag">i</span> picture<span class="selector-class">.png</span> -filter_complex overlay=<span class="string">"(main_w/2)-(overlay_w/2):(main_h/2)-(overlay_h)/2"</span> output.mp4</span><br></pre></td></tr></table></figure><p><code>picture.png</code> 为水印图片， <code>overlay</code> 为水印位置</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;平时使用ffmpeg对视频解码成图片比较多，就稍微简单的了解一些ffmpeg常用的相关指令。下面是一些相关指令的介绍笔记。&lt;/p&gt;
&lt;h3 id=&quot;安装&quot;&gt;&lt;a href=&quot;#安装&quot; class=&quot;headerlink&quot; title=&quot;安装&quot;&gt;&lt;/a&gt;安装&lt;/h3&gt;&lt;p&gt;Debian/Ubuntu/Linux Mint 下安装 ffmpeg 很简单：&lt;/p&gt;
&lt;figure class=&quot;highlight routeros&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;apt-&lt;span class=&quot;builtin-name&quot;&gt;get&lt;/span&gt; install ffmpeg&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;其他操作系统安装方法，参考&lt;a href=&quot;https://www.ffmpeg.org/download.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官网&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;如果想要手工编译 ffmpeg 可以参考官方 &lt;a href=&quot;https://trac.ffmpeg.org/wiki#CompilingFFmpeg&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;wiki&lt;/a&gt;。 Ubuntu/Debian/Mint 系手工编译 ffmpeg 参考 &lt;a href=&quot;https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;wiki&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id=&quot;常用指令&quot;&gt;&lt;a href=&quot;#常用指令&quot; class=&quot;headerlink&quot; title=&quot;常用指令&quot;&gt;&lt;/a&gt;常用指令&lt;/h3&gt;&lt;h4 id=&quot;1-显示文件信息&quot;&gt;&lt;a href=&quot;#1-显示文件信息&quot; class=&quot;headerlink&quot; title=&quot;1.显示文件信息&quot;&gt;&lt;/a&gt;1.显示文件信息&lt;/h4&gt;&lt;p&gt;显示视频信息&lt;/p&gt;
&lt;figure class=&quot;highlight stylus&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;ffmpeg -&lt;span class=&quot;selector-tag&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;selector-tag&quot;&gt;input&lt;/span&gt;.avi&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h4 id=&quot;2-将视频拆分图片-批量截图&quot;&gt;&lt;a href=&quot;#2-将视频拆分图片-批量截图&quot; class=&quot;headerlink&quot; title=&quot;2.将视频拆分图片 批量截图&quot;&gt;&lt;/a&gt;2.将视频拆分图片 批量截图&lt;/h4&gt;
    
    </summary>
    
      <category term="linux" scheme="https://nicehuster.github.io/categories/linux/"/>
    
    
      <category term="ffmpeg" scheme="https://nicehuster.github.io/tags/ffmpeg/"/>
    
  </entry>
  
  <entry>
    <title>bash命令并行</title>
    <link href="https://nicehuster.github.io/2019/10/18/0060-bash-parallel/"/>
    <id>https://nicehuster.github.io/2019/10/18/0060-bash-parallel/</id>
    <published>2019-10-18T11:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>&#160; &#160; &#160; &#160;在bash中，使用后台任务来实现任务的“多进程化”。在不加控制的模式下，不管有多少任务，全部都后台执行。也就是说，在这种情况下，有多少任务就有多少“进程”在同时执行。<br><a id="more"></a><br><strong>实例一：正常情况脚本</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ((i=0;i&lt;5;i++));<span class="keyword">do</span></span><br><span class="line">        &#123;</span><br><span class="line">                sleep 3;<span class="built_in">echo</span> 1&gt;&gt;aa &amp;&amp; <span class="built_in">echo</span> <span class="string">"done!"</span></span><br><span class="line">        &#125; </span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">wait</span></span><br><span class="line">cat aa|wc -l</span><br><span class="line">rm aa</span><br></pre></td></tr></table></figure></p><p>&#160; &#160; &#160; &#160;这种情况下，程序顺序执行，每个循环3s，共需15s左右。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ time bash test.sh </span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">5</span><br><span class="line"></span><br><span class="line">real    0m15.030s</span><br><span class="line">user    0m0.002s</span><br><span class="line">sys     0m0.003s</span><br></pre></td></tr></table></figure></p><p><strong>实例二：“多进程”实现</strong><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ((i=0;i&lt;5;i++));<span class="keyword">do</span></span><br><span class="line">        &#123;</span><br><span class="line">                sleep 3;<span class="built_in">echo</span> 1&gt;&gt;aa &amp;&amp; <span class="built_in">echo</span> <span class="string">"done!"</span></span><br><span class="line">        &#125;&amp;</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">wait</span></span><br><span class="line">cat aa|wc -l</span><br><span class="line">rm aa</span><br></pre></td></tr></table></figure></p><p>&#160; &#160; &#160; &#160;这个实例实际上就在上面基础上多加了一个后台执行&amp;符号，此时应该是5个循环任务并发执行，最后需要3s左右时间。<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ time bash test.sh </span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">done!</span><br><span class="line">5</span><br><span class="line"></span><br><span class="line">real    0m3.109s</span><br><span class="line">user    0m0.008s</span><br><span class="line">sys     0m0.100s</span><br></pre></td></tr></table></figure></p><p>&#160; &#160; &#160; &#160;效果非常明显。这里需要说明一下wait的左右。wait是等待前面的后台任务全部完成才往下执行，否则程序本身是不会等待的，这样对后面依赖前面任务结果的命令来说就可能出错。</p><p>&#160; &#160; &#160; &#160;以上所讲的实例都是进程数目不可控制的情况，下面描述如何准确控制并发的进程数目。</p><p> **实例三：“多进程可控”实现<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">tmp_fifofile=<span class="string">"/tmp/$$.fifo"</span></span><br><span class="line">mkfifo <span class="variable">$tmp_fifofile</span>      <span class="comment"># 新建一个fifo类型的文件</span></span><br><span class="line"><span class="built_in">exec</span> 6&lt;&gt;<span class="variable">$tmp_fifofile</span>      <span class="comment"># 将fd6指向fifo类型</span></span><br><span class="line">rm <span class="variable">$tmp_fifofile</span></span><br><span class="line">thread=15 <span class="comment"># 此处定义线程数</span></span><br><span class="line"><span class="keyword">for</span> ((i=0;i&lt;<span class="variable">$thread</span>;i++));<span class="keyword">do</span> </span><br><span class="line"><span class="built_in">echo</span></span><br><span class="line"><span class="keyword">done</span> &gt;&amp;6 <span class="comment"># 事实上就是在fd6中放置了$thread个回车符</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ((i=0;i&lt;50;i++));<span class="keyword">do</span> <span class="comment"># 50次循环</span></span><br><span class="line"><span class="built_in">read</span> -u6 </span><br><span class="line"><span class="comment"># 一个read -u6命令执行一次，就从fd6中减去一个回车符，然后向下执行，</span></span><br><span class="line"><span class="comment"># fd6中没有回车符的时候，就停在这了，从而实现了线程数量控制</span></span><br><span class="line"></span><br><span class="line">&#123; <span class="comment"># 此处子进程开始执行，被放到后台</span></span><br><span class="line">      &#123;sleep 3&#125; &amp;&amp; &#123; <span class="built_in">echo</span> <span class="string">"a_sub is finished"</span>&#125;</span><br><span class="line"></span><br><span class="line">     <span class="built_in">echo</span> &gt;&amp;6 <span class="comment"># 当进程结束以后，再向fd6中加上一个回车符，即补上了read -u6减去的那个</span></span><br><span class="line">&#125; &amp;</span><br><span class="line"></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">wait</span> <span class="comment"># 等待所有的后台子进程结束</span></span><br><span class="line"><span class="built_in">exec</span> 6&gt;&amp;- <span class="comment"># 关闭df6</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">exit</span> 0</span><br></pre></td></tr></table></figure></p><p>sleep 3s，线程数为15，一共循环50次，所以，此脚本一共的执行时间大约为12秒</p><p>即：<br>15x3=45, 所以 3x3s=9s<br>(50-45=5)&lt;15, 所以1x3s=3s<br>所以9s+3s = 12s</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ time bash multithread.sh </span><br><span class="line"></span><br><span class="line">real        0m12.025s</span><br><span class="line">user        0m0.020s</span><br><span class="line">sys         0m0.064s</span><br></pre></td></tr></table></figure><p>而当不使用多线程技巧的时候，执行时间为：50 x 3s = 150s。</p><p>注：此文转载自<a href="http://www.cnitblog.com/sysop/archive/2008/11/03/50974.aspx" target="_blank" rel="noopener">这里</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;#160; &amp;#160; &amp;#160; &amp;#160;在bash中，使用后台任务来实现任务的“多进程化”。在不加控制的模式下，不管有多少任务，全部都后台执行。也就是说，在这种情况下，有多少任务就有多少“进程”在同时执行。&lt;br&gt;
    
    </summary>
    
      <category term="project" scheme="https://nicehuster.github.io/categories/project/"/>
    
    
      <category term="parallel programming" scheme="https://nicehuster.github.io/tags/parallel-programming/"/>
    
  </entry>
  
  <entry>
    <title>图像分类算法优化技巧</title>
    <link href="https://nicehuster.github.io/2019/06/13/0059-cnn_tricks/"/>
    <id>https://nicehuster.github.io/2019/06/13/0059-cnn_tricks/</id>
    <published>2019-06-13T11:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>上午看了一下<a href="https://arxiv.org/pdf/1812.01187.pdf" target="_blank" rel="noopener">Bag of Tricks for Image Classification with Convolutional Neural Networks</a>这篇文章，该文提到了许多关于如何提升CNN性能的trick。trick在CNN中起着非常重要的作用，然而很少有文章详细地介绍这些trick的使用。而这篇文章则对一些常用的trick做了详尽的介绍以及使用trick前后带来的对比，可以说是满满的干货，对应的代码实现，在<a href="https://github.com/dmlc/gluon-cv" target="_blank" rel="noopener">这里</a>。文章提到的trick包括：模型结构调整，数据增强，学习率调整，训练速度优化等等。<br><a id="more"></a></p><h3 id="加快模型训练"><a href="#加快模型训练" class="headerlink" title="加快模型训练"></a>加快模型训练</h3><p>目前加快模型训练有效的两种方式是：（1）使用大的batch size；（2）采用低精度训练；</p><p>（1）<strong>使用大的batch size训练</strong><br>使用较大的 batch size 时，如果还是使用同样的 epochs 数量进行运算，则准确度往往低于 batch size 较小的场景。为了确保使用较大的 batch size 训练能够在相同 epochs 前提下获得与较小的 batch size 相近的测试准确度。这部分具体可以参考<a href="https://arxiv.org/pdf/1706.02677.pdf" target="_blank" rel="noopener">Accurate, Large Minibatch SGD</a>这篇文章。作者总结了如下几种解决方案：</p><p><strong>A.增大学习率</strong>：如果我们将 batch size 由 B 增加至 kB，我们亦需要将学习率由η增加至 kη（其中 k 为倍数）。</p><p><strong>B.warm up</strong>：先用一个小的学习率先训几个epoch（warmup），因为网络的参数是随机初始化的，假如一开始就采用较大的学习率容易出现数值不稳定，这是使用warmup的原因。等到训练过程基本稳定了就可以使用原先设定的初始学习率进行训练了。论文中提到了warmup采用线性增加策略实现。举例而言，假设，初始学习率为η,选择前m个batch用于warm up，则在第i个batch时, 1 ≤ i ≤ m，设置学习率为 iη/m。</p><p><strong>C.每个残差模块最后的BN层中γ设置为0</strong>：BN层的γ、β参数是用来对标准化后的输入做线性变换的，也就是γx^+β，一般γ参数都会初始化为1，β参数会初始化为0，作者认为初始化为0更有利于模型的训练。</p><p><strong>D.不对bias执行weight decay操作</strong>：weight decay通常用于所有网络层的可学习参数（包括weight和bias）。其作用等效于对所有参数做L2正则化，以达到减少模型过拟合的作用。作者推荐仅仅只对weight进行weight decay操作。</p><p>（2）<strong>使用低精度训练</strong><br>采用低精度比如FP16训练可以在数值层面上对网络训练加速。通过大多数网络训练都是采用FP32精度训练，是因为网络的输入，网络参数以及网络输出都是采用FP32。如果能使用16位浮点型参数进行训练，就可以大大加快模型的训练速度。下表是使用大的batch size和半精度FP16进行训练的前后对比，其中baseline 使用BS=256,FP32，efficient使用BS=1024,FP16：<br><img src="/img/cnn_trick.png" alt><br>从实验结果可以看出，相比baseline,训练速度得到明显提升，而且准确率上也得到了一定程度提升。更加详细的对比实验如下：<br><img src="/img/cnn_trick2.png" alt></p><h3 id="模型结构调整"><a href="#模型结构调整" class="headerlink" title="模型结构调整"></a>模型结构调整</h3><p>这种模型结构调整是在不显著增加计算量的情况下对模型性能进行进一步提升。作者以ResNet为例进行优化。ResNet通常包括一个input stem，4个stage和1个output。下图展示的是原始resnet50结构。<br><img src="/img/cnn_trick_resnet.png" alt><br>作者在downsampling 部分，input stem以及Path B部分做了一些小的调整，对应如下图(a),(b),(c)<br><img src="/img/cnn_trick_resnet1.png" alt><br>实验结果对比如下：<br><img src="/img/cnn_trick_resnet-res.png" alt><br>从实验结果可以看出，这些结构上微小的调整，并没有对计算量带来较大的变化，但对accuracy带来不小的提升。</p><h3 id="模型训练调优"><a href="#模型训练调优" class="headerlink" title="模型训练调优"></a>模型训练调优</h3><p>作者在这一部分提到了四种调优技巧：</p><p><strong>采用cosine学习率衰减策略</strong>：实验对比结果如下，相比于常用的step decay,cosine decay在起始阶段开始衰减学习率，在step decay的学习率下降了10x时，cosine依然可以保持较大的学习率，这潜在的提高了训练速度。<br><img src="/img/cosine.png" alt></p><p><strong>知识蒸馏</strong>：使用一个效果更好的teacher model训练student model，使得student model在模型结构不改变的情况下提升效果。作者采用ResNet-152作为teacher model，用ResNet-50作为student model。代码上通过在ResNet网络后添加一个蒸馏损失函数实现，这个损失函数用来评价teacher model输出和student model输出的差异，因此整体的损失函数原损失函数和蒸馏损失函数的结合：<br><img src="/img/distill.png" alt><br>其中p表示真实标签，z表示student model的全连接层输出，r表示teacher model的全连接层输出，T是超参数，用来平滑softmax函数的输出。</p><p><strong>mixup</strong>：mixup也是一种数据增强操作，其大致操作是，每次读取2张输入图像，假设用（xi，yi）和（xj，yj）表示，那么通过如下公式就可以合成得到一张新的图像（x，y），然后用这张新图像进行训练：<br><img src="/img/mixup.png" alt><br>其中λ属于[0,1]服从Beta分布。实验结果如下<br><img src="/img/cnn_trick_res.png" alt></p><p>此外，作者将这些trick应用于其他图像任务中，比如目标检测，图像分割上同样有效，可以带来2-3个点的提升。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上午看了一下&lt;a href=&quot;https://arxiv.org/pdf/1812.01187.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Bag of Tricks for Image Classification with Convolutional Neural Networks&lt;/a&gt;这篇文章，该文提到了许多关于如何提升CNN性能的trick。trick在CNN中起着非常重要的作用，然而很少有文章详细地介绍这些trick的使用。而这篇文章则对一些常用的trick做了详尽的介绍以及使用trick前后带来的对比，可以说是满满的干货，对应的代码实现，在&lt;a href=&quot;https://github.com/dmlc/gluon-cv&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这里&lt;/a&gt;。文章提到的trick包括：模型结构调整，数据增强，学习率调整，训练速度优化等等。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://nicehuster.github.io/categories/paper-reading/"/>
    
    
      <category term="deep learning" scheme="https://nicehuster.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>细粒度图像识别</title>
    <link href="https://nicehuster.github.io/2019/06/12/0058-fine-grain/"/>
    <id>https://nicehuster.github.io/2019/06/12/0058-fine-grain/</id>
    <published>2019-06-12T11:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>一般而言，图像识别分为两种：传统图像识别和细粒度图像识别。前者指的是对一些大的类别比如汽车、动物、植物等大的类别进行分类，这是属于粗粒度的图像识别。而后者则是在某个类别下做进一步分类。比如在狗的类别下区分狗的品种是哈士奇、柯基、萨摩还是阿拉斯加等等，这是属于细粒度图像识别。<br><img src="/img/fine-grain.png" alt><br><a id="more"></a></p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>在细粒度图像识别领域，经典的基准数据集包括：</p><ul><li>鸟类数据集CUB200-2011，11788张图像，200个细粒度分类</li><li>狗类数据集Stanford Dogs，20580张图像，120个细粒度分类</li><li>花类数据集Oxford Flowers，8189张图像，102个细粒度分类</li><li>飞机数据集Aircrafts，10200张图像，100个细粒度分类</li><li>汽车数据集Stanford Cars，16185张图像，196个细粒度分类</li></ul><p>细粒度图像分类作为一个热门的研究方向，每年的计算机视觉顶会都会举办一些workshop和挑战赛，比如Workshop on Fine-Grained Visual Categorization和iFood Classification Challenge。</p><h3 id="挑战"><a href="#挑战" class="headerlink" title="挑战"></a>挑战</h3><p><img src="/img/fine-grain-challenge.png" alt><br>上图展示的是CUB20鸟类数据集的部分图片。不同行表示的不同的鸟类别。很明显，这些鸟类数据集在同一类别上存在巨大差异，比如上图中每一行所展示的一样，这些差异包括姿态、背景等差异。但在不同类别的鸟类上却又存在着差异性小的问题，比如上图展示的第一列，第一列虽然分别属于不同类别，但却又十分相似。</p><p>因此可以看出，细粒度图像识别普遍存在类内差异性大（large intra-class variance）和类间差异性小（small inter-class variance）的特点。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>细粒度图像识别同样是作为图像分类任务，因此也可以直接使用通用图像识别中一些算法来做，比如直接使用resnet,vgg等网络模型直接训练识别，通常在数据集上，比如CUB200上就可以达到75%的准确率，但这种方法离目前的SOTA方法的精度至少差了10个点。</p><p>目前细粒度图像识别方法大致可以分为两类：</p><p>1.<strong>基于强监督学习方法</strong>：这里指的强监督信息是指bounding box或者landmark，举个例子，针对某一种鸟类，他和其他的类别的差异一般在于它的嘴巴、腿部，羽毛颜色等<br><img src="/img/v2-f04c284bb40f1fc3da258bb6764d4728_hd.jpg" alt><br>主流的方法像Part-based R-CNN，Pose Normalized CNN,Part-Stacked CNN等。</p><p>2.<strong>基于弱监督学习方法</strong>：什么是弱监督信息呢？就是说没有bounding box或者landmark信息，只有类别信息，开山之作应该属于2015年Bilinear CNN，这个模型当时在CUB200上是state of the art，即使和强监督学习方法相比也只是差1个点左右。</p><p>关于前几年细粒度图像分析的综述，可以参考<a href="https://zhuanlan.zhihu.com/p/24738319" target="_blank" rel="noopener">这里</a>。由于强监督学习方法中对于大规模数据集来说，bounding box和landmark标注成本较高，因此，现在主流的研究方法都是是基于弱监督学习方法。</p><p>下面是我要介绍的近1/2年来比较有代表性的顶会paper，这些paper都是基于弱监督信息，自主去挖掘Discriminative Region。</p><h3 id="Look-Closer-to-See-Better-Recurrent-Attention-Convolutional-Neural-Network-for-Fine-grained-Image-Recognition"><a href="#Look-Closer-to-See-Better-Recurrent-Attention-Convolutional-Neural-Network-for-Fine-grained-Image-Recognition" class="headerlink" title="Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-grained Image Recognition"></a><center>Look Closer to See Better: Recurrent Attention Convolutional Neural Network for Fine-grained Image Recognition</center></h3><p>代码链接：<a href="https://github.com/Jianlong-Fu/Recurrent-Attention-CNN" target="_blank" rel="noopener">https://github.com/Jianlong-Fu/Recurrent-Attention-CNN</a><br>这篇文章是CVPR2017的一篇oral paper。细粒度图像识别的挑战主要包括两个方面：判别力区域定位以及从判别力区域学习精细化特征。RA-CNN以一种相互强化的方式递归地学习判别力区域attention和基于区域的特征表示。具体模型结构如下：<br><img src="/img/ra-cnn.png" alt></p><h4 id="主要思路"><a href="#主要思路" class="headerlink" title="主要思路"></a>主要思路</h4><p>如上图，每一行表示一个普通的CNN网络，<br>（1）图片a1进入b1（堆叠多个卷积层）之后，分成两个部分，一个部分到c1连接fc+softmax进行普通的分类；另一个部分进入d1，Attention Proposal Network得到一个region proposal。<br>（2）在原图上利用d1提出的region proposal，在原图上crop出一个更有判别性的小区域，插值之后得到a2,同样的道理得到a3。<br>可以看出特征区域经过两个APN之后不断放大和精细化，为了使得APN选取的特征区域是图像中最具有判别性的区域，作者引入了一个Ranking loss：即强迫a1、a2、a3区域的分类confidence score越来越高(图片最后一列的对应Pt概率越来越大)。这样以来，联合普通的分类损失，使网络不断细化discriminative attention region。</p><h4 id="部分细节"><a href="#部分细节" class="headerlink" title="部分细节"></a>部分细节</h4><p><strong>attention 定位和放大</strong><br>作者使用二维boxcar函数作为attention mask与原图相乘得到候选区域位置。这样做的目的在于实现APN的端对端训练。因为普通的crop操作不可导。<br><strong>损失函数</strong><br>该模型的损失函数包含两个部分，一部分是每一路经过fc和softmax之后的一个分类误差；一部分是Ranking loss使得越精细化的区域得到了置信度分数越高。<br><img src="/img/ra-cnn-loss.png" alt><br>对于ranking loss，<br><img src="/img/rank-loss.png" alt><br>在训练的过程中，迫使$p^{(s+1)}_t &gt;p^{(s)}_t$。</p><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p>在CUB-200-2011数据集上<br><img src="/img/ra-cnn-res.png" alt></p><h3 id="Pairwise-Confusion-for-Fine-Grained-Visual-Classification"><a href="#Pairwise-Confusion-for-Fine-Grained-Visual-Classification" class="headerlink" title="Pairwise Confusion for Fine-Grained Visual Classification"></a><center>Pairwise Confusion for Fine-Grained Visual Classification</center></h3><p>代码链接：<a href="https://github.com/abhimanyudubey/confusion" target="_blank" rel="noopener">https://github.com/abhimanyudubey/confusion</a><br>这是ECCV2018的一篇文章，这篇文章提出了一种Pairwise Confusion正则化方法，主要用于解决在细粒度图像分类问题上类间相似性和样本少导致过拟合的问题。在通用图像分类问题上，由于数据集一般较大，直接使用交叉熵损失函数就可以迫使网络学习类间差异性。然而对于细粒度图像分类问题而言，数据集小，且普遍存在类间差异较小，类内差异较大的特点。假如对于两张鸟类图像样本，内容相似却有着不同的标签，直接最小化交叉熵损失将会迫使网络去学习图像本身的差异比如一些差异性较大的背景，而不能很好的挖掘不同鸟类的细粒度区别。<br><img src="/img/pc.png" alt><br>因此，作者提出了Pairwise Confusion方法 ，网络结构如上图所示。网络采用Siamese结构共享权值，对于一个Batch的图片会分成两部分，组成很多“图片对”，如果这些图片对属于相同的label，那么就把两张图片分别求Cross entropy loss；如果有一对图片属于不同的label，那么在分别对他们求Cross Entropy Loss的同时还要附加一个Euclidean Confusion作为惩罚项</p><p>论文的主要出发点还是制约不同类别图片表示的特征向量之间的距离。作为一个涨点的trick。可以加在任何细粒度识别算法中。下面是一些实验对比结果，可以看出，添加PC之后在每个数据集都能带来1-2个点。<br><img src="/img/pc-res.png" alt></p><h3 id="Learning-to-Navigate-for-Fine-grained-Classification"><a href="#Learning-to-Navigate-for-Fine-grained-Classification" class="headerlink" title="Learning to Navigate for Fine-grained Classification"></a><center>Learning to Navigate for Fine-grained Classification</center></h3><p>代码链接：<a href="https://github.com/yangze0930/NTS-Net" target="_blank" rel="noopener">https://github.com/yangze0930/NTS-Net</a><br>这也是ECCV2018的文章，这篇文章借鉴了RPN的思路，通过在原图上生成anchors，利用rank loss选出信息量最大的一些proposal，然后crop出这些区域，和原图一起提取特征然后进行决策判断。这是这篇文章方法的一个大致结构。<br><img src="/img/ntsnet.png" alt></p><h4 id="Navigator"><a href="#Navigator" class="headerlink" title="Navigator"></a>Navigator</h4><p>这个结构和FPN类似，在三个不同尺度的feature map上生成候选框，Navigator就是给每一个候选区域的“信息量”打分,信息量大的区域分数高。</p><h4 id="Teacher"><a href="#Teacher" class="headerlink" title="Teacher"></a>Teacher</h4><p>这个就是对topN分数的候选区域进行Feature Extractor + FC + softmax，判断这些候选区域属于目标的概率；</p><h4 id="Scrutinizer"><a href="#Scrutinizer" class="headerlink" title="Scrutinizer"></a>Scrutinizer</h4><p>这个是把所有候选区域part_feats和原图的raw_feats提取出来然后concat在一起，经过fc输出200个对应类别。</p><h4 id="实现流程"><a href="#实现流程" class="headerlink" title="实现流程"></a>实现流程</h4><p>下面是作者给出的源码一个具体实现流程：<br>（1）输入大小448x448尺寸原图输入feature extractor（resnet50），得到（14x14x2048）feature maps，经过Global Pooling之后2048维的Feature以及经过Global Pooling+ FC分类之后的200维的raw_logits;<br>（2）预设的RPN在14x14,7x7,4x4这三种尺度的feature map上根据不同的scale和ratio生成对应的Anchors 一共1614个（14x14x6+7x7x6+4x4x9=1614）;<br>（3）用步骤（1）得到的14x14x2048大小的feature map经过navigator对每个anchors进行打分，使用NMS进行处理，只保留topN（N=6）个proposal。<br>（4）把topN个proposal使用bilinear到224x224，输入feature extractor，得到这些局部区域的part_features,part_features经过全连接层可以到part_logits;<br>（5）把步骤（1）得到的全局Feature和步骤（4）得到的局部的part_features拼接在一起，经过全连接层得到concat_logits，用于最终的推断决策;</p><h4 id="监督训练"><a href="#监督训练" class="headerlink" title="监督训练"></a>监督训练</h4><p>（1）交叉熵损失：步骤（1）中的<strong>raw_logits</strong>, 步骤（4）中的<strong>part_logits</strong>,步骤5中的<strong>concat_logits</strong>都是直接使用交叉熵损失函数监督。<br>（2）<strong>ranking loss</strong>：步骤（3）中的信息量打分需要用步骤（4）中的part分类概率进行监督，即对于步骤（4）中的判断的属于目标Label概率高的局部区域，必须在步骤（3）中判断的信息量也高。<br><img src="/img/navigate.png" alt><br>值得注意的是，在原文中，作者提到的级联训练损失函数只由rank_loss，concat_logits以及part_logits三个部分组成，但在代码实现上加入了raw_logits部分，每个部分的权重都为1。在CUB200数据集上实验结果如下：<br><img src="/img/ntsnet-res.png" alt></p><p>从上面看到的几篇paper都可以看到ranking loss的影子，ranking loss是Learning to Rank提出的一种机器学习算法，通过训练模型来解决排序问题，具体介绍可以看<a href="https://www.cnblogs.com/bentuwuying/p/6681943.html" target="_blank" rel="noopener">这里</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一般而言，图像识别分为两种：传统图像识别和细粒度图像识别。前者指的是对一些大的类别比如汽车、动物、植物等大的类别进行分类，这是属于粗粒度的图像识别。而后者则是在某个类别下做进一步分类。比如在狗的类别下区分狗的品种是哈士奇、柯基、萨摩还是阿拉斯加等等，这是属于细粒度图像识别。&lt;br&gt;&lt;img src=&quot;/img/fine-grain.png&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://nicehuster.github.io/categories/paper-reading/"/>
    
    
      <category term="fine-grain" scheme="https://nicehuster.github.io/tags/fine-grain/"/>
    
  </entry>
  
  <entry>
    <title>了解零和博弈/贸易顺差逆差</title>
    <link href="https://nicehuster.github.io/2019/05/16/0057-zero-game/"/>
    <id>https://nicehuster.github.io/2019/05/16/0057-zero-game/</id>
    <published>2019-05-16T13:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近几天比较热的话题莫过于中美贸易谈判，看懂官方（人民日报，央视新闻等）给出的评论以及一些国际锐评，就不可避免地需要了解零和博弈和贸易逆差等关键字。</p><h3 id="零和博弈"><a href="#零和博弈" class="headerlink" title="零和博弈"></a>零和博弈</h3><p>零和博弈（zero-sum game），又称零和游戏，与非零和博弈相对，是博弈论的一个概念，属非合作博弈。指参与博弈的各方，在严格竞争下，一方的收益必然意味着另一方的损失，博弈各方的收益和损失相加总和永远为“零”，双方不存在合作的可能。</p><a id="more"></a><p>零和博弈的结果是一方吃掉另一方，一方的所得正是另一方的所失，整个社会的利益并不会因此而增加一分。也可以说：自己的幸福是建立在他人的痛苦之上的，二者的大小完全相等，因而双方都想尽一切办法以实现“损人利己”。与“零和”对应的“双赢”的基本理论就是“利己”不“损人”，通过谈判、合作达到皆大欢喜的结果。</p><p>零和博弈比较常见的例子就是打扑克/麻将，打扑克的人的钱总数是不变，变的只是钱从一个口袋转移至另一个口袋而已。</p><h3 id="非零和博弈"><a href="#非零和博弈" class="headerlink" title="非零和博弈"></a>非零和博弈</h3><p>非零和博弈是一种合作下的博弈，博弈中各方的收益或损失的总和不是零值，它区别于零和博弈。在经济学研究中比较有用。 在这种状况时，自己的所得并不与他人的损失的大小相等，连自己的幸福也未必建立在他人的痛苦之上，即使伤害他人也可能“损人不利己”，所以博弈双方存在 “双赢”的可能，进而达成合作。</p><p>非零和博弈既有可能是<strong>正和博弈</strong>，也有可能是<strong>负和博弈</strong>。<br><strong>正和博弈</strong>：指博弈双方的利益都有所增加，或者至少是一方的利益增加，而另一方的利益不受损害，因而整体的利益有所增加。<br><strong>负和博弈</strong>：指博弈双方都有损失，整体的利益有所减少。</p><p>非零和博弈比较具有代表性的例子就是“<strong>囚徒困境</strong>”。“囚徒困境”是1950年美国兰德公司提出的博弈论模型。两个共犯被关入监狱，在不能互相沟通情况。如果两个人都不揭发对方，则由于证据不确定，每个人都坐牢一年；若一人揭发，而另一人保持沉默，则揭发者因为立功而立即获释，沉默者因不合作而入狱五年；若互相揭发，则因证据确实，二者都判刑五年。由于囚徒无法信任对方，因此倾向于互相揭发，而不是同守沉默。<br>我们可以用图表来表示上述情况，如果立即获释，得0分，每人获刑1年，则-1分；如果获刑两年则-2分，如果获刑五年则-5分。结果如下：<br><img src="/img/no-zero-game.png" alt><br>囚徒困境是博弈论的非零和博弈中具代表性的例子，反映<strong>个人最佳选择并非团体最佳选择</strong>。</p><p>目前中美贸易谈判失败的一个原因在于两国的立足点从本质上就矛盾的。中国认为，目前经济全球化下，中美合作就是双赢，可以互相推动经济发展，一起赚钱。而美国则认为，中美合作就是零和博弈，全球资源总数是不变的，蛋糕就这么大，你吃了一点，那我吃的就少一点，况且，我本来就强，为什么要和你一起合作吃大蛋糕呢。这就是典型的霸权主义。近年来，中国发展迅速，赚的钱远比美国多。所以美国贸易谈判不肯合作，是说的过去的，不愧是作为生意人出身的特朗普，很有生意头脑。其实，中国在对待南海问题上也是这个态度，搁置争议，共同开发。毕竟，当前实现中华民族伟大复兴是最重要的。</p><h3 id="贸易顺差逆差"><a href="#贸易顺差逆差" class="headerlink" title="贸易顺差逆差"></a>贸易顺差逆差</h3><p><strong>贸易顺差逆差</strong>是指在一定时间内，国家的出口贸易总额大于进口贸易总额，又称“出超”，反之，则是贸易逆差，又称“入超”或者“贸易赤字”。</p><p>贸易顺差逆差会影响一国货币的汇率变化。以中国为例，贸易顺差大则就意味着商品出口增加，人民币面临升值的压力，反之当贸易逆差较大时，人民币就容易贬值，当一个国家出现贸易顺差时，说明该国对外贸易中是净赚的，对国民经济来说，在推动就业发展，增加就业，推进出口增长，增加外汇储备等方面有着积极的作用，但贸易顺差过大则意味着对国际市场依赖性较强，一旦国际市场不买账了，国内经济就会受到影响，另外为了应对本国货币的升值压力，国家通常会增加货币发行量，造成通货膨胀，除此以外，长期过大的贸易顺差还容易导致与贸易伙伴国之间的摩擦，毕竟谁愿意跟一个总是赚自己钱的人做生意呢。</p><p>而当一个国家出现贸易逆差时，说明该国在对外贸易中处于不利地位，为了支付进口产生的债务，国民收入就会外流，经济表现转弱，但贸易逆差又并非一无是处，适当的逆差有利于降低国内的通货膨胀压力，对缓解短期贸易纠纷，促进长期稳定增长都有着积极的作用。</p><p>总的来说，贸易顺差和逆差并没有孰优孰劣，对一个国家来说，平衡才是最好的状态。目前，中美贸易谈判失败的另一个原因在于，美国认为，美中两国之间存在巨额的贸易逆差。个人认为，美国的巨额贸易逆差并非因中国而生，也不会因中国而终。产生的根本原因很多，比如财政赤字，过度消费等等。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近几天比较热的话题莫过于中美贸易谈判，看懂官方（人民日报，央视新闻等）给出的评论以及一些国际锐评，就不可避免地需要了解零和博弈和贸易逆差等关键字。&lt;/p&gt;
&lt;h3 id=&quot;零和博弈&quot;&gt;&lt;a href=&quot;#零和博弈&quot; class=&quot;headerlink&quot; title=&quot;零和博弈&quot;&gt;&lt;/a&gt;零和博弈&lt;/h3&gt;&lt;p&gt;零和博弈（zero-sum game），又称零和游戏，与非零和博弈相对，是博弈论的一个概念，属非合作博弈。指参与博弈的各方，在严格竞争下，一方的收益必然意味着另一方的损失，博弈各方的收益和损失相加总和永远为“零”，双方不存在合作的可能。&lt;/p&gt;
    
    </summary>
    
      <category term="杂谈" scheme="https://nicehuster.github.io/categories/%E6%9D%82%E8%B0%88/"/>
    
    
      <category term="摘记" scheme="https://nicehuster.github.io/tags/%E6%91%98%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>HRNet详解</title>
    <link href="https://nicehuster.github.io/2019/05/15/0056-HRNet/"/>
    <id>https://nicehuster.github.io/2019/05/15/0056-HRNet/</id>
    <published>2019-05-15T13:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> Ke Sun,USTC &amp; MSRA, CVPR2019<br><strong>代码链接：</strong> <a href="https://github.com/leoxiaobin/deep-high-resolution-net.pytorch" target="_blank" rel="noopener">https://github.com/leoxiaobin/deep-high-resolution-net.pytorch</a><br><strong>整体信息：</strong> HRNet（High Resolution Network）由一个中科大的学生在MSRA实习时的一篇工作。这篇工作设计了一种新的人体姿态估计模型。在姿态估计中，图像feature map的分辨率大小至关重要，以往的姿态估计方法都是采用串行的高—&gt;低—&gt;高的方式来获得语义信息强的高分辨feature map。而HRNet不同的是，其自始至终都保持的高分辨率。并且这非常有效。刷新 COCO keypoint detection数据集和the MPII Human Pose数据集。<br><img src="/img/other-hrnet.png" alt><br><a id="more"></a></p><p>上图表示的姿态估计方法中几种经典结构，（a）是Hourglass中的结构，（b）是CPN中的结构，（c）是simpleBaseline，（d）则是DeepCut结构，可以看这几种经典的结构中，图像的feature map都经历了high-to-low和low-to-high的两个变化过程，前者的目的在于生成低分辨率（low-resolution）以及高层表示（high-level representation），后者目的则是生成高分辨率表示（high-resolution representation），feature map大小变化是串行的。而且，从高到低和从低到高是分开的。</p><p>其实这些网络的设计模式不外乎：1）对称的high-to-low和low-to-high过程，比如hourglass；2）Heavy high-to-low 和 light low-to-high过程；后者这类方法high-to-low过程一般采用分类网络（比如ResNet,VGG）是一个heavy部分，而low-to-high过程则是简单地堆叠 bilinear-upsampling层或者或者transpose卷积层。</p><h3 id="HRNet"><a href="#HRNet" class="headerlink" title="HRNet"></a>HRNet</h3><p><img src="/img/HRNet.png" alt><br>HRNet的网络结构如上图所示，非常清晰易懂。一共有三个不同feature map大小的branch。每个branch在前向传播过程中，feature map大小不发生变化。三个branch虽然有着不同大小feature map但是每个branch之间会存在信息的交流。例如在前向的过程中，上面branch会把自己的feature map大小减半，然后传入到下面的branch中，而下面的branch也会通过upsample把变大后的feature送给上面的branch，两个操作可以在同一个阶段同时进行。</p><p>可以看出，不同于其他方法，HRNet通过并行的方式连接高-低分辨率特征图，因此可以直接在高分辨率的特征图上预测姿态，而不需要通过降采样再升采样来预测姿态，而且在整个过程中，不停地融合各种不同尺度的表征，实现了多尺度融合，提高了高分辨率的语义信息。</p><p>实验结果，在两种输入分辨率上，大模型HRNet-W48和小模型HRNet-W32，都刷新了COCO纪录。其中，大模型在384x288的输入分辨率上，拿到了76.3的mAP。<br><img src="/img/HRNet-res.png" alt></p><h3 id="姿态估计评估指标"><a href="#姿态估计评估指标" class="headerlink" title="姿态估计评估指标"></a>姿态估计评估指标</h3><p>最后提一下姿态估计中一些常用的评估指标。不得不提的两个指标就是OKS和PCK。</p><h4 id="OKS（Object-Keypoint-Similarity）"><a href="#OKS（Object-Keypoint-Similarity）" class="headerlink" title="OKS（Object Keypoint Similarity）"></a>OKS（Object Keypoint Similarity）</h4><p>这个是coco姿态估计挑战赛提出的一个评估指标，基于对象关键点相似度的mAP,常用于coco姿态估计中。<br><img src="/img/oks.png" alt><br>其中，di表示预测的关键点与ground truth之间的欧式距离。vi是ground truth的可见性标志，s是目标尺度，此值等于该人在ground truth中的面积的平方根,ki控制衰减的每个关键点常量。</p><p>简而言之，OKS扮演的角色与IoU在目标检测中扮演的角色相同。它是根据人的尺度标准化的预测点和标准真值点之间的距离计算出来的。在coco<a href="http://cocodataset.org/#keypoints-leaderboard" target="_blank" rel="noopener">官网</a>可以看到和目标检测一样的评估指标AP[0.5:0.95]和AR[0.5:95]。从官网目前给出的leaderboard可以看到mAP最高是0.764。</p><h4 id="PCK（Probability-of-Correct-Keypoint）"><a href="#PCK（Probability-of-Correct-Keypoint）" class="headerlink" title="PCK（Probability of Correct Keypoint）"></a>PCK（Probability of Correct Keypoint）</h4><p>预测的关键点与其对应的 groundtruth 之间的归一化距离小于设定阈值的比例。如果预测关节与真实关节之间的距离在特定阈值内，则检测到的关节被认为是正确的。阈值可以是：</p><ul><li>PCK@0.2表示以躯干（torso size）直径最为归一化参考，如果归一化后的距离大于阈值0.2，则认为改点预测正确。FLIC数据集评估指标采用的就是PCK@0.2。</li><li>PCKh@0.5表示以头部长度（head length）作为归一化参考，如果归一化后的距离大于阈值0.5，则认为改点预测正确。MPII数据集的评估指标采用的就是PCKh@0.5，目前MPII数据集PCKh最高为92.5；</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; Ke Sun,USTC &amp;amp; MSRA, CVPR2019&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt; &lt;a href=&quot;https://github.com/leoxiaobin/deep-high-resolution-net.pytorch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/leoxiaobin/deep-high-resolution-net.pytorch&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt; HRNet（High Resolution Network）由一个中科大的学生在MSRA实习时的一篇工作。这篇工作设计了一种新的人体姿态估计模型。在姿态估计中，图像feature map的分辨率大小至关重要，以往的姿态估计方法都是采用串行的高—&amp;gt;低—&amp;gt;高的方式来获得语义信息强的高分辨feature map。而HRNet不同的是，其自始至终都保持的高分辨率。并且这非常有效。刷新 COCO keypoint detection数据集和the MPII Human Pose数据集。&lt;br&gt;&lt;img src=&quot;/img/other-hrnet.png&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://nicehuster.github.io/categories/paper-reading/"/>
    
    
      <category term="pose estimation" scheme="https://nicehuster.github.io/tags/pose-estimation/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch常用API解析</title>
    <link href="https://nicehuster.github.io/2019/05/14/0055-pytorch-api/"/>
    <id>https://nicehuster.github.io/2019/05/14/0055-pytorch-api/</id>
    <published>2019-05-14T11:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>好记性不如烂笔头，主要是纪录一些Pytorch中常用函数用法以及自定义数据读取collate_fn()介绍。</p><h3 id="常用数学类操作符"><a href="#常用数学类操作符" class="headerlink" title="常用数学类操作符"></a>常用数学类操作符</h3><h4 id="torchTensor-transpose-amp-Tensor-permute"><a href="#torchTensor-transpose-amp-Tensor-permute" class="headerlink" title="torchTensor.transpose() &amp; Tensor.permute()"></a>torchTensor.transpose() &amp; Tensor.permute()</h4><p>transpose只能操作矩阵的两个维度。只接受两个维度参数。若要对多个维度进行操作，使用permute更加灵活方便。<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a=torch.rand(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)  #torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">b=a.transpose(<span class="number">0</span>,<span class="number">1</span>)   #torch.Size([<span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">c=a.transpose(<span class="number">0</span>,<span class="number">1</span>).transpose(<span class="number">1</span>,<span class="number">2</span>)  #torch.Size([<span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>])</span><br><span class="line">d=a.permute(<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)                 #torch.Size([<span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure></p><p>同样是对三个维度进行变换，transpose需要操作三次，而是用permute只需要操作一次，因此对于高维度的矩阵变化，permute更加方便。<br><a id="more"></a></p><h4 id="toch-cat-amp-torch-stack"><a href="#toch-cat-amp-torch-stack" class="headerlink" title="toch.cat() &amp; torch.stack()"></a>toch.cat() &amp; torch.stack()</h4><p>cat是对数据沿着某一维度进行拼接。cat后数据的总维数不变。二维的矩阵拼接后依旧是二维的矩阵。如下面代码对两个2维tensor（分别为2<em>3,4</em>3）进行拼接，拼接完后变为3*3还是2维的tensor。<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=torch.rand(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">y=torch.rand(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">z=torch.cat((x,y),<span class="number">0</span>)  #torch.Size([<span class="number">6</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure></p><p><strong>注意</strong>：对维度进行拼接是必须满足维度一致的要求，除了指定拼接的维度除外，其他维度必须都相等。否则就会出现维度不匹配问题。<br>stack是增加新的维度进行堆叠。这个比较常见，比如在pytorch中对数据进行加载组成一个batch时常用到。比较好理解。<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a=torch.rand(<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">b=torch.rand(<span class="number">3</span>,<span class="number">224</span>,<span class="number">224</span>)</span><br><span class="line">c=torch.stack((a,b),<span class="number">0</span>)  # torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>])</span><br><span class="line">d=torch.stack((a,b),<span class="number">3</span>)  # torch.Size([<span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure></p><p>注意cat()和stack()两者区别就在于后者会增加维度。</p><h4 id="torch-squeeze-amp-torch-unsqueeze"><a href="#torch-squeeze-amp-torch-unsqueeze" class="headerlink" title="torch.squeeze() &amp; torch.unsqueeze()"></a>torch.squeeze() &amp; torch.unsqueeze()</h4><p>squeeze(dim_n)压缩，即去掉元素数量为1的dim_n维度。同理unsqueeze(dim_n)，增加dim_n维度，元素数量为1。<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#减少维度</span><br><span class="line">a=torch.rand(<span class="number">2</span>,<span class="number">1</span>,<span class="number">4</span>)  #torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">a_=a.squeeze()       #torch.Size([<span class="number">2</span>, <span class="number">4</span>])，不加参数，去掉所有为元素个数为<span class="number">1</span>的维度</span><br><span class="line">a_=a.squeeze(<span class="number">0</span>)      #torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>])加上参数，去掉第一维的元素为<span class="number">1</span>，不起作用，因为第一维有<span class="number">2</span>个元素</span><br><span class="line">a_=a.squeeze(<span class="number">1</span>)      #torch.Size([<span class="number">2</span>, <span class="number">4</span>])，这样就可以</span><br><span class="line"></span><br><span class="line">#增加维度</span><br><span class="line">a_=a.unsqueeze(<span class="number">0</span>)    #torch.Size([<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure></p><h4 id="Tensor-expand-as"><a href="#Tensor-expand-as" class="headerlink" title="Tensor.expand_as()"></a>Tensor.expand_as()</h4><p>expand_as()这是tensor变量的一个内置方法，如果使用b.expand_as(a)就是将b进行扩充，扩充到a的维度，需要说明的是a的低维度需要比b大，例如b的shape是3<em>1，如果a的shape是3</em>2不会出错，但是是2*2就会报错了<br><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a=torch.Tensor(<span class="string">[[1],[2],[3]]</span>)</span><br><span class="line">b=torch.Tensor(<span class="string">[[4]]</span>)</span><br><span class="line">c=b.expand_as(a)</span><br><span class="line">#tensor(<span class="string">[[4.],</span></span><br><span class="line"><span class="string">        [4.],</span></span><br><span class="line"><span class="string">        [4.]]</span>)</span><br></pre></td></tr></table></figure></p><h4 id="torch-contiguous"><a href="#torch-contiguous" class="headerlink" title="torch.contiguous()"></a>torch.contiguous()</h4><p>contiguous：view只能用在contiguous的variable上。如果在view之前用了transpose, permute等，需要用contiguous()来返回一个contiguous copy。 因为view需要tensor的内存是整块的。有些tensor并不是占用一整块内存，而是由不同的数据块组成，而tensor的view()操作依赖于内存是整块的，这时只需要执行contiguous()这个函数，把tensor变成在内存中连续分布的形式。判断是否contiguous用torch.Tensor.is_contiguous()函数<br><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=torch.ones(<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line">x.is_contiguous()  #<span class="literal">True</span></span><br><span class="line">x.transpose(<span class="number">0</span>,<span class="number">1</span>).is_contiguous()  #<span class="literal">False</span></span><br><span class="line">x.transpose(<span class="number">0</span>, <span class="number">1</span>).contiguous().is_contiguous()  #<span class="literal">True</span></span><br></pre></td></tr></table></figure></p><p><strong>因此，在调用view之前最好先contiguous一下，x.contiguous().view()</strong> 。</p><h3 id="数据读取"><a href="#数据读取" class="headerlink" title="数据读取"></a>数据读取</h3><p>在pytorch中一个现有的数据读取方法就是torchvision.datasets.ImageFolder()，这个api主要用于做分类问题。将每一类数据放到同一个文件夹中，比如有10个类别，那么就在一个大的文件夹下面建立10个子文件夹，每个子文件夹里面放的是同一类的数据。从api的实现可以发现ImageFolder该类继承自torch.utils.data.Dataset。</p><h4 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h4><p>Dataset的定义如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Dataset</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""An abstract class representing a Dataset.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    All other datasets should subclass it. All subclasses should override</span></span><br><span class="line"><span class="string">    ``__len__``, that provides the size of the dataset, and ``__getitem__``,</span></span><br><span class="line"><span class="string">    supporting integer indexing in range from 0 to len(self) exclusive.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__add__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> ConcatDataset([self, other])</span><br></pre></td></tr></table></figure></p><p>从上面的注释知道，这个是代表数据集的一个抽象类。有关于数据集的类都可以定义为其子类，只需要重写<strong>getitem</strong>和<strong>len</strong>就可以了。<br>那么定义好了数据集我们不可能将所有的数据集都放到内存，这样内存肯定就爆了，我们需要定义一个迭代器，每一步产生一个batch，这里PyTorch已经为我们实现好了，就是下面的torch.utils.data.DataLoader。</p><h4 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h4><p>DataLoader能够为我们自动生成一个多线程的迭代器。DataLoader的函数定义如下：DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0, collate_fn=default_collate, pin_memory=False, drop_last=False)</p><ul><li>dataset：加载的数据集(Dataset对象)</li><li>batch_size：batch size</li><li>shuffle:：是否将数据打乱</li><li>sampler： 样本抽样，后续会详细介绍</li><li>num_workers：使用多进程加载的进程数，0代表不使用多进程</li><li>collate_fn： 如何将多个样本数据拼接成一个batch，一般使用默认的拼接方式即可</li><li>pin_memory：是否将数据保存在pin memory区，pin memory中的数据转到GPU会快一些</li><li>drop_last：dataset中的数据个数可能不是batch_size的整数倍，drop_last为True会将多出来不足一个batch的数据丢弃</li></ul><h4 id="collate-fn"><a href="#collate-fn" class="headerlink" title="collate_fn()"></a>collate_fn()</h4><p>到这里，我们可以发现pytorch数据载入很简单，基本上对于简单地任务都不会出啥问题，然而数据载入容易出错的地方往往在于collate_fn函数。尤其是对于batch大小不一致的情况，比如目标检测，需要检测出图片中所有的目标。如下：<br><img src="/img/yolo2_result.png" alt><br>问题就是，输入的是一张张图片，大小不同可以resize成一样大小组成一个batch，但是它的label是每个目标的boxes和categories。而且每张图片中的目标个数也不一样，怎么组成一个batch。这个时候我们就需要自定义实现一个collate_fn函数。这里可以使用任何名字，只要在DataLoader里面传入就可以了。<br><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">def collate_fn(self, batch):</span><br><span class="line">    <span class="string">""</span><span class="comment">"</span></span><br><span class="line">    Since each image may have <span class="keyword">a</span> different <span class="keyword">number</span> of objects, we need <span class="keyword">a</span> collate <span class="function"><span class="keyword">function</span> <span class="params">(to be passed to the DataLoader)</span>.</span></span><br><span class="line"></span><br><span class="line">    This describes how <span class="keyword">to</span> combine these tensors of different sizes. We use lists.</span><br><span class="line"></span><br><span class="line">    Note: this need not <span class="keyword">be</span> defined in this Class, can <span class="keyword">be</span> standalone.</span><br><span class="line"></span><br><span class="line">    :param batch: <span class="keyword">an</span> iterable of <span class="keyword">N</span> sets from __getitem__()</span><br><span class="line">    :<span class="keyword">return</span>: <span class="keyword">a</span> tensor of images, lists of varying-size tensors of bounding boxes, labels, <span class="built_in">and</span> difficulties</span><br><span class="line">    <span class="string">""</span><span class="comment">"</span></span><br><span class="line">    </span><br><span class="line">    images = <span class="keyword">list</span>()</span><br><span class="line">    boxes = <span class="keyword">list</span>()</span><br><span class="line">    labels = <span class="keyword">list</span>()</span><br><span class="line">    difficulties = <span class="keyword">list</span>()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> <span class="keyword">b</span> in batch:</span><br><span class="line">        images.<span class="keyword">append</span>(<span class="keyword">b</span>[<span class="number">0</span>])</span><br><span class="line">        boxes.<span class="keyword">append</span>(<span class="keyword">b</span>[<span class="number">1</span>])</span><br><span class="line">        labels.<span class="keyword">append</span>(<span class="keyword">b</span>[<span class="number">2</span>])</span><br><span class="line">        difficulties.<span class="keyword">append</span>(<span class="keyword">b</span>[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    images = torch.stack(images, dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> images, boxes, labels, difficulties  # tensor (<span class="keyword">N</span>, <span class="number">3</span>, <span class="number">300</span>, <span class="number">300</span>), <span class="number">3</span> lists of <span class="keyword">N</span> tensors each</span><br></pre></td></tr></table></figure></p><h4 id="pin-memory"><a href="#pin-memory" class="headerlink" title="pin_memory()"></a>pin_memory()</h4><p>上面提到了pin_memory参数，pin_memory就是锁页内存，创建DataLoader时，设置pin_memory=True，则意味着生成的Tensor数据最开始是属于内存中的锁页内存，这样将内存的Tensor转义到GPU的显存就会更快一些。</p><p>主机中的内存，有两种存在方式，一是锁页，二是不锁页，锁页内存存放的内容在任何情况下都不会与主机的虚拟内存进行交换（注：虚拟内存就是硬盘），而不锁页内存在主机内存不足时，数据会存放在虚拟内存中。</p><p>而显卡中的显存全部是锁页内存！</p><p>当计算机的内存充足的时候，可以设置pin_memory=True。当系统卡住，或者交换内存使用过多的时候，设置pin_memory=False。因为pin_memory与电脑硬件性能有关，pytorch开发者不能确保每一个炼丹玩家都有高端设备，因此pin_memory默认为False。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;好记性不如烂笔头，主要是纪录一些Pytorch中常用函数用法以及自定义数据读取collate_fn()介绍。&lt;/p&gt;
&lt;h3 id=&quot;常用数学类操作符&quot;&gt;&lt;a href=&quot;#常用数学类操作符&quot; class=&quot;headerlink&quot; title=&quot;常用数学类操作符&quot;&gt;&lt;/a&gt;常用数学类操作符&lt;/h3&gt;&lt;h4 id=&quot;torchTensor-transpose-amp-Tensor-permute&quot;&gt;&lt;a href=&quot;#torchTensor-transpose-amp-Tensor-permute&quot; class=&quot;headerlink&quot; title=&quot;torchTensor.transpose() &amp;amp; Tensor.permute()&quot;&gt;&lt;/a&gt;torchTensor.transpose() &amp;amp; Tensor.permute()&lt;/h4&gt;&lt;p&gt;transpose只能操作矩阵的两个维度。只接受两个维度参数。若要对多个维度进行操作，使用permute更加灵活方便。&lt;br&gt;&lt;figure class=&quot;highlight lsl&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;a=torch.rand(&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;)  #torch.Size([&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;b=a.transpose(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;)   #torch.Size([&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;c=a.transpose(&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;).transpose(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;)  #torch.Size([&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;d=a.permute(&lt;span class=&quot;number&quot;&gt;1&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;,&lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;)                 #torch.Size([&lt;span class=&quot;number&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;number&quot;&gt;2&lt;/span&gt;])&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;同样是对三个维度进行变换，transpose需要操作三次，而是用permute只需要操作一次，因此对于高维度的矩阵变化，permute更加方便。&lt;br&gt;
    
    </summary>
    
      <category term="project" scheme="https://nicehuster.github.io/categories/project/"/>
    
    
      <category term="PyTorch" scheme="https://nicehuster.github.io/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>SSD中的数据增强细节</title>
    <link href="https://nicehuster.github.io/2019/05/11/0054-ssd-dataaug/"/>
    <id>https://nicehuster.github.io/2019/05/11/0054-ssd-dataaug/</id>
    <published>2019-05-11T11:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>在SSD中数据增强起着至关重要的作用，从原文的实验结果可以看出，数据增强可以为提高8.8%mAP。如下表所示：<br><img src="/img/img_5b340d37f2563.png" alt><br>数据增强对于提高小目标的检测精度尤为重要，因为它在分类器可以看到更多对象结构的图像中创建放大图像。数据增强还可用于处理包含被遮挡对象的图像，通过将裁剪的图像包括在训练数据中，在训练数据中只能看到对象的一部分。在SSD中数据增强包括如下步骤：<br><a id="more"></a></p><ul><li>光度畸变（Photometric Distortions）<ul><li>随机亮度（Random Brightness）</li><li>随机对比度，色调和饱和度（Random Contrast, Hue, Saturation）</li><li>随机光噪声（RandomLightingNoise）</li></ul></li><li>几何畸变（Geometric Distortions）<ul><li>随机扩展（RandomExpand）</li><li>随机裁剪（RandomCrop）</li><li>随机翻转（RandomMirror）</li></ul></li></ul><p><img src="/img/img_5b345c68e6487-768x489.png" alt></p><h3 id="Photometric-Distortions"><a href="#Photometric-Distortions" class="headerlink" title="Photometric Distortions"></a><strong>Photometric Distortions</strong></h3><h4 id="Random-Brightness"><a href="#Random-Brightness" class="headerlink" title="Random Brightness"></a><strong>Random Brightness</strong></h4><p>这个是从$[-\delta,\delta]$中随机选择一个值添加到图像的所有像素中，默认值设置为32。<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def __call__(self, <span class="built_in">image</span>, boxes=None, <span class="built_in">labels</span>=None):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">random</span>.randint(<span class="number">2</span>):</span><br><span class="line">        <span class="built_in">delta</span> = <span class="built_in">random</span>.uniform(-self.<span class="built_in">delta</span>, self.<span class="built_in">delta</span>)</span><br><span class="line">        <span class="built_in">image</span> += <span class="built_in">delta</span></span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">image</span>, boxes, <span class="built_in">labels</span></span><br></pre></td></tr></table></figure></p><p><img src="/img/img_5b345fd789997.png" alt></p><h4 id="Random-Contrast-Hue-Saturation"><a href="#Random-Contrast-Hue-Saturation" class="headerlink" title="Random Contrast, Hue, Saturation"></a><strong>Random Contrast, Hue, Saturation</strong></h4><p>对于Contrast, Hue, Saturation的应用，有两种选择：Contrast + Hue + Saturation和Hue + Saturation + Contrast。每种选择概率为0.5.<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">self<span class="selector-class">.pd</span> = [</span><br><span class="line">            RandomContrast(),</span><br><span class="line">            ConvertColor(<span class="attribute">transform</span>=<span class="string">'HSV'</span>),</span><br><span class="line">            RandomSaturation(),</span><br><span class="line">            RandomHue(),</span><br><span class="line">            ConvertColor(current=<span class="string">'HSV'</span>, <span class="attribute">transform</span>=<span class="string">'BGR'</span>),</span><br><span class="line">            RandomContrast()</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line"> im, boxes, labels = self.rand_brightness(im, boxes, labels)</span><br><span class="line">        <span class="keyword">if</span> random.randint(<span class="number">2</span>):</span><br><span class="line">            distort = Compose(self<span class="selector-class">.pd</span>[:-<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            distort = Compose(self<span class="selector-class">.pd</span>[<span class="number">1</span>:])</span><br><span class="line">        im, boxes, labels = distort(im, boxes, labels)</span><br></pre></td></tr></table></figure></p><p>需要注意的是，Contrast应用于RGB空间，而Hue, Saturation则是应用HSV空间。因此，在应用每个操作之前，必须执行适当的颜色空间转换。应用随机contrast之后，随机hue and 随机saturation操作与随机Brightness类似。随机saturation如下：<br><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def __call__(self, <span class="built_in">image</span>, boxes=None, <span class="built_in">labels</span>=None):</span><br><span class="line">       <span class="keyword">if</span> <span class="built_in">random</span>.randint(<span class="number">2</span>):</span><br><span class="line">           <span class="built_in">image</span>[:, :, <span class="number">1</span>] *= <span class="built_in">random</span>.uniform(self.lower, self.upper)</span><br><span class="line"></span><br><span class="line">       <span class="built_in">return</span> <span class="built_in">image</span>, boxes, <span class="built_in">labels</span></span><br></pre></td></tr></table></figure></p><p><img src="/img/img_5b3463b4d93e0.png" alt></p><h4 id="RandomLightingNoise"><a href="#RandomLightingNoise" class="headerlink" title="RandomLightingNoise"></a><strong>RandomLightingNoise</strong></h4><p>最后一个 光度畸变是随机光噪声，这种操作主要是颜色通道的随机交换。三个通道有六种jiaohua<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">self.perms = ((<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>), (<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>),</span><br><span class="line">                      (<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), (<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>),</span><br><span class="line">                      (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>), (<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>))</span><br></pre></td></tr></table></figure></p><p>随机交换后如下：<br><img src="/img/img_5b34650f2861b.png" alt></p><h3 id="Geometric-Distortions"><a href="#Geometric-Distortions" class="headerlink" title="Geometric Distortions"></a><strong>Geometric Distortions</strong></h3><h4 id="RandomExpand"><a href="#RandomExpand" class="headerlink" title="RandomExpand"></a><strong>RandomExpand</strong></h4><p>随机扩展的发生概率为0.5，随机扩展的步骤如下<br><img src="/img/img_5b35382c10419.png" alt></p><h4 id="RandomCrop"><a href="#RandomCrop" class="headerlink" title="RandomCrop"></a><strong>RandomCrop</strong></h4><p>这一步主要是在上一步随机扩展的基础上，对目标进行随机裁剪，有利于缓解目标遮挡问题。随机生成裁剪框，如果框与目标的iou不满足要求则删除，如果iou满足要求，而目标中心点不包含在随机裁剪框也删除。<br><img src="/img/img_5b3544b2a975a.png" alt><br>部分例子如下：<br><img src="/img/img_5b35476dbeb90.png" alt><br>其中，红色表示ground truth，绿色表示随机裁剪的框。</p><h4 id="RandomMirror"><a href="#RandomMirror" class="headerlink" title="RandomMirror"></a><strong>RandomMirror</strong></h4><p>随机裁剪主要是镜像翻转。很简单。</p><p>最后，对经过数据增强的图像resize到（300,300），然后减均值。<br>代码参考：  <a href="https://github.com/amdegroot/ssd.pytorch/" target="_blank" rel="noopener">https://github.com/amdegroot/ssd.pytorch/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在SSD中数据增强起着至关重要的作用，从原文的实验结果可以看出，数据增强可以为提高8.8%mAP。如下表所示：&lt;br&gt;&lt;img src=&quot;/img/img_5b340d37f2563.png&quot; alt&gt;&lt;br&gt;数据增强对于提高小目标的检测精度尤为重要，因为它在分类器可以看到更多对象结构的图像中创建放大图像。数据增强还可用于处理包含被遮挡对象的图像，通过将裁剪的图像包括在训练数据中，在训练数据中只能看到对象的一部分。在SSD中数据增强包括如下步骤：&lt;br&gt;
    
    </summary>
    
      <category term="project" scheme="https://nicehuster.github.io/categories/project/"/>
    
    
      <category term="object detection" scheme="https://nicehuster.github.io/tags/object-detection/"/>
    
  </entry>
  
  <entry>
    <title>SSD详解以及代码实现</title>
    <link href="https://nicehuster.github.io/2019/05/09/0053-ssd-explained%20with%20code/"/>
    <id>https://nicehuster.github.io/2019/05/09/0053-ssd-explained with code/</id>
    <published>2019-05-09T11:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>&#160; &#160; &#160; &#160;SSD作为one-stage目标检测算法中比较经典且具有代表性的一种方法。其精度可以与Faster R-CNN相匹敌，而速度达到了惊人的59FPS，速度上完爆 Faster R-CNN。相比于Faster R-CNN,其速度快的根本原因在于移除了region proposals的步骤以及后续的像素采样或特征采样步骤。论文连接：<a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noopener">SSD: Single Shot MultiBox Detector</a>，作者开源的代码连接：<a href="https://github.com/weiliu89/caffe/tree/ssd" target="_blank" rel="noopener">code</a>。由于作者开源代码使用caffe实现，这里以pytorch源码的方式实现。作者在论文中实现了两种不同输入大小的SSD模型：SSD300（输入图像大小统一为300x300）以及SSD512（输入图像大小统一为512x512）.这里主要针对SSD300,下面主要分四个部分介绍SSD300以及代码具体实现。<br><img src="/img/ssd.png" alt><br><a id="more"></a></p><h3 id="Base-Convolutions"><a href="#Base-Convolutions" class="headerlink" title="Base Convolutions"></a>Base Convolutions</h3><p>如上图所示，在SSD算法中基础网络采用的是VGG-16结构作为其基础网络。与VGG-16不同的是，把fc6和fc7替换成了卷积层。替换后的backbone结构如下图所示<br><img src="/img/modifiedvgg.png" alt><br>对应代码实现如下：<br><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VGGBase</span>(<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    VGG base convolutions to produce lower-level feature maps.</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span></span>(<span class="keyword">self</span>):</span><br><span class="line">        <span class="keyword">super</span>(VGGBase, <span class="keyword">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Standard convolutional layers in VGG16</span></span><br><span class="line">        <span class="keyword">self</span>.conv1_1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)  <span class="comment"># stride = 1, by default</span></span><br><span class="line">        <span class="keyword">self</span>.conv1_2 = nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.pool1 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.conv2_1 = nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv2_2 = nn.Conv2d(<span class="number">128</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.pool2 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.conv3_1 = nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv3_2 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv3_3 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.pool3 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, ceil_mode=True)  <span class="comment"># ceiling (not floor) here for even dims</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.conv4_1 = nn.Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv4_2 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv4_3 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.pool4 = nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.conv5_1 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv5_2 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv5_3 = nn.Conv2d(<span class="number">512</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.pool5 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)  <span class="comment"># retains size because stride is 1 (and padding)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Replacements for FC6 and FC7 in VGG16</span></span><br><span class="line">        <span class="keyword">self</span>.conv6 = nn.Conv2d(<span class="number">512</span>, <span class="number">1024</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">6</span>, dilation=<span class="number">6</span>)  <span class="comment"># atrous convolution</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.conv7 = nn.Conv2d(<span class="number">1024</span>, <span class="number">1024</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Load pretrained layers</span></span><br><span class="line">        <span class="keyword">self</span>.load_pretrained_layers()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span></span>(<span class="keyword">self</span>, image):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        Forward propagation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param image: images, a tensor of dimensions (N, 3, 300, 300)</span></span><br><span class="line"><span class="string">        :return: lower-level feature maps conv4_3 and conv7</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv1_1(image))  <span class="comment"># (N, 64, 300, 300)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv1_2(<span class="keyword">out</span>))  <span class="comment"># (N, 64, 300, 300)</span></span><br><span class="line">        <span class="keyword">out</span> = <span class="keyword">self</span>.pool1(<span class="keyword">out</span>)  <span class="comment"># (N, 64, 150, 150)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv2_1(<span class="keyword">out</span>))  <span class="comment"># (N, 128, 150, 150)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv2_2(<span class="keyword">out</span>))  <span class="comment"># (N, 128, 150, 150)</span></span><br><span class="line">        <span class="keyword">out</span> = <span class="keyword">self</span>.pool2(<span class="keyword">out</span>)  <span class="comment"># (N, 128, 75, 75)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv3_1(<span class="keyword">out</span>))  <span class="comment"># (N, 256, 75, 75)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv3_2(<span class="keyword">out</span>))  <span class="comment"># (N, 256, 75, 75)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv3_3(<span class="keyword">out</span>))  <span class="comment"># (N, 256, 75, 75)</span></span><br><span class="line">        <span class="keyword">out</span> = <span class="keyword">self</span>.pool3(<span class="keyword">out</span>)  <span class="comment"># (N, 256, 38, 38), it would have been 37 if not for ceil_mode = True</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv4_1(<span class="keyword">out</span>))  <span class="comment"># (N, 512, 38, 38)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv4_2(<span class="keyword">out</span>))  <span class="comment"># (N, 512, 38, 38)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv4_3(<span class="keyword">out</span>))  <span class="comment"># (N, 512, 38, 38)</span></span><br><span class="line">        conv4_3_feats = <span class="keyword">out</span>  <span class="comment"># (N, 512, 38, 38)</span></span><br><span class="line">        <span class="keyword">out</span> = <span class="keyword">self</span>.pool4(<span class="keyword">out</span>)  <span class="comment"># (N, 512, 19, 19)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv5_1(<span class="keyword">out</span>))  <span class="comment"># (N, 512, 19, 19)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv5_2(<span class="keyword">out</span>))  <span class="comment"># (N, 512, 19, 19)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv5_3(<span class="keyword">out</span>))  <span class="comment"># (N, 512, 19, 19)</span></span><br><span class="line">        <span class="keyword">out</span> = <span class="keyword">self</span>.pool5(<span class="keyword">out</span>)  <span class="comment"># (N, 512, 19, 19), pool5 does not reduce dimensions</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv6(<span class="keyword">out</span>))  <span class="comment"># (N, 1024, 19, 19)</span></span><br><span class="line"></span><br><span class="line">        conv7_feats = F.relu(<span class="keyword">self</span>.conv7(<span class="keyword">out</span>))  <span class="comment"># (N, 1024, 19, 19)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Lower-level feature maps</span></span><br><span class="line">        <span class="keyword">return</span> conv4_3_feats, conv7_feats</span><br></pre></td></tr></table></figure></p><p>值得注意的是：</p><ul><li>pool3采用的是向上取整操作，ceil(75/2)=38；</li><li>将原始pool5（2x2,stride 2）修改为pool5（3x3,stride 1），这样改的结果是，pool5操作不改变特征图大小；</li><li>将VGG16中的fc6,fc7替换成卷积层conv6,和conv7，此外，conv6使用了atrous卷积；</li><li>去掉了所有的dropout层和fc8层；</li><li>VGGBase输出的是conv4_3和conv_7的feature map；</li></ul><p>Atrous/dilated 卷积使用说明：<br>通常卷积过程中为了使特征图尺寸特征图尺寸保持不变，通过会在边缘打padding，但人为加入的padding值会引入噪声，因此，使用atrous卷积能够在保持感受野不变的条件下，减少padding噪声。</p><h3 id="Auxiliary-Convolutions"><a href="#Auxiliary-Convolutions" class="headerlink" title="Auxiliary Convolutions"></a>Auxiliary Convolutions</h3><p>为了能够充分利用不同大小的feature map用于目标检测，作者在Base Convolutions的基础上，在conv_7后面又堆叠了8层卷积用于生成不同大小的特征图。如下图所示<br><img src="/img/auxconv.jpg" alt><br>对应代码实现：<br><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AuxiliaryConvolutions</span>(<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    Additional convolutions to produce higher-level feature maps.</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span></span>(<span class="keyword">self</span>):</span><br><span class="line">        <span class="keyword">super</span>(AuxiliaryConvolutions, <span class="keyword">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Auxiliary/additional convolutions on top of the VGG base</span></span><br><span class="line">        <span class="keyword">self</span>.conv8_1 = nn.Conv2d(<span class="number">1024</span>, <span class="number">256</span>, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>)  <span class="comment"># stride = 1, by default</span></span><br><span class="line">        <span class="keyword">self</span>.conv8_2 = nn.Conv2d(<span class="number">256</span>, <span class="number">512</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)  <span class="comment"># dim. reduction because stride &gt; 1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.conv9_1 = nn.Conv2d(<span class="number">512</span>, <span class="number">128</span>, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv9_2 = nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)  <span class="comment"># dim. reduction because stride &gt; 1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.conv10_1 = nn.Conv2d(<span class="number">256</span>, <span class="number">128</span>, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv10_2 = nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">0</span>)  <span class="comment"># dim. reduction because padding = 0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.conv11_1 = nn.Conv2d(<span class="number">256</span>, <span class="number">128</span>, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">self</span>.conv11_2 = nn.Conv2d(<span class="number">128</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">0</span>)  <span class="comment"># dim. reduction because padding = 0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Initialize convolutions' parameters</span></span><br><span class="line">        <span class="keyword">self</span>.init_conv2d()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_conv2d</span></span>(<span class="keyword">self</span>):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        Initialize convolution parameters.</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        <span class="keyword">for</span> c in <span class="keyword">self</span>.children():</span><br><span class="line">            <span class="keyword">if</span> isinstance(c, nn.Conv2d):</span><br><span class="line">                nn.init.xavier_uniform_(c.weight)</span><br><span class="line">                nn.init.constant_(c.bias, <span class="number">0</span>.)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span></span>(<span class="keyword">self</span>, conv7_feats):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        Forward propagation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param conv7_feats: lower-level conv7 feature map, a tensor of dimensions (N, 1024, 19, 19)</span></span><br><span class="line"><span class="string">        :return: higher-level feature maps conv8_2, conv9_2, conv10_2, and conv11_2</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv8_1(conv7_feats))  <span class="comment"># (N, 256, 19, 19)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv8_2(<span class="keyword">out</span>))  <span class="comment"># (N, 512, 10, 10)</span></span><br><span class="line">        conv8_2_feats = <span class="keyword">out</span>  <span class="comment"># (N, 512, 10, 10)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv9_1(<span class="keyword">out</span>))  <span class="comment"># (N, 128, 10, 10)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv9_2(<span class="keyword">out</span>))  <span class="comment"># (N, 256, 5, 5)</span></span><br><span class="line">        conv9_2_feats = <span class="keyword">out</span>  <span class="comment"># (N, 256, 5, 5)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv10_1(<span class="keyword">out</span>))  <span class="comment"># (N, 128, 5, 5)</span></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv10_2(<span class="keyword">out</span>))  <span class="comment"># (N, 256, 3, 3)</span></span><br><span class="line">        conv10_2_feats = <span class="keyword">out</span>  <span class="comment"># (N, 256, 3, 3)</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">out</span> = F.relu(<span class="keyword">self</span>.conv11_1(<span class="keyword">out</span>))  <span class="comment"># (N, 128, 3, 3)</span></span><br><span class="line">        conv11_2_feats = F.relu(<span class="keyword">self</span>.conv11_2(<span class="keyword">out</span>))  <span class="comment"># (N, 256, 1, 1)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Higher-level feature maps</span></span><br><span class="line">        <span class="keyword">return</span> conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats</span><br></pre></td></tr></table></figure></p><p>如SSD论文在section 3.1提到的一样，新添加的卷积层的初始化采用的xavier初始化方法。AuxiliaryConvolutions输出的是不同大小的feature map：conv8_2, conv9_2, conv10_2, and conv11_2。对于SSD512，作者在conv11_2后面来额外添加了两层conv12。<br>SSD算法中使用到了conv4_3,conv_7，conv8_2,conv7_2,conv8_2,conv9_2,conv10_2,conv11_2这些大小不同的feature maps，其<strong>目的是为了能够准确的检测到不同尺度的物体</strong>，因为在低层的feature map,感受野比较小，高层的感受野比较大，在不同的feature map进行卷积，可以达到多尺度的目的。</p><h3 id="Priors"><a href="#Priors" class="headerlink" title="Priors"></a>Priors</h3><p>在继续进行预测卷积之前，我们必须首先了解我们预测的是什么。当然，这是物体和它们的位置，但是以什么形式？在这里，我们必须了解priors以及它们在SSD中所起的关键作用。priors和anchor的概念类似，也是预先在feature map上定义一些不同大小形状的box，然后再这个基础上进行回归预测分类目标。如下图所示<br><img src="/img/defaultbox.png" alt><br>如上图所示，在特征图的每个位置预测K个box，对于每一个box，预测C个类别得分，以及相对于prior box的4个偏移量值，这样总共需要（C+4）<em> K个预测器，则在m</em>n的特征图上面将会产生（C+4）<em> K </em> m * n个预测值。在SSD300中，分别在6个不同尺度的feature map上生成prior box。对应prior box如下：          </p><div class="table-container"><table><thead><tr><th style="text-align:center">Feature Map From</th><th style="text-align:center">Feature Map Dimensions</th><th style="text-align:center">Prior Scale</th><th style="text-align:center">Aspect Ratios</th><th style="text-align:center">Number of Priors per Position</th><th style="text-align:center">Total Number of Priors on this Feature Map</th></tr></thead><tbody><tr><td style="text-align:center"><code>conv4_3</code></td><td style="text-align:center">38, 38</td><td style="text-align:center">0.1</td><td style="text-align:center">1:1, 2:1, 1:2 + an extra prior</td><td style="text-align:center">4</td><td style="text-align:center">38x38x4=5776</td></tr><tr><td style="text-align:center"><code>conv7</code></td><td style="text-align:center">19, 19</td><td style="text-align:center">0.2</td><td style="text-align:center">1:1, 2:1, 1:2, 3:1, 1:3 + an extra prior</td><td style="text-align:center">6</td><td style="text-align:center">19x19x6=2166</td></tr><tr><td style="text-align:center"><code>conv8_2</code></td><td style="text-align:center">10, 10</td><td style="text-align:center">0.375</td><td style="text-align:center">1:1, 2:1, 1:2, 3:1, 1:3 + an extra prior</td><td style="text-align:center">6</td><td style="text-align:center">10x10x6=600</td></tr><tr><td style="text-align:center"><code>conv9_2</code></td><td style="text-align:center">5, 5</td><td style="text-align:center">0.55</td><td style="text-align:center">1:1, 2:1, 1:2, 3:1, 1:3 + an extra prior</td><td style="text-align:center">6</td><td style="text-align:center">5x5x6=150</td></tr><tr><td style="text-align:center"><code>conv10_2</code></td><td style="text-align:center">3,  3</td><td style="text-align:center">0.725</td><td style="text-align:center">1:1, 2:1, 1:2 + an extra prior</td><td style="text-align:center">4</td><td style="text-align:center">3x3x4=36</td></tr><tr><td style="text-align:center"><code>conv11_2</code></td><td style="text-align:center">1, 1</td><td style="text-align:center">0.9</td><td style="text-align:center">1:1, 2:1, 1:2 + an extra prior</td><td style="text-align:center">4</td><td style="text-align:center">1x1x4=4</td></tr><tr><td style="text-align:center"><strong>Grand Total</strong></td><td style="text-align:center">–</td><td style="text-align:center">–</td><td style="text-align:center">–</td><td style="text-align:center">–</td><td style="text-align:center"><strong>8732 priors</strong></td></tr></tbody></table></div><p>Prior Scale计算公式如下：<br><img src="/img/ssd_scale.png" alt><br>其中，$s_{min}=0.2,s_{max}=0.9$，conv4_3设置为0.1。<br>值得注意的是，对于aspect ratio为1的时候，作者额外添加了一个extra prior，对应的additional_scale：</p><script type="math/tex; mode=display">\acute{s}_k=\sqrt{s_k\times s_{k+1}}</script><p>因此在SSD300中总共有9732个prior boxes ！<br>对于每一个priorbox，对应宽度w和高度h的计算如下：<br><img src="/img/wh1.jpg" alt><br>求解w和h可得：<br><img src="/img/wh2.jpg" alt><br>然后以每个feature map上的每个点的中心点为中心，就可以生成一系列不同scale和aspect ratio的prior box。对应代码实现如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_prior_boxes</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Create the 8732 prior (default) boxes for the SSD300, as defined in the paper.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :return: prior boxes in center-size coordinates, a tensor of dimensions (8732, 4)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    fmap_dims = &#123;<span class="string">'conv4_3'</span>: <span class="number">38</span>,</span><br><span class="line">                 <span class="string">'conv7'</span>: <span class="number">19</span>,</span><br><span class="line">                 <span class="string">'conv8_2'</span>: <span class="number">10</span>,</span><br><span class="line">                 <span class="string">'conv9_2'</span>: <span class="number">5</span>,</span><br><span class="line">                 <span class="string">'conv10_2'</span>: <span class="number">3</span>,</span><br><span class="line">                 <span class="string">'conv11_2'</span>: <span class="number">1</span>&#125;</span><br><span class="line"></span><br><span class="line">    obj_scales = &#123;<span class="string">'conv4_3'</span>: <span class="number">0.1</span>,</span><br><span class="line">                      <span class="string">'conv7'</span>: <span class="number">0.2</span>,</span><br><span class="line">                      <span class="string">'conv8_2'</span>: <span class="number">0.375</span>,</span><br><span class="line">                      <span class="string">'conv9_2'</span>: <span class="number">0.55</span>,</span><br><span class="line">                      <span class="string">'conv10_2'</span>: <span class="number">0.725</span>,</span><br><span class="line">                      <span class="string">'conv11_2'</span>: <span class="number">0.9</span>&#125;</span><br><span class="line"></span><br><span class="line">    aspect_ratios = &#123;<span class="string">'conv4_3'</span>: [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">0.5</span>],</span><br><span class="line">                         <span class="string">'conv7'</span>: [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">0.5</span>, <span class="number">.333</span>],</span><br><span class="line">                         <span class="string">'conv8_2'</span>: [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">0.5</span>, <span class="number">.333</span>],</span><br><span class="line">                         <span class="string">'conv9_2'</span>: [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">0.5</span>, <span class="number">.333</span>],</span><br><span class="line">                         <span class="string">'conv10_2'</span>: [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">0.5</span>],</span><br><span class="line">                         <span class="string">'conv11_2'</span>: [<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">0.5</span>]&#125;</span><br><span class="line"></span><br><span class="line">    fmaps = list(fmap_dims.keys())</span><br><span class="line"></span><br><span class="line">    prior_boxes = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> k, fmap <span class="keyword">in</span> enumerate(fmaps):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(fmap_dims[fmap]):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(fmap_dims[fmap]):</span><br><span class="line">                cx = (j + <span class="number">0.5</span>) / fmap_dims[fmap]</span><br><span class="line">                cy = (i + <span class="number">0.5</span>) / fmap_dims[fmap]</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> ratio <span class="keyword">in</span> aspect_ratios[fmap]:</span><br><span class="line">                    prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] / sqrt(ratio)])</span><br><span class="line"></span><br><span class="line">                        <span class="comment"># For an aspect ratio of 1, use an additional prior whose scale is the geometric mean of the</span></span><br><span class="line">                        <span class="comment"># scale of the current feature map and the scale of the next feature map</span></span><br><span class="line">                    <span class="keyword">if</span> ratio == <span class="number">1.</span>:</span><br><span class="line">                        <span class="keyword">try</span>:</span><br><span class="line">                            additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k + <span class="number">1</span>]])</span><br><span class="line">                            <span class="comment"># For the last feature map, there is no "next" feature map</span></span><br><span class="line">                        <span class="keyword">except</span> IndexError:</span><br><span class="line">                            additional_scale = <span class="number">1.</span></span><br><span class="line">                        prior_boxes.append([cx, cy, additional_scale, additional_scale])</span><br><span class="line"></span><br><span class="line">    prior_boxes = torch.FloatTensor(prior_boxes).to(device)  <span class="comment"># (8732, 4)</span></span><br><span class="line">    prior_boxes.clamp_(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># (8732, 4)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> prior_boxes</span><br></pre></td></tr></table></figure></p><h3 id="Prediction-convolutions"><a href="#Prediction-convolutions" class="headerlink" title="Prediction convolutions"></a>Prediction convolutions</h3><p>前面提及我们使用回归预测来找到目标的bounding box，当然，priors并不能表示我们最终的预测boxes。不过可以priors作为回归的起始框，然后找出需要调整多少才能获得更精确的边界框预测。而我们的目标就是预测这个偏差offsets,如下图示：<br><img src="/img/ecs2.png" alt><br>通过预测这四个偏差(g_c_x, g_c_y, g_w, g_h)来回归出bounding box的坐标位置。除了预测offsets之外，我们还需要输出对应目标是哪一类的分数scores。以conv9_2为例，对应输出为：<br><img src="/img/predconv2.jpg" alt><br>对应6个不同尺度feature map上的输出同样也输出对应的offset和scores，具体代码实现如下：<br><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">PredictionConvolutions</span>(<span class="title">nn</span>.<span class="type">Module</span>):</span></span><br><span class="line"><span class="class">    """</span></span><br><span class="line"><span class="class">    <span class="type">Convolutions</span> to predict <span class="keyword">class</span> scores and bounding boxes using lower and higher-level feature maps.</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    <span class="type">The</span> bounding boxes (<span class="title">locations</span>) are predicted as encoded offsets w.r.t each of the 8732 prior (<span class="title">default</span>) boxes.</span></span><br><span class="line"><span class="class">    <span class="type">See</span> 'cxcy_to_gcxgcy' in utils.py for the encoding definition.</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    <span class="type">The</span> <span class="keyword">class</span> scores represent the scores of each object <span class="keyword">class</span> in each of the 8732 bounding boxes located.</span></span><br><span class="line"><span class="class">    <span class="type">A</span> high score for 'background' = no object.</span></span><br><span class="line"><span class="class">    """</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>, <span class="title">n_classes</span>):</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        :param n_classes: number of different types of objects</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        super(<span class="type">PredictionConvolutions</span>, <span class="title">self</span>).__init__()</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        self.n_classes = n_classes</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        # <span class="type">Number</span> of prior-boxes we are considering per position in each feature map</span></span><br><span class="line"><span class="class">        n_boxes = &#123;'conv4_3': 4,</span></span><br><span class="line"><span class="class">                   'conv7': 6,</span></span><br><span class="line"><span class="class">                   'conv8_2': 6,</span></span><br><span class="line"><span class="class">                   'conv9_2': 6,</span></span><br><span class="line"><span class="class">                   'conv10_2': 4,</span></span><br><span class="line"><span class="class">                   'conv11_2': 4&#125;</span></span><br><span class="line"><span class="class">        # 4 prior-boxes implies we use 4 different aspect ratios, etc.</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        # <span class="type">Localization</span> prediction convolutions (<span class="title">predict</span> <span class="title">offsets</span> <span class="title">w</span>.<span class="title">r</span>.<span class="title">t</span> <span class="title">prior</span>-<span class="title">boxes</span>)</span></span><br><span class="line"><span class="class">        self.loc_conv4_3 = nn.<span class="type">Conv2d</span>(512, <span class="title">n_boxes</span>['<span class="title">conv4_3'</span>] * 4, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.loc_conv7 = nn.<span class="type">Conv2d</span>(1024, <span class="title">n_boxes</span>['<span class="title">conv7'</span>] * 4, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.loc_conv8_2 = nn.<span class="type">Conv2d</span>(512, <span class="title">n_boxes</span>['<span class="title">conv8_2'</span>] * 4, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.loc_conv9_2 = nn.<span class="type">Conv2d</span>(256, <span class="title">n_boxes</span>['<span class="title">conv9_2'</span>] * 4, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.loc_conv10_2 = nn.<span class="type">Conv2d</span>(256, <span class="title">n_boxes</span>['<span class="title">conv10_2'</span>] * 4, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.loc_conv11_2 = nn.<span class="type">Conv2d</span>(256, <span class="title">n_boxes</span>['<span class="title">conv11_2'</span>] * 4, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        # <span class="type">Class</span> prediction convolutions (<span class="title">predict</span> <span class="title">classes</span> <span class="title">in</span> <span class="title">localization</span> <span class="title">boxes</span>)</span></span><br><span class="line"><span class="class">        self.cl_conv4_3 = nn.<span class="type">Conv2d</span>(512, <span class="title">n_boxes</span>['<span class="title">conv4_3'</span>] * <span class="title">n_classes</span>, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.cl_conv7 = nn.<span class="type">Conv2d</span>(1024, <span class="title">n_boxes</span>['<span class="title">conv7'</span>] * <span class="title">n_classes</span>, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.cl_conv8_2 = nn.<span class="type">Conv2d</span>(512, <span class="title">n_boxes</span>['<span class="title">conv8_2'</span>] * <span class="title">n_classes</span>, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.cl_conv9_2 = nn.<span class="type">Conv2d</span>(256, <span class="title">n_boxes</span>['<span class="title">conv9_2'</span>] * <span class="title">n_classes</span>, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.cl_conv10_2 = nn.<span class="type">Conv2d</span>(256, <span class="title">n_boxes</span>['<span class="title">conv10_2'</span>] * <span class="title">n_classes</span>, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        self.cl_conv11_2 = nn.<span class="type">Conv2d</span>(256, <span class="title">n_boxes</span>['<span class="title">conv11_2'</span>] * <span class="title">n_classes</span>, <span class="title">kernel_size</span>=3, <span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        # <span class="type">Initialize</span> convolutions' parameters</span></span><br><span class="line"><span class="class">        self.init_conv2d()</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def init_conv2d(<span class="title">self</span>):</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        <span class="type">Initialize</span> convolution parameters.</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        for c in self.children():</span></span><br><span class="line"><span class="class">            if isinstance(<span class="title">c</span>, <span class="title">nn</span>.<span class="type">Conv2d</span>):</span></span><br><span class="line"><span class="class">                nn.init.xavier_uniform_(<span class="title">c</span>.<span class="title">weight</span>)</span></span><br><span class="line"><span class="class">                nn.init.constant_(<span class="title">c</span>.<span class="title">bias</span>, 0.)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>, <span class="title">conv4_3_feats</span>, <span class="title">conv7_feats</span>, <span class="title">conv8_2_feats</span>, <span class="title">conv9_2_feats</span>, <span class="title">conv10_2_feats</span>, <span class="title">conv11_2_feats</span>):</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        <span class="type">Forward</span> propagation.</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        :param conv4_3_feats: conv4_3 feature map, a tensor of dimensions (<span class="type">N</span>, 512, 38, 38)</span></span><br><span class="line"><span class="class">        :param conv7_feats: conv7 feature map, a tensor of dimensions (<span class="type">N</span>, 1024, 19, 19)</span></span><br><span class="line"><span class="class">        :param conv8_2_feats: conv8_2 feature map, a tensor of dimensions (<span class="type">N</span>, 512, 10, 10)</span></span><br><span class="line"><span class="class">        :param conv9_2_feats: conv9_2 feature map, a tensor of dimensions (<span class="type">N</span>, 256, 5, 5)</span></span><br><span class="line"><span class="class">        :param conv10_2_feats: conv10_2 feature map, a tensor of dimensions (<span class="type">N</span>, 256, 3, 3)</span></span><br><span class="line"><span class="class">        :param conv11_2_feats: conv11_2 feature map, a tensor of dimensions (<span class="type">N</span>, 256, 1, 1)</span></span><br><span class="line"><span class="class">        :return: 8732 locations and <span class="keyword">class</span> scores (<span class="title">i</span>.<span class="title">e</span>. <span class="title">w</span>.<span class="title">r</span>.<span class="title">t</span> <span class="title">each</span> <span class="title">prior</span> <span class="title">box</span>) for each image</span></span><br><span class="line"><span class="class">        """</span></span><br><span class="line"><span class="class">        batch_size = conv4_3_feats.size(0)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        # <span class="type">Predict</span> localization boxes' bounds (<span class="title">as</span> <span class="title">offsets</span> <span class="title">w</span>.<span class="title">r</span>.<span class="title">t</span> <span class="title">prior</span>-<span class="title">boxes</span>)</span></span><br><span class="line"><span class="class">        l_conv4_3 = self.loc_conv4_3(<span class="title">conv4_3_feats</span>)  # (<span class="type">N</span>, 16, 38, 38)</span></span><br><span class="line"><span class="class">        l_conv4_3 = l_conv4_3.permute(0, 2, 3,</span></span><br><span class="line"><span class="class">                                      1).contiguous()  # (<span class="type">N</span>, 38, 38, 16), to match prior-box order (<span class="title">after</span> .<span class="title">view</span>())</span></span><br><span class="line"><span class="class">        # (.<span class="title">contiguous</span>() ensures it is stored in a contiguous chunk of memory, needed for .view() below)</span></span><br><span class="line"><span class="class">        l_conv4_3 = l_conv4_3.view(<span class="title">batch_size</span>, -1, 4)  # (<span class="type">N</span>, 5776, 4), there are a total 5776 boxes on this feature map</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        l_conv7 = self.loc_conv7(<span class="title">conv7_feats</span>)  # (<span class="type">N</span>, 24, 19, 19)</span></span><br><span class="line"><span class="class">        l_conv7 = l_conv7.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 19, 19, 24)</span></span><br><span class="line"><span class="class">        l_conv7 = l_conv7.view(<span class="title">batch_size</span>, -1, 4)  # (<span class="type">N</span>, 2166, 4), there are a total 2116 boxes on this feature map</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        l_conv8_2 = self.loc_conv8_2(<span class="title">conv8_2_feats</span>)  # (<span class="type">N</span>, 24, 10, 10)</span></span><br><span class="line"><span class="class">        l_conv8_2 = l_conv8_2.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 10, 10, 24)</span></span><br><span class="line"><span class="class">        l_conv8_2 = l_conv8_2.view(<span class="title">batch_size</span>, -1, 4)  # (<span class="type">N</span>, 600, 4)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        l_conv9_2 = self.loc_conv9_2(<span class="title">conv9_2_feats</span>)  # (<span class="type">N</span>, 24, 5, 5)</span></span><br><span class="line"><span class="class">        l_conv9_2 = l_conv9_2.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 5, 5, 24)</span></span><br><span class="line"><span class="class">        l_conv9_2 = l_conv9_2.view(<span class="title">batch_size</span>, -1, 4)  # (<span class="type">N</span>, 150, 4)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        l_conv10_2 = self.loc_conv10_2(<span class="title">conv10_2_feats</span>)  # (<span class="type">N</span>, 16, 3, 3)</span></span><br><span class="line"><span class="class">        l_conv10_2 = l_conv10_2.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 3, 3, 16)</span></span><br><span class="line"><span class="class">        l_conv10_2 = l_conv10_2.view(<span class="title">batch_size</span>, -1, 4)  # (<span class="type">N</span>, 36, 4)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        l_conv11_2 = self.loc_conv11_2(<span class="title">conv11_2_feats</span>)  # (<span class="type">N</span>, 16, 1, 1)</span></span><br><span class="line"><span class="class">        l_conv11_2 = l_conv11_2.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 1, 1, 16)</span></span><br><span class="line"><span class="class">        l_conv11_2 = l_conv11_2.view(<span class="title">batch_size</span>, -1, 4)  # (<span class="type">N</span>, 4, 4)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        # <span class="type">Predict</span> classes in localization boxes</span></span><br><span class="line"><span class="class">        c_conv4_3 = self.cl_conv4_3(<span class="title">conv4_3_feats</span>)  # (<span class="type">N</span>, 4 * <span class="title">n_classes</span>, 38, 38)</span></span><br><span class="line"><span class="class">        c_conv4_3 = c_conv4_3.permute(0, 2, 3,</span></span><br><span class="line"><span class="class">                                      1).contiguous()  # (<span class="type">N</span>, 38, 38, 4 * <span class="title">n_classes</span>), to match prior-box order (<span class="title">after</span> .<span class="title">view</span>())</span></span><br><span class="line"><span class="class">        c_conv4_3 = c_conv4_3.view(<span class="title">batch_size</span>, -1,</span></span><br><span class="line"><span class="class">                                   <span class="title">self</span>.<span class="title">n_classes</span>)  # (<span class="type">N</span>, 5776, <span class="title">n_classes</span>), there are a total 5776 boxes on this feature map</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        c_conv7 = self.cl_conv7(<span class="title">conv7_feats</span>)  # (<span class="type">N</span>, 6 * <span class="title">n_classes</span>, 19, 19)</span></span><br><span class="line"><span class="class">        c_conv7 = c_conv7.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 19, 19, 6 * <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class">        c_conv7 = c_conv7.view(<span class="title">batch_size</span>, -1,</span></span><br><span class="line"><span class="class">                               <span class="title">self</span>.<span class="title">n_classes</span>)  # (<span class="type">N</span>, 2166, <span class="title">n_classes</span>), there are a total 2116 boxes on this feature map</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        c_conv8_2 = self.cl_conv8_2(<span class="title">conv8_2_feats</span>)  # (<span class="type">N</span>, 6 * <span class="title">n_classes</span>, 10, 10)</span></span><br><span class="line"><span class="class">        c_conv8_2 = c_conv8_2.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 10, 10, 6 * <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class">        c_conv8_2 = c_conv8_2.view(<span class="title">batch_size</span>, -1, <span class="title">self</span>.<span class="title">n_classes</span>)  # (<span class="type">N</span>, 600, <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        c_conv9_2 = self.cl_conv9_2(<span class="title">conv9_2_feats</span>)  # (<span class="type">N</span>, 6 * <span class="title">n_classes</span>, 5, 5)</span></span><br><span class="line"><span class="class">        c_conv9_2 = c_conv9_2.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 5, 5, 6 * <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class">        c_conv9_2 = c_conv9_2.view(<span class="title">batch_size</span>, -1, <span class="title">self</span>.<span class="title">n_classes</span>)  # (<span class="type">N</span>, 150, <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        c_conv10_2 = self.cl_conv10_2(<span class="title">conv10_2_feats</span>)  # (<span class="type">N</span>, 4 * <span class="title">n_classes</span>, 3, 3)</span></span><br><span class="line"><span class="class">        c_conv10_2 = c_conv10_2.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 3, 3, 4 * <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class">        c_conv10_2 = c_conv10_2.view(<span class="title">batch_size</span>, -1, <span class="title">self</span>.<span class="title">n_classes</span>)  # (<span class="type">N</span>, 36, <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        c_conv11_2 = self.cl_conv11_2(<span class="title">conv11_2_feats</span>)  # (<span class="type">N</span>, 4 * <span class="title">n_classes</span>, 1, 1)</span></span><br><span class="line"><span class="class">        c_conv11_2 = c_conv11_2.permute(0, 2, 3, 1).contiguous()  # (<span class="type">N</span>, 1, 1, 4 * <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class">        c_conv11_2 = c_conv11_2.view(<span class="title">batch_size</span>, -1, <span class="title">self</span>.<span class="title">n_classes</span>)  # (<span class="type">N</span>, 4, <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        # <span class="type">A</span> total of 8732 boxes</span></span><br><span class="line"><span class="class">        # <span class="type">Concatenate</span> in this specific order (<span class="title">i</span>.<span class="title">e</span>. <span class="title">must</span> <span class="title">match</span> <span class="title">the</span> <span class="title">order</span> <span class="title">of</span> <span class="title">the</span> <span class="title">prior</span>-<span class="title">boxes</span>)</span></span><br><span class="line"><span class="class">        locs = torch.cat([<span class="title">l_conv4_3</span>, <span class="title">l_conv7</span>, <span class="title">l_conv8_2</span>, <span class="title">l_conv9_2</span>, <span class="title">l_conv10_2</span>, <span class="title">l_conv11_2</span>], <span class="title">dim</span>=1)  # (<span class="type">N</span>, 8732, 4)</span></span><br><span class="line"><span class="class">        classes_scores = torch.cat([<span class="title">c_conv4_3</span>, <span class="title">c_conv7</span>, <span class="title">c_conv8_2</span>, <span class="title">c_conv9_2</span>, <span class="title">c_conv10_2</span>, <span class="title">c_conv11_2</span>],</span></span><br><span class="line"><span class="class">                                   <span class="title">dim</span>=1)  # (<span class="type">N</span>, 8732, <span class="title">n_classes</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        return locs, classes_scores</span></span><br></pre></td></tr></table></figure></p><h3 id="Multibox-loss"><a href="#Multibox-loss" class="headerlink" title="Multibox loss"></a>Multibox loss</h3><p>与常见的 Object Detection模型的目标函数相同，SSD算法的目标函数分为两部分：计算相应的prior box与目标类别的confidence loss以及相应的位置回归Localization loss。<br><img src="/img/multibox_loss.png" alt><br>loss的计算过程如下：</p><h4 id="Localization-loss"><a href="#Localization-loss" class="headerlink" title="Localization loss"></a>Localization loss</h4><ul><li>计算8732个prior box与groud truth的IOU重叠值，保留重合度大于0.5的所有priors，对应的prior也成为positive prior；在论文中换了个名词Jaccard overlaps，其实和IOU是一个意思;</li><li>由于网络输出的是相对于priors的预测偏差(g_c_x, g_c_y, g_w, g_h)，因此我们需要将positive prior转换为绝对坐标位置，然后与groud truth计算smooth l1 loss;</li><li>smooth l1 loss损失计算如下：<br><img src="/img/locloss.jpg" alt></li></ul><h4 id="Confidence-loss"><a href="#Confidence-loss" class="headerlink" title="Confidence loss"></a>Confidence loss</h4><p>在计算Localization loss中，只计算了positive prior的损失，没有计算negative prior的损失是因为标签中只有positive prior对应的gt。而在计算Confidence loss的不一样，每一次预测，不管正样本和是负样本都存在一个与之对应的标签。注意，这里的正负样本都是基于positive prior，iou&gt;0.5的prior box，正确分类为目标类别的为正样本，否则为负样本。在SSD中使用了hard negative mining。这是因为在positive prior中，分类正样本和负样本存在大量不平衡问题，因此使用hard negative mining来缓解这个问题。在这里，正样本与负样本的数量比例为1:3，负样本选择分类置信度靠前的。Confidence loss使用的交叉熵损失函数，如下：<br><img src="/img/confloss.jpg" alt></p><p>在SSD中，最后整个算法模型的损失函数为：<br><img src="/img/totalloss.jpg" alt><br>具体代码实现如下：<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">class MultiBoxLoss(nn.Module):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    The MultiBox loss, a loss function for object detection.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This is a combination of:</span></span><br><span class="line"><span class="string">    (1) a localization loss for the predicted locations of the boxes, and</span></span><br><span class="line"><span class="string">    (2) a confidence loss for the predicted class scores.</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line">    def __init__(self, priors_cxcy, <span class="attr">threshold=0.5,</span> <span class="attr">neg_pos_ratio=3,</span> <span class="attr">alpha=1.):</span></span><br><span class="line">        super(MultiBoxLoss, self).__init__()</span><br><span class="line">        self.<span class="attr">priors_cxcy</span> = priors_cxcy  <span class="comment">#priors_cxcy即上面create_prior_boxes（）生成的8732 priorbox</span></span><br><span class="line">        print(self.priors_cxcy[<span class="number">0</span>])</span><br><span class="line">        self.<span class="attr">priors_xy</span> = cxcy_to_xy(priors_cxcy)</span><br><span class="line">        self.<span class="attr">threshold</span> = threshold</span><br><span class="line">        self.<span class="attr">neg_pos_ratio</span> = neg_pos_ratio</span><br><span class="line">        self.<span class="attr">alpha</span> = alpha</span><br><span class="line"></span><br><span class="line">        self.<span class="attr">smooth_l1</span> = nn.L1Loss()</span><br><span class="line">        self.<span class="attr">cross_entropy</span> = nn.CrossEntropyLoss(<span class="attr">reduce=False)</span></span><br><span class="line"></span><br><span class="line">    def forward(self, predicted_locs, predicted_scores, boxes, labels):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        Forward propagation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)</span></span><br><span class="line"><span class="string">        :param predicted_scores: class scores for each of the encoded locations/boxes, a tensor of dimensions (N, 8732, n_classes)</span></span><br><span class="line"><span class="string">        :param boxes: true  object bounding boxes in boundary coordinates, a list of N tensors</span></span><br><span class="line"><span class="string">        :param labels: true object labels, a list of N tensors</span></span><br><span class="line"><span class="string">        :return: multibox loss, a scalar</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        <span class="attr">batch_size</span> = predicted_locs.size(<span class="number">0</span>)</span><br><span class="line">        <span class="attr">n_priors</span> = self.priors_cxcy.size(<span class="number">0</span>)</span><br><span class="line">        print (self.priors_xy[<span class="number">0</span>:<span class="number">2</span>])</span><br><span class="line">        <span class="attr">n_classes</span> = predicted_scores.size(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> <span class="attr">n_priors</span> == predicted_locs.size(<span class="number">1</span>) == predicted_scores.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="attr">true_locs</span> = torch.zeros((batch_size, n_priors, <span class="number">4</span>), <span class="attr">dtype=torch.float).to(device)</span>  <span class="comment"># (N, 8732, 4)</span></span><br><span class="line">        <span class="attr">true_classes</span> = torch.zeros((batch_size, n_priors), <span class="attr">dtype=torch.long).to(device)</span>  <span class="comment"># (N, 8732)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># For each image</span></span><br><span class="line">        for i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">            <span class="attr">n_objects</span> = boxes[i].size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            <span class="attr">overlap</span> = find_jaccard_overlap(boxes[i],</span><br><span class="line">                                           self.priors_xy)  <span class="comment"># (n_objects, 8732)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># For each prior, find the object that has the maximum overlap</span></span><br><span class="line">            overlap_for_each_prior, <span class="attr">object_for_each_prior</span> = overlap.max(<span class="attr">dim=0)</span>  <span class="comment"># (8732)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># We don't want a situation where an object is not represented in our positive (non-background) priors -</span></span><br><span class="line">            <span class="comment"># 1. An object might not be the best object for all priors, and is therefore not in object_for_each_prior.</span></span><br><span class="line">            <span class="comment"># 2. All priors with the object may be assigned as background based on the threshold (0.5).</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># To remedy this -</span></span><br><span class="line">            <span class="comment"># First, find the prior that has the maximum overlap for each object.</span></span><br><span class="line">            _, <span class="attr">prior_for_each_object</span> = overlap.max(<span class="attr">dim=1)</span>  <span class="comment"># (N_o)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Then, assign each object to the corresponding maximum-overlap-prior. (This fixes 1.)</span></span><br><span class="line">            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># To ensure these priors qualify, artificially give them an overlap of greater than 0.5. (This fixes 2.)</span></span><br><span class="line">            overlap_for_each_prior[prior_for_each_object] = <span class="number">1</span>.</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Labels for each prior</span></span><br><span class="line">            <span class="attr">label_for_each_prior</span> = labels[i][object_for_each_prior]  <span class="comment"># (8732)</span></span><br><span class="line">            <span class="comment"># Set priors whose overlaps with objects are less than the threshold to be background (no object)</span></span><br><span class="line">            label_for_each_prior[overlap_for_each_prior &lt; self.threshold] = <span class="number">0</span>  <span class="comment"># (8732)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Store</span></span><br><span class="line">            true_classes[i] = label_for_each_prior</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Encode center-size object coordinates into the form we regressed predicted boxes to</span></span><br><span class="line">            true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy)  <span class="comment"># (8732, 4)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Identify priors that are positive (object/non-background)</span></span><br><span class="line">        <span class="attr">positive_priors</span> = true_classes != <span class="number">0</span>  <span class="comment"># (N, 8732)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># LOCALIZATION LOSS</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Localization loss is computed only over positive (non-background) priors</span></span><br><span class="line">        <span class="attr">loc_loss</span> = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])  <span class="comment"># (), scalar</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Note: indexing with a torch.uint8 (byte) tensor flattens the tensor when indexing is across multiple dimensions (N &amp; 8732)</span></span><br><span class="line">        <span class="comment"># So, if predicted_locs has the shape (N, 8732, 4), predicted_locs[positive_priors] will have (total positives, 4)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># CONFIDENCE LOSS</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Confidence loss is computed over positive priors and the most difficult (hardest) negative priors in each image</span></span><br><span class="line">        <span class="comment"># That is, FOR EACH IMAGE,</span></span><br><span class="line">        <span class="comment"># we will take the hardest (neg_pos_ratio * n_positives) negative priors, i.e where there is maximum loss</span></span><br><span class="line">        <span class="comment"># This is called Hard Negative Mining - it concentrates on hardest negatives in each image, and also minimizes pos/neg imbalance</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Number of positive and hard-negative priors per image</span></span><br><span class="line">        <span class="attr">n_positives</span> = positive_priors.sum(<span class="attr">dim=1)</span>  <span class="comment"># (N)</span></span><br><span class="line">        <span class="attr">n_hard_negatives</span> = self.neg_pos_ratio * n_positives  <span class="comment"># (N)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># First, find the loss for all priors</span></span><br><span class="line">        <span class="attr">conf_loss_all</span> = self.cross_entropy(predicted_scores.view(-<span class="number">1</span>, n_classes), true_classes.view(-<span class="number">1</span>))  <span class="comment"># (N * 8732)</span></span><br><span class="line">        <span class="attr">conf_loss_all</span> = conf_loss_all.view(batch_size, n_priors)  <span class="comment"># (N, 8732)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># We already know which priors are positive</span></span><br><span class="line">        <span class="attr">conf_loss_pos</span> = conf_loss_all[positive_priors]  <span class="comment"># (sum(n_positives))</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Next, find which priors are hard-negative</span></span><br><span class="line">        <span class="comment"># To do this, sort ONLY negative priors in each image in order of decreasing loss and take top n_hard_negatives</span></span><br><span class="line">        <span class="attr">conf_loss_neg</span> = conf_loss_all.clone()  <span class="comment"># (N, 8732)</span></span><br><span class="line">        conf_loss_neg[positive_priors] = <span class="number">0</span>.  <span class="comment"># (N, 8732), positive priors are ignored (never in top n_hard_negatives)</span></span><br><span class="line">        conf_loss_neg, <span class="attr">_</span> = conf_loss_neg.sort(<span class="attr">dim=1,</span> <span class="attr">descending=True)</span>  <span class="comment"># (N, 8732), sorted by decreasing hardness</span></span><br><span class="line">        <span class="attr">hardness_ranks</span> = torch.LongTensor(range(n_priors)).unsqueeze(<span class="number">0</span>).expand_as(conf_loss_neg).to(device)  <span class="comment"># (N, 8732)</span></span><br><span class="line">        <span class="attr">hard_negatives</span> = hardness_ranks &lt; n_hard_negatives.unsqueeze(<span class="number">1</span>)  <span class="comment"># (N, 8732)</span></span><br><span class="line">        <span class="attr">conf_loss_hard_neg</span> = conf_loss_neg[hard_negatives]  <span class="comment"># (sum(n_hard_negatives))</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># As in the paper, averaged over positive priors only, although computed over both positive and hard-negative priors</span></span><br><span class="line">        <span class="attr">conf_loss</span> = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()  <span class="comment"># (), scalar</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># TOTAL LOSS</span></span><br><span class="line"></span><br><span class="line">        return conf_loss + self.alpha * loc_loss</span><br></pre></td></tr></table></figure></p><h3 id="Processing-predictions"><a href="#Processing-predictions" class="headerlink" title="Processing predictions"></a>Processing predictions</h3><p>对模型进行训练后，我们可以将其应用于图像目标检测。然而，模型预测输出的是包含8732个priors的偏移量和类别分数。因此需要对这些进行后处理，以获得最终的目标检测的边界框。大致过程如下：<br>（1）在获得了8732个priors的预测偏移量(g_c_x, g_c_y, g_w, g_h)之后，通过如下公式进行逆运算转换成绝对坐标：<br><img src="/img/ecs2.png" alt><br>（2）通过NMS（非最大值抑制）过滤掉背景和得分不是很高的框（这个是为了避免重复预测），得到最终的预测。<br>具体代码实现如下：<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line">def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    Decipher the 8732 locations and class scores (output of ths SSD300) to detect objects.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For each class, perform Non-Maximum Suppression (NMS) on boxes that are above a minimum threshold.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)</span></span><br><span class="line"><span class="string">    :param predicted_scores: class scores for each of the encoded locations/boxes, a tensor of dimensions (N, 8732, n_classes)</span></span><br><span class="line"><span class="string">    :param min_score: minimum threshold for a box to be considered a match for a certain class</span></span><br><span class="line"><span class="string">    :param max_overlap: maximum overlap two boxes can have so that the one with the lower score is not suppressed via NMS</span></span><br><span class="line"><span class="string">    :param top_k: if there are a lot of resulting detection across all classes, keep only the top 'k'</span></span><br><span class="line"><span class="string">    :return: detections (boxes, labels, and scores), lists of length batch_size</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    <span class="attr">batch_size</span> = predicted_locs.size(<span class="number">0</span>)</span><br><span class="line">    <span class="attr">n_priors</span> = self.priors_cxcy.size(<span class="number">0</span>)</span><br><span class="line">    <span class="attr">predicted_scores</span> = F.softmax(predicted_scores, <span class="attr">dim=2)</span>  <span class="comment"># (N, 8732, n_classes)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Lists to store final predicted boxes, labels, and scores for all images</span></span><br><span class="line">    <span class="attr">all_images_boxes</span> = list()</span><br><span class="line">    <span class="attr">all_images_labels</span> = list()</span><br><span class="line">    <span class="attr">all_images_scores</span> = list()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> <span class="attr">n_priors</span> == predicted_locs.size(<span class="number">1</span>) == predicted_scores.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    for i <span class="keyword">in</span> range(batch_size):</span><br><span class="line">        <span class="comment"># Decode object coordinates from the form we regressed predicted boxes to</span></span><br><span class="line">        <span class="attr">decoded_locs</span> = cxcy_to_xy(</span><br><span class="line">            gcxgcy_to_cxcy(predicted_locs[i], self.priors_cxcy))  <span class="comment"># (8732, 4), these are fractional pt. coordinates</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Lists to store boxes and scores for this image</span></span><br><span class="line">        <span class="attr">image_boxes</span> = list()</span><br><span class="line">        <span class="attr">image_labels</span> = list()</span><br><span class="line">        <span class="attr">image_scores</span> = list()</span><br><span class="line"></span><br><span class="line">        max_scores, <span class="attr">best_label</span> = predicted_scores[i].max(<span class="attr">dim=1)</span>  <span class="comment"># (8732)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Check for each class</span></span><br><span class="line">        for c <span class="keyword">in</span> range(<span class="number">1</span>, self.n_classes):</span><br><span class="line">            <span class="comment"># Keep only predicted boxes and scores where scores for this class are above the minimum score</span></span><br><span class="line">            <span class="attr">class_scores</span> = predicted_scores[i][:, c]  <span class="comment"># (8732)</span></span><br><span class="line">            <span class="attr">score_above_min_score</span> = class_scores &gt; min_score  <span class="comment"># torch.uint8 (byte) tensor, for indexing</span></span><br><span class="line">            <span class="attr">n_above_min_score</span> = score_above_min_score.sum().item()</span><br><span class="line">            <span class="keyword">if</span> <span class="attr">n_above_min_score</span> == <span class="number">0</span>:</span><br><span class="line">                continue</span><br><span class="line">            <span class="attr">class_scores</span> = class_scores[score_above_min_score]  <span class="comment"># (n_qualified), n_min_score &lt;= 8732</span></span><br><span class="line">            <span class="attr">class_decoded_locs</span> = decoded_locs[score_above_min_score]  <span class="comment"># (n_qualified, 4)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Sort predicted boxes and scores by scores</span></span><br><span class="line">            class_scores, <span class="attr">sort_ind</span> = class_scores.sort(<span class="attr">dim=0,</span> <span class="attr">descending=True)</span>  <span class="comment"># (n_qualified), (n_min_score)</span></span><br><span class="line">            <span class="attr">class_decoded_locs</span> = class_decoded_locs[sort_ind]  <span class="comment"># (n_min_score, 4)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Find the overlap between predicted boxes</span></span><br><span class="line">            <span class="attr">overlap</span> = find_jaccard_overlap(class_decoded_locs, class_decoded_locs)  <span class="comment"># (n_qualified, n_min_score)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Non-Maximum Suppression (NMS)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># A torch.uint8 (byte) tensor to keep track of which predicted boxes to suppress</span></span><br><span class="line">            <span class="comment"># 1 implies suppress, 0 implies don't suppress</span></span><br><span class="line">            <span class="attr">suppress</span> = torch.zeros((n_above_min_score), <span class="attr">dtype=torch.uint8).to(device)</span>  <span class="comment"># (n_qualified)</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Consider each box in order of decreasing scores</span></span><br><span class="line">            for box <span class="keyword">in</span> range(class_decoded_locs.size(<span class="number">0</span>)):</span><br><span class="line">                <span class="comment"># If this box is already marked for suppression</span></span><br><span class="line">                <span class="keyword">if</span> suppress[box] == <span class="number">1</span>:</span><br><span class="line">                    continue</span><br><span class="line"></span><br><span class="line">                <span class="comment"># Suppress boxes whose overlaps (with this box) are greater than maximum overlap</span></span><br><span class="line">                <span class="comment"># Find such boxes and update suppress indices</span></span><br><span class="line">                <span class="attr">suppress</span> = torch.max(suppress, overlap[box] &gt; max_overlap)</span><br><span class="line">                <span class="comment"># The max operation retains previously suppressed boxes, like an 'OR' operation</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># Don't suppress this box, even though it has an overlap of 1 with itself</span></span><br><span class="line">                suppress[box] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Store only unsuppressed boxes for this class</span></span><br><span class="line">            image_boxes.append(class_decoded_locs[<span class="number">1</span> - suppress])</span><br><span class="line">            image_labels.append(torch.LongTensor((<span class="number">1</span> - suppress).sum().item() * [c]).to(device))</span><br><span class="line">            image_scores.append(class_scores[<span class="number">1</span> - suppress])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If no object in any class is found, store a placeholder for 'background'</span></span><br><span class="line">        <span class="keyword">if</span> len(image_boxes) == <span class="number">0</span>:</span><br><span class="line">            image_boxes.append(torch.FloatTensor([[<span class="number">0</span>., <span class="number">0</span>., <span class="number">1</span>., <span class="number">1</span>.]]).to(device))</span><br><span class="line">            image_labels.append(torch.LongTensor([<span class="number">0</span>]).to(device))</span><br><span class="line">            image_scores.append(torch.FloatTensor([<span class="number">0</span>.]).to(device))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Concatenate into single tensors</span></span><br><span class="line">        <span class="attr">image_boxes</span> = torch.cat(image_boxes, <span class="attr">dim=0)</span>  <span class="comment"># (n_objects, 4)</span></span><br><span class="line">        <span class="attr">image_labels</span> = torch.cat(image_labels, <span class="attr">dim=0)</span>  <span class="comment"># (n_objects)</span></span><br><span class="line">        <span class="attr">image_scores</span> = torch.cat(image_scores, <span class="attr">dim=0)</span>  <span class="comment"># (n_objects)</span></span><br><span class="line">        <span class="attr">n_objects</span> = image_scores.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Keep only the top k objects</span></span><br><span class="line">        <span class="keyword">if</span> n_objects &gt; top_k:</span><br><span class="line">            image_scores, <span class="attr">sort_ind</span> = image_scores.sort(<span class="attr">dim=0,</span> <span class="attr">descending=True)</span></span><br><span class="line">            <span class="attr">image_scores</span> = image_scores[:top_k]  <span class="comment"># (top_k)</span></span><br><span class="line">            <span class="attr">image_boxes</span> = image_boxes[sort_ind][:top_k]  <span class="comment"># (top_k, 4)</span></span><br><span class="line">            <span class="attr">image_labels</span> = image_labels[sort_ind][:top_k]  <span class="comment"># (top_k)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Append to lists that store predicted boxes and scores for all images</span></span><br><span class="line">        all_images_boxes.append(image_boxes)</span><br><span class="line">        all_images_labels.append(image_labels)</span><br><span class="line">        all_images_scores.append(image_scores)</span><br><span class="line"></span><br><span class="line">    return all_images_boxes, all_images_labels, all_images_scores  <span class="comment"># lists of length batch_size</span></span><br></pre></td></tr></table></figure></p><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>在SSD中作者使用VOC2007和VOC2012数据集进行实验验证，两个数据集是互斥，不相容的。在SSD论文中针对 VOC2007和VOC2012 的具体用法有以下几种：</p><blockquote><ul><li><strong>07</strong>： VOC2007 trainval ，做训练数据；</li><li><strong>07+12</strong>： VOC2007 trainval + VOC200712 trainval，两者组合作为训练数据；</li><li><strong>07++12</strong>： VOC2007 trainval + VOC200712 trainval + VOC2007 test， 三者组合作为训练数据；</li></ul></blockquote><p>此外，还有先在coco进行预训练，然后再VOC上进行finetune的用法：</p><blockquote><ul><li><strong>07+12+coco</strong>： VOC2007 trainval + VOC200712 trainval + coco trainval135k, 表示先在coco trainval135k上训练，然后使用07+12进行finetune；</li><li><strong>07++12+coco</strong>： VOC2007 trainval + VOC2007 test + VOC200712 trainval + coco trainval135k, 表示先在coco trainval135k上训练，然后使用07++12进行finetune；</li></ul></blockquote><h3 id="code"><a href="#code" class="headerlink" title="code"></a>code</h3><p>SSD300实现的完整代码：<a href="https://github.com/nicehuster/ssd.pytorch" target="_blank" rel="noopener">https://github.com/nicehuster/ssd.pytorch</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;#160; &amp;#160; &amp;#160; &amp;#160;SSD作为one-stage目标检测算法中比较经典且具有代表性的一种方法。其精度可以与Faster R-CNN相匹敌，而速度达到了惊人的59FPS，速度上完爆 Faster R-CNN。相比于Faster R-CNN,其速度快的根本原因在于移除了region proposals的步骤以及后续的像素采样或特征采样步骤。论文连接：&lt;a href=&quot;https://arxiv.org/abs/1512.02325&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SSD: Single Shot MultiBox Detector&lt;/a&gt;，作者开源的代码连接：&lt;a href=&quot;https://github.com/weiliu89/caffe/tree/ssd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;code&lt;/a&gt;。由于作者开源代码使用caffe实现，这里以pytorch源码的方式实现。作者在论文中实现了两种不同输入大小的SSD模型：SSD300（输入图像大小统一为300x300）以及SSD512（输入图像大小统一为512x512）.这里主要针对SSD300,下面主要分四个部分介绍SSD300以及代码具体实现。&lt;br&gt;&lt;img src=&quot;/img/ssd.png&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="project" scheme="https://nicehuster.github.io/categories/project/"/>
    
    
      <category term="object detection" scheme="https://nicehuster.github.io/tags/object-detection/"/>
    
  </entry>
  
  <entry>
    <title>Faster rcnn Explained with code</title>
    <link href="https://nicehuster.github.io/2019/05/08/0052-Faster-rcnn%20Explained%20with%20code/"/>
    <id>https://nicehuster.github.io/2019/05/08/0052-Faster-rcnn Explained with code/</id>
    <published>2019-05-08T11:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>&#160; &#160; &#160; &#160;最近在看mmdetection源码，对于有些代码实现原理不是很清楚。由于mmdetection中实现的算法大多是two-stage方法，Faster rcnn作为这类方法的鼻祖，十分经典，详细地了解其实现具体原理十分重要。下面主要是推荐两篇关于Faster RCNN的blog，这两篇blog都是基于同一个代码实现来讲解的。<br><strong>代码地址</strong>：<a href="https://github.com/jwyang/faster-rcnn.pytorch" target="_blank" rel="noopener">https://github.com/jwyang/faster-rcnn.pytorch</a><br><strong>blog 1</strong> :<a href="http://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/" target="_blank" rel="noopener">Object Detection and Classification using R-CNNs</a><br><strong>blog 2</strong>：<a href="https://towardsdatascience.com/fasterrcnn-explained-part-1-with-code-599c16568cff" target="_blank" rel="noopener">FasterRCNN Explained with Code</a>  </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;#160; &amp;#160; &amp;#160; &amp;#160;最近在看mmdetection源码，对于有些代码实现原理不是很清楚。由于mmdetection中实现的算法大多是two-stage方法，Faster rcnn作为这类方法的鼻祖，十分经典，详细地了解其实现具体原理十分重要
      
    
    </summary>
    
      <category term="project" scheme="https://nicehuster.github.io/categories/project/"/>
    
    
      <category term="object detection" scheme="https://nicehuster.github.io/tags/object-detection/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：Objects as Points</title>
    <link href="https://nicehuster.github.io/2019/05/07/0051-CornerNet-Objects%20as%20Points/"/>
    <id>https://nicehuster.github.io/2019/05/07/0051-CornerNet-Objects as Points/</id>
    <published>2019-05-07T11:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> Xingyi Zhou,UT Austin, arxiv1904.07852<br><strong>代码链接：</strong> <a href="https://github.com/xingyizhou/CenterNet" target="_blank" rel="noopener">https://github.com/xingyizhou/CenterNet</a><br><strong>整体信息：</strong> 这篇文章题目言简意赅，就非常吸引人眼球。不同于CornerNet预测一对角点得到bbox，以及基于CornerNet改进的CenterNet预测三个点得到bbox，文章提出的CenterNet同样作为一种anchor-free的目标检测方法，化繁去简，直接将目标作为一个点去预测，彻底丢掉了nms等后处理操作，而且将这种方法应用到姿态估计和3D目标检测问题上。不得不说，足够Innovative，考虑精度和速度trade-off，在coco上也取得不俗的成绩：28.1%mAP/142FPS，37.4%mAP/52FPS，45.1%mAP/1.4FPS。<br><img src="/img/xcenternet.png" alt><br><a id="more"></a></p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>&#160; &#160; &#160; &#160;现有的目标检测方法，不论是oe-stage方法还是two-stage方法，在得到所有潜在目标框之后，都需要使用NMS（非极大值抑制）来处理同类目标重复的检测框，这种做法出列计算耗时之外，而且这种后处理操作不可导，无法做到端对端的训练。作者通过将目标作为一个点即目标的中心点，采用关键点估计方法预测中心点，再去预测目标尺寸。这种方法，相比于基于边界框的检测方法，可以做到端对端可导，更简单，更快速和更准确。如上图所示。</p><h3 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h3><p><img src="/img/implicit-anchor.png" alt="图1"><br>&#160; &#160; &#160; &#160;本文的CenterNet方法和anchor-based的one-stage目标检测方法类似，中心点可以看做是shape-agnostic（形状未知）的锚点，可以看做是一种隐式地anchors。和anchor-based方法的区别如上图示。</p><ul><li>在CenterNet中，分配的锚点仅仅是放在位置上，没有尺寸框。不需要手动设置阈值做前后景分类。（像Faster RCNN会将与GT IOU &gt;0.7的作为前景，&lt;0.3的作为背景）；</li><li>每个目标仅仅有一个正的锚点，因此不会用到NMS，通过提取关键点heatmap上局部峰值点（local peaks）；</li><li>CenterNet相比较于其他目标检测方法而言（一般缩放16倍尺度），使用更大分辨率的输出特征图（缩放了4倍），因此无需用到多重特征图锚点；</li><li>于其他基于关键点的目标检测方法相比（比如CornerNet,ExtremeNet），无需grouping过程以及后处理操作；</li></ul><h3 id="Preliminary"><a href="#Preliminary" class="headerlink" title="Preliminary"></a>Preliminary</h3><p>指定</p><script type="math/tex; mode=display">I\in R^{W\times H\times 3 }</script><p>表示输入图像，宽W，高H。生成的关键点heatmaps为</p><script type="math/tex; mode=display">\hat{Y}\in [0,1]^{\frac{W}{R}\times \frac{H}{R}\times C}</script><p>其中R表示输出stride，在CenterNet中设置为4，C表示关键点类别，比如在coco中C为80类目标。在预测的heatmaps中，</p><script type="math/tex; mode=display">\hat{Y}_{x,y,c}=1</script><p>表示检测的关键点，</p><script type="math/tex; mode=display">\hat{Y}_{x,y,c}=0</script><p>表示为背景。在CenterNet中，作者使用了三种全卷积encoder-decoder网络：hourglass，Resnet和DLA来预测$\hat{Y}$。关键点损失函数采用的是focal loss，如下公式所示：<br><img src="/img/centernet-loss.png" alt><br>此外，为了弥补因为输出stride造成的量化误差，作者另外设计了一个局部offset来补偿，其损失函数采用L1loss如下：<br><img src="/img/centernet-offset.png" alt></p><h3 id="Objects-as-Points"><a href="#Objects-as-Points" class="headerlink" title="Objects as Points"></a>Objects as Points</h3><p>指定</p><script type="math/tex; mode=display">(x_{1}^{(k)},y_{1}^{(k)},x_{2}^{(k)},y_{2}^{(k)})</script><p>表示目标k（其类别为$c_k$）,对应中心位置为</p><script type="math/tex; mode=display">p_k=(\frac{x_1^{(k)}+x_2^{(k)}}{2},\frac{y_1^{(k)}+y_2^{(k)}}{2})</script><p>使用关键点预测器$\hat{Y}$预测所有关键点。此外回归每个目标k的大小</p><script type="math/tex; mode=display">s_k=(x_2^{(k)}-x_1^{(k)},y_2^{(k)}-y_1^{(k)})</script><p>为了计算减少计算，d对所有目标使用单尺度预测，$\hat{S}\in R^{\frac{W}{R}\times \frac{H}{R}\times 2}$，因此可以在中心点位置添加L1 Loss：</p><script type="math/tex; mode=display">L_{size}=\frac{1}{N}\sum_{k=1}^{N}\left | \hat s_{pk}-s_k \right |</script><p>整个目标损失函数如下：</p><script type="math/tex; mode=display">L_{det}=L_k+\lambda _{size}L_{size}+\lambda _{off}L_{off}</script><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>&#160; &#160; &#160; &#160;在coco上训练了三种不同backbone的CenterNet：Resnet-101和DLA-34花费2.5天，hourglass-104花费5天。实验结果对比如下：<br><img src="/img/centernet-res.png" alt><br>在精度和速度上的trade-off上相比，CenterNet相对来说还是比较优秀。此外，作者针对输入分辨率，目标中心点重叠、size损失函数、size损失的超参数和训练策略等方面做了很多ablation study，使得整个实验结果比较solid。</p><h3 id="个人观点"><a href="#个人观点" class="headerlink" title="个人观点"></a>个人观点</h3><p>（1）这篇Objects as Points 的CenterNet通过预测一个中心点解决了2D目标检测，3D目标检测以及姿态估计任务，完美的把这三个任务统一起来，从广度上来讲，很强很强。<br>（2）值得和另一篇CenterNet（Keypoint Triplets for Object Detection）进行比较，Keypoint Triplets则是通过中心点预测抑制CornerNet中存在的大量误检，这个更多的是incremental。<br>（3）从放出来的几篇基于keypoint的目标检测方法在coco上训练耗时来看，这类方法普遍存在训练耗时长的问题。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; Xingyi Zhou,UT Austin, arxiv1904.07852&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt; &lt;a href=&quot;https://github.com/xingyizhou/CenterNet&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/xingyizhou/CenterNet&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt; 这篇文章题目言简意赅，就非常吸引人眼球。不同于CornerNet预测一对角点得到bbox，以及基于CornerNet改进的CenterNet预测三个点得到bbox，文章提出的CenterNet同样作为一种anchor-free的目标检测方法，化繁去简，直接将目标作为一个点去预测，彻底丢掉了nms等后处理操作，而且将这种方法应用到姿态估计和3D目标检测问题上。不得不说，足够Innovative，考虑精度和速度trade-off，在coco上也取得不俗的成绩：28.1%mAP/142FPS，37.4%mAP/52FPS，45.1%mAP/1.4FPS。&lt;br&gt;&lt;img src=&quot;/img/xcenternet.png&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://nicehuster.github.io/categories/paper-reading/"/>
    
    
      <category term="object detection" scheme="https://nicehuster.github.io/tags/object-detection/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：Keypoint Triplets for Object Detection(CenterNet)</title>
    <link href="https://nicehuster.github.io/2019/05/06/0050-CenterNet/"/>
    <id>https://nicehuster.github.io/2019/05/06/0050-CenterNet/</id>
    <published>2019-05-06T11:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> Kaiwen Duan,CAS, arxiv:1904.08189<br><strong>代码链接：</strong> <a href="https://github.com/Duankaiwen/CenterNet" target="_blank" rel="noopener">https://github.com/Duankaiwen/CenterNet</a><br><strong>整体信息：</strong> 这是在CornerNet的基础上进行改进的一篇文章，原始的CornerNet仅仅是通过左上角点右下角点来确定目标，但是没有有效的利用目标的内部特征信息，因此产生容易产生许多误检。作者在CornerNet基础上添加了一个center keypoint,使得网络具备了感知物体内部信息的能力，从而能有效抑制误检。同样作为一种one-stage方法，在MSCOCO上上取得47%mAP。<br><img src="/img/cor-centernet.png" alt><br><a id="more"></a></p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>作者认为CornerNet存在的一些不足之处：</p><blockquote><ul><li>在CornerNet中，其容易产生一些无效框，同时这些无效框的中心位置往往不在目标上。因此可以通过预测物体中心点，来对CornerNet的bbox进行过滤，滤除那些中心点不在物体上的bbox；</li><li>在CornerNet中，使用了corner pooling，这种方式使得网络更加关注于目标的边缘信息，缺乏对目标内部信息的关注，因此可以根据corner pooling设计center pooling使得网络不仅关注于边界信息，而且能有效地利用目标内部特征；</li></ul></blockquote><h3 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h3><p>&#160; &#160; &#160; &#160;上图表示的是CenterNet的模型结构。整个模型结构比较容易理解。<br>（1）使用Hourglass作为backbone提取图像的feature。<br>（2）使用Cascade Corner Pooling模块提取图像的Corner heatmaps，并采用与CornerNet中一样的方法，根据左上角和右下角点，得到物体的bounding box。<br>（3）使用center pooling得到目标的center heatmap，进而计算得到目标的中心点。<br>（4）对步骤2中得到的bbox使用中心点信息对bbox进一步过滤。</p><h3 id="Center-Pooling-amp-amp-Cascade-Corner-Pooling"><a href="#Center-Pooling-amp-amp-Cascade-Corner-Pooling" class="headerlink" title="Center Pooling &amp;&amp; Cascade Corner Pooling"></a>Center Pooling &amp;&amp; Cascade Corner Pooling</h3><p><img src="/img/center-pool.png" alt><br><img src="/img/center-pool1.png" alt></p><h4 id="Center-Pooling"><a href="#Center-Pooling" class="headerlink" title="Center Pooling"></a>Center Pooling</h4><p>&#160; &#160; &#160; &#160;物体的几何中心不一定能传达出recognizable的视觉信息（例如，人的头部有很强的视觉模式，但中心关键点通常位于人体的中间）。为了解决这个问题，作者提出了Center Pooling方法。如上图1 (a)所示，center pooling计算此点所在的水平方向最大值和垂直方向最大值之和。<br>&#160; &#160; &#160; &#160;作者在corner pooling基础上实现了center pooling。如上图2（a）所示，为了取得一个方向的最大值比如水平方向上，先使用从左至右的Corner Pooling，再跟着一个从右至左的Corner Pooling，即可直接得到水平方向的最大值。同理，也可以得到竖直方向的最大值，再将水平Pooling和竖直Pooling的结果相加即可。</p><h4 id="Cascade-Corner-Pooling"><a href="#Cascade-Corner-Pooling" class="headerlink" title="Cascade Corner Pooling"></a>Cascade Corner Pooling</h4><p>物体bbox的两个角点往往在物体外部，缺少对物体局部特征的描述。因为Corner Pooling目的是找到物体边界部分的最大值来确定bbox，因此其对边缘非常敏感。作者为了解决这个问题，让物体在关注目标边界信息的同时也能关注目标内部信息。如上图2（c）所示，首先和Corner Pooling一样，首先沿边界查找边界最大值，然后沿边界最大值的位置查找内部最大值，最后将两个最大值相加。通过这种方式，使得角点可以同时获得物体的边界信息和内部视觉信息。其具体实现如上图2（b）给出了Cascade Top Pooling的具体实现，可以发现其首先使用Left Corner Pooling，获得每个点右边的最大值，然后使用Top Corner Pooling。</p><h3 id="Object-Detection-as-Keypoint-Triplets"><a href="#Object-Detection-as-Keypoint-Triplets" class="headerlink" title="Object Detection as Keypoint Triplets"></a>Object Detection as Keypoint Triplets</h3><p>&#160; &#160; &#160; &#160;CenterNet利用检测到的中心点并采用以下步骤来有效地过滤掉错误的bbox。<br>（1）根据中心点的heatmap的响应值得分，选择top-k个center keypoints<br>（2）使用对应的offsets对remap到原始图像上的关键点进行微调，得到更加精确地中心点位置。<br>（3）根据left-top 和right-bottom点得到所有的bounding box。并为每一个bbox定义一个中心区域。<br>（4）如果（1）中得到的中心点在bbox的中心区域，则bbox的分数为三个点的分数之和；否则，移除相应bbox。<br>&#160; &#160; &#160; &#160;中心区域大小的确定十分重要，因为中心区域定义过小会导致小目标的recall过低，中心区域定义的过大则会导致大目标的precision过低。在CenterNet中，作者采用了一种scale-aware方法来自适应地确定中心区域大小。对于大目标则给予相对小的中心区域，对于小目标则给予相对大的中心区域。如下图示。<br><img src="/img/central-region.png" alt><br>中心区域左上角和右下角坐标计算公式如下：<br><img src="/img/central-region1.png" alt><br>其中n为奇数，在论文中，当bbox的size大于150时，n=5,小于150时，n=3。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/img/centernet-exp.png" alt><br>&#160; &#160; &#160; &#160;只从单尺度实验比较上可以看出，CenterNet相比CornerNet在mAP提高了3.8%，可以看出来提高的幅度还是相对可观的，在多尺度以及不同的backbone上提高的幅度也有4-5%。从整体上看在one-stage方法中，算是效果最好的。个人认为，CornerNet由于只关注目标的边界问题，而且确定bbox的角点往往在目标外面，无法利用目标的内部信息，这个drawback不难想到，不过，以一种比较effective的方法来解决这个问题确实有点难，作者恰巧在corner pooling的的基础上设计出了center pooling，结合Center point和Corner point进行目标检测，确实不容易。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; Kaiwen Duan,CAS, arxiv:1904.08189&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt; &lt;a href=&quot;https://github.com/Duankaiwen/CenterNet&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/Duankaiwen/CenterNet&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt; 这是在CornerNet的基础上进行改进的一篇文章，原始的CornerNet仅仅是通过左上角点右下角点来确定目标，但是没有有效的利用目标的内部特征信息，因此产生容易产生许多误检。作者在CornerNet基础上添加了一个center keypoint,使得网络具备了感知物体内部信息的能力，从而能有效抑制误检。同样作为一种one-stage方法，在MSCOCO上上取得47%mAP。&lt;br&gt;&lt;img src=&quot;/img/cor-centernet.png&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://nicehuster.github.io/categories/paper-reading/"/>
    
    
      <category term="object detection" scheme="https://nicehuster.github.io/tags/object-detection/"/>
    
  </entry>
  
  <entry>
    <title>使用mmdetection训练supervisely-person-datasets</title>
    <link href="https://nicehuster.github.io/2019/04/28/0049-supervisely-person-datasets/"/>
    <id>https://nicehuster.github.io/2019/04/28/0049-supervisely-person-datasets/</id>
    <published>2019-04-28T11:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>&#160; &#160; &#160; &#160;supervisely-person-datasets是<a href="https://deepsystems.ai/" target="_blank" rel="noopener">deepsystems.ai</a>开源的一个人像分割数据集。专门用于人像的精细化语义分割。该数据集包含5711张图片，有6884个高质量的标注的人体实例。关于该数据集的下载，<a href="https://www.leiphone.com/news/201804/h2LP6OeEwgmGghER.html" target="_blank" rel="noopener">终于！Supervise.ly 发布人像分割数据集啦（免费开源）</a>这篇文章介绍的比较清楚。一开始下载下来的数据集标注文件使用的是.json格式保存，部分人体标注信息采用polygon的形式给出的，而大部分则是通过bitmap形式给出的，这和常用数据集比如COCO，PASCAL VOC数据集相比，可读性非常差，好在supervisely开源了数据集使用的API接口。本文就主要介绍这个数据集使用的API接口。<br><img src="/img/supervisely-example.png" alt><br><a id="more"></a></p><h3 id="supervisely-person-datasets数据集下载"><a href="#supervisely-person-datasets数据集下载" class="headerlink" title="supervisely-person-datasets数据集下载"></a>supervisely-person-datasets数据集下载</h3><p>&#160; &#160; &#160; &#160;这个数据集包含13个文件夹（ds1~ds13），每个文件夹中包含img和ann两个文件夹，前者是图片集合后者是对应图片集合的.json格式标注文件。整个数据集下载下来大约7.2G左右，不过该数据集需要使用外网才能下载下来，而且数据集下载很慢。建议分部分下载，这样下载效率更高。我自己下载下来并上传至百度云盘，考虑到版权问题，不予公开，如果您的网络下载不了，可以邮箱私信我。数据集部分图片如下所示<br><img src="/img/supervisely-example1.png" alt><br>数据集下载下来的结构如下<br><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">niceliu<span class="variable">@ise</span><span class="symbol">:/mnt/software/datasets/supervisely-person-datasets</span><span class="variable">$ </span>tree -L <span class="number">2</span></span><br><span class="line">.</span><br><span class="line">├── ds1</span><br><span class="line">│   ├── ann</span><br><span class="line">│   └── img</span><br><span class="line">...</span><br><span class="line">├── ds11</span><br><span class="line">│   ├── ann</span><br><span class="line">│   └── img</span><br><span class="line">├── ds12</span><br><span class="line">│   ├── ann</span><br><span class="line">│   └── img</span><br><span class="line">├── ds13</span><br><span class="line">│   ├── ann</span><br><span class="line">│   └── img</span><br><span class="line">└── meta.json</span><br></pre></td></tr></table></figure></p><p>这个数据集中所有图片都是在<a href="https://www.pexels.com/" target="_blank" rel="noopener">Pexels</a>上下载的，每张图片均为高清图片，可做壁纸哈。deepsystem.io开源了这个网站的图片下载工具<a href="https://github.com/DeepSystems/pexels_downloader" target="_blank" rel="noopener">Pexels Downloader</a>。注意：这个数据集下载下来之后包含meta.json文件不可删除，不然后续API无法解析这个数据集。</p><h3 id="API下载及使用"><a href="#API下载及使用" class="headerlink" title="API下载及使用"></a>API下载及使用</h3><p>&#160; &#160; &#160; &#160;<a href="https://github.com/supervisely/supervisely" target="_blank" rel="noopener">supervisely</a>这个repository中的supervisely_lib包含了supervisely数据格式的API接口。把这个repository克隆下来。在这个文件help/jupyterlab_scripts/src/tutorials/01_project_structure/project.ipynb详细地介绍了supervisely数据格式的读取以及可视化。这里主要列举一些与supervisely-person-datasets相关的接口。</p><h4 id="数据集载入jiy以及基本信息读取"><a href="#数据集载入jiy以及基本信息读取" class="headerlink" title="数据集载入jiy以及基本信息读取"></a>数据集载入jiy以及基本信息读取</h4><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Supervisely Python SDK</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> supervisely_lib <span class="keyword">as</span> sly  </span><br><span class="line">################打开数据集</span><br><span class="line">project = sly.Project(<span class="string">'/mnt/software/datasets/supervisely-person-datasets/'</span>, sly.OpenMode.READ)</span><br><span class="line">#打印数据集相关信息</span><br><span class="line">print(<span class="string">"Project name: "</span>, project.name) </span><br><span class="line">print(<span class="string">"Project directory: "</span>, project.directory)</span><br><span class="line">print(<span class="string">"Total images: "</span>, project.total_items)</span><br><span class="line">print(<span class="string">"Dataset names: "</span>, project.datasets.keys())</span><br></pre></td></tr></table></figure><p>打印信息如下：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Project <span class="string">name:</span>  supervisely-person-datasets</span><br><span class="line">Project <span class="string">directory:</span>  <span class="regexp">/mnt/</span>software<span class="regexp">/datasets/</span>supervisely-person-datasets</span><br><span class="line">Total <span class="string">images:</span>  <span class="number">5711</span></span><br><span class="line">Dataset <span class="string">names:</span>  [<span class="string">'ds7'</span>, <span class="string">'ds3'</span>, <span class="string">'ds4'</span>, <span class="string">'ds1'</span>, <span class="string">'ds8'</span>, <span class="string">'ds9'</span>, <span class="string">'ds2'</span>, <span class="string">'ds5'</span>, <span class="string">'ds13'</span>, <span class="string">'ds6'</span>, <span class="string">'ds12'</span>, <span class="string">'ds11'</span>, <span class="string">'ds10'</span>]</span><br></pre></td></tr></table></figure></p><h4 id="数据集遍历"><a href="#数据集遍历" class="headerlink" title="数据集遍历"></a>数据集遍历</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> dataset <span class="keyword">in</span> project:</span><br><span class="line">    <span class="builtin-name">print</span>(<span class="string">"Dataset: "</span>, dataset.name)</span><br><span class="line">    <span class="keyword">for</span> item_name <span class="keyword">in</span> dataset:</span><br><span class="line">        <span class="attribute">ann_path</span>=dataset.get_ann_path(item_name)</span><br><span class="line">        img_path = dataset.get_img_path(item_name)</span><br><span class="line">        <span class="builtin-name">print</span>(<span class="string">"  image &amp; anno: "</span>, img_path,ann_path)</span><br></pre></td></tr></table></figure><p>打印输出的是单张图片路径以及对应标注文件路径：<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image &amp; <span class="string">anno:</span>  <span class="regexp">/mnt/</span>software<span class="regexp">/datasets/</span>supervisely-person-datasets<span class="regexp">/ds10/</span>img<span class="regexp">/pexels-photo-450596.png /</span>mnt<span class="regexp">/software/</span>datasets<span class="regexp">/supervisely-person-datasets/</span>ds10<span class="regexp">/ann/</span>pexels-photo<span class="number">-450596.</span>png.json</span><br></pre></td></tr></table></figure></p><h4 id="标注信息读取并显示"><a href="#标注信息读取并显示" class="headerlink" title="标注信息读取并显示"></a>标注信息读取并显示</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">display_images</span><span class="params">(images, figsize=None)</span>:</span></span><br><span class="line">    plt.figure(figsize=(figsize <span class="keyword">if</span> (figsize <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>) <span class="keyword">else</span> (<span class="number">15</span>, <span class="number">15</span>)))</span><br><span class="line">    <span class="keyword">for</span> i, img <span class="keyword">in</span> enumerate(images, start=<span class="number">1</span>):</span><br><span class="line">        plt.subplot(<span class="number">1</span>, len(images), i)</span><br><span class="line">        plt.imshow(img)</span><br><span class="line"><span class="comment"># 定位并加载图像标签数据。</span></span><br><span class="line">item_paths = project.datasets.get(<span class="string">'ds1'</span>).get_item_paths(<span class="string">'pexels-photo-708392.png'</span>)</span><br><span class="line">ann = sly.Annotation.load_json_file(item_paths.ann_path, project.meta)</span><br><span class="line"><span class="comment"># 检查标记的对象并打印出基本属性。</span></span><br><span class="line"><span class="keyword">for</span> label <span class="keyword">in</span> ann.labels:</span><br><span class="line">    print(<span class="string">'Found label object: '</span> + label.obj_class.name)  </span><br><span class="line">    print(<span class="string">'   geometry type: '</span> + label.geometry.geometry_name()) </span><br><span class="line">    print(<span class="string">'   object area: '</span> + str(label.geometry.area)) </span><br><span class="line">    </span><br><span class="line"><span class="comment"># 读取底层原始图像以进行显示。</span></span><br><span class="line">img = sly.image.read(item_paths.img_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用meta.json文件中的颜色呈现所有标签。</span></span><br><span class="line">ann_render = np.zeros(ann.img_size + (<span class="number">3</span>,), dtype=np.uint8)</span><br><span class="line">ann.draw(ann_render)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分别绘制标记的对象轮廓。</span></span><br><span class="line">ann_contours = np.zeros(ann.img_size + (<span class="number">3</span>,), dtype=np.uint8)</span><br><span class="line">ann.draw_contour(ann_contours, thickness=<span class="number">7</span>)</span><br><span class="line"><span class="comment">#显示</span></span><br><span class="line">display_images([img, ann_render, ann_contours])</span><br></pre></td></tr></table></figure><p>对应显示结果如下：<br><img src="/img/display.png" alt><br>打印信息如下：<br><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Found</span> label <span class="class"><span class="keyword">object</span></span>: person_bmp</span><br><span class="line">   geometry <span class="class"><span class="keyword">type</span></span>: bitmap</span><br><span class="line">   <span class="class"><span class="keyword">object</span> <span class="title">area</span></span>: <span class="number">87746.0</span></span><br><span class="line"><span class="type">Found</span> label <span class="class"><span class="keyword">object</span></span>: person_bmp</span><br><span class="line">   geometry <span class="class"><span class="keyword">type</span></span>: bitmap</span><br><span class="line">   <span class="class"><span class="keyword">object</span> <span class="title">area</span></span>: <span class="number">153198.0</span></span><br><span class="line"><span class="type">Found</span> label <span class="class"><span class="keyword">object</span></span>: person_bmp</span><br><span class="line">   geometry <span class="class"><span class="keyword">type</span></span>: bitmap</span><br><span class="line">   <span class="class"><span class="keyword">object</span> <span class="title">area</span></span>: <span class="number">92894.0</span></span><br><span class="line"><span class="type">Found</span> label <span class="class"><span class="keyword">object</span></span>: person_bmp</span><br><span class="line">   geometry <span class="class"><span class="keyword">type</span></span>: bitmap</span><br><span class="line">   <span class="class"><span class="keyword">object</span> <span class="title">area</span></span>: <span class="number">109706.0</span></span><br></pre></td></tr></table></figure></p><p>值得注意的是，在这个数据集中geometry类型除了bitmap格式还有polygon格式。polygon格式对应显示如下：<br><img src="/img/display1.png" alt></p><h4 id="获取边框"><a href="#获取边框" class="headerlink" title="获取边框"></a>获取边框</h4><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">rendered_bboxes = np.zeros(ann.img_size + (<span class="number">3</span>,), dtype=np.uint8)</span><br><span class="line"></span><br><span class="line">for <span class="keyword">label</span><span class="bash"> <span class="keyword">in</span> ann.labels:</span></span><br><span class="line"><span class="bash">    <span class="built_in">print</span>(<span class="string">'Label type: '</span> + label.geometry.geometry_name())</span></span><br><span class="line"><span class="bash">    </span></span><br><span class="line"><span class="bash">    <span class="comment"># Same call for any label type.</span></span></span><br><span class="line"><span class="bash">    bbox = label.geometry.to_bbox()</span></span><br><span class="line"><span class="bash">    <span class="built_in">print</span>(<span class="string">'Label bounding box: [&#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;]'</span>.format(</span></span><br><span class="line"><span class="bash">        bbox.top, bbox.left, bbox.bottom, bbox.right))</span></span><br><span class="line"><span class="bash">    </span></span><br><span class="line"><span class="bash">    <span class="comment"># Draw the bounding boxes.</span></span></span><br><span class="line"><span class="bash">    bbox.draw_contour(rendered_bboxes, color = label.obj_class.color, thickness=20)</span></span><br><span class="line"><span class="bash">    </span></span><br><span class="line"><span class="bash">    <span class="comment"># Draw the labels themselves too to make sure the bounding boxes are correct.</span></span></span><br><span class="line"><span class="bash">    label.geometry.draw(rendered_bboxes, color=[int(x/2) <span class="keyword">for</span> x <span class="keyword">in</span> label.obj_class.color])</span></span><br><span class="line"><span class="bash"></span></span><br><span class="line"><span class="bash">display_images([img, rendered_bboxes])</span></span><br></pre></td></tr></table></figure><p>边框显示如下：<br><img src="/img/display2.png" alt><br>熟悉了这个数据集标注信息的读取尤其是边框box以及掩膜mask,接下来就可以使用一些现有的模型进行训练。</p><h3 id="使用mmdetection训练"><a href="#使用mmdetection训练" class="headerlink" title="使用mmdetection训练"></a>使用mmdetection训练</h3><p>在使用mmdetection训练这个数据集的时候，主要修改如下部分：<br>（1）选择合适模型，并编写该模型的配置文件，比如maskrcnn，修改相关参数，比如数据集相关参数，以及优化器学习率，总的迭代次数；<br>（2）在mmdet/datasets文件夹中实现自己的数据集加载部分代码，可参考coco.py的实现，实现之后，在mmdet/datasets/<strong>init</strong>.py中添加自己的数据集加载类；<br>（3）在mmdet/core/evaluation/class_names.py中添加自己的数据集类别名声明</p><p>之后就可以开始训练自己的数据集。由于这个数据集规模算比较小，而且考虑到计算资源问题，为了让模型更加快速的收敛，我选择在已经在coco上训练好的maskrcnn预训练模型来训练该数据集。测试结果如下：<br><img src="/img/pretrained_supervise_res.jpg" alt><br>最后，使用mmdetection训练supervisely person 数据集的源码在这里，<a href="https://github.com/nicehuster/mmdetection-supervisely-person-datasets" target="_blank" rel="noopener">https://github.com/nicehuster/mmdetection-supervisely-person-datasets</a></p><p><strong>数据集下载地址：</strong> 链接: <a href="https://pan.baidu.com/s/14J3uXxLaVHOvAToTIDeRCw" target="_blank" rel="noopener">https://pan.baidu.com/s/14J3uXxLaVHOvAToTIDeRCw</a> 提取码: iu9v </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;#160; &amp;#160; &amp;#160; &amp;#160;supervisely-person-datasets是&lt;a href=&quot;https://deepsystems.ai/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;deepsystems.ai&lt;/a&gt;开源的一个人像分割数据集。专门用于人像的精细化语义分割。该数据集包含5711张图片，有6884个高质量的标注的人体实例。关于该数据集的下载，&lt;a href=&quot;https://www.leiphone.com/news/201804/h2LP6OeEwgmGghER.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;终于！Supervise.ly 发布人像分割数据集啦（免费开源）&lt;/a&gt;这篇文章介绍的比较清楚。一开始下载下来的数据集标注文件使用的是.json格式保存，部分人体标注信息采用polygon的形式给出的，而大部分则是通过bitmap形式给出的，这和常用数据集比如COCO，PASCAL VOC数据集相比，可读性非常差，好在supervisely开源了数据集使用的API接口。本文就主要介绍这个数据集使用的API接口。&lt;br&gt;&lt;img src=&quot;/img/supervisely-example.png&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="project" scheme="https://nicehuster.github.io/categories/project/"/>
    
    
      <category term="object detection" scheme="https://nicehuster.github.io/tags/object-detection/"/>
    
  </entry>
  
  <entry>
    <title>mmdetection(pytorch0.4.1版本)模型构建部分源码解析</title>
    <link href="https://nicehuster.github.io/2019/04/25/0048-mmdetection_build/"/>
    <id>https://nicehuster.github.io/2019/04/25/0048-mmdetection_build/</id>
    <published>2019-04-25T11:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>&#160; &#160; &#160; &#160;在mmdetection中，实现了许多现有two-stage目标检测方法以及one-stage目标检测方法，且包含完整的数据载入、模型构建、模型训练以及模型测试部分的源码。因此非常适合在此基础上扩展实现其他目标检测算法。关于数据载入、模型训练以及模型测试部分的源码，在上一篇博客中有详细介绍。后期打算在mmdetection基础上添加其他算法。因此了解模型构建这部分源码十分重要。在mmdetecion官方<a href="https://github.com/open-mmlab/mmdetection" target="_blank" rel="noopener">github</a>介绍模型主要由四部分组成：backbone, neck, head, 以及ROI extractor。<br><a id="more"></a><br>&#160; &#160; &#160; &#160;阅读mmdetection源码最简单地主线就是根据模型配置文件configs/*.py文件中model部分去阅读。在mmdetection中，two-stage类目标检测方法model部分内容通常包含：backbone，neck，rpn_head，bbox_roi_extractor和bbox_head共五个部分。当然，在maskrcnn中还包括mask_roi_extractor和mask_head。以maskrcnn模型的配置文件中model部分为例，介绍模型构建部分，下面是maskrcnn模型中model部分配置：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">model = dict(</span><br><span class="line">    <span class="attribute">type</span>=<span class="string">'MaskRCNN'</span>,  #模型名，对应模型声明在mmdet/modelsp/detectors/mask_rcnn.py文件中</span><br><span class="line">    <span class="attribute">pretrained</span>=<span class="string">'modelzoo://resnet50'</span>, #backbone的预训练模型</span><br><span class="line">    <span class="attribute">backbone</span>=dict(</span><br><span class="line">        <span class="attribute">type</span>=<span class="string">'ResNet'</span>,</span><br><span class="line">        <span class="attribute">depth</span>=50, </span><br><span class="line">        <span class="attribute">num_stages</span>=4,</span><br><span class="line">        out_indices=(0, 1, 2, 3),</span><br><span class="line">        <span class="attribute">frozen_stages</span>=1, </span><br><span class="line">        <span class="attribute">style</span>=<span class="string">'pytorch'</span>, </span><br><span class="line">        <span class="attribute">dcn</span>=dict(        </span><br><span class="line">            <span class="attribute">modulated</span>=<span class="literal">False</span>,</span><br><span class="line">            <span class="attribute">deformable_groups</span>=1,</span><br><span class="line">            <span class="attribute">fallback_on_stride</span>=<span class="literal">False</span>),</span><br><span class="line">        stage_with_dcn=(<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>)),</span><br><span class="line">    <span class="attribute">neck</span>=dict(                   </span><br><span class="line">        <span class="attribute">type</span>=<span class="string">'FPN'</span>,</span><br><span class="line">        in_channels=[256, 512, 1024, 2048],</span><br><span class="line">        <span class="attribute">out_channels</span>=256,</span><br><span class="line">        <span class="attribute">num_outs</span>=5),</span><br><span class="line">    <span class="attribute">rpn_head</span>=dict(</span><br><span class="line">        <span class="attribute">type</span>=<span class="string">'RPNHead'</span>,</span><br><span class="line">        <span class="attribute">in_channels</span>=256,</span><br><span class="line">        <span class="attribute">feat_channels</span>=256,</span><br><span class="line">        anchor_scales=[8],</span><br><span class="line">        anchor_ratios=[0.5, 1.0, 2.0],</span><br><span class="line">        anchor_strides=[4, 8, 16, 32, 64],</span><br><span class="line">        target_means=[.0, .0, .0, .0],</span><br><span class="line">        target_stds=[1.0, 1.0, 1.0, 1.0],</span><br><span class="line">        <span class="attribute">use_sigmoid_cls</span>=<span class="literal">True</span>),</span><br><span class="line">    <span class="attribute">bbox_roi_extractor</span>=dict(</span><br><span class="line">        <span class="attribute">type</span>=<span class="string">'SingleRoIExtractor'</span>,</span><br><span class="line">        <span class="attribute">roi_layer</span>=dict(type='RoIAlign', <span class="attribute">out_size</span>=7, <span class="attribute">sample_num</span>=2),</span><br><span class="line">        <span class="attribute">out_channels</span>=256,</span><br><span class="line">        featmap_strides=[4, 8, 16, 32]),</span><br><span class="line">    <span class="attribute">bbox_head</span>=dict(</span><br><span class="line">        <span class="attribute">type</span>=<span class="string">'SharedFCBBoxHead'</span>,</span><br><span class="line">        <span class="attribute">num_fcs</span>=2,</span><br><span class="line">        <span class="attribute">in_channels</span>=256,</span><br><span class="line">        <span class="attribute">fc_out_channels</span>=1024,</span><br><span class="line">        <span class="attribute">roi_feat_size</span>=7,</span><br><span class="line">        <span class="attribute">num_classes</span>=81,</span><br><span class="line">        target_means=[0., 0., 0., 0.],</span><br><span class="line">        target_stds=[0.1, 0.1, 0.2, 0.2],</span><br><span class="line">        <span class="attribute">reg_class_agnostic</span>=<span class="literal">False</span>),</span><br><span class="line">    <span class="attribute">mask_roi_extractor</span>=dict(</span><br><span class="line">        <span class="attribute">type</span>=<span class="string">'SingleRoIExtractor'</span>,</span><br><span class="line">        <span class="attribute">roi_layer</span>=dict(type='RoIAlign', <span class="attribute">out_size</span>=14, <span class="attribute">sample_num</span>=2),</span><br><span class="line">        <span class="attribute">out_channels</span>=256,</span><br><span class="line">        featmap_strides=[4, 8, 16, 32]),</span><br><span class="line">    <span class="attribute">mask_head</span>=dict(</span><br><span class="line">        <span class="attribute">type</span>=<span class="string">'FCNMaskHead'</span>,</span><br><span class="line">        <span class="attribute">num_convs</span>=4,</span><br><span class="line">        <span class="attribute">in_channels</span>=256,</span><br><span class="line">        <span class="attribute">conv_out_channels</span>=256,</span><br><span class="line">        <span class="attribute">num_classes</span>=81))</span><br></pre></td></tr></table></figure></p><p><strong>注意</strong>：</p><blockquote><ul><li>上面的pretrained参数只是针对backbone部分的预训练权重，在mmdetection中除了支持pytorch的modelzoo之外，mmdetection也提供了部分预训练权重，具体可以查看mmcv/runner/checkpoint.py文件。</li><li>如果想对整个模型进行预训练，比如我使用coco数据集预训练模型去训练pascal voc，则可以在configs/*.py文件中load_from参数，来指定coco预训练模型权重文件路径。</li></ul></blockquote><h3 id="Backbone-amp-amp-neck"><a href="#Backbone-amp-amp-neck" class="headerlink" title="Backbone &amp;&amp; neck"></a><strong>Backbone &amp;&amp; neck</strong></h3><p>&#160; &#160; &#160; &#160;目前官方实现了backbone包括：resnet,resnext以及ssd_vgg。作者在resnet基础上还实现了dcn。至于neck部分目前只有FPN，而且对于two-stage方法，neck只能设置为FPN，对应代码实现部分在mmdet/models/necks/fpn.py。backbone和neck相关的配置参数如下：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">backbone</span>=dict(</span><br><span class="line">    <span class="attribute">type</span>=<span class="string">'ResNet'</span>,</span><br><span class="line">    <span class="attribute">depth</span>=50, </span><br><span class="line">    <span class="attribute">num_stages</span>=4,</span><br><span class="line">    out_indices=(0, 1, 2, 3),</span><br><span class="line">    <span class="attribute">frozen_stages</span>=1,  #由于stage1的feature map比较大，因此冻结住，训练阶段可以节省内存</span><br><span class="line">    <span class="attribute">style</span>=<span class="string">'pytorch'</span>,  #预训练模型载入来源，可以是caffe</span><br><span class="line">    <span class="attribute">dcn</span>=dict(         #dcn参数配置，如果仅仅使用普通resnet50，可以设置<span class="attribute">dcn</span>=None</span><br><span class="line">        <span class="attribute">modulated</span>=<span class="literal">False</span>,</span><br><span class="line">        <span class="attribute">deformable_groups</span>=1,</span><br><span class="line">        <span class="attribute">fallback_on_stride</span>=<span class="literal">False</span>),</span><br><span class="line">    stage_with_dcn=(<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>))，</span><br><span class="line"><span class="attribute">neck</span>=dict(                   </span><br><span class="line">    <span class="attribute">type</span>=<span class="string">'FPN'</span>,</span><br><span class="line">    in_channels=[256, 512, 1024, 2048], #对应于resnet的4个stage（C2,C3,C4,C5）输出通道数</span><br><span class="line">    <span class="attribute">out_channels</span>=256,                   #对应(P2,P3,P4,P5,P6)的输出通道数</span><br><span class="line">    <span class="attribute">num_outs</span>=5),                        #5个输出，对应P2-P6</span><br></pre></td></tr></table></figure></p><p>&#160; &#160; &#160; &#160;backbone部分代码实现部分在mmdet/models/backbones/*.py文件中。neck部分代码实现部分在mmdet/models/necks/fpn.py文件中。这两部分代码的实现不难，不予介绍，FPN结构如下图所示。值得注意的是，RetinaNet模型中neck部分也是采用的FPN，对应neck部分配置却不一样，RetinaNet模型中neck部分参数如下：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">neck</span>=dict(</span><br><span class="line">    <span class="attribute">type</span>=<span class="string">'FPN'</span>,</span><br><span class="line">    in_channels=[256, 512, 1024, 2048],</span><br><span class="line">    <span class="attribute">out_channels</span>=256,</span><br><span class="line">    <span class="attribute">start_level</span>=1,</span><br><span class="line">    <span class="attribute">add_extra_convs</span>=<span class="literal">True</span>,</span><br><span class="line">    <span class="attribute">num_outs</span>=5)</span><br></pre></td></tr></table></figure></p><p><img src="/img/fpn.png" alt><br>&#160; &#160; &#160; &#160;额外添加了start_level和add_extra_convs两个参数。start_level=1表示从C3开始，因为RetinaNet模型中FPN的输出并不是P2-P6这5个输出，在RetinaNet中FPN的输出采用的是P3-P7这5个输出。在maskrcnn的FPN结构中是直接通过对P5进行maxpool得到P6，而在RetinaNet中，P6通过对P5进行stride=2卷积操作得到的，而P7则是同样对P6进行stride=2卷积操作得到。这一部分的区别，可以看源码mmdet/models/necks/fpn.py文件中第72行。</p><h3 id="rpn-head"><a href="#rpn-head" class="headerlink" title="rpn_head"></a><strong>rpn_head</strong></h3><p>&#160; &#160; &#160; &#160;这一部分主要涉及到anchor的相关操作，在mmdet/models/anchor_heads文件夹下实现了多种anchor_heads，其中包括基类AnchorHead(实现在anchor_head.py文件中），RetinaNet模型相关的RetinaHead类(实现在retina_head.py文件中），two-stage方法的RPNHead类（实现在rpn_head.py文件中）以及SSD方法SSDHead（实现在ssd_head.py文件中）。后面三个类都继承自AnchorHead基类。在maskrcnn中使用的anchor_head是RPNHead类，对应配置参数如下：<br><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">rpn_head=dict(</span></span><br><span class="line">    <span class="attr">type='RPNHead',</span> <span class="comment"># anchor_head类型为RPNHead</span></span><br><span class="line">    <span class="attr">in_channels=256,</span> <span class="comment"># FPN中每个level的输出通道数，通常都为256</span></span><br><span class="line">    <span class="attr">feat_channels=256,</span> <span class="comment"># RPN的feature map的通道数</span></span><br><span class="line">    <span class="attr">anchor_scales=[8],</span> <span class="comment"># anchor的尺度，在使用FPN结构时，每个level只有一种尺度</span></span><br><span class="line">    <span class="attr">anchor_ratios=[0.5,</span> <span class="number">1.0</span>, <span class="number">2.0</span>], <span class="comment">#anchor比例：1:2,1:1,2:1这三种</span></span><br><span class="line">    <span class="attr">anchor_strides=[4,</span> <span class="number">8</span>, <span class="number">16</span>, <span class="number">32</span>, <span class="number">64</span>], <span class="comment">#对应于FPN中每个level的featuremap上每个pixel的感受野大小；</span></span><br><span class="line">    <span class="attr">target_means=[.0,</span> .<span class="number">0</span>, .<span class="number">0</span>, .<span class="number">0</span>],</span><br><span class="line">    <span class="attr">target_stds=[1.0,</span> <span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">1.0</span>],</span><br><span class="line">    <span class="attr">use_sigmoid_cls=True)</span> <span class="comment">#anchor前景背景分类采用sigmoid，在早期的fasterrcnn采用的是softmax,在mmdetection中没有实现softmax分类</span></span><br></pre></td></tr></table></figure></p><p>对应RPNHead类的实现如下：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RPNHead</span>(<span class="title">AnchorHead</span>): <span class="comment">#继承自AnchorHead类</span></span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, in_channels, **kwargs)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">super</span>(RPNHead, <span class="keyword">self</span>).__init_<span class="number">_</span>(<span class="number">2</span>, in_channels, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_layers</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.rpn_conv = nn.Conv2d(</span><br><span class="line">            <span class="keyword">self</span>.in_channels, <span class="keyword">self</span>.feat_channels, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.rpn_cls = nn.Conv2d(<span class="keyword">self</span>.feat_channels,</span><br><span class="line">                                 <span class="keyword">self</span>.num_anchors * <span class="keyword">self</span>.cls_out_channels, <span class="number">1</span>)</span><br><span class="line">                                 </span><br><span class="line">        <span class="keyword">self</span>.rpn_reg = nn.Conv2d(<span class="keyword">self</span>.feat_channels, <span class="keyword">self</span>.num_anchors * <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        normal_init(<span class="keyword">self</span>.rpn_conv, std=<span class="number">0</span>.<span class="number">01</span>)</span><br><span class="line">        normal_init(<span class="keyword">self</span>.rpn_cls, std=<span class="number">0</span>.<span class="number">01</span>)</span><br><span class="line">        normal_init(<span class="keyword">self</span>.rpn_reg, std=<span class="number">0</span>.<span class="number">01</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_single</span><span class="params">(<span class="keyword">self</span>, x)</span></span>:  <span class="comment">#表示输入FPN的一个level，比如P6输入，P6：1*256*13*16</span></span><br><span class="line">        x = <span class="keyword">self</span>.rpn_conv(x)      <span class="comment">#conv不改变featuremap大小，且feat_channels=256，故x:1*256*13*16</span></span><br><span class="line">        x = F.relu(x, inplace=True)</span><br><span class="line">        rpn_cls_score = <span class="keyword">self</span>.rpn_cls(x)  <span class="comment">#对应于anchor的前景背景分类 1*3*13*16</span></span><br><span class="line">        rpn_bbox_pred = <span class="keyword">self</span>.rpn_reg(x)  <span class="comment">#对应于anchor的回归 1*12*13*16</span></span><br><span class="line">        <span class="keyword">return</span> rpn_cls_score, rpn_bbox_pred  </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(<span class="keyword">self</span>, cls_scores, bbox_preds, gt_bboxes, img_metas, cfg)</span></span><span class="symbol">:</span></span><br><span class="line"></span><br><span class="line">        losses = <span class="keyword">super</span>(RPNHead, <span class="keyword">self</span>).loss(cls_scores, bbox_preds, gt_bboxes,</span><br><span class="line">                                           None, img_metas, cfg)</span><br><span class="line">        <span class="keyword">return</span> dict(</span><br><span class="line">            loss_rpn_cls=losses[<span class="string">'loss_cls'</span>], loss_rpn_reg=losses[<span class="string">'loss_reg'</span>])</span><br></pre></td></tr></table></figure></p><p>RPNHead类中实现的函数只是对AnchorHead类中部分函数的重写。由于RPNHead继承自AnchorHead类，在实际训练和测试中，直接调用的是还是继承自AnchorHead类的forward的函数。在RPN中通过在13x16的featuremap上生成13x16x3个anchors（注意，这里使用的是FPN结构，在不同level上，每个点只生成3个anchor，因此生成的anchors数是13x16x3）。经过后处理选择出proposal进行后续的bbox_roi_extractor。其中后处理操作包括：</p><blockquote><ul><li>删除背景框（13x16x3）；</li><li>NMS处理去除重合度大的框中分数低的那个；</li><li>取分数最大的K（top-k）个框用于后续bbox_roi_extractor；</li></ul></blockquote><p>这部分比较复杂，还涉及到一些RPN损失函数，比如smoth_l1_loss，focal loss以及delta2box的转换等等操作，直接看源码比较容易理解。关于Anchors这部分，<a href="https://www.cnblogs.com/q735613050/p/10631619.html" target="_blank" rel="noopener">推荐博客</a>讲解的比较清楚，推荐看。</p><h3 id="ROI-extractor"><a href="#ROI-extractor" class="headerlink" title="ROI extractor"></a><strong>ROI extractor</strong></h3><p>&#160; &#160; &#160; &#160;在maskrcnn模型中，这部分涉及到box的操作以及mask的操作：bbox_roi_extractor和mask_roi_extractor。相关的配置如下：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">bbox_roi_extractor</span>=dict(</span><br><span class="line">    <span class="attribute">type</span>=<span class="string">'SingleRoIExtractor'</span>,</span><br><span class="line">    <span class="attribute">roi_layer</span>=dict(type='RoIAlign', <span class="attribute">out_size</span>=7, <span class="attribute">sample_num</span>=2), #roi输出大小为7<span class="number">*7</span></span><br><span class="line">    <span class="attribute">out_channels</span>=256, #roi输出通道数</span><br><span class="line">    featmap_strides=[4, 8, 16, 32]),  #对应4个level的proposal</span><br><span class="line"><span class="attribute">mask_roi_extractor</span>=dict(</span><br><span class="line">    <span class="attribute">type</span>=<span class="string">'SingleRoIExtractor'</span>,</span><br><span class="line">    <span class="attribute">roi_layer</span>=dict(type='RoIAlign', <span class="attribute">out_size</span>=14, <span class="attribute">sample_num</span>=2),</span><br><span class="line">    <span class="attribute">out_channels</span>=256,</span><br><span class="line">    featmap_strides=[4, 8, 16, 32]),</span><br></pre></td></tr></table></figure></p><p>在上面的配置参数可以看出，bbox_roi_extractor和mask_roi_extractor的roi_layer层类型都是SingleRoIExtractor。这个层的实现在mmdet/models/roi_extractors/single_level.py文件中。在FPN+RPN分别输出4个不同level的proposal，因此或生成4个对应的roi_layer，每个roi_layer输出的特征通道数out_channels=256。对于box，每个roi的大小为7x7；对于mask，每个roi的输出大小14x14。</p><p>经过ROI extractor操作之后就是bbox的回归和分类以及mask的生成分类。这部分的配置参数如下：<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">bbox_head</span>=dict(</span><br><span class="line">    <span class="attribute">type</span>=<span class="string">'SharedFCBBoxHead'</span>,</span><br><span class="line">    <span class="attribute">num_fcs</span>=2, #2个全连接层</span><br><span class="line">    <span class="attribute">in_channels</span>=256, #roi输出特征通道数</span><br><span class="line">    <span class="attribute">fc_out_channels</span>=1024, #2个全连接层的节点数</span><br><span class="line">    <span class="attribute">roi_feat_size</span>=7,</span><br><span class="line">    <span class="attribute">num_classes</span>=81, #81个类别输出（80目标+1背景）</span><br><span class="line">    target_means=[0., 0., 0., 0.],</span><br><span class="line">    target_stds=[0.1, 0.1, 0.2, 0.2],</span><br><span class="line">    <span class="attribute">reg_class_agnostic</span>=<span class="literal">False</span>),</span><br><span class="line"><span class="attribute">mask_head</span>=dict(</span><br><span class="line">    <span class="attribute">type</span>=<span class="string">'FCNMaskHead'</span>,</span><br><span class="line">    <span class="attribute">num_convs</span>=4, #4个卷积层</span><br><span class="line">    <span class="attribute">in_channels</span>=256, #roi输出特征通道数</span><br><span class="line">    <span class="attribute">conv_out_channels</span>=256, #4个卷积层的通道数均为256</span><br><span class="line">    <span class="attribute">num_classes</span>=81))    #mask的类别是81，</span><br></pre></td></tr></table></figure></p><p>这一部分的实现对应于下图，该图截图自maskrcnn原文，如下图所示<br><img src="/img/head_arch.png" alt><br>值得注意的是，在上面mask branch中倒数第二层应该是28x28x256的上采样层,在上面的参数配置中没有体现，但在mmdetection的代码有实现，可以看mmdet/models/mask_heads/fcn_mask_head.py中FCNMaskHead默认的上采样层类型是deconv，层的实现在第58行。</p><h3 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a><strong>模型构建</strong></h3><p>&#160; &#160; &#160; &#160;上面根据模型配置文件结构介绍了构建maskrcnn模型的4个子模块，如何将这四个子模块拼接构成一个完整的maskrcnn模型？在mmdetecion中实例化一个具体模型，需要在mmdet/models/detectors/文件夹下中实现相应的模型，比如maskrcnn模型的实现在该文件夹下mask_rcnn.py文件中，实现如下：<br><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">from <span class="string">.two_stage</span> import TwoStageDetector</span><br><span class="line">from <span class="string">..registry</span> import DETECTORS</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@DETECTORS.register_module</span><br><span class="line">class MaskRCNN<span class="params">(TwoStageDetector)</span>:</span><br><span class="line"></span><br><span class="line">    def __init__<span class="params">(self,</span></span><br><span class="line"><span class="params">                 backbone,</span></span><br><span class="line"><span class="params">                 neck,</span></span><br><span class="line"><span class="params">                 rpn_head,</span></span><br><span class="line"><span class="params">                 bbox_roi_extractor,</span></span><br><span class="line"><span class="params">                 bbox_head,</span></span><br><span class="line"><span class="params">                 mask_roi_extractor,</span></span><br><span class="line"><span class="params">                 mask_head,</span></span><br><span class="line"><span class="params">                 train_cfg,</span></span><br><span class="line"><span class="params">                 test_cfg,</span></span><br><span class="line"><span class="params">                 <span class="attr">pretrained</span>=None)</span>:</span><br><span class="line">        super<span class="params">(MaskRCNN, self)</span><span class="string">.__init__</span><span class="params">(</span></span><br><span class="line"><span class="params">            <span class="attr">backbone</span>=backbone,</span></span><br><span class="line"><span class="params">            <span class="attr">neck</span>=neck,</span></span><br><span class="line"><span class="params">            <span class="attr">rpn_head</span>=rpn_head,</span></span><br><span class="line"><span class="params">            <span class="attr">bbox_roi_extractor</span>=bbox_roi_extractor,</span></span><br><span class="line"><span class="params">            <span class="attr">bbox_head</span>=bbox_head,</span></span><br><span class="line"><span class="params">            <span class="attr">mask_roi_extractor</span>=mask_roi_extractor,</span></span><br><span class="line"><span class="params">            <span class="attr">mask_head</span>=mask_head,</span></span><br><span class="line"><span class="params">            <span class="attr">train_cfg</span>=train_cfg,</span></span><br><span class="line"><span class="params">            <span class="attr">test_cfg</span>=test_cfg,</span></span><br><span class="line"><span class="params">            <span class="attr">pretrained</span>=pretrained)</span></span><br></pre></td></tr></table></figure></p><p>从上面可以看出maskrcnn模型的构建继承自TwoStageDetector类。TwoStageDetector类是一个通用的two-stage目标检测方法的基类，具体实现如下：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line">from .base import BaseDetector</span><br><span class="line">from .test_mixins import RPNTestMixin, BBoxTestMixin, MaskTestMixin</span><br><span class="line">from .. import builder</span><br><span class="line">from ..registry import DETECTORS</span><br><span class="line">from mmdet.core import bbox2roi, bbox2result, build_assigner, build_sampler</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@DETECTORS.register_module</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoStageDetector</span>(<span class="title">BaseDetector</span>, <span class="title">RPNTestMixin</span>, <span class="title">BBoxTestMixin</span>,</span></span><br><span class="line">                       MaskTestMixin)<span class="symbol">:</span></span><br><span class="line"><span class="comment">#继承了四个基类BaseDetector（主要是一些模型常见训练，单多尺度测试相关操作的虚函数），RPNTestMixin，BBoxTestMixin，MaskTestMixin主要是模型单尺度/多尺度测试</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 backbone,</span></span></span><br><span class="line"><span class="function"><span class="params">                 neck=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 rpn_head=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 bbox_roi_extractor=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 bbox_head=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 mask_roi_extractor=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 mask_head=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 train_cfg=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 test_cfg=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 pretrained=None)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">super</span>(TwoStageDetector, <span class="keyword">self</span>).__init_<span class="number">_</span>()</span><br><span class="line">        <span class="keyword">self</span>.backbone = builder.build_backbone(backbone)  <span class="comment">#根据配置文件构架backbone</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> neck is <span class="keyword">not</span> <span class="symbol">None:</span></span><br><span class="line">            <span class="keyword">self</span>.neck = builder.build_neck(neck)        <span class="comment">#根据配置文件构架neck,这里主要是FPN</span></span><br><span class="line">        <span class="symbol">else:</span></span><br><span class="line">            raise NotImplementedError           </span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> rpn_head is <span class="keyword">not</span> <span class="symbol">None:</span></span><br><span class="line">            <span class="keyword">self</span>.rpn_head = builder.build_head(rpn_head) <span class="comment">#根据配置文件构架rpn</span></span><br><span class="line">        <span class="comment">#根据配置文件构架bbox_roi_extractor,bbox_head</span></span><br><span class="line">        <span class="keyword">if</span> bbox_head is <span class="keyword">not</span> <span class="symbol">None:</span></span><br><span class="line">            <span class="keyword">self</span>.bbox_roi_extractor = builder.build_roi_extractor(</span><br><span class="line">                bbox_roi_extractor)</span><br><span class="line">            <span class="keyword">self</span>.bbox_head = builder.build_head(bbox_head)</span><br><span class="line">        <span class="comment">#根据配置文件构架mask_roi_extractor,mask_head</span></span><br><span class="line">        <span class="keyword">if</span> mask_head is <span class="keyword">not</span> <span class="symbol">None:</span></span><br><span class="line">            <span class="keyword">self</span>.mask_roi_extractor = builder.build_roi_extractor(</span><br><span class="line">                mask_roi_extractor)</span><br><span class="line">            <span class="keyword">self</span>.mask_head = builder.build_head(mask_head)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.train_cfg = train_cfg</span><br><span class="line">        <span class="keyword">self</span>.test_cfg = test_cfg</span><br><span class="line">        <span class="comment">#模型参数权重初始化</span></span><br><span class="line">        <span class="keyword">self</span>.init_weights(pretrained=pretrained)</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">with_rpn</span><span class="params">(<span class="keyword">self</span>)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">return</span> hasattr(<span class="keyword">self</span>, <span class="string">'rpn_head'</span>) <span class="keyword">and</span> <span class="keyword">self</span>.rpn_head is <span class="keyword">not</span> None</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(<span class="keyword">self</span>, pretrained=None)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">super</span>(TwoStageDetector, <span class="keyword">self</span>).init_weights(pretrained)</span><br><span class="line">        <span class="keyword">self</span>.backbone.init_weights(pretrained=pretrained)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">with_neck:</span></span><br><span class="line">            <span class="keyword">if</span> isinstance(<span class="keyword">self</span>.neck, nn.Sequential)<span class="symbol">:</span></span><br><span class="line">                <span class="keyword">for</span> m <span class="keyword">in</span> <span class="keyword">self</span>.<span class="symbol">neck:</span></span><br><span class="line">                    m.init_weights()</span><br><span class="line">            <span class="symbol">else:</span></span><br><span class="line">                <span class="keyword">self</span>.neck.init_weights()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">with_rpn:</span></span><br><span class="line">            <span class="keyword">self</span>.rpn_head.init_weights()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">with_bbox:</span></span><br><span class="line">            <span class="keyword">self</span>.bbox_roi_extractor.init_weights()</span><br><span class="line">            <span class="keyword">self</span>.bbox_head.init_weights()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">with_mask:</span></span><br><span class="line">            <span class="keyword">self</span>.mask_roi_extractor.init_weights()</span><br><span class="line">            <span class="keyword">self</span>.mask_head.init_weights()</span><br><span class="line">    <span class="comment">#提取特征（包括经过bakcbone,neck）</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extract_feat</span><span class="params">(<span class="keyword">self</span>, img)</span></span><span class="symbol">:</span></span><br><span class="line">        x = <span class="keyword">self</span>.backbone(img)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">with_neck:</span></span><br><span class="line">            x = <span class="keyword">self</span>.neck(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_train</span><span class="params">(<span class="keyword">self</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      img,</span></span></span><br><span class="line"><span class="function"><span class="params">                      img_meta,</span></span></span><br><span class="line"><span class="function"><span class="params">                      gt_bboxes,</span></span></span><br><span class="line"><span class="function"><span class="params">                      gt_bboxes_ignore,</span></span></span><br><span class="line"><span class="function"><span class="params">                      gt_labels,</span></span></span><br><span class="line"><span class="function"><span class="params">                      gt_masks=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                      proposals=None)</span></span><span class="symbol">:</span></span><br><span class="line">        x = <span class="keyword">self</span>.extract_feat(img)</span><br><span class="line">      </span><br><span class="line">        losses = dict()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># RPN 前向计算和损失</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">with_rpn:</span></span><br><span class="line">            rpn_outs = <span class="keyword">self</span>.rpn_head(x)</span><br><span class="line">            rpn_loss_inputs = rpn_outs + (gt_bboxes, img_meta,</span><br><span class="line">                                          <span class="keyword">self</span>.train_cfg.rpn)</span><br><span class="line">            rpn_losses = <span class="keyword">self</span>.rpn_head.loss(*rpn_loss_inputs)</span><br><span class="line">            losses.update(rpn_losses)</span><br><span class="line"></span><br><span class="line">            proposal_inputs = rpn_outs + (img_meta, <span class="keyword">self</span>.test_cfg.rpn)</span><br><span class="line">            proposal_list = <span class="keyword">self</span>.rpn_head.get_bboxes(*proposal_inputs)</span><br><span class="line">        <span class="symbol">else:</span></span><br><span class="line">            proposal_list = proposals</span><br><span class="line"></span><br><span class="line">        <span class="comment"># assign gts and sample proposals</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.with_bbox <span class="keyword">or</span> <span class="keyword">self</span>.<span class="symbol">with_mask:</span></span><br><span class="line">            bbox_assigner = build_assigner(<span class="keyword">self</span>.train_cfg.rcnn.assigner)</span><br><span class="line">            bbox_sampler = build_sampler(</span><br><span class="line">                <span class="keyword">self</span>.train_cfg.rcnn.sampler, context=<span class="keyword">self</span>)</span><br><span class="line">            num_imgs = img.size(<span class="number">0</span>)</span><br><span class="line">            sampling_results = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(num_imgs)<span class="symbol">:</span></span><br><span class="line">                assign_result = bbox_assigner.assign(</span><br><span class="line">                    proposal_list[i], gt_bboxes[i], gt_bboxes_ignore[i],</span><br><span class="line">                    gt_labels[i])</span><br><span class="line">                sampling_result = bbox_sampler.sample(</span><br><span class="line">                    assign_result,</span><br><span class="line">                    proposal_list[i],</span><br><span class="line">                    gt_bboxes[i],</span><br><span class="line">                    gt_labels[i],</span><br><span class="line">                    feats=[lvl_feat[i][None] <span class="keyword">for</span> lvl_feat <span class="keyword">in</span> x])</span><br><span class="line">                sampling_results.append(sampling_result)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># bbox head forward and loss</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">with_bbox:</span></span><br><span class="line">            rois = bbox2roi([res.bboxes <span class="keyword">for</span> res <span class="keyword">in</span> sampling_results])</span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span> a more flexible way to decide which feature maps to use</span></span><br><span class="line">            bbox_feats = <span class="keyword">self</span>.bbox_roi_extractor(</span><br><span class="line">                x[<span class="symbol">:self</span>.bbox_roi_extractor.num_inputs], rois)</span><br><span class="line">            cls_score, bbox_pred = <span class="keyword">self</span>.bbox_head(bbox_feats)</span><br><span class="line"></span><br><span class="line">            bbox_targets = <span class="keyword">self</span>.bbox_head.get_target(</span><br><span class="line">                sampling_results, gt_bboxes, gt_labels, <span class="keyword">self</span>.train_cfg.rcnn)</span><br><span class="line">            loss_bbox = <span class="keyword">self</span>.bbox_head.loss(cls_score, bbox_pred,</span><br><span class="line">                                            *bbox_targets)</span><br><span class="line">            losses.update(loss_bbox)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># mask head forward and loss</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">with_mask:</span></span><br><span class="line">            pos_rois = bbox2roi([res.pos_bboxes <span class="keyword">for</span> res <span class="keyword">in</span> sampling_results])</span><br><span class="line">            mask_feats = <span class="keyword">self</span>.mask_roi_extractor(</span><br><span class="line">                x[<span class="symbol">:self</span>.mask_roi_extractor.num_inputs], pos_rois)</span><br><span class="line">            mask_pred = <span class="keyword">self</span>.mask_head(mask_feats)</span><br><span class="line"></span><br><span class="line">            mask_targets = <span class="keyword">self</span>.mask_head.get_target(</span><br><span class="line">                sampling_results, gt_masks, <span class="keyword">self</span>.train_cfg.rcnn)</span><br><span class="line">            pos_labels = torch.cat(</span><br><span class="line">                [res.pos_gt_labels <span class="keyword">for</span> res <span class="keyword">in</span> sampling_results])</span><br><span class="line">            loss_mask = <span class="keyword">self</span>.mask_head.loss(mask_pred, mask_targets,</span><br><span class="line">                                            pos_labels)</span><br><span class="line">            losses.update(loss_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> losses</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">simple_test</span><span class="params">(<span class="keyword">self</span>, img, img_meta, proposals=None, rescale=False)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="string">""</span><span class="string">"Test without augmentation."</span><span class="string">""</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">aug_test</span><span class="params">(<span class="keyword">self</span>, imgs, img_metas, rescale=False)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="string">""</span><span class="string">"Test with augmentations.</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&amp;#160; &amp;#160; &amp;#160; &amp;#160;在mmdetection中，实现了许多现有two-stage目标检测方法以及one-stage目标检测方法，且包含完整的数据载入、模型构建、模型训练以及模型测试部分的源码。因此非常适合在此基础上扩展实现其他目标检测算法。关于数据载入、模型训练以及模型测试部分的源码，在上一篇博客中有详细介绍。后期打算在mmdetection基础上添加其他算法。因此了解模型构建这部分源码十分重要。在mmdetecion官方&lt;a href=&quot;https://github.com/open-mmlab/mmdetection&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;github&lt;/a&gt;介绍模型主要由四部分组成：backbone, neck, head, 以及ROI extractor。&lt;br&gt;
    
    </summary>
    
      <category term="project" scheme="https://nicehuster.github.io/categories/project/"/>
    
    
      <category term="object detection" scheme="https://nicehuster.github.io/tags/object-detection/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：Detecting Objects as Paired Keypoints(CornerNet)</title>
    <link href="https://nicehuster.github.io/2019/04/23/0047-CornerNet/"/>
    <id>https://nicehuster.github.io/2019/04/23/0047-CornerNet/</id>
    <published>2019-04-23T11:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> Hei Law,Princeton, ECCV2018<br><strong>代码链接：</strong> <a href="https://github.com/princeton-vl/CornerNet" target="_blank" rel="noopener">https://github.com/princeton-vl/CornerNet</a><br><strong>整体框架：</strong> 这篇文章应该是首次将目标检测的框点位问题转化为一对顶点（左上角和右下角）的定位问题，给人眼前一亮有木有。通过将目标检测问题转换为点的定位问题消除了anchor-based方法中的对anchor的依赖以及anchor相关的所有问题而且在coco上取得42.2%mAP不凡的成绩。相比于one-stage方法有较明显的提高。<br><img src="/img/cornernet.png" alt><br><a id="more"></a></p><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p>CornerNet认为基于anchor-based方法最为明显的缺点是：</p><blockquote><ul><li>密集采样anchor boxes，导致存在大量的样本不平衡问题；</li><li>anchor boxes带来大量超参数，比如anchor boxes数量，尺度，比率等影响模型训练推断速度；</li></ul></blockquote><p>&#160; &#160; &#160; &#160;论文提出CornerNet,作为一种新的one-stage方法，消除了anchor boxes，将目标检测转化为一对顶点（左上角，右下角）的定位问题。即使用单一卷积模型生成heatmap和embedding vector：所有目标的左上角heatmap和embedding vector以及所有目标的右下角heatmap和embedding vector。如上图表示的是CornerNet框架。<br>&#160; &#160; &#160; &#160;作者认为相比于检测框中心或proposal的方法，检测角点有效的原因在于：1）目标框中心难以确定因为取决于目标的四个边，而角点只依赖于目标的两个边，相对来说定位更容易；2）角点提供了更为有效的离散边界空间，对角点而言为$O(wh)$，而对anchor boxes而言为$O(w^2h^2)$。</p><h3 id="CornerNet"><a href="#CornerNet" class="headerlink" title="CornerNet"></a>CornerNet</h3><p><img src="/img/cornernet_overview.png" alt><br>&#160; &#160; &#160; &#160;上图表示的是CornerNet的模型结构。整个模型结构包括三个部分：Hourglass Network，Top-left Corner Prediction Module以及Bottom-right Prediction Module。Hourglass Network是人体姿态估计的典型架构。该网络前半部分通过卷积池化处理，进行多次下采样操作，获得一些分辨率较低的特征，从而使计算复杂度降低。下采样不断变小feature map，后面则通过上采样增大feature map恢复到输入图像大小，上采样操作使得图像的分辨率增高，同时更有能力预测物体的准确位置。</p><h4 id="Detecting-Corners"><a href="#Detecting-Corners" class="headerlink" title="Detecting Corners"></a>Detecting Corners</h4><p><img src="/img/detecting_corner.png" alt><br>&#160; &#160; &#160; &#160;<strong>Prediction Module的第一个输出是heatmaps</strong>，论文模型输出的heatmaps包含C个通道（C是目标的类别，没有背景类别），每个channel是二进制掩膜，表示相应类别的角点位置。对于每个角点，只有一个ground-truth，其他位置都是负样本。在训练过程，模型减少负样本，在每个ground-truth角点设定半径r区域内都是正样本，这是因为落在半径r区域内的顶点依然可以生成有效的边界定位框，论文中设置IoU=0.7。如上图所示。对于角点定位损失函数采用的是focal loss的形式。如下：<br><img src="/img/corner_focal_loss.png" alt><br>&#160; &#160; &#160; &#160;其中，$p_{cij}$表示在位置(i,j)，对于类别c的预测heatmaps。$y_{cij}$表示对应的ground-truth。<br>&#160; &#160; &#160; &#160;<strong>Prediction Module的第二个输出是offsets</strong>，由于存在下采样，模型生成的heatmap大小相比原始输入图像小。原始图像上点$(i,j)$映射到heatmaps上的位置变成$(\lfloor x/n \rfloor,\lfloor y/n \rfloor)$。将heatmaps上的点重新映射到原始图像上存在量化映射误差，导致映射的角点位置出现偏移。这种偏移会严重影响小目标IOU计算。这个问题和mask-rcnn中提出ROIAlign的出发点是一样的，也是为了解决存在的量化误差问题。论文提出偏移损失函数，用于微调检测角点和ground-truth偏移。<br><img src="/img/cornernet_offset.png" alt></p><h4 id="Grouping-Corners"><a href="#Grouping-Corners" class="headerlink" title="Grouping Corners"></a>Grouping Corners</h4><p>&#160; &#160; &#160; &#160;<strong>Prediction Module的第二个输出是embeddings</strong>，输入图像中会有多个目标，相应生成多个目标的左上角和右下角角点。如何确定顶点A和顶点B是属于同一个目标呢。作者在论文中引入了Associative Embedding思想。网络在训练的时候，两个branch在分别输出角点的heatmap的同时也分别输出对应角点的embedding vector。如果这两个角点属于同一个目标，则对应embedding vector的距离最小。这样通过角点距离大小判断两个角点是否属于同一个目标。指定etk表示目标k的左上角顶点，ebk表示右下角顶点。作者设计了两类损失函数，Lpull损失函数使同一目标的顶点进行分组，Lpush损失函数用于分离不同目标的顶点。<br><img src="/img/group_corner_loss.png" alt></p><h4 id="Corner-Pooling"><a href="#Corner-Pooling" class="headerlink" title="Corner Pooling"></a>Corner Pooling</h4><p><img src="/img/corner_pool_fig.png" alt><br>&#160; &#160; &#160; &#160;为什么要引入corner pooling呢？如上图所示，一个目标通常并没有明显的边界，没有规律可循。因此对于定位目标的左上角和右下角顶点非常困难。考虑到左上角角点的右边有目标顶端的特征信息（第一张图的头顶），左上角角点的下边有目标左侧的特征信息（第一张图的手），因此如果左上角角点经过池化操作后能有这两个信息，那么就有利于该点的预测，这就有了corner pooling。对应操作示意图如下图所示。<br><img src="/img/corner_pool_fig1.png" alt><br>上图展示的是对左上角的角点进行corner pooling操作示例图，具体的例子可以看下图。<br><img src="/img/corner_pool_fig2.png" alt><br>过程比较容易理解，一个从下至上的方向逐元素取每列的最大值；一个是从右到左逐元素取每行的最大值，然后进行相加。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>&#160; &#160; &#160; &#160;作者优化CornerNet时损失函数的设计如下：<br><img src="/img/cornernet_loss.png" alt><br>&#160; &#160; &#160; &#160;论文做了许多ablation实验，其中包括Corner pooling，ground-truth的半径设置影响，Hourglass网络等实验对比。下表展示的是与其他方法的实验对比。<br><img src="/img/cornernet_res.png" alt><br>&#160; &#160; &#160; &#160;可以看出作者主要和one-stage方法进行了对比。可以看出和YOLO类方法，SSD类方法相比效果提高显著。但是却没和这类方法进行速度的比较。论文也有提及网络推断速度，在Titan x GPU上每张图片的推断时间244ms。为了改善这个速度问题，最近新出了<a href="https://arxiv.org/pdf/1904.08900.pdf" target="_blank" rel="noopener">CornerNet-Lites</a>，打算这几天拜读一下。最后放几张论文中在coco上的检测结果。<br><img src="/img/cornernet_res_pic.png" alt></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; Hei Law,Princeton, ECCV2018&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt; &lt;a href=&quot;https://github.com/princeton-vl/CornerNet&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/princeton-vl/CornerNet&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体框架：&lt;/strong&gt; 这篇文章应该是首次将目标检测的框点位问题转化为一对顶点（左上角和右下角）的定位问题，给人眼前一亮有木有。通过将目标检测问题转换为点的定位问题消除了anchor-based方法中的对anchor的依赖以及anchor相关的所有问题而且在coco上取得42.2%mAP不凡的成绩。相比于one-stage方法有较明显的提高。&lt;br&gt;&lt;img src=&quot;/img/cornernet.png&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://nicehuster.github.io/categories/paper-reading/"/>
    
    
      <category term="object detection" scheme="https://nicehuster.github.io/tags/object-detection/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：Fully Convolutional One-Stage Object Detection(FCOS)</title>
    <link href="https://nicehuster.github.io/2019/04/15/0046-FCOS/"/>
    <id>https://nicehuster.github.io/2019/04/15/0046-FCOS/</id>
    <published>2019-04-15T11:13:39.000Z</published>
    <updated>2020-09-13T06:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> Zhi Tian,Adelaide<br><strong>代码链接：</strong><a href="https://github.com/tianzhi0549/FCOS" target="_blank" rel="noopener">https://github.com/tianzhi0549/FCOS</a><br><strong>整体框架：</strong>提出了一种全卷积one-stage目标检测算法（FCOS: Fully Convolutional One-Stage Object Detection）。通过逐像素的方式预测来解决目标检测问题。类似于语义分割。以往sota方法比如RetinaNet,Faster RCNN,SSD和YOLO等等都是属于一种anchor-based方法,都依赖与预定义的锚框（anchor boxes）。与之相比，FCOS则属于anchor-free方法，通过消除预定义的anchor,避免了大量与anchor相关的复杂计算。例如在训练过程做计算重叠且显著降低现存，更重要的是避免了对检测性能敏感的与anchor有关的超参数。<br><img src="/img/fcos.png" alt><br><a id="more"></a><br>上图表示的是FCOS的网络结构图，从上面的结构可以看出，FCOS由<strong>backbone</strong>+<strong>FPN</strong>+<strong>classification+Regression</strong>组成。整个网络结构比较简单，与RetinaNet比较相似，因为作者就是在RetinaNet的基础上进行改进的。在了解FCOS算法前，有必要了解一下作者的出发点，即anchor-based方法存在一些缺点：</p><blockquote><ul><li>anchor超参数（比如base size,scale,aspect ratio等）影响检测性能；</li><li>anchor预先定义的scale和aspect ratio无法很好的适应目标形状变化，尤其是小目标；</li><li>为了实现高recall，会密集采样anchor，导致出现正负样本不平衡的问题；</li><li>大量的anchor在计算IOU的时候极大地增加了计算量和计算显存；</li></ul></blockquote><p>对于anchor-based方法，最著名的应属YOLOv1算法，这是该算法的recall太低，无太大实用价值，所以在其基础上提出了基于anchor的YOLOv2算法。本文提出的算法主要由包括三点：</p><h3 id="逐像素回归"><a href="#逐像素回归" class="headerlink" title="逐像素回归"></a>逐像素回归</h3><p>在YOLOv1中虽然也是使用的anchor-free的策略，但是YOLOv1只预测目标中心点附近的框回归，预测的框少了，自然召回率低。与之不同的是，本文提出的FCOS则是对目标框中的所有点来预测bbox。<br><img src="/img/fcos_reg_.png" alt><br><img src="/img/fcos_reg.png" alt><br>如上图所示，FCOS在进行目标框回归的时候，是通过计算框中的点到四个边长之间的距离进行回归。当然，不可避免会出现一个点落在两个目标框中,如上图右边所示，这种样本作者称为ambiguous sample，作者在这里简单地选择小目标的框作为其回归目标。作者后面也提到了为了解决这种问题采用了基于FPN的多层次预测来解决中这个问题。训练损失函数如下：<br><img src="/img/fcos_reg_loss.png" alt><br>其中，第一项表示focal loss，第二项表示的IOU loss（这个不是很清楚，后续看代码了解）。<br>由于FCOS算法是基于目标物体框中的点进行逐像素回归的，因此执行回归的目标都是正样本，所以作者使用了exp()函数将回归目标进行拉伸，我个人认为此操作是为了最终的特征空间更大，辨识度更强。作者最后提及到，逐像素回归预测的方式除了能够带来更多的框以外，更重要的是利用了尽可能多的前景样本来训练回归器，而传统的基于anchor的检测器，只考虑具有足够高的IOU的anchor box作为正样本。作者认为，这可能是FCOS优于基于anchor的同类检测器的原因之一。</p><h3 id="基于FPN多层次预测"><a href="#基于FPN多层次预测" class="headerlink" title="基于FPN多层次预测"></a>基于FPN多层次预测</h3><p>在CNN中，大的降采样（比如最后一层的feature map往往为16x）容易导致较低的recall。对于anchor-based方法来说，可以通过降低IOU阈值来获取更多的正样本进行弥补以提高recall。对于FCOS这种anchor-free方法而言，recall肯定比前者更低，因为，由于stride过大，在最后的feature map上很难找到目标的位置编码。然而作者发现，在没有使用FPN的情况下，FCOS依然可以达到95.55%的recall，一点也不逊色于RetinaNet。如上第一张网络结构图所示，作者使用了{$P_3$,$P_4$,$P_5$,$P_6$,$P_7$}六个尺度的feature map。对应的stride分别为8,32,64,128。<br>与anchor-based方法类似,FCOS也是采用分层级的方式来逐像素的预测回归。在<a href="https://arxiv.org/pdf/1904.01355v2.pdf" target="_blank" rel="noopener">论文</a>中介绍的比较清楚，这里直接截图如下所示：<br><img src="/img/fcos_fpn.png" alt><br>大致意思就是如果当前像素点的四个变量的回归值的最大值满足max(l<em>,t</em>,r<em>,b</em>)大于mi-1小于mi，才对该点进行回归。这样就实现了不同特征层回归不同的目标尺度范围。此外，作者认为在不同的特征层使用相同的输出激活函数是不合理的，因此作者没有使用标准的exp(x),而是使用了exp($xs_i$),其中s为可训练的标量。通过si来自动调整不同层级特征的指数函数的基数，提高检测性能。</p><h3 id="center-ness"><a href="#center-ness" class="headerlink" title="center-ness"></a>center-ness</h3><p>作者发现，即使使用了FPN后，FCOS和RetinaNet相比，还是存在2%的差距。作者通过观察发现，这个主要是因为FCOS在远离目标中心预测了许多低质量的框。基于此，作者提出了一个简单而有效的策略center-ness来抑制这些低质量检测到的边界框，且该策略不引入任何超参数。<br><img src="/img/fcos_centerness.png" alt><br>如上图所示，center-ness策略在每一个层级预测中添加了一个分支，该分支与分类并行，相当于给网络添加了一个损失，而该损失保证了预测的边界框尽可能的靠近中心。该损失的公式如下，其中l，r，t，b表示的为如下图左图中所示的预测值。<br><img src="/img/fcos_centerness_loss.png" alt><br>而该策略之所以能够有效，主要是在训练的过程中我们会约束上述公式中的值，使得其接近于0，这就导致上图中蓝色框中的短边能够向黄边靠近，使得分布在目标位置边缘的低质量框能够尽可能的靠近中心。这样的话，在最终使用该网络的过程中，非极大值抑制(NMS)就可以滤除这些低质量的边界框，提高检测性能。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>作者做了许多ablation的实验和RetinaNet进行对比。图表众多，这里主要列一下最终的实验对比。<br><img src="/img/fcos_result.png" alt><br>最后，作者开源了本文相关的代码以及训练模型，打算后期将代码阅读一遍。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; Zhi Tian,Adelaide&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/tianzhi0549/FCOS&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/tianzhi0549/FCOS&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体框架：&lt;/strong&gt;提出了一种全卷积one-stage目标检测算法（FCOS: Fully Convolutional One-Stage Object Detection）。通过逐像素的方式预测来解决目标检测问题。类似于语义分割。以往sota方法比如RetinaNet,Faster RCNN,SSD和YOLO等等都是属于一种anchor-based方法,都依赖与预定义的锚框（anchor boxes）。与之相比，FCOS则属于anchor-free方法，通过消除预定义的anchor,避免了大量与anchor相关的复杂计算。例如在训练过程做计算重叠且显著降低现存，更重要的是避免了对检测性能敏感的与anchor有关的超参数。&lt;br&gt;&lt;img src=&quot;/img/fcos.png&quot; alt&gt;&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://nicehuster.github.io/categories/paper-reading/"/>
    
    
      <category term="object detection" scheme="https://nicehuster.github.io/tags/object-detection/"/>
    
  </entry>
  
</feed>
