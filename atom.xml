<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一起打怪升级呀</title>
  <icon>https://www.gravatar.com/avatar/2555127dc0de830d31ceeb98d8565ac8</icon>
  <subtitle>别整太大鸭力,多鸡立自己qaq</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://blog.nicehuster.cn/"/>
  <updated>2022-04-13T04:30:07.119Z</updated>
  <id>https://blog.nicehuster.cn/</id>
  
  <author>
    <name>nicehuster</name>
    <email>nicehuster@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>增量目标检测方法 Faster ILOD</title>
    <link href="https://blog.nicehuster.cn/2022/03/24/Faster_ILOD/"/>
    <id>https://blog.nicehuster.cn/2022/03/24/Faster_ILOD/</id>
    <published>2022-03-24T11:13:39.000Z</published>
    <updated>2022-04-13T04:30:07.119Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/abs/2003.03901" target="_blank" rel="noopener">Faster ILOD: Incremental Learning for Object Detectors based on Faster RCNN</a><br><strong>代码链接：</strong><a href="https://github.com/CanPeng123/Faster-ILOD" target="_blank" rel="noopener">https://github.com/CanPeng123/Faster-ILOD</a><br><strong>整体信息：</strong>这是发表在PRL2020上的一篇文章关于增量目标检测的文章，作者是来自The University of Queensland，这篇文章基于Faster RCNN，使用multi-network 自适应蒸馏，设计了一种end2end的增量目标检测方法。</p><a id="more"></a><h2 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h2><p>增量目标检测包含S个增量steps，在每个增量step，训练数据只包含新类别Cn ，给定一个在旧类别 C0 上已训练好的目标检测模型，增量目标检测的任务是在新类别 Cn 数据上重新训练模型(一般是fine-tune)，同时维持模型在旧类别 C0 上的性能，在增量训练过程中，旧类别 C0 不可见。</p><p><img src="/img/faster_ilod_1.png" alt="img"></p><p>上图展示的是一个在VOC数据集上的增量目标检测的示例，模型首先在前15类训练，然后逐步增加每个类别。增量训练过程中，只提供当前新类别的标注，其他类别不可见。Normal Training是使用所有数据（旧数据和新数据）从头开始重新训练模型，该模型在测试集上的指标即为增量目标检测的指标上界。 Catastrophic forgetting是使用已训练好的旧类模型直接在新类数据上fine-tune的结果，可以看到在不断进行增量训练时，总体指标在逐渐下降，即出现了灾难性遗忘的问题( Catastrophic forgetting)。</p><h2 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h2><p><img src="/img/faster_ilod_method.png" alt="img"></p><p>作者使用Multi-network自适应蒸馏的方法来解决增量目标检测中出现的Catastrophic forgetting问题。具体做法如上图所示，上面是旧模型即teacher模型T，下面是新模型即student模型S，中间为蒸馏过程。</p><p>（1）<strong>特征蒸馏</strong>，所有feature map进行减均值归一化，然后使用L1 loss进行蒸馏。具体地，针对特征图上每个激活值大小进行比较，确定是否蒸馏，如下公式所示，</p><script type="math/tex; mode=display">\mathcal{L}_{F_{-} D i s t}=\frac{1}{\mathcal{M}} \sum \begin{cases}\left\|\tilde{f}_{t e}-\tilde{f}_{s t}\right\|_{1}, & \text { if } \tilde{f}_{t e}>\tilde{f}_{s t} \\ 0, & \text { otherwise }\end{cases}</script><p>（2）<strong>RPN蒸馏</strong>，同样地，使用T模型RPN的输出作为下界进行自适应蒸馏，使用的是L2 loss。</p><script type="math/tex; mode=display">\begin{aligned} &\mathcal{L}_{R P N_{-} D i s t}=\frac{1}{\mathcal{N}} \sum \begin{cases}\left\|q_{t e}-q_{s t}\right\|_{2}^{2}+\beta\left\|r_{t e}-r_{s t}\right\|_{2}^{2}, & \text { if } q_{t e}>q_{s t} \\ 0, & \text { otherwise }\end{cases} \\ &\text { where } \\ &\beta= \begin{cases}1, & \text { if } q_{t e}>\left(q_{s t}+\mathcal{T}\right) \\ 0, & \text { otherwise. }\end{cases} \end{aligned}</script><p>（3）<strong>RCNN蒸馏</strong>，具体做法和ILOD做法一致，在T模型中背景分数最小的128个ROI中随机选择64个proposal进行蒸馏，即只蒸馏背景信息，新类别不参与RCNN蒸馏，具体地，如下公式所示，</p><script type="math/tex; mode=display">\mathcal{L}_{R C N_{-} D i s t}=\frac{1}{\mathcal{K} \times C_{o}} \sum\left[\left\|\tilde{p}_{t e}-\tilde{p}_{s t}\right\|_{2}^{2}+\left\|t_{t e}-t_{s t}\right\|_{2}^{2}\right]</script><p>最后，总的loss为三者相加，</p><script type="math/tex; mode=display">\mathcal{L}_{\text {total }}=\mathcal{L}_{R C N N}+\lambda_{1} \mathcal{L}_{F_{-} \text {Dist }}+\lambda_{2} \mathcal{L}_{R P N_{-} \text {Dist }}+\lambda_{3} \mathcal{L}_{R C N_{-} \text {Dist }}</script><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ul><li>数据集(指标)：PASCAL VOC(AP@0.5)，COCO(mAP)</li><li>settings：one-step 和 multi-step</li><li><p>对比方法，ILOD</p></li><li><p>对比方法，ILOD</p></li></ul><p><img src="/img/faster_ilod_exp1.png" alt="img"></p><p><img src="/img/faster_ilod_exp2.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2003.03901&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Faster ILOD: Incremental Learning for Object Detectors based on Faster RCNN&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/CanPeng123/Faster-ILOD&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/CanPeng123/Faster-ILOD&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这是发表在PRL2020上的一篇文章关于增量目标检测的文章，作者是来自The University of Queensland，这篇文章基于Faster RCNN，使用multi-network 自适应蒸馏，设计了一种end2end的增量目标检测方法。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="Continual learning" scheme="https://blog.nicehuster.cn/tags/Continual-learning/"/>
    
  </entry>
  
  <entry>
    <title>增量目标检测方法调研</title>
    <link href="https://blog.nicehuster.cn/2022/03/21/IOD/"/>
    <id>https://blog.nicehuster.cn/2022/03/21/IOD/</id>
    <published>2022-03-21T11:13:39.000Z</published>
    <updated>2022-04-12T11:36:14.547Z</updated>
    
    <content type="html"><![CDATA[<p>最近参加了CVPR2022 有关增量学习的一个Workshop：<a href="https://sites.google.com/view/clvision2022/overview" target="_blank" rel="noopener">Workshop on Continual Learning in Computer Vision</a>，这个workshop有三个赛道：(1)Instance Classification Track，(2)Category Detection Track；(3)Instance Detection Track，三个赛道的任务介绍可以看官网。个人是做检测方向出身，因此关注了一下track2，并基于增量目标检测方向做了部分调研，并简单介绍其中部分算法。</p><a id="more"></a><h3 id="增量目标检测方法调研"><a href="#增量目标检测方法调研" class="headerlink" title="增量目标检测方法调研"></a>增量目标检测方法调研</h3><p><img src="/img/iod_survey.png" alt="img"></p><h3 id="部分方法介绍"><a href="#部分方法介绍" class="headerlink" title="部分方法介绍"></a>部分方法介绍</h3><p><img src="/img/IOD_1.PNG" alt="IOD (1)"></p><p><img src="/img/IOD_2.PNG" alt="IOD (2)"></p><p><img src="/img/IOD_3.PNG" alt="IOD (3)"></p><p><img src="/img/IOD_4.PNG" alt="IOD (4)"></p><p><img src="/img/IOD_5.PNG" alt="IOD (5)"></p><p><img src="/img/IOD_6.PNG" alt="IOD (6)"></p><p><img src="/img/IOD_7.PNG" alt="IOD (7)"></p><p><img src="/img/IOD_8.PNG" alt="IOD (8)"></p><p><img src="/img/IOD_9.PNG" alt="IOD (9)"></p><p><img src="/img/IOD_10.PNG" alt="IOD (10)"></p><p><img src="/img/IOD_11.PNG" alt="IOD (11)"></p><p><img src="/img/IOD_12.PNG" alt="IOD (12)"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近参加了CVPR2022 有关增量学习的一个Workshop：&lt;a href=&quot;https://sites.google.com/view/clvision2022/overview&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Workshop on Continual Learning in Computer Vision&lt;/a&gt;，这个workshop有三个赛道：(1)Instance Classification Track，(2)Category Detection Track；(3)Instance Detection Track，三个赛道的任务介绍可以看官网。个人是做检测方向出身，因此关注了一下track2，并基于增量目标检测方向做了部分调研，并简单介绍其中部分算法。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="Continual learning" scheme="https://blog.nicehuster.cn/tags/Continual-learning/"/>
    
  </entry>
  
  <entry>
    <title>Panoptic Segmentation详解</title>
    <link href="https://blog.nicehuster.cn/2022/01/07/PSeg/"/>
    <id>https://blog.nicehuster.cn/2022/01/07/PSeg/</id>
    <published>2022-01-07T11:13:39.000Z</published>
    <updated>2022-04-12T04:35:47.174Z</updated>
    
    <content type="html"><![CDATA[<p>最近在知乎上频繁地刷到有关Mask2Former地帖子，这么多人吹捧地必是精品，就跟一波风看了一下Mask2Former，顺带地也了解一下Panoptic Segmentation这个任务。众所周知，图像分割主要有两个方向：</p><ul><li><strong>语义分割（semantic segmentation）</strong>，常用来识别天空、草地、道路等没有固定形状的不可数<strong>事物（stuff）</strong>。语义分割的标记方法通常是给每个像素加上标签。</li><li><strong>实例分割（instance segmentation）</strong>，人、动物或工具等可数的、独立的明显物体<strong>（things）</strong>。实例分割通常用包围盒或分割掩码标记目标。</li></ul><p><strong>全景分割（Panoptic Segmentation）</strong>其实就是把这两个方向结合起来，生成统一的、全局的分割图像，既识别事物，也识别物体。</p><p><img src="/img/ps.png" alt="img"></p><a id="more"></a><p><strong>标记方法</strong></p><p>全景分割的标记方法结合了语义分割和实例分割，给每个像素加上标签$\left(l_{i}, z_{i}\right)$,其中i表示第i个像素，l表示语义类别，z表示实例ID。语义类别由两部分组成，事物类别$L^{ST}$和$L^{TH}$分别为stuff和thing的简写）。当$l_{i} \in L^{S T}$，忽略$z_i$（事物类别）；</p><p><strong>评估标准</strong></p><p>首先是常规的IoU &gt; 0.5，然后结合TF、FN、FP搞出了一个PQ标准。（PQ是Panoptic Quality，即全景质量的简称。)</p><p><img src="/img/metric.png" alt="img"></p><p>PQ的具体公式为：</p><script type="math/tex; mode=display">\mathrm{PQ}=\frac{\sum_{(p, g) \in T P} \operatorname{IoU}(p, g)}{|T P|+\frac{1}{2}|F P|+\frac{1}{2}|F N|}</script><p>另外，PQ可以分解为<strong>分割质量（segmentation quality，SQ）</strong>和<strong>识别质量（recognition quality，RQ）</strong>的乘积，便于进一步评估分割和识别环节的表现。</p><script type="math/tex; mode=display">\mathrm{PQ}=\underbrace{\frac{\sum_{(p, g) \in T P} \operatorname{IoU}(p, g)}{|T P|}}_{\text {segmentation quality }(\mathrm{SQ})} \times \underbrace{\frac{|T P|}{|T P|+\frac{1}{2}|F P|+\frac{1}{2}|F N|}}_{\text {recognition quality }(\mathrm{RQ})}</script><p><strong>数据集</strong></p><p>全景分割数据集需要既有语义分割标注，也有实例分割标注。</p><ul><li>Cityscapes(19classes)：5000张街景图片，97%的图片有像素标注，共有19个类别，其中8个类别符合语义分割的特征；</li><li>ADE20k(150classes)：图像总量超过25000张，并经过公开标注。其中包括100种物体和59种事物。</li><li>Mapillary Vistas(65classes)：25000张分辨率不同的街景照片。其中98%的图片都经过了像素标注，涵盖28种事物与37种物体。</li><li>COCO：知名数据集COCO最近加入了全景分割标注。</li></ul><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h4 id="1-MaskFormer"><a href="#1-MaskFormer" class="headerlink" title="1.MaskFormer"></a>1.<strong>MaskFormer</strong></h4><p>Per-Pixel Classification is Not All You Need for Semantic Segmentation</p><p>篇文章提出了一个新的分割输出端范式，传统方法会将每个像素点预测成一种类别来完成分割任务，而本文则是会输出很多个二分类分割图，如下图所示：</p><p><img src="/img/mask_cls.png" alt="img"></p><p>上图左侧为传统方法，右侧为本文方法，不是只输出 K 个类别二值分类图，而是提前设定一个较大值，同时与分类图一同预测的还有一个预测类别，这个类别可以为空，即该二值分类图没有用。因此损失函数由两个组成，一个是预测分类图与真值图的损失，另一个是预测类别的交叉熵损失.</p><script type="math/tex; mode=display">\mathcal{L}_{\text {mask-cls }}\left(z, z^{\mathrm{gt}}\right)=\sum_{j=1}^{N}\left[-\log p_{\sigma(j)}\left(c_{j}^{\mathrm{gt}}\right)+\mathbb{1}_{c_{j}^{\mathrm{gt}} \neq \varnothing} \mathcal{L}_{\text {mask }}\left(m_{\sigma(j)}, m_{j}^{\mathrm{gt}}\right)\right]</script><p>匹配策略上依旧采用匈牙利匹配，匹配成本：</p><script type="math/tex; mode=display">-p_{i}\left(c_{j}^{\mathrm{gt}}\right)+\mathcal{L}_{\text {mask }}\left(m_{i}, m_{j}^{\mathrm{gt}}\right)</script><p>其中，$L_{mask}$为二类交叉熵损失。MaskFormer具体结构如下：</p><p><img src="/img/maskformer.png" alt="img"></p><h4 id="2-Mask2Former"><a href="#2-Mask2Former" class="headerlink" title="2.Mask2Former"></a>2.Mask2Former</h4><p>Masked-attention Mask Transformer for Universal Image Segmentation</p><p>一个model去建模所有的分割任务：语义分割，实例分割和以及全景分割。一个模型取得三个不同分割任务STOA.。具体而言本文方法沿用了上一篇的分割图分别产生方式，另外有两个主要创新点，包括每个transformer decoder层都会使用pixel-decoder的金字塔对应结构，以及 Mask Attention 形式。可参考下图：</p><p><img src="/img/mask2former.png" alt="img"></p><p>因此，作者基于上述的问题和最初的motivation(一个模型取得三个不同分割任务STOA)，提出了几个改进，节省训练的时间(MaskFormer训练需要300epoch)和cost的同时能够提升性能。</p><p><strong>第一个改进</strong>：Mask Attention加速收敛,相比于之前的cross attention，这个里面的attention affinity是一种稀疏的attention，其实就是将上一层预测的分割图使用阈值0.5转换成[0,1]mask图，将转换后的mask进一步转换成[-inf,0]，然后和原始att相加，过softmax得到att_mask，相当于不计算原始分割图中为0的区域att.</p><p>standard cross-attention:  $\mathbf{X}_{l}=\operatorname{softmax}\left(\mathbf{Q}_{l} \mathbf{K}_{l}^{\mathrm{T}}\right) \mathbf{V}_{l}+\mathbf{X}_{l-1}$</p><p> masked cross-attention: </p><script type="math/tex; mode=display">\mathbf{X}_{l}=\operatorname{softmax}\left(\mathcal{M}_{l-1}+\mathbf{Q}_{l} \mathbf{K}_{l}^{\mathrm{T}}\right) \mathbf{V}_{l}+\mathbf{X}_{l-1}</script><script type="math/tex; mode=display">\mathcal{M}_{l-1}(x, y)=\left\{\begin{array}{ll}0 & \text { if } \mathbf{M}_{l-1}(x, y)=1 \\-\infty & \text { otherwise }\end{array} .\right.</script><p>具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiScaleMaskedTransformerDecoder</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">       ...</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask_features, mask = None)</span>:</span></span><br><span class="line">       ...</span><br><span class="line">       outputs_class, outputs_mask, attn_mask = self.forward_prediction_heads(output, mask_features, attn_mask_target_size=size_list[<span class="number">0</span>])</span><br><span class="line">       <span class="comment">#print(outputs_class.shape,outputs_mask.shape,attn_mask.shape)</span></span><br><span class="line">       predictions_class.append(outputs_class)</span><br><span class="line">       predictions_mask.append(outputs_mask)</span><br><span class="line"></span><br><span class="line">       <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_layers):</span><br><span class="line">           level_index = i % self.num_feature_levels</span><br><span class="line">           attn_mask[torch.where(attn_mask.sum(<span class="number">-1</span>) == attn_mask.shape[<span class="number">-1</span>])] = <span class="keyword">False</span></span><br><span class="line">           <span class="comment"># attention: cross-attention first</span></span><br><span class="line">           output = self.transformer_cross_attention_layers[i](</span><br><span class="line">               output, src[level_index],</span><br><span class="line">               memory_mask=attn_mask,</span><br><span class="line">               memory_key_padding_mask=<span class="keyword">None</span>,  <span class="comment"># here we do not apply masking on padded region</span></span><br><span class="line">               pos=pos[level_index], query_pos=query_embed</span><br><span class="line">           )</span><br><span class="line"></span><br><span class="line">           output = self.transformer_self_attention_layers[i](</span><br><span class="line">               output, tgt_mask=<span class="keyword">None</span>,</span><br><span class="line">               tgt_key_padding_mask=<span class="keyword">None</span>,</span><br><span class="line">               query_pos=query_embed</span><br><span class="line">           )</span><br><span class="line"></span><br><span class="line">           <span class="comment"># FFN</span></span><br><span class="line">           output = self.transformer_ffn_layers[i](</span><br><span class="line">               output</span><br><span class="line">           )</span><br><span class="line"></span><br><span class="line">           outputs_class, outputs_mask, attn_mask = self.forward_prediction_heads(output, mask_features, attn_mask_target_size=size_list[(i + <span class="number">1</span>) % self.num_feature_levels])</span><br><span class="line">           predictions_class.append(outputs_class)</span><br><span class="line">           predictions_mask.append(outputs_mask)</span><br><span class="line">           ...</span><br><span class="line">           </span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward_prediction_heads</span><span class="params">(self, output, mask_features, attn_mask_target_size)</span>:</span></span><br><span class="line">       decoder_output = self.decoder_norm(output)</span><br><span class="line">       decoder_output = decoder_output.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">       outputs_class = self.class_embed(decoder_output)</span><br><span class="line">       mask_embed = self.mask_embed(decoder_output)</span><br><span class="line">       outputs_mask = torch.einsum(<span class="string">"bqc,bchw-&gt;bqhw"</span>, mask_embed, mask_features)</span><br><span class="line"></span><br><span class="line">       <span class="comment"># <span class="doctag">NOTE:</span> prediction is of higher-resolution</span></span><br><span class="line">       <span class="comment"># [B, Q, H, W] -&gt; [B, Q, H*W] -&gt; [B, h, Q, H*W] -&gt; [B*h, Q, HW]</span></span><br><span class="line">       attn_mask = F.interpolate(outputs_mask, size=attn_mask_target_size, mode=<span class="string">"bilinear"</span>, align_corners=<span class="keyword">False</span>)</span><br><span class="line">       <span class="comment">#print(outputs_mask.shape,attn_mask.shape)</span></span><br><span class="line">       <span class="comment"># must use bool type</span></span><br><span class="line">       <span class="comment"># If a BoolTensor is provided, positions with ``True`` are not allowed to attend while ``False`` values will be unchanged.</span></span><br><span class="line">       attn_mask = (attn_mask.sigmoid().flatten(<span class="number">2</span>).unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, self.num_heads, <span class="number">1</span>, <span class="number">1</span>).flatten(<span class="number">0</span>, <span class="number">1</span>) &lt; <span class="number">0.5</span>).bool()</span><br><span class="line">       attn_mask = attn_mask.detach()</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> outputs_class, outputs_mask, attn_mask</span><br></pre></td></tr></table></figure><p><strong>第二个改进是多尺度特征改善小目标分割</strong>，对应于pixel-decoder，作者使用了类似于Deformable DETR decoder端的设置，在decoder端采用了multi scale的特征输入做attention。这个步骤对于提升small object的segmentation帮助很大。具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MSDeformAttnPixelDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_features</span><span class="params">(self, features)</span>:</span> <span class="comment">#[res3,res4,res5]</span></span><br><span class="line">        srcs = []</span><br><span class="line">        pos = []</span><br><span class="line">        <span class="comment"># Reverse feature maps into top-down order (from low to high resolution)</span></span><br><span class="line">        <span class="keyword">for</span> idx, f <span class="keyword">in</span> enumerate(self.transformer_in_features[::<span class="number">-1</span>]):</span><br><span class="line">            x = features[f].float()  <span class="comment"># deformable detr does not support half precision</span></span><br><span class="line">            srcs.append(self.input_proj[idx](x))</span><br><span class="line">            pos.append(self.pe_layer(x))</span><br><span class="line"></span><br><span class="line">        y, spatial_shapes, level_start_index = self.transformer(srcs, pos) <span class="comment">#MSDeformAttnTransformerEncoderOnly</span></span><br><span class="line"></span><br><span class="line">        bs = y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        split_size_or_sections = [<span class="keyword">None</span>] * self.transformer_num_feature_levels</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.transformer_num_feature_levels):</span><br><span class="line">            <span class="keyword">if</span> i &lt; self.transformer_num_feature_levels - <span class="number">1</span>:</span><br><span class="line">                split_size_or_sections[i] = level_start_index[i + <span class="number">1</span>] - level_start_index[i]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                split_size_or_sections[i] = y.shape[<span class="number">1</span>] - level_start_index[i]</span><br><span class="line">        y = torch.split(y, split_size_or_sections, dim=<span class="number">1</span>) [x3,x4,x5]</span><br><span class="line">        </span><br><span class="line">        out = []</span><br><span class="line">        multi_scale_features = []</span><br><span class="line">        num_cur_levels = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, z <span class="keyword">in</span> enumerate(y):</span><br><span class="line">            zz = z.transpose(<span class="number">1</span>, <span class="number">2</span>).view(bs, <span class="number">-1</span>, spatial_shapes[i][<span class="number">0</span>], spatial_shapes[i][<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            out.append(zz)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># append `out` with extra FPN levels</span></span><br><span class="line">        <span class="comment"># Reverse feature maps into top-down order (from low to high resolution)</span></span><br><span class="line">        <span class="keyword">for</span> idx, f <span class="keyword">in</span> enumerate(self.in_features[:self.num_fpn_levels][::<span class="number">-1</span>]):</span><br><span class="line"></span><br><span class="line">            x = features[f].float()</span><br><span class="line"></span><br><span class="line">            lateral_conv = self.lateral_convs[idx]</span><br><span class="line">            output_conv = self.output_convs[idx]</span><br><span class="line">            cur_fpn = lateral_conv(x)</span><br><span class="line">            <span class="comment"># Following FPN implementation, we use nearest upsampling here</span></span><br><span class="line">            y = cur_fpn + F.interpolate(out[<span class="number">-1</span>], size=cur_fpn.shape[<span class="number">-2</span>:], mode=<span class="string">"bilinear"</span>, align_corners=<span class="keyword">False</span>)</span><br><span class="line">            y = output_conv(y)</span><br><span class="line">            out.append(y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> o <span class="keyword">in</span> out:</span><br><span class="line">            <span class="keyword">if</span> num_cur_levels &lt; self.maskformer_num_feature_levels:</span><br><span class="line">                multi_scale_features.append(o)</span><br><span class="line">                num_cur_levels += <span class="number">1</span>      </span><br><span class="line">        <span class="keyword">return</span> self.mask_features(out[<span class="number">-1</span>]), out[<span class="number">0</span>], multi_scale_features</span><br></pre></td></tr></table></figure><p>此外，文章还额外提出了三个小改进：</p><ul><li>将 Self 和 Cross Attention 的顺序做一个变换，让 Cross 在前面，因为在图像特征没加入计算时自身做 Self 效率会较低；</li><li>文章将 Transformer 解码器的初始序列设置为可学版本；</li><li>整个模型将不使用 dropout 操作。</li><li>使用point-rend head 改善分割边界质量，同时降低显存；</li></ul><p>mask2former/modeling/meta_arch/mask_former_head.py </p><p><strong>backbone:res50</strong></p><p>800x800(short size=800) —&gt;res2[1, 256, 200, 200],res3[1, 512, 100, 100],res4[1, 1024, 50, 50],res5[1, 2048, 25, 25],</p><p><strong>pixel decoder:MSDeformAttnPixelDecoder</strong>(6 layers)</p><p> res3,res4,res5—&gt;MSDeformAttnTransformerEncoderOnly—&gt;x1[1, 256, 25, 25],x2[1, 256, 50, 50],x3[1, 256, 100, 100],</p><p>x1,x2,x3+FPN(res2) —-&gt;x1[1, 256, 25, 25],x2[1, 256, 50, 50],x3[1, 256, 100, 100],x4[1, 256, 200, 200]</p><p> res3,res4,res5—&gt;  <strong>mask_features</strong>:conv[x4],<strong>transformer_encoder_features</strong>:x4,<strong>multi_scale_features</strong>:[x1,x2,x3]</p><p><strong>Transformer decoder:MultiScaleMaskedTransformerDecoder</strong></p><p>input：multi_scale_features,mask_features </p><p>query_feat(100x256)，mask_features[1, 256, 200, 200]  —&gt;forward_prediction_heads—&gt;outputs_class[1, 100, 134],outputs_mask[1, 100, 200, 200],attn_mask[8, 100, 625]，attn_mask是有outputs_mask插值降采样得到</p><h4 id="3-PointRend"><a href="#3-PointRend" class="headerlink" title="3.PointRend"></a>3.PointRend</h4><p>文中使用了各种术语，比如rend、subdivision、ray-tracing，是想说明一个问题: 图像是对真实目标的一个离散化表达，真实目标的一些属性，如区域联通性、边缘连续性，在图像中同样存在。那么分割问题就可以看作预测一个真实目标在离散化后的图像中所占的区域，即：point-wise label prediction（点对点分类）。连续性体现在：图像中的像素可以通过插值得到与真实目标一一对应的坐标点。分割是在离散化的网格区域点对点的分类，但是有些点很难分类的准确，这些点大部分处在目标的边缘。pointrend方法的提出是对这些模糊分割的点，做更进一步的预测，即：精细分割。主要分成3步，如下图所示：1）候选点（或模糊分割点）选取；2）点特征提取；3）点分割预测。</p><p><img src="/img/pointRend.png" alt="img"></p><h5 id="1-候选点选取"><a href="#1-候选点选取" class="headerlink" title="1.候选点选取"></a><strong>1.候选点选取</strong></h5><p>候选点选取在训练和测试过程是不一样的，其中推断过程是通过迭代的方式从一个低分辨率的分割图像得到一个高分辨率的图像，这种方式不适合训练过程中的梯度反传，故采用另一种方式。</p><p><strong>Point select for training</strong></p><ul><li>随机生成kN个点，其中k&gt;1。</li><li>估计这kN个点的不确定程度，并根据不确定程度，筛选前βN个点，β取值范围为[0, 1]。其中不确定程度的估计方式与推断过程估计方式相同，使用低分辨率的分割置信度。</li><li>在剩余的点中，均匀采用得到（1-β）N个点。</li></ul><p><img src="/img/point_select_train.png" alt="img"></p><p>在PointRend中，采用了k=3和$\beta=0.75$的采样策略参数，采样得到$14^{2}$个点；从 coarse prediction 中插值的 GT 类别的概率和 0.5 之间的距离作为 point-wise 不确定性度量.</p><p><strong>Point select for inference</strong></p><p>推断过程是迭代进行的，具体过程如下图所示，首先通过上采样，将模型直接输出的低分辨率的分割图的长宽各扩大2倍得到高分辨的分割图，如从4x4的特征图上采样得到8x8的特征图；然后在高分辨分割图中，筛选N个分割模糊的点，即分割置信度（若置信度区间为[0,1]）在0.5左右的点，如在下图8x8的高分辨率分割图中，点集中在边缘附近。这N个点为最终筛选出来进行再次确认的点。以此类推，逐步迭代，得到最终目标分辨率的分割图。</p><p><img src="/img/subdivision.png" alt="img"></p><p>在PointRend中，对于预测类别 c 的矩形框，除非特别说明，均采用 adaptive subdivision 来通过 5 steps 将 7x7 coarse 预测精细化到 224x224. 每一次迭代，选择并更新 $N=28^2$个基于预测值和 0.5 之间的差异性距离得到最不确定的点.</p><h5 id="2-点特征提取"><a href="#2-点特征提取" class="headerlink" title="2.点特征提取"></a><strong>2.点特征提取</strong></h5><p>点特征提取包括fine-grained特征和coarse特征，其中coarse为K维，来自低分辨的分割mask，fine-grained特征来自原cnn backbone某个stage或者多个stage的组合特征。文中使用P2层的卷积特征作为fine-grained特征提取的特征图，具体流程如下图。</p><p><img src="/img/pointRend-ppl.png" alt="img"></p><h5 id="3-点分割预测"><a href="#3-点分割预测" class="headerlink" title="3. 点分割预测"></a>3. 点分割预测</h5><p>如上图所示紫色箭头，得到候选点的特征表达后，经过一组MLP，来得到最后的N个点的分割预测结果。其中，在实验中使用了4个全连接层，其中<strong>每个全连接层的输出，都联合coarse feature</strong>，作为下一个全连接层的输入。</p><h5 id="5-Results"><a href="#5-Results" class="headerlink" title="5. Results"></a>5. Results</h5><p><img src="/img/pointRend-res.png" alt="img"></p><p>其中左边一列图为mask rcnn的结果，右边一列图为PointRend的结果，从视觉效果看，PointRend对物体边缘描述更加精细化。</p><h4 id="4-Panoptic-SegFormer"><a href="#4-Panoptic-SegFormer" class="headerlink" title="4.Panoptic SegFormer"></a>4.Panoptic SegFormer</h4><p> Delving Deeper into Panoptic Segmentation with Transformers</p><p><img src="/img/PSFormer-ppl.png" alt="img"></p><p>该结构与DETR类似，不同之处在于：</p><p>（1）backbone使用多尺度特征(C3,C4,C5);</p><p>（2）针对decoder中query做了进一步精细划分,解耦location和Mask。针对Thing Query使用location Decoder捕获thing类别位置信息对Thing Query进行refine；此后使用refine后的Thing Query和Stuff Query作为Mask Decoder 输出mask结果。其中location Decoders使用bbox信息辅助监督，可以加速网络收敛；</p><p>（3）后处理部分，使用Mask-wise merge策略融合things和stuff获取最终的mask结果.</p><p>下面详细讲一下location decoder、mask decoder和mask-wise merge部分。</p><p><strong>Location Decoder</strong></p><p>给定N个初始化queries，训练阶段，在location decoder后面添加一个辅助MLP来预测位置和尺寸，location decoder的输出称为location-aware queries；推理阶段，去除辅助MLP。这一个辅助loss，可以帮助网络快速收敛，每个query关注区域指向性更明确。</p><p><strong>Mask Decoder</strong></p><p>mask decoder将location decoder的输出location-wise queries当作query，和MaskFormer预测mask和类别不同的是，Panoptic SegFormer预测mask需要先将attention map拆分成A3，A4，A5，然后都上采样到H/8xW/8的分辨率，concat在一起得到A_fuse，最后通过1x1卷积得到mask预测结果。</p><p><strong>Mask-wise merge</strong></p><p><img src="/img/mask-wise-merge.png" alt="img"></p><p>之前的分割去重，一般都是使用pixel-wise argmax策略，也就是重叠部分保留预测分数最大的类别。本文提出的mask-wise merge策略，对于重叠部分进行舍弃，上图是伪代码。</p><p>很喜欢作者在conclusion里面提到的一句话， Given the similarities and differences among the various segmentation tasks, “seek common ground while reserving differences” is a more reasonable guiding ideology.  完全的统一框架不见得是最好的选择，“求同存异”才是一个更合理的指导思想。</p><p>代码链接：<a href="https://github.com/zhiqi-li/Panoptic-SegFormer" target="_blank" rel="noopener">https://github.com/zhiqi-li/Panoptic-SegFormer</a></p><p>结果复现：</p><div class="table-container"><table><thead><tr><th>Method</th><th>PQ</th><th>SQ</th><th>RQ</th><th>N</th></tr></thead><tbody><tr><td>All(paper)</td><td>49.600</td><td>81.600</td><td>59.900</td><td>133</td></tr><tr><td>All(rep)</td><td>49.900</td><td>81.500</td><td>60.200</td><td>133</td></tr></tbody></table></div><h4 id="5-K-Net"><a href="#5-K-Net" class="headerlink" title="5. K-Net"></a>5. K-Net</h4><p>K-Net: Towards Unified Image Segmentation</p><p><img src="/img/knet-simple.png" alt="img"></p><p>K-Net的目标也是在于统一实例分割和语义分割。如上图所示，语义分割核心结构就是由一组kernel来负责语义mask的生成，让kernel数量与数据集类别数据保持一致，每个kernel负责一个固定类别masker的生成。受此启发，在实例分割中，可以通过同样方式引入一组卷积核来负责 mask 的生成，限定一个 kernel 只分割一个物体，每个kernel负责分割不同的物体，实例分割任务统一到一个框架内。</p><p><strong>Group-Aware Kernels</strong></p><p>理论上一组instance kernel就可以得到实例分割结果，但实验结果却相差甚远，与sem seg 相比，ins seg需要的kernel要求更高：</p><p>(1)在sem seg中，每个单独的sem kernel与类别(sem class)是绑定的，在每张图上都可以学习分割同一个类别，而ins seg不具备，而是通过Bipartite matching 来做的 target assignment,这导致每个kernel在每张图上学习的目标是根据当前的预测情况动态分配的。</p><p>(2)ins kernel需要区分appearence和scale变化的物体，需要具备更强的判别特性；</p><p>基于此，作者设计了<strong>Kernel Update Head</strong> 基于 mask 和特征图来将 kernel 动态化；如下图所示，Kernel Update Head 首先获得每个 kernel 对应 pixel group 的 feature，然后以某种方式动态地更新当前kernel。</p><p><img src="/img/kernel-update-head.png" alt="img"></p><p>此外，为了使得kernel可以modeling全局信息，作者还新增kernel interaction模块，最终得到的特征可用于class prediction, dynamic kernels 和mask predictions. </p><p><img src="/img/knet-ppl.png" alt="img"></p><p>为了得到更精细化的mask，可以通过叠加多个Kernel Update Head对mask和kernel进行迭代式refine.最终K-Net pipeline如上图所示，在论文中使用了3个Kernel Update Head和100个ins kernel.</p><p>注：在COCO-Panoptic上多尺度训练36epoch，训练一个K-Net，使用16张V1100需要两天半，在两台机器的情况下，训练时间有点长。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在知乎上频繁地刷到有关Mask2Former地帖子，这么多人吹捧地必是精品，就跟一波风看了一下Mask2Former，顺带地也了解一下Panoptic Segmentation这个任务。众所周知，图像分割主要有两个方向：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;语义分割（semantic segmentation）&lt;/strong&gt;，常用来识别天空、草地、道路等没有固定形状的不可数&lt;strong&gt;事物（stuff）&lt;/strong&gt;。语义分割的标记方法通常是给每个像素加上标签。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实例分割（instance segmentation）&lt;/strong&gt;，人、动物或工具等可数的、独立的明显物体&lt;strong&gt;（things）&lt;/strong&gt;。实例分割通常用包围盒或分割掩码标记目标。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;全景分割（Panoptic Segmentation）&lt;/strong&gt;其实就是把这两个方向结合起来，生成统一的、全局的分割图像，既识别事物，也识别物体。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/ps.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="transformer" scheme="https://blog.nicehuster.cn/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>Transformer 杂记</title>
    <link href="https://blog.nicehuster.cn/2021/12/05/transformer-soup/"/>
    <id>https://blog.nicehuster.cn/2021/12/05/transformer-soup/</id>
    <published>2021-12-05T11:13:39.000Z</published>
    <updated>2022-04-12T11:39:50.984Z</updated>
    
    <content type="html"><![CDATA[<p>近期断断续续的看了一些transformer相关的paper，看的比较杂，有些是对应领域比较有代表性地工作。偷个懒就不详细介绍每篇Paper，简单地记录一下这些paper大致要解决地问题。</p><a id="more"></a><h4 id="1-MAE-Masked-Autoencoders-Are-Scalable-Vision-Learners"><a href="#1-MAE-Masked-Autoencoders-Are-Scalable-Vision-Learners" class="headerlink" title="1. MAE:Masked Autoencoders Are Scalable Vision Learners"></a>1. MAE:Masked Autoencoders Are Scalable Vision Learners</h4><p>自监督学习方法，核心思想是以一定比例随机 mask 掉图片中的一些图像块(patch)然后重建这些部分的像素值</p><h4 id="2-SegFormer-Simple-and-Efficient-Design-for-Semantic-Segmentation-with-Transformers"><a href="#2-SegFormer-Simple-and-Efficient-Design-for-Semantic-Segmentation-with-Transformers" class="headerlink" title="2.SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers"></a>2.SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</h4><p>设计多层次backbone MIT，丢弃PE，优化self-att加速推理，此外从SETR痛点出发设计轻量级MLP解码器</p><h4 id="3-Early-Convolutions-Help-Transformers-See-Better"><a href="#3-Early-Convolutions-Help-Transformers-See-Better" class="headerlink" title="3.Early Convolutions Help Transformers See Better"></a>3.Early Convolutions Help Transformers See Better</h4><p>Vit训练不稳定在于Patch Embedding时使用大卷积核以及大步长导致，进一步提出使用step-wise conv stem进行替换，以此改进vit训练稳定性问题</p><h4 id="4-Visformer-The-Vision-friendly-Transformer"><a href="#4-Visformer-The-Vision-friendly-Transformer" class="headerlink" title="4.Visformer: The Vision-friendly Transformer"></a>4.Visformer: The Vision-friendly Transformer</h4><p>提升transformer方法的性能下限，即使是小数据集依然可以得到很好的性能</p><h4 id="5-Conditional-Positional-Encodings-for-Vision-Transformers"><a href="#5-Conditional-Positional-Encodings-for-Vision-Transformers" class="headerlink" title="5.Conditional Positional Encodings for Vision Transformers"></a>5.Conditional Positional Encodings for Vision Transformers</h4><p>利用卷积+zero-padding来编码局部位置信息，从而丢弃现有的PE，解决输入大小变化时需要对PE进行插值和fine-tune的问题</p><h4 id="6-MetaFormer-is-Actually-What-You-Need-for-Vision"><a href="#6-MetaFormer-is-Actually-What-You-Need-for-Vision" class="headerlink" title="6.MetaFormer is Actually What You Need for Vision"></a>6.MetaFormer is Actually What You Need for Vision</h4><p>transformer优于cnn在于其结构，而不是attention，即使替换成pooling，也能达到不错的性能</p><h4 id="7-Per-Pixel-Classification-is-Not-All-You-Need-for-Semantic-Segmentation"><a href="#7-Per-Pixel-Classification-is-Not-All-You-Need-for-Semantic-Segmentation" class="headerlink" title="7.Per-Pixel Classification is Not All You Need for Semantic Segmentation"></a>7.Per-Pixel Classification is Not All You Need for Semantic Segmentation</h4><p>提出了一种新的分割范式，解耦分割和分类，统一语义分割和实例分割任务</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;近期断断续续的看了一些transformer相关的paper，看的比较杂，有些是对应领域比较有代表性地工作。偷个懒就不详细介绍每篇Paper，简单地记录一下这些paper大致要解决地问题。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="transformer" scheme="https://blog.nicehuster.cn/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>Swin-transformer</title>
    <link href="https://blog.nicehuster.cn/2021/08/12/swin/"/>
    <id>https://blog.nicehuster.cn/2021/08/12/swin/</id>
    <published>2021-08-12T11:13:39.000Z</published>
    <updated>2022-04-12T04:14:34.226Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/pdf/2103.14030.pdf" target="_blank" rel="noopener">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a><br><strong>代码链接：</strong><a href="https://github.com/microsoft/Swin-Transformer" target="_blank" rel="noopener">https://github.com/microsoft/Swin-Transformer</a><br><strong>整体信息：</strong> Swin Transformer 提出了一种针对视觉任务的通用的 Transformer 架构，Transformer 架构在 NLP 任务中已经算得上一种通用的架构，但是如果想迁移到视觉任务中有一个比较大的困难就是处理数据的尺寸不一样。作者分析表明，Transformer 从 NLP 迁移到 CV 上没有大放异彩主要有两点原因：(1)两个领域涉及的scale不同，NLP的scale是标准固定的，而CV的scale变化范围非常大。(2) CV比起NLP需要更大的分辨率，而且CV中使用Transformer的计算复杂度是图像尺度的平方，这会导致计算量过于庞大。为了解决这两个问题，Swin Transformer相比之前的ViT做了两个改进：1.引入CNN中常用的层次化构建方式构建层次化Transformer 2.引入locality思想，对无重合的window区域内进行self-attention计算。</p><p><img src="/img/swin.png" alt="swin"></p><a id="more"></a><p>相比于ViT，Swin Transfomer计算复杂度大幅度降低，具有输入图像大小线性计算复杂度。Swin Transformer随着深度加深，逐渐合并图像块来构建层次化Transformer，可以作为通用的视觉骨干网络，应用于图像分类、目标检测、语义分割等任务。</p><h3 id="1-整体结构"><a href="#1-整体结构" class="headerlink" title="1. 整体结构"></a>1. 整体结构</h3><p>我们先看下Swin Transformer的整体架构:</p><p><img src="/img/swin-arch.png" alt="swin"></p><p>整个模型采取层次化的设计，一共包含4个Stage，每个stage都会缩小输入特征图的分辨率，像CNN一样逐层扩大感受野。</p><ul><li>在输入开始的时候，做了一个<code>Patch Embedding</code>，将图片切成一个个图块，并嵌入到<code>Embedding</code>。</li><li>在每个Stage里，由<code>Patch Merging</code>和多个Block组成。</li><li>其中<code>Patch Merging</code>模块主要在每个Stage一开始降低图片分辨率。</li><li>而Block具体结构如右图所示，主要是<code>LayerNorm</code>，<code>MLP</code>，<code>Window Attention</code> 和 <code>Shifted Window Attention</code>组成 (为了方便讲解，我会省略掉一些参数)</li></ul><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SwinTransformer</span>(<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(...)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">super</span>().__init_<span class="number">_</span>()</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># absolute position embedding</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">ape:</span></span><br><span class="line">            <span class="keyword">self</span>.absolute_pos_embed = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches, embed_dim))</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">self</span>.pos_drop = nn.Dropout(p=drop_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># build layers</span></span><br><span class="line">        <span class="keyword">self</span>.layers = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i_layer <span class="keyword">in</span> range(<span class="keyword">self</span>.num_layers)<span class="symbol">:</span></span><br><span class="line">            layer = BasicLayer(...)</span><br><span class="line">            <span class="keyword">self</span>.layers.append(layer)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.norm = norm_layer(<span class="keyword">self</span>.num_features)</span><br><span class="line">        <span class="keyword">self</span>.avgpool = nn.AdaptiveAvgPool1d(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.head = nn.Linear(<span class="keyword">self</span>.num_features, num_classes) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_features</span><span class="params">(<span class="keyword">self</span>, x)</span></span><span class="symbol">:</span></span><br><span class="line">        x = <span class="keyword">self</span>.patch_embed(x)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">ape:</span></span><br><span class="line">            x = x + <span class="keyword">self</span>.absolute_pos_embed</span><br><span class="line">        x = <span class="keyword">self</span>.pos_drop(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="keyword">self</span>.<span class="symbol">layers:</span></span><br><span class="line">            x = layer(x)</span><br><span class="line"></span><br><span class="line">        x = <span class="keyword">self</span>.norm(x)  <span class="comment"># B L C</span></span><br><span class="line">        x = <span class="keyword">self</span>.avgpool(x.transpose(<span class="number">1</span>, <span class="number">2</span>))  <span class="comment"># B C 1</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(<span class="keyword">self</span>, x)</span></span><span class="symbol">:</span></span><br><span class="line">        x = <span class="keyword">self</span>.forward_features(x)</span><br><span class="line">        x = <span class="keyword">self</span>.head(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>其中有几个地方处理方法与ViT不同：</p><blockquote><ul><li>ViT在输入会给embedding进行位置编码。而Swin-T这里则是作为一个<strong>可选项</strong>（<code>self.ape</code>），Swin-T是在计算Attention的时候做了一个<code>相对位置编码</code>;</li><li>ViT会单独加上一个可学习参数，作为分类的token。而Swin-T则是<strong>直接做平均</strong>，输出分类，有点类似CNN最后的全局平均池化层;</li></ul></blockquote><h3 id="2-Patch-Embedding"><a href="#2-Patch-Embedding" class="headerlink" title="2. Patch Embedding"></a>2. Patch Embedding</h3><p>在输入进Block前，我们需要将图片切成一个个patch，然后嵌入向量。具体做法是对原始图片裁成一个个 <code>window_size * window_size</code>的窗口大小，然后进行嵌入。这里可以通过二维卷积层，<strong>将stride，kernelsize设置为window_size大小</strong>。设定输出通道来确定嵌入向量的大小。最后将H,W维度展开，并移动到第一维度:</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> torch</span><br><span class="line"><span class="built_in">import</span> torch.nn as nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class PatchEmbed(nn.Module):</span><br><span class="line">    def __init__(self, <span class="attr">img_size=224,</span> <span class="attr">patch_size=4,</span> <span class="attr">in_chans=3,</span> <span class="attr">embed_dim=96,</span> <span class="attr">norm_layer=None):</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="attr">img_size</span> = to_2tuple(img_size) <span class="comment"># -&gt; (img_size, img_size)</span></span><br><span class="line">        <span class="attr">patch_size</span> = to_2tuple(patch_size) <span class="comment"># -&gt; (patch_size, patch_size)</span></span><br><span class="line">        <span class="attr">patches_resolution</span> = [img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>]]</span><br><span class="line">        self.<span class="attr">img_size</span> = img_size</span><br><span class="line">        self.<span class="attr">patch_size</span> = patch_size</span><br><span class="line">        self.<span class="attr">patches_resolution</span> = patches_resolution</span><br><span class="line">        self.<span class="attr">num_patches</span> = patches_resolution[<span class="number">0</span>] * patches_resolution[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        self.<span class="attr">in_chans</span> = in_chans</span><br><span class="line">        self.<span class="attr">embed_dim</span> = embed_dim</span><br><span class="line"></span><br><span class="line">        self.<span class="attr">proj</span> = nn.Conv2d(in_chans, embed_dim, <span class="attr">kernel_size=patch_size,</span> <span class="attr">stride=patch_size)</span></span><br><span class="line">        <span class="keyword">if</span> norm_layer is not None:</span><br><span class="line">            self.<span class="attr">norm</span> = norm_layer(embed_dim)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.<span class="attr">norm</span> = None</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        <span class="comment"># 假设采取默认参数</span></span><br><span class="line">        <span class="attr">x</span> = self.proj(x) <span class="comment"># 出来的是(N, 96, 224/4, 224/4) </span></span><br><span class="line">        <span class="attr">x</span> = torch.flatten(x, <span class="number">2</span>) <span class="comment"># 把HW维展开，(N, 96, 56*56)</span></span><br><span class="line">        <span class="attr">x</span> = torch.transpose(x, <span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 把通道维放到最后 (N, 56*56, 96)</span></span><br><span class="line">        <span class="keyword">if</span> self.norm is not None:</span><br><span class="line">            <span class="attr">x</span> = self.norm(x)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure><h3 id="3-Patch-Merging"><a href="#3-Patch-Merging" class="headerlink" title="3.Patch Merging"></a>3.Patch Merging</h3><p>该模块的作用是在每个Stage开始前做降采样，用于缩小分辨率，调整通道数 进而形成层次化的设计，同时也能节省一定运算量。在CNN中，则是在每个Stage开始前用<code>stride=2</code>的卷积/池化层来降低分辨率。每次降采样是两倍，因此<strong>在行方向和列方向上，间隔2选取元素</strong>。然后拼接在一起作为一整个张量，最后展开。<strong>此时通道维度会变成原先的4倍</strong>（因为H,W各缩小2倍），此时再通过一个<strong>全连接层再调整通道维度为原来的两倍</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PatchMerging</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_resolution, dim, norm_layer=nn.LayerNorm)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.input_resolution = input_resolution</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.reduction = nn.Linear(<span class="number">4</span> * dim, <span class="number">2</span> * dim, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.norm = norm_layer(<span class="number">4</span> * dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        x: B, H*W, C</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        H, W = self.input_resolution</span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">"input feature has wrong size"</span></span><br><span class="line">        <span class="keyword">assert</span> H % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> W % <span class="number">2</span> == <span class="number">0</span>, <span class="string">f"x size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) are not even."</span></span><br><span class="line"></span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line"></span><br><span class="line">        x0 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x1 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x2 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x3 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x = torch.cat([x0, x1, x2, x3], <span class="number">-1</span>)  <span class="comment"># B H/2 W/2 4*C</span></span><br><span class="line">        x = x.view(B, <span class="number">-1</span>, <span class="number">4</span> * C)  <span class="comment"># B H/2*W/2 4*C</span></span><br><span class="line"></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.reduction(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>下面是一个示意图（输入张量N=1, H=W=8, C=1，不包含最后的全连接层调整）</p><p><img src="/img/patch_merging.png" alt="swin"></p><h3 id="3-Window-Attention"><a href="#3-Window-Attention" class="headerlink" title="3. Window Attention"></a>3. Window Attention</h3><p>这是这篇文章的关键。传统的Transformer都是<strong>基于全局来计算注意力的</strong>，因此计算复杂度十分高。而Swin Transformer则将<strong>注意力的计算限制在每个窗口内</strong>，进而减少了计算量。我们先简单看下公式：</p><script type="math/tex; mode=display">\operatorname{Attention}(Q, K, V)=\operatorname{SoftMax}\left(Q K^{T} / \sqrt{d}+B\right) V</script><p>主要区别是在原始计算Attention的公式中的Q,K时<strong>加入了相对位置编码</strong>。后续实验有证明相对位置编码的加入提升了模型性能。</p><h3 id="4-Shifted-Window-Attention"><a href="#4-Shifted-Window-Attention" class="headerlink" title="4. Shifted Window Attention"></a>4. Shifted Window Attention</h3><p>前面的Window Attention是在每个窗口下计算注意力的，为了更好的和其他window进行信息交互，Swin Transformer还引入了shifted window操作。</p><p><img src="/img/shifted.png" alt="swin"></p><p>左边是没有重叠的Window Attention，而右边则是将窗口进行移位的Shift Window Attention。可以看到移位后的窗口包含了原本相邻窗口的元素。但这也引入了一个新问题，即<strong>window的个数翻倍了</strong>，由原本四个窗口变成了9个窗口。在实际代码里，我们是<strong>通过对特征图移位，并给Attention设置mask来间接实现的</strong>。能在<strong>保持原有的window个数下</strong>，最后的计算结果等价。</p><p><img src="/img/partition.png" alt="swin"></p><h4 id="4-1-特征图移位操作"><a href="#4-1-特征图移位操作" class="headerlink" title="4.1 特征图移位操作"></a>4.1 特征图移位操作</h4><p>代码里对特征图移位是通过<code>torch.roll</code>来实现的，下面是示意图</p><p><img src="/img/shift_opt.png" alt="swin"></p><blockquote><p>第一位操作是针对行进行移位，第二位操作时针对列进行移位操作。如果需要<code>reverse cyclic shift</code>的话只需把参数<code>shifts</code>设置为对应的正数值。</p></blockquote><h4 id="4-2-attention-mask"><a href="#4-2-attention-mask" class="headerlink" title="4.2 attention mask"></a>4.2 attention mask</h4><p>我认为这是Swin Transformer的精华，通过设置合理的mask，让<code>Shifted Window Attention</code>在与<code>Window Attention</code>相同的窗口个数下，达到等价的计算结果。首先我们对Shift Window后的每个窗口都给上index，并且做一个<code>roll</code>操作（window_size=2, shift_size=1）</p><p><img src="/img/shift_index.jpg" alt="swin"></p><p>我们希望在计算Attention的时候，<strong>让具有相同index QK进行计算，而忽略不同index QK计算结果</strong>。最后正确的结果如下图所示.</p><p><strong>例1：</strong>比如右上角这个 window，如下图所示。它由4个 patch 组成，所以应该计算出的 attention map是4×4的。但是6和4是2个不同的 sub-window，我们又不想让它们的 attention 发生交叠。所以我们希望的 attention map 和attention  mask如下图所示。</p><p><img src="/img/att_mask.png" alt="swin"></p><p><strong>例2：</strong>比如右下角这个 window，对应的 attention map 和attention  mask是下面这个样子。</p><p><img src="/img/att_mask2.png" alt="swin"></p><h3 id="5-transformer-block-整体结构"><a href="#5-transformer-block-整体结构" class="headerlink" title="5. transformer block 整体结构"></a>5. transformer block 整体结构</h3><p><img src="/img/block.png" alt="swin"></p><p>两个连续的Block架构如上图所示，需要注意的是一个Stage包含的Block个数必须是偶数，因为需要交替包含一个含有<code>Window Attention</code>的Layer和含有<code>Shifted Window Attention</code>的Layer。我们看下Block的前向代码：</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, x):</span><br><span class="line">    H, <span class="attr">W</span> = self.input_resolution</span><br><span class="line">    B, L, <span class="attr">C</span> = x.shape</span><br><span class="line">    <span class="keyword">assert</span> <span class="attr">L</span> == H * W, <span class="string">"input feature has wrong size"</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">shortcut</span> = x</span><br><span class="line">    <span class="attr">x</span> = self.norm1(x)</span><br><span class="line">    <span class="attr">x</span> = x.view(B, H, W, C)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># cyclic shift</span></span><br><span class="line">    <span class="keyword">if</span> self.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="attr">shifted_x</span> = torch.roll(x, <span class="attr">shifts=(-self.shift_size,</span> -self.shift_size), <span class="attr">dims=(1,</span> <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="attr">shifted_x</span> = x</span><br><span class="line"></span><br><span class="line">    <span class="comment"># partition windows</span></span><br><span class="line">    <span class="attr">x_windows</span> = window_partition(shifted_x, self.window_size)  <span class="comment"># nW*B, window_size, window_size, C</span></span><br><span class="line">    <span class="attr">x_windows</span> = x_windows.view(-<span class="number">1</span>, self.window_size * self.window_size, C)  <span class="comment"># nW*B, window_size*window_size, C</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># W-MSA/SW-MSA</span></span><br><span class="line">    <span class="attr">attn_windows</span> = self.attn(x_windows, <span class="attr">mask=self.attn_mask)</span>  <span class="comment"># nW*B, window_size*window_size, C</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># merge windows</span></span><br><span class="line">    <span class="attr">attn_windows</span> = attn_windows.view(-<span class="number">1</span>, self.window_size, self.window_size, C)</span><br><span class="line">    <span class="attr">shifted_x</span> = window_reverse(attn_windows, self.window_size, H, W)  <span class="comment"># B H' W' C</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># reverse cyclic shift</span></span><br><span class="line">    <span class="keyword">if</span> self.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="attr">x</span> = torch.roll(shifted_x, <span class="attr">shifts=(self.shift_size,</span> self.shift_size), <span class="attr">dims=(1,</span> <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="attr">x</span> = shifted_x</span><br><span class="line">    <span class="attr">x</span> = x.view(B, H * W, C)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># FFN</span></span><br><span class="line">    <span class="attr">x</span> = shortcut + self.drop_path(x)</span><br><span class="line">    <span class="attr">x</span> = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line"></span><br><span class="line">    return x</span><br></pre></td></tr></table></figure><p>整体流程如下：</p><ul><li>先对特征图进行LayerNorm</li><li>通过<code>self.shift_size</code>决定是否需要对特征图进行shift</li><li>然后将特征图切成一个个窗口</li><li>计算Attention，通过<code>self.attn_mask</code>来区分<code>Window Attention</code>还是<code>Shift Window Attention</code></li><li>将各个窗口合并回来</li><li>如果之前有做shift操作，此时进行<code>reverse shift</code>，把之前的shift操作恢复</li><li>做dropout和残差连接</li><li>再通过一层LayerNorm+全连接层，以及dropout和残差连接</li></ul><h3 id="6-experiments"><a href="#6-experiments" class="headerlink" title="6. experiments"></a>6. experiments</h3><p><img src="/img/ink21.png" alt="swin"></p><p>在ImageNet22K数据集上，准确率能达到惊人的86.4%。另外在检测，分割等任务上表现也很优异，感兴趣的可以翻看论文最后的实验部分。</p><h3 id="7-conclusion"><a href="#7-conclusion" class="headerlink" title="7. conclusion"></a>7. conclusion</h3><p>这篇文章创新点很棒，引入window这一个概念，将CNN的局部性引入，还能控制模型整体计算量。在Shift Window Attention部分，用一个mask和移位操作，很巧妙的实现计算等价。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2103.14030.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Swin Transformer: Hierarchical Vision Transformer using Shifted Windows&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/microsoft/Swin-Transformer&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/microsoft/Swin-Transformer&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt; Swin Transformer 提出了一种针对视觉任务的通用的 Transformer 架构，Transformer 架构在 NLP 任务中已经算得上一种通用的架构，但是如果想迁移到视觉任务中有一个比较大的困难就是处理数据的尺寸不一样。作者分析表明，Transformer 从 NLP 迁移到 CV 上没有大放异彩主要有两点原因：(1)两个领域涉及的scale不同，NLP的scale是标准固定的，而CV的scale变化范围非常大。(2) CV比起NLP需要更大的分辨率，而且CV中使用Transformer的计算复杂度是图像尺度的平方，这会导致计算量过于庞大。为了解决这两个问题，Swin Transformer相比之前的ViT做了两个改进：1.引入CNN中常用的层次化构建方式构建层次化Transformer 2.引入locality思想，对无重合的window区域内进行self-attention计算。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/swin.png&quot; alt=&quot;swin&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="transformer" scheme="https://blog.nicehuster.cn/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>Transformer 在图像领域应用的开拓者VIT</title>
    <link href="https://blog.nicehuster.cn/2021/07/21/vit/"/>
    <id>https://blog.nicehuster.cn/2021/07/21/vit/</id>
    <published>2021-07-21T11:13:39.000Z</published>
    <updated>2022-04-12T04:11:36.659Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/pdf/2010.11929.pdf" target="_blank" rel="noopener">An image is worth 16x16 words: Transformers for image recognition at scale</a><br><strong>代码链接：</strong><a href="https://github.com/google-research/vision_transformer" target="_blank" rel="noopener">https://github.com/google-research/vision_transformer</a><br><strong>整体信息：</strong>ViT（vision transformer）是Google在2020年提出的直接将transformer应用在图像分类的模型，后面很多的工作都是基于ViT进行改进的。ViT的思路很简单：直接把图像分成固定大小的patchs，然后通过线性变换得到patch embedding，这就类比NLP的words和word embedding，由于transformer的输入就是a sequence of token embeddings，所以将图像的patch embeddings送入transformer后就能够进行特征提取从而分类了。</p><p><img src="/img/arch.png" alt="vit"></p><a id="more"></a><p>ViT模型原理如上图所示，其实ViT模型只是用了transformer的Encoder来提取特征（原始的transformer还有decoder部分，用于实现sequence to sequence，比如机器翻译）。下面将分别对各个部分做详细的介绍。</p><h3 id="1-Patch-Embedding"><a href="#1-Patch-Embedding" class="headerlink" title="1.Patch Embedding"></a>1.Patch Embedding</h3><p>对于ViT来说，首先要将原始patch形式的2-D图像转换成一系列1-D的patch embeddings，这就好似NLP中的word embedding。输入的2-D图像记为$\mathbf{x} \in \mathbb{R}^{H \times W \times C}$,其中$H$和$W$分别是图像的高和宽，而$C$为通道数，对于RGB图像为3。如果将图像分为大小为$P \times P$的patchs,可以通过reshape等操作得到一系列patchs：$\mathbf{x}_{p} \in \mathbb{R}^{N \times\left(P^{2} \cdot C\right)}$ ,总共可以得到的patch数是$N=H W / P^{2}$,这个就是序列的长度。注意这里直接将patch拉平为1-D，其特征大小为$P^{2} \cdot C$ .然后通过一个简单的线性变换将patchs映射成D大小的维度，这就是patch embeddings:$\mathbf{x}_{\mathbf{p}}^{\prime} \in \mathbb{R}^{N \times D}$, 在实现上等同于对于x进行一个$P \times P$ 且stride为$P$ 的卷积操作。下面是具体的实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PatchEmbed</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" Image to Patch Embedding</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, embed_dim=<span class="number">768</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        img_size = to_2tuple(img_size)</span><br><span class="line">        patch_size = to_2tuple(patch_size)</span><br><span class="line">        num_patches = (img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>]) * (img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>])</span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.num_patches = num_patches</span><br><span class="line"></span><br><span class="line">        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        <span class="comment"># FIXME look at relaxing size constraints</span></span><br><span class="line">        <span class="keyword">assert</span> H == self.img_size[<span class="number">0</span>] <span class="keyword">and</span> W == self.img_size[<span class="number">1</span>], \</span><br><span class="line">            <span class="string">f"Input image size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) doesn't match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>*<span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>)."</span></span><br><span class="line">        x = self.proj(x).flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="2-Position-Embedding"><a href="#2-Position-Embedding" class="headerlink" title="2. Position Embedding"></a>2. Position Embedding</h3><p>除了patch embeddings，模型还需要另外一个特殊的position embedding。transformer和CNN不同，需要position embedding来编码tokens的位置信息，这主要是因为self-attention是permutation-invariant，即打乱sequence里的tokens的顺序并不会改变结果。如果不给模型提供patch的位置信息，那么模型就需要通过patchs的语义来学习拼图，这就额外增加了学习成本。ViT论文中对比了几种不同的position embedding方案(如下），最后发现如果不提供positional embedding效果会差，但其它各种类型的positional embedding效果都接近，这主要是因为ViT的输入是相对较大的patchs而不是pixels，所以学习位置信息相对容易很多。</p><ul><li>无positional embedding</li><li>1-D positional embedding：把2-D的patchs看成1-D序列</li><li>2-D positional embedding：考虑patchs的2-D位置（x, y）</li><li>Relative positional embeddings：patchs的相对位置</li></ul><p>在ViT中默认采用学习（训练的）的1-D positional embedding，在输入transformer的encoder之前直接将patch embeddings和positional embedding相加:</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里多1是为了后面要说的class token，embed_dim即patch embed_dim</span></span><br><span class="line"><span class="keyword">self</span>.pos_embed = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches + <span class="number">1</span>, embed_dim)) </span><br><span class="line"></span><br><span class="line"><span class="comment"># patch emded + pos_embed</span></span><br><span class="line">x = x + <span class="keyword">self</span>.pos_embed</span><br></pre></td></tr></table></figure><p>论文中也对学习到的positional embedding进行了可视化，发现相近的patchs的positional embedding比较相似，而且同行或同列的positional embedding也相近：</p><p><img src="/img/position_embedding.png" alt="vit"></p><p>这里额外要注意的一点，如果改变图像的输入大小，ViT不会改变patchs的大小，那patch的数量$N$也会发生变化，那么之前学习的pos_embed就维度对不上了，ViT采用的方案是通过插值来解决这个问题。但是这种情形一般会造成性能少许损失，可以通过finetune模型来解决。另外最新的论文<a href="https://arxiv.org/pdf/2102.10882.pdf" target="_blank" rel="noopener">CPVT</a>通过implicit Conditional Position encoding来解决这个问题（插入Conv来隐式编码位置信息，zero padding让Conv学习到绝对位置信息）。</p><h3 id="3-Class-Token"><a href="#3-Class-Token" class="headerlink" title="3. Class Token"></a>3. Class Token</h3><p>除了patch token，ViT借鉴BERT还增加了一个特殊的class token。后面会说，transformer的encoder输入是a sequence patch embeddings，输出也是同样长度的a sequence patch features，但图像分类最后需要获取image feature，简单的策略是采用pooling，比如求patch features的平均来获取image feature，但是ViT并没有采用类似的pooling策略，而是直接增加一个特殊的class token，其最后输出的特征加一个linear classifier就可以实现对图像的分类（ViT的pre-training时是接一个MLP head），所以输入ViT的sequence长度是$N+1$,class token对应的embedding在训练时随机初始化，然后通过训练得到，具体实现如下：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机初始化</span></span><br><span class="line">self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Classifier head</span></span><br><span class="line">self.head = nn.Linear(self.num_features, num_classes) if num_classes &gt; 0 <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 具体forward过程</span></span><br><span class="line">B = x.shape[0]</span><br><span class="line">x = self.patch_embed(x)</span><br><span class="line">cls_tokens = self.cls_token.expand(B, -1, -1)  <span class="comment"># stole cls_tokens impl from Phil Wang, thanks</span></span><br><span class="line">x = torch.cat((cls_tokens, x), dim=1)</span><br><span class="line">x = x + self.pos_embed</span><br></pre></td></tr></table></figure><h3 id="4-Transformer-Encoder"><a href="#4-Transformer-Encoder" class="headerlink" title="4. Transformer Encoder"></a>4. Transformer Encoder</h3><p>transformer最核心的操作就是self-attention，其实attention机制很早就在NLP和CV领域应用了，比如带有attention机制的seq2seq模型，但是transformer完全摒弃RNN或LSTM结构，直接采用attention机制反而取得了更好的效果：attention is all you need！简单来说，attention就是根据当前查询对输入信息赋予不同的权重来聚合信息，从操作上看就是一种“加权平均”。attention中共有3个概念：query, key和value，其中key和value是成对的，对于一个给定的query向量$q \in \mathbb{R}^{d}$,通过计算内积来匹配k个key向量(维度也是d,$K \in \mathbb{R}^{k \times d}$),得到的内积通过softmax来归一化得到k个权重，那么对于query其attention的输出就是k个key向量对应的value向量（即矩阵$V \in \mathbb{R}^{k \times d}$),对于一系列的N个query(即矩阵$Q \in \mathbb{R}^{N \times d}$)，可以通过矩阵计算它们的attention输出：</p><script type="math/tex; mode=display">\operatorname{Attention}(Q, K, V)=\operatorname{Softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><p>这里的$\sqrt{d_{k}}$为缩放因子以避免点击带来的方差影响。上述的Attention机制称为<strong>Scaled dot product attention</strong>，其实attention机制的变种有很多，但基本原理是相似的。如果$Q,K,V$都是从一个包含$N$个向量的sequence($X \in \mathbb{R}^{N \times D}$)变换得到：</p><script type="math/tex; mode=display">Q=X W_{Q}, K=X W_{K}, V=X W_{V}</script><p>那么此时就变成了<strong>self-attention</strong>，这个时候就有N个(key,value)对,self-attention是transformer最核心部分，self-attention其实就是输入向量之间进行相互attention来学习到新特征。前面说过我们已经得到图像的patch sequence，那么送入self-attention就能到同样size的sequence输出，只不过特征改变了。更进一步，transformer采用的是<strong>multi-head self-attention (MSA）</strong>，所谓的MSA就是采用定义h个attention heads，即采用h个self-attention应用在输入sequence上，在操作上可以将sequence拆分成h个size为$N \times d$，这里$D=h d$,不同的heads得到的输出concat在一起然后通过线性变换得到最终的输出,size也是$N \times D$:</p><script type="math/tex; mode=display">\operatorname{MSA}(\mathbf{z})=\left[\mathrm{SA}_{1}(z) ; \mathrm{SA}_{2}(z) ; \cdots ; \mathrm{SA}_{k}(z)\right] \mathrm{U}_{m s a} \quad \mathrm{U}_{m s a} \in \mathbb{R}^{k \cdot D_{h} \times D}</script><p>MSA的计算量是和$N^{2}$成比例的，所以ViT的输入是patch embeddings，而不是pixel embeddings，这有计算量上的考虑。在实现上，MSA是可以并行计算各个head的，具体代码如下：</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">Attention</span>(<span class="title">nn</span>.<span class="type">Module</span>):</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>, <span class="title">dim</span>, <span class="title">num_heads</span>=8, <span class="title">qkv_bias</span>=<span class="type">False</span>, <span class="title">qk_scale</span>=<span class="type">None</span>, <span class="title">attn_drop</span>=0., <span class="title">proj_drop</span>=0.):</span></span><br><span class="line"><span class="class">        super().__init__()</span></span><br><span class="line"><span class="class">        self.num_heads = num_heads</span></span><br><span class="line"><span class="class">        head_dim = dim // num_heads</span></span><br><span class="line"><span class="class">    </span></span><br><span class="line"><span class="class">        self.scale = qk_scale or head_dim ** -0.5</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        self.qkv = nn.<span class="type">Linear</span>(<span class="title">dim</span>, <span class="title">dim</span> * 3, <span class="title">bias</span>=<span class="title">qkv_bias</span>)</span></span><br><span class="line"><span class="class">        self.attn_drop = nn.<span class="type">Dropout</span>(<span class="title">attn_drop</span>)</span></span><br><span class="line"><span class="class">        self.proj = nn.<span class="type">Linear</span>(<span class="title">dim</span>, <span class="title">dim</span>)</span></span><br><span class="line"><span class="class">        # 这里包含了dropout</span></span><br><span class="line"><span class="class">        self.proj_drop = nn.<span class="type">Dropout</span>(<span class="title">proj_drop</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>, <span class="title">x</span>):</span></span><br><span class="line"><span class="class">        <span class="type">B</span>, <span class="type">N</span>, <span class="type">C</span> = x.shape</span></span><br><span class="line"><span class="class">        qkv = self.qkv(<span class="title">x</span>).reshape(<span class="type">B</span>, <span class="type">N</span>, 3, <span class="title">self</span>.<span class="title">num_heads</span>, <span class="type">C</span> // <span class="title">self</span>.<span class="title">num_heads</span>).permute(2, 0, 3, 1, 4)</span></span><br><span class="line"><span class="class">        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (<span class="title">cannot</span> <span class="title">use</span> <span class="title">tensor</span> <span class="title">as</span> <span class="title">tuple</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        attn = (<span class="title">q</span> @ <span class="title">k</span>.<span class="title">transpose</span>(-2, -1)) * self.scale</span></span><br><span class="line"><span class="class">        attn = attn.softmax(<span class="title">dim</span>=-1)</span></span><br><span class="line"><span class="class">        attn = self.attn_drop(<span class="title">attn</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        x = (<span class="title">attn</span> @ <span class="title">v</span>).transpose(1, 2).reshape(<span class="type">B</span>, <span class="type">N</span>, <span class="type">C</span>)</span></span><br><span class="line"><span class="class">        x = self.proj(<span class="title">x</span>)</span></span><br><span class="line"><span class="class">        x = self.proj_drop(<span class="title">x</span>)</span></span><br><span class="line"><span class="class">        return x</span></span><br></pre></td></tr></table></figure><p>在transformer中，MSA后跟一个FFN（Feed-forward network），这个FFN包含两个FC层，第一个FC层将特征从维度D变换成4D，后一个FC层将特征从维度4D变成D，中间的非线性激活函数采用GeLU，其实这就是一个MLP，具体实现如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mlp</span>(<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=<span class="number">0</span>.)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">super</span>().__init_<span class="number">_</span>()</span><br><span class="line">        out_features = out_features <span class="keyword">or</span> in_features</span><br><span class="line">        hidden_features = hidden_features <span class="keyword">or</span> in_features</span><br><span class="line">        <span class="keyword">self</span>.fc1 = nn.Linear(in_features, hidden_features)</span><br><span class="line">        <span class="keyword">self</span>.act = act_layer()</span><br><span class="line">        <span class="keyword">self</span>.fc2 = nn.Linear(hidden_features, out_features)</span><br><span class="line">        <span class="keyword">self</span>.drop = nn.Dropout(drop)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(<span class="keyword">self</span>, x)</span></span><span class="symbol">:</span></span><br><span class="line">        x = <span class="keyword">self</span>.fc1(x)</span><br><span class="line">        x = <span class="keyword">self</span>.act(x)</span><br><span class="line">        x = <span class="keyword">self</span>.drop(x)</span><br><span class="line">        x = <span class="keyword">self</span>.fc2(x)</span><br><span class="line">        x = <span class="keyword">self</span>.drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>那么一个完成transformer encoder block就包含一个MSA后面接一个FFN，其实MSA和FFN均包含和ResNet一样的skip connection，另外MSA和FFN后面都包含layer norm层，具体实现如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class Block(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, dim, num_heads, <span class="attribute">mlp_ratio</span>=4., <span class="attribute">qkv_bias</span>=<span class="literal">False</span>, <span class="attribute">qk_scale</span>=None, <span class="attribute">drop</span>=0., <span class="attribute">attn_drop</span>=0.,</span><br><span class="line">                 <span class="attribute">drop_path</span>=0., <span class="attribute">act_layer</span>=nn.GELU, <span class="attribute">norm_layer</span>=nn.LayerNorm):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.norm1 = norm_layer(dim)</span><br><span class="line">        self.attn = Attention(</span><br><span class="line">            dim, <span class="attribute">num_heads</span>=num_heads, <span class="attribute">qkv_bias</span>=qkv_bias, <span class="attribute">qk_scale</span>=qk_scale, <span class="attribute">attn_drop</span>=attn_drop, <span class="attribute">proj_drop</span>=drop)</span><br><span class="line">        # NOTE: drop path <span class="keyword">for</span> stochastic depth, we shall see <span class="keyword">if</span> this is better than dropout here</span><br><span class="line">        self.drop_path = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; 0. <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.norm2 = norm_layer(dim)</span><br><span class="line">        mlp_hidden_dim = int(dim * mlp_ratio)</span><br><span class="line">        self.mlp = Mlp(<span class="attribute">in_features</span>=dim, <span class="attribute">hidden_features</span>=mlp_hidden_dim, <span class="attribute">act_layer</span>=act_layer, <span class="attribute">drop</span>=drop)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = x + self.drop_path(self.attn(self.norm1(x)))</span><br><span class="line">        x = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure><h3 id="5-ViT"><a href="#5-ViT" class="headerlink" title="5. ViT"></a>5. ViT</h3><p>对于ViT模型来说，就类似CNN那样，不断堆积transformer encoder blocks，最后提取class token对应的特征用于图像分类，论文中也给出了模型的公式表达，其中（1）就是提取图像的patch embeddings，然后和class token对应的embedding拼接在一起并加上positional embedding；（2）是MSA，而（3）是MLP，（2）和（3）共同组成了一个transformer encoder block，共有L层；（4)是对class token对应的输出做layer norm，然后就可以用来图像分类。</p><p><img src="/img/vit-procedu.png" alt="vit"></p><p>ViT模型的超参数主要包括以下，这些超参数直接影响模型参数以及计算量：</p><blockquote><ol><li>Layers：block的数量；</li><li>Hidden size D：隐含层特征，D在各个block是一直不变的；</li><li>MLP size：一般设置为4D大小；</li><li>Heads：MSA中的heads数量；</li><li>Patch size：模型输入的patch size，ViT中共有两个设置：14x14和16x16，这个只影响计算量；</li></ol></blockquote><p>类似BERT，ViT共定义了3中不同大小的模型：Base，Large和Huge，其对应的模型参数不同，如下所示。如ViT-L/16指的是采用Large结构，输入的patch size为16x16。类似BERT，ViT共定义了3中不同大小的模型：Base，Large和Huge，其对应的模型参数不同，如下所示。如ViT-L/16指的是采用Large结构，输入的patch size为16x16。</p><p><img src="/img/models.png" alt="vit"></p><h3 id="6-Experiments"><a href="#6-Experiments" class="headerlink" title="6. Experiments"></a>6. Experiments</h3><p>ViT并不像CNN那样具有inductive bias，论文中发现如果如果直接在ImageNet上训练，同level的ViT模型效果要差于ResNet，但是如果在比较大的数据集上petraining，然后再finetune，效果可以超越ResNet。比如ViT在Google私有的300M JFT数据集上pretrain后，在ImageNet上的最好Top-1 acc可达88.55%，这已经和ImageNet上的SOTA相当了（Noisy Student EfficientNet-L2效果为88.5%，Google最新的SOTA是Meta Pseudo Labels，效果可达90.2%）：</p><p><img src="/img/exp.png" alt="vit"></p><p>那么ViT至少需要多大的数据量才能和CNN旗鼓相当呢？这个论文也做了实验，结果如下图所示，从图上所示这个预训练所使用的数据量要达到100M时才能显示ViT的优势。transformer的一个特色是它的scalability：当模型和数据量提升时，性能持续提升。在大数据面前，ViT可能会发挥更大的优势。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2010.11929.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;An image is worth 16x16 words: Transformers for image recognition at scale&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/google-research/vision_transformer&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/google-research/vision_transformer&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;ViT（vision transformer）是Google在2020年提出的直接将transformer应用在图像分类的模型，后面很多的工作都是基于ViT进行改进的。ViT的思路很简单：直接把图像分成固定大小的patchs，然后通过线性变换得到patch embedding，这就类比NLP的words和word embedding，由于transformer的输入就是a sequence of token embeddings，所以将图像的patch embeddings送入transformer后就能够进行特征提取从而分类了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/arch.png&quot; alt=&quot;vit&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="transformer" scheme="https://blog.nicehuster.cn/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>长尾目标检测-Seesaw Loss[转载]</title>
    <link href="https://blog.nicehuster.cn/2021/05/30/seesaw-loss/"/>
    <id>https://blog.nicehuster.cn/2021/05/30/seesaw-loss/</id>
    <published>2021-05-30T11:13:39.000Z</published>
    <updated>2022-04-22T09:01:40.695Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/abs/2008.10032" target="_blank" rel="noopener">Seesaw Loss for Long-Tailed Instance Segmentation</a><br><strong>代码链接：</strong><a href="https://github.com/open-mmlab/mmdetection/tree/master/configs/seesaw_loss" target="_blank" rel="noopener">https://github.com/open-mmlab/mmdetection/tree/master/configs/seesaw_loss</a><br><strong>整体信息：</strong>这是 MMDet 团队参加 LVIS 2020 竞赛获得rank1时提出地损失函数，这篇文章指出了限制检测器在长尾分布数据上性能的一个关键原因：施加在尾部类别（tail class上的<strong>正负样本梯度的比例</strong>是不均衡的。因此，我们提出 <strong>Seesaw Loss</strong> <strong>来动态地抑制尾部类别上过量的负样本梯度，同时补充对误分类样本的惩罚</strong>。 Seesaw Loss 显著提升了尾部类别的分类准确率，进而为检测器在长尾数据集上的整体性能带来可观的增益。</p><a id="more"></a><p>由于原作在知乎上已有讲解，我就不班门弄斧，直接搬运过来，侵权删。</p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>在长尾分布的数据集中（例如：LVIS），大部分训练样本来自头部类别（head class），而只有少量样本来自尾部类别（tail class）。因此在训练过程中，来自头部类别的样本会对尾部类别施加过量的负样本梯度，淹没了来自尾部类别自身的正样本梯度。这种不平衡的学习过程导致分类器倾向于给予尾部类别很低的响应，以降低训练的loss。如下图所示，我们统计了在 LVIS v1.0 上训练Mask R-CNN过程中，施加在每个类别的分类器上正负样本累计梯度的分布。</p><p><img src="/img/seesaw-distribution.png" alt="img"></p><p>显然，头部类别获得的正负样本梯度比例接近1.0，而越是稀有的尾部类别，其获得的正负样本梯度的比例就越小。由此带来的结果就是分类的准确率随着样本数的减少而急剧下降，进而严重影响了检测器的性能。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>为了方便直观理解，我们可以把正负样本梯度不均衡的问题，类比于一个一边放有较重物体而另一边放有较轻物体的跷跷板（Seesaw），如下图所示。为了平衡这个跷跷板，一个简单可行的方案就是缩短重物一侧跷跷板的臂长，即减少重物的重量在平衡过程中的权重。回到正负样本梯度不均衡的问题，我们提出了 Seesaw Loss 来动态地减少由头部类别施加在尾部类别上过量的负样本梯度的权重，从而达到正负样本梯度相对平衡的效果。</p><p><img src="/img/seesaw-ce.png" alt="img"></p><p>Seesaw Loss的数学表达如下，</p><script type="math/tex; mode=display">\begin{aligned}&L_{\text {seesaw }}(\mathbf{z})=-\sum_{i=1}^{C} y_{i} \log \left(\widehat{\sigma}_{i}\right) \\&\text { with } \widehat{\sigma}_{i}=\frac{e^{z_{i}}}{\sum_{j \neq i}^{C} \mathcal{S}_{i j} e^{z_{j}}+e^{z_{i}}}\end{aligned}</script><p>$y_{i} \in\{0,1\}$ 是one-hot label, $\mathbf{z}=\left[z_{1}, z_{2}, \ldots, z_{C}\right]$ ,是每一类预测的 logit。此时，对于一个第 i类的样本，它施加在第 j类上的负样本梯度为</p><script type="math/tex; mode=display">\frac{\partial L_{\text {seesaw }}(\mathbf{z})}{\partial z_{j}}=\mathcal{S}_{i j} \frac{e^{z_{j}}}{e^{z_{i}}} \widehat{\sigma}_{i}</script><p>这里我们可以发现 $S_{ij}$就像一个平衡系数，通过调节 $S_{ij}$，我们可以达到放大或者缩小第 i类施加在第 j 类上的负样本梯度的效果。这样，我们就可以通过选择合适 $S_{ij}$来达到平衡正负样本梯度的目的。</p><p>在 Seesaw Loss 的设计中，我们考虑了两方面的因素，一方面我们需要考虑类别间样本分布的关系（class-wise），并据此减少头部类别对尾部类别的”惩罚” （负样本梯度）；另一方面，盲目减少对尾部类别的惩罚会增加错误分类的风险，因为部分误分类的样本受到的惩罚变小了，因此对于那些在训练过程中误分类的样本我们需要保证其受到足够的”惩罚”。据此， $S_{ij}$由两项相乘得到，</p><script type="math/tex; mode=display">\mathcal{S}_{i j}=\mathcal{M}_{i j} \cdot \mathcal{C}_{i j}</script><p>$M_{ij}$ <strong>（Mitigation Factor）</strong>用来缓解尾部类别上过量的负样本梯度, $C_{ij}$ (<strong>Compensation Factor）</strong>用来补充那些错误分类样本上的”惩罚”。</p><h4 id="1-Mitigation-Factor"><a href="#1-Mitigation-Factor" class="headerlink" title="1. Mitigation Factor"></a>1. Mitigation Factor</h4><p><img src="/img/seesaw-mitigation.png" alt="img"></p><p>既然正负样本梯度不平衡的问题来自于样本数量的不平衡，那么一种直接有效的办法就是根据不同类别之间样本数量的相对比例来进行调节。在训练过程中，Seesaw Loss在线地统计每一类的累计训练样本数量 $N_{ij}$ 并根据如下公式计算$M_{ij}$ ,</p><script type="math/tex; mode=display">\mathcal{M}_{i j}=\left\{\begin{array}{cc}1, & \text { if } N_{i} \leq N_{j} \\\left(\frac{N_{j}}{N_{i}}\right)^{p}, & \text { if } N_{i}>N_{j}\end{array}\right.</script><p>也就是说当第 i类比第j类出现地 就会自动根据两类之间不平衡的程度来减少第 i类对第j类施加的负样本梯度。此外，我们在线地累计样本数量，而非使用预先统计的数据集样本分布，这样的设计主要是因为一些高级的样本 sampling 方式会改变数据集的分布（例如：repeat factor sampler, class balanced sampler 等)。在这种情况下，预先统计的方式无法反映训练过程中数据的真实分布。</p><h4 id="2-Compensation-Factor"><a href="#2-Compensation-Factor" class="headerlink" title="2. Compensation Factor"></a>2. Compensation Factor</h4><p><img src="/img/seesaw-compensation.png" alt="img"></p><p>为了防止过度减少负样本梯度而带来的分类错误，Seesaw Loss会增加对那些错误分类样本的惩罚。具体来说，如果一个第i 类的样本错误分给了第 j类，Seesaw Loss会根据两类之间的分类置信度的相对比值来适当的增加对第 j类的惩罚。$C_{ij}$的计算如下，</p><script type="math/tex; mode=display">\mathcal{C}_{i j}=\left\{\begin{array}{cc}1, & \text { if } \sigma_{j} \leq \sigma_{i} \\\left(\frac{\sigma_{j}}{\sigma_{i}}\right)^{q}, & \text { if } \sigma_{j}>\sigma_{i}\end{array}\right.</script><h4 id="3-Normalized-Linear-Activation"><a href="#3-Normalized-Linear-Activation" class="headerlink" title="3. Normalized Linear Activation"></a>3. Normalized Linear Activation</h4><p>受到face recognition，few-shot learning等领域的启发，Seesaw Loss在预测分类logit的时候对weight和feature进行了归一化处理，即</p><script type="math/tex; mode=display">\begin{gathered}z=\tau \widetilde{\mathcal{W}}^{T} \tilde{x}+b \\\widetilde{\mathcal{W}}_{i}=\frac{\mathcal{W}_{i}}{\left\|\mathcal{W}_{i}\right\|_{2}}, i \in C, \tilde{x}=\frac{x}{\|x\|_{2}}\end{gathered}</script><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p><img src="/img/seesaw-exp.png" alt="img"></p><p>Seesaw相比于 EQL 和 BAGS 两种专门为 LVIS 数据设计的方法取得了显著的性能优势，在 end-to-end 训练的情况下在 test-dev 上取得高达30.0 AP的精度。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2008.10032&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Seesaw Loss for Long-Tailed Instance Segmentation&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmdetection/tree/master/configs/seesaw_loss&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/open-mmlab/mmdetection/tree/master/configs/seesaw_loss&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这是 MMDet 团队参加 LVIS 2020 竞赛获得rank1时提出地损失函数，这篇文章指出了限制检测器在长尾分布数据上性能的一个关键原因：施加在尾部类别（tail class上的&lt;strong&gt;正负样本梯度的比例&lt;/strong&gt;是不均衡的。因此，我们提出 &lt;strong&gt;Seesaw Loss&lt;/strong&gt; &lt;strong&gt;来动态地抑制尾部类别上过量的负样本梯度，同时补充对误分类样本的惩罚&lt;/strong&gt;。 Seesaw Loss 显著提升了尾部类别的分类准确率，进而为检测器在长尾数据集上的整体性能带来可观的增益。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="Long-tailed object detection" scheme="https://blog.nicehuster.cn/tags/Long-tailed-object-detection/"/>
    
  </entry>
  
  <entry>
    <title>VFNet高性能的密集目标检测器</title>
    <link href="https://blog.nicehuster.cn/2021/04/12/vfnet/"/>
    <id>https://blog.nicehuster.cn/2021/04/12/vfnet/</id>
    <published>2021-04-12T11:13:39.000Z</published>
    <updated>2022-04-12T04:03:05.352Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_VarifocalNet_An_IoU-Aware_Dense_Object_Detector_CVPR_2021_paper.pdf" target="_blank" rel="noopener">VarifocalNet  An IoU-aware Dense Object Detector</a><br><strong>代码链接：</strong><a href="https://github.com/hyz-xmaster/VarifocalNet" target="_blank" rel="noopener">https://github.com/hyz-xmaster/VarifocalNet</a></p><p><img src="/img/vfnet.png" alt="vfnet"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_VarifocalNet_An_IoU-Aware_Dense_Object_Detect
      
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://blog.nicehuster.cn/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>TTFNet 快速训练版的CenterNet</title>
    <link href="https://blog.nicehuster.cn/2021/03/13/ttfnet/"/>
    <id>https://blog.nicehuster.cn/2021/03/13/ttfnet/</id>
    <published>2021-03-13T11:13:39.000Z</published>
    <updated>2022-04-12T03:54:48.981Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/pdf/1909.00700.pdf" target="_blank" rel="noopener">Training-Time-Friendly Network for Real-Time Object Detection</a><br><strong>代码链接：</strong><a href="https://github.com/ZJULearning/ttfnet" target="_blank" rel="noopener">https://github.com/ZJULearning/ttfnet</a><br><strong>问题</strong>： 这篇论文主要是解决CenterNet存在的一些问题，CenterNet在推理速度精度上表现不差，但存在训练耗时的问题，CenterNet官方代码训练coco需要2.5天，因此这篇论文主要是如何改进CenterNet训练耗时长的问题。作者基于CenterNet，在回归box时，作者认为由于CenterNet进行回归时仅仅使用中心点的那些目标是次优的选择，同时作者借助了FCOS是使用Ground Truth回归所有样本的思想，这里作者想到使用高斯核来选择回归样本，一方面可以增加回归的信息量，提升检测精度，同时作者指出这样有增加batch size的效果，可以使用更大的学习率以及有更短的训练时间。并且没有引入FCOS中耗时的NMS后处理过程，使得Inference时间减小。</p><a id="more"></a><p>该方法优势：</p><blockquote><ol><li>使用高斯核编码训练样本，使得网络能够更快地收敛；</li><li>能够在不引入其他组件，比如FPN的基础上，降低ambiguous 以及low-quality的样本;</li><li>不需要any offset predictions来矫正结果;<br><img src="https://github.com/ZJULearning/ttfnet/raw/master/imgs/structure.png" alt="undefined"><br>具体思路：<br>给定一张图片，利用网络分别预测物体中心在什么位置 $\hat{H} \in R^{N \times C \times \frac{H}{r} \times \frac{W}{r}}$和回归box size的向量 $\hat{S} \in R^{N \times 4 \times \frac{\dot{H}}{r} \times \frac{W}{r}}$ 在分类和回归时同时使用高斯核，并通过 $\alpha$ 和 $\beta$  控制高斯核的尺寸。这里和CenterNet一样，将目标检测分为两个部分：object localization和 size regression；</li></ol></blockquote><h3 id="1-object-localization"><a href="#1-object-localization" class="headerlink" title="1.object  localization"></a>1.object  localization</h3><p>假设一张图上有M个annotated box，其中第m个标注box属于$c_m$  类别，二维的高斯核：</p><script type="math/tex; mode=display">K_{m}(x, y)=\exp \left(-\frac{\left(x-x_{0}^{2}\right)}{2 \sigma_{2}^{x}}-\frac{\left(y-y_{0}^{2}\right)}{2 \sigma_{2}^{y}}\right)</script><p>高斯分布的峰值，也就是box中心，被视为正目标，而任何其他像素被视为负目标。对那些与较大的分配值对应的负目标的惩罚将会减轻。使用修正 focal loss.</p><h3 id="2-size-regression"><a href="#2-size-regression" class="headerlink" title="2. size regression"></a>2. size regression</h3><p><img src="/img/ttfnet_sample_define.png" alt="img"><br>给定特征图尺度上的m个ground truth box，采用另一个高斯核生成 $S_{m} \in R^{1 \times \frac{H}{r} \times \frac{W}{r}}$,核大小是由β 如上所述控制。Sm中的非零部分称为高斯区域Am，如图3所示。由于Am总是在它们的框中，所以在本文的其余部分中，它也被称为子区域。将子区域中的每个像素作为训练样本。损失函数部分使用的是GIOU loss。</p><h3 id="3-experiment"><a href="#3-experiment" class="headerlink" title="3. experiment"></a>3. experiment</h3><p><img src="/img/ttfnet_exp.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/1909.00700.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Training-Time-Friendly Network for Real-Time Object Detection&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/ZJULearning/ttfnet&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/ZJULearning/ttfnet&lt;/a&gt;&lt;br&gt;&lt;strong&gt;问题&lt;/strong&gt;： 这篇论文主要是解决CenterNet存在的一些问题，CenterNet在推理速度精度上表现不差，但存在训练耗时的问题，CenterNet官方代码训练coco需要2.5天，因此这篇论文主要是如何改进CenterNet训练耗时长的问题。作者基于CenterNet，在回归box时，作者认为由于CenterNet进行回归时仅仅使用中心点的那些目标是次优的选择，同时作者借助了FCOS是使用Ground Truth回归所有样本的思想，这里作者想到使用高斯核来选择回归样本，一方面可以增加回归的信息量，提升检测精度，同时作者指出这样有增加batch size的效果，可以使用更大的学习率以及有更短的训练时间。并且没有引入FCOS中耗时的NMS后处理过程，使得Inference时间减小。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://blog.nicehuster.cn/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>Generalized Focal Loss &amp;&amp; Generalized Focal Loss V2</title>
    <link href="https://blog.nicehuster.cn/2021/03/12/gfl/"/>
    <id>https://blog.nicehuster.cn/2021/03/12/gfl/</id>
    <published>2021-03-12T11:13:39.000Z</published>
    <updated>2022-04-12T03:38:57.426Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/pdf/2006.04388.pdf" target="_blank" rel="noopener">Generalized Focal Loss</a> &amp;&amp; <a href="https://arxiv.org/pdf/2011.12885.pdf" target="_blank" rel="noopener">Generalized Focal Loss V2</a><br><strong>代码链接：</strong> <a href="https://github.com/implus/GFocal" target="_blank" rel="noopener">https://github.com/implus/GFocal</a> &amp;&amp; <a href="https://github.com/implus/GFocalV2" target="_blank" rel="noopener">https://github.com/implus/GFocalV2</a><br><strong>整体信息：</strong>这倆篇论文都是出自南理工的李翔，从定位质量估计和边框表示的角度上对现有检测方法进行改进。在任意one-stage检测器上都能涨1-2点。GFLv1解决的是目前检测方法存在的两个问题：1）在训练和推理的时候，分类和质量估计的不一致性；2）狄拉克分布针对复杂场景下（模糊和不确定性边界）存在不灵活的问题。GFLv2是在v1的基础上，进一步地，使用概率分布的方式评估检测框质量，相比v1更有效，v2在ATSS上可以无痛涨点2-3点。</p><a id="more"></a><h3 id="GFLv1"><a href="#GFLv1" class="headerlink" title="GFLv1"></a>GFLv1</h3><p>方法的出发点主要是解决俩个问题：1）classification score 和 IoU/centerness score 训练测试不一致；2）bbox regression 采用的表示不够灵活，没有办法建模复杂场景下的uncertainty；</p><p><strong>问题1：训练测试不一致性</strong></p><p><img src="/img/gflv1_problem.png" alt="img"></p><p>不一致主要体先在俩方面：</p><blockquote><ol><li>用法不一致。训练的时候，分类和质量估计各自训记几个儿的，但测试的时候却又是乘在一起作为NMS score排序的依据，这个操作显然没有end-to-end，必然存在一定的gap；</li><li>对象不一致。借助Focal Loss的力量，分类分支能够使得少量的正样本和大量的负样本一起成功训练，但是质量估计（回归）通常就只针对正样本训练；</li></ol></blockquote><p><strong>问题2：bbox regression不灵活，无法建模复杂场景</strong><br>在复杂场景中，边界框的表示具有很强的不确定性，而现有的框回归本质都是建模了非常单一的狄拉克分布，非常不flexible。我们希望用一种general的分布去建模边界框的表示。问题二如图所示（比如被水模糊掉的滑板，以及严重遮挡的大象）：<br><img src="/img/gflv1_problem2.png" alt="img"></p><p>解决方法：</p><p><img src="/img/gflv1_solution.png" alt="img"></p><p>具体地，</p><h4 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h4><p>在分类分支上不再使用one-hot型类别标签监督，而是使用soft-one-hot 标签，标签值使用地iou值；如此，完美的将类别和位置信息结合利用起来了；传统地交叉熵、focal loss只能处理离散标签；因此作者在focal loss基础上提出 Quality Focal Loss，具体形式如下：</p><script type="math/tex; mode=display">\mathbf{Q F L}(\sigma)=-|y-\sigma|^{\beta}((1-y) \log (1-\sigma)+y \log (\sigma))</script><h4 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h4><p>考虑到真实的分布通常不会距离标注的位置太远，作者又额外加了个loss，希望网络能够快速地聚焦到标注位置附近的数值，使得他们概率尽可能大。Distribution Focal Loss：</p><script type="math/tex; mode=display">\mathbf{D F L}\left(\mathcal{S}_{i}, \mathcal{S}_{i+1}\right)=-\left(\left(y_{i+1}-y\right) \log \left(\mathcal{S}_{i}\right)+\left(y-y_{i}\right) \log \left(\mathcal{S}_{i+1}\right)\right)</script><p>其形式上与QFL的右半部分很类似，含义是以类似交叉熵的形式去优化与标签y最接近的一左一右两个位置的概率，从而让网络快速地聚焦到目标位置的邻近区域的分布中去。简单来讲，就是预测目标框边界附近地n个位置地概率值，然后基于概率值和n个位置进行加权，得到目标框边界值；目标的四个边框均采用上述操作，然后基于组合的目标框使用GIOU loss进行优化。</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><script type="math/tex; mode=display">\mathcal{L}=\frac{1}{N_{\text {pos }}} \sum_{z} \mathcal{L}_{\mathcal{Q}}+\frac{1}{N_{\text {pos }}} \sum_{z} \mathbf{1}_{\left\{c_{z}^{*}>0\right\}}\left(\lambda_{0} \mathcal{L}_{\mathcal{B}}+\lambda_{1} \mathcal{L}_{\mathcal{D}}\right)</script><p>上面是训练loss函数，第一部分是分类分支的QFL；第二部分是回归分支的GIOU loss和Distribution Focal Loss；</p><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p><img src="/img/gflv1_exp.png" alt="img"></p><h3 id="GFLv2"><a href="#GFLv2" class="headerlink" title="GFLv2"></a>GFLv2</h3><p>问题：在GFLv1中，作者提出了对边界框进行一个一般化的分布表示建模。有了这个可以学习的表示之后，基本上那些非常清晰明确的边界，它的分布都很尖锐；而模糊定义不清的边界它们学习到的分布基本上会平下来，而且有的时候还经常出现双峰的情况。作者为了充分利用这分布，提出了GFLv2，利用分布形状的统计量去知道最终最终定位质量的估计。具体网络结构如下：<br><img src="/img/gflv2_framework.png" alt="img"></p><h4 id="核心思路"><a href="#核心思路" class="headerlink" title="核心思路"></a>核心思路</h4><p>直接取学习到的分布（分布是用离散化的多个和为1的回归数值表示的，详情参考GFLV1）的Topk数值。其实理解起来也不难，因为所有数值和为1，如果分布非常尖锐的话，Topk这几个数通常就会很大；反之Topk就会比较小。选择Topk还有一个重要的原因就是它可以使得我们的特征与对象的scale尽可能无关，如下图所示：<br><img src="/img/gflv2_topk.png" alt="img"><br>简单来说就是长得差不多形状的分布要出差不多结果的数值，不管它峰值时落在小scale还是大scale。我们把4条边的分布的Topk concat在一起形成一个维度非常低的输入特征向量（可能只有10+或20+），用这个向量再接一个非常小的fc层（通常维度为32、64)，最后再变成一个Sigmoid之后的scalar乘到原来的分类表征中。具体model参考上图，其中红色框就是比GFLV1多出来的Distribution-Guided Quality Predictor部分，这也就是本文的核心。</p><h4 id="实验结果-1"><a href="#实验结果-1" class="headerlink" title="实验结果"></a>实验结果</h4><p><img src="/img/gflv2_exp.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2006.04388.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Generalized Focal Loss&lt;/a&gt; &amp;amp;&amp;amp; &lt;a href=&quot;https://arxiv.org/pdf/2011.12885.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Generalized Focal Loss V2&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt; &lt;a href=&quot;https://github.com/implus/GFocal&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/implus/GFocal&lt;/a&gt; &amp;amp;&amp;amp; &lt;a href=&quot;https://github.com/implus/GFocalV2&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/implus/GFocalV2&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这倆篇论文都是出自南理工的李翔，从定位质量估计和边框表示的角度上对现有检测方法进行改进。在任意one-stage检测器上都能涨1-2点。GFLv1解决的是目前检测方法存在的两个问题：1）在训练和推理的时候，分类和质量估计的不一致性；2）狄拉克分布针对复杂场景下（模糊和不确定性边界）存在不灵活的问题。GFLv2是在v1的基础上，进一步地，使用概率分布的方式评估检测框质量，相比v1更有效，v2在ATSS上可以无痛涨点2-3点。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://blog.nicehuster.cn/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>ATSS自适应选择正负样本的目标检测方法</title>
    <link href="https://blog.nicehuster.cn/2021/02/23/atss/"/>
    <id>https://blog.nicehuster.cn/2021/02/23/atss/</id>
    <published>2021-02-23T11:13:39.000Z</published>
    <updated>2021-02-24T08:09:44.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/abs/1912.02424" target="_blank" rel="noopener">Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection</a><br><strong>代码链接：</strong><a href="https://github.com/sfzhang15/ATSS" target="_blank" rel="noopener">https://github.com/sfzhang15/ATSS</a><br><strong>整体信息：</strong>这篇论文是中科院张士峰发表在CVPR2020上的一篇文章，论文通过大量实验证明指出anchor-based方法和anchor-free方法的性能差异主要来自于正负样本的选取上，基于此提出ATSS(Adaptive Training Sample Selection)方法，该方法通过自动统计标注GT上的相关统计特征自适应的选择合适的anchor box作为正负样本，在不带来额外计算量和参数的情况下，能够大幅提升模型的性能。</p><a id="more"></a><h3 id="1-差异分析"><a href="#1-差异分析" class="headerlink" title="1.差异分析"></a>1.差异分析</h3><p>在不考虑各种trick的情况下，anchor-based方法RetinaNet和anchor-free方法FCOS主要存在两个差异：1）正负样本选取；2）回归起始状态；为公平比较，降低两者方法的差异性，在实验中将RetinaNet的anchor数改为1降低差异性，方便与FCOS比较，作者后续在实验中也验证不同anchor数对模型性能的影响。</p><h4 id="1-Inconsistency-Removal"><a href="#1-Inconsistency-Removal" class="headerlink" title="1).Inconsistency Removal"></a>1).Inconsistency Removal</h4><p><img src="/img/atss_Inconsistency%20Removal.png" alt="atss_Inconsistency Removal"></p><p>由于FCOS加入了很多trick，为了与RetinaNet进行公平比较，对FCOS上的trick进行对齐，包括GroupNorm、GIoU loss、限制正样本必须在GT内、Centerness branch以及添加可学习的标量控制FPN的各层的尺寸。具体结果如上面表格所示，最终的RetinaNet与FCOS相差无几，但仍然存在0.8个点的差异。</p><h4 id="2-Essential-Difference"><a href="#2-Essential-Difference" class="headerlink" title="2).Essential Difference"></a>2).Essential Difference</h4><p>在经过上述实验对齐之后，RetinaNet和FCOS仅存在两个差异：1）正负样本选取；2）回归起始状态；针对这两个差异，论文通过实验做进一步分析。</p><p><img src="/img/image-20210224115350117.png" alt="image-20210224115350117"></p><ul><li><p><strong>Classification</strong></p><p>在RetinaNet中，通过划分IOU阈值区间来区分正负样本，$\mathrm{IoU}&gt;\theta_{p}$ 置为正样本，$\mathrm{IoU}&lt;\theta_{n}$ 置为负样本，至于区间内所有样本忽略；在FCOS使用空间尺寸和尺寸限制来区分正负anchor point，正样本首先必须在GT box内，其次需要是GT尺寸对应的层，其余均为负样本；</p></li></ul><p><img src="/img/image-20210224115743063.png" alt="image-20210224115743063"></p><ul><li><p><strong>Regression</strong></p><p>在RetinaNet中，模型预测的是相对anchor box的4个偏移量，通过4个偏移量对anchor box进行调整；而在FCOS则预测的是相对于anchor point的4个偏移量，基于anchor point和4个偏移量组成bbox；</p><p><img src="/img/image-20210224143241390.png" alt="image-20210224143241390"></p></li><li><p><strong>Conclusion</strong></p><p>基于上述两个差异，论文做了实验进行交叉对比，实验结果如上表格2所示，在相同正负样本定义下的RetinaNet和FCOS性能几乎一样，不同的定义方法性能差异较大，具体可以看下表格的纵向对比结果；而从横向对比结果上来看，无论是基于box或基于Point的形式，指标基本不发生变化，这也印证了回归初始状态的不同对模型性能影响不大。所以，基本可以确定正负样本的选取是影响模型性能的关键；</p></li></ul><h3 id="2-自适应样本选取（Adaptive-Training-Sample-Selection）"><a href="#2-自适应样本选取（Adaptive-Training-Sample-Selection）" class="headerlink" title="2. 自适应样本选取（Adaptive Training Sample Selection）"></a>2. 自适应样本选取（Adaptive Training Sample Selection）</h3><p><img src="/img/image-20210224151209904.png" alt="image-20210224151209904"></p><h4 id="1-算法流程"><a href="#1-算法流程" class="headerlink" title="1).算法流程"></a>1).算法流程</h4><p>论文提出基于统计特征自适应样本选取的方法具体流程如上图所示，具体来说，对于每个标注gt框g，首先针对每个FPN层级基于L2距离找到与目标中心点最近的k个anchor box，计算anchor box与gt box的IOU值，计算所有iou值的均值$m_{g}$和标准差$v_{g}$，基于均值和阈值的计算结果，得到IOU阈值 $t_{g}=m_{g}+v_{g}$ ，最后选择阈值大于等于$t_{g}$ 且中心点位于gt box内的box作为最后的输出。如果anchor box对应多个gt，则选择IoU最大的gt。</p><h4 id="2-算法思想"><a href="#2-算法思想" class="headerlink" title="2).算法思想"></a>2).算法思想</h4><p>从思想上来看，自适应样本选取的方法遵循如下几点：</p><blockquote><ul><li>基于目标中心距离远近程度来选择样本；在RetinaNet中，anchor box与gt中心点越近一般IoU越高，而在FCOS中，中心点越近一般预测的质量越高；</li><li>使用使用iou的统计量均值和方差之和作为iou阈值；均反映的是预设的anchor box与gt的匹配程度，均值高则应当提高阈值来调整正样本，均值低则应当降低阈值来调整正样本。标准差反映的是适合GT的FPN层数，标准差高则表示高质量的anchor box集中在一个层中，应将阈值加上标准差来过滤其他层的anchor box，低则表示多个层都适合该gt，将阈值加上标准差来选择合适的层的anchor box，均值和标准差结合作为IoU阈值能够很好地自动选择对应的特征层上合适的anchor box；</li><li>正样本的中心必须位于目标内；若anchor box的中心点不在gt区域内，则其会使用非gt区域的背景特征进行预测，影响模型性能，这种情况应予以剔除；</li><li>几乎无超参数；从上面的算法流程可以看出，唯一的超参数就是k，论文后面也通过实验验证ATSS算法对于k值的选取不明感；</li></ul></blockquote><h4 id="3-ATSS实验对比"><a href="#3-ATSS实验对比" class="headerlink" title="3).ATSS实验对比"></a>3).ATSS实验对比</h4><p><img src="/img/image-20210224154804455.png" alt="image-20210224154804455"></p><p>将ATSS应用到RetinaNet和FCOS上测试效果：</p><ul><li>将RetinaNet中的正负样本替换为ATSS，AP提升了2.3%；</li><li>在FCOS上的应用主要用两种：lite版本采用ATSS的思想，从选取GT内的anchor point改为选取每层离GT最近的topk个候选anchor point，提升了0.8%AP；full版本将FCOS的anchor point改为长宽为8S的anchor box来根据ATSS选择正负样本，但仍然使用原始的回归方法，提升了1.4%AP。两种方法找到的anchor point在空间位置上大致相同，但是在FPN层上的选择不太一样。从结果来看，自适应的选择方法比固定的方法更有效。至于与其他SOTA方法的对比，可以直接参考论文。</li></ul><h3 id="3-Discussion"><a href="#3-Discussion" class="headerlink" title="3.Discussion"></a>3.Discussion</h3><p>上面的实验对比可以看出RetinaNet实验中没有涉及任何关于anchor宽高比ratio和尺度scale相关的参数，而且在实验中仅使用了一个anchor,在原始RetinaNet中，一个目标选择的是9个预定义的不同尺度和宽高比的anchor box，因此论文补充实验测试了在不同尺度、不同宽高比以及不同anchors数量下的实验结果。具体实验结果，可以直接参考原文，从论文po出来的实验结果来看，ATSS对于anchor尺度和宽高比并不敏感，而且在每个位置设定多个anchor box是无用的操作，关键在于选择合适的正样本。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/1912.02424&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/sfzhang15/ATSS&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/sfzhang15/ATSS&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这篇论文是中科院张士峰发表在CVPR2020上的一篇文章，论文通过大量实验证明指出anchor-based方法和anchor-free方法的性能差异主要来自于正负样本的选取上，基于此提出ATSS(Adaptive Training Sample Selection)方法，该方法通过自动统计标注GT上的相关统计特征自适应的选择合适的anchor box作为正负样本，在不带来额外计算量和参数的情况下，能够大幅提升模型的性能。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://blog.nicehuster.cn/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读： A Simple and Strong Anchor-freeObject Detector(FCOS_imprv)</title>
    <link href="https://blog.nicehuster.cn/2021/02/22/FCOS_improv/"/>
    <id>https://blog.nicehuster.cn/2021/02/22/FCOS_improv/</id>
    <published>2021-02-22T11:13:39.000Z</published>
    <updated>2021-02-23T02:46:30.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> Zhi Tian,Adelaide<br><strong>代码链接：</strong><a href="https://github.com/aim-uofa/AdelaiDet" target="_blank" rel="noopener">https://github.com/aim-uofa/AdelaiDet</a><br><strong>整体框架：</strong> 这篇论文是fcos原作者重新修改后发表在PAMI上面的一篇文章，fcos_imprv和fcos整体思想是一致的，只是在原作上做了部分修改，是网络性能得到进一步提升，在backbone为ResNet-101-FPN上面的性能从41.0提升到43.2，提升明显。本文主要是记录一下fcos_imprv相比原方法的一些改进和提升方法。<br><a id="more"></a></p><h3 id="改进点"><a href="#改进点" class="headerlink" title="改进点"></a>改进点</h3><p>对于原始fcos方法的原理介绍，可以参考<a href="https://nicehuster.github.io/2019/04/15/FCOS/" target="_blank" rel="noopener">这里</a> ，这里详细说一下改进的地方：</p><h4 id="1-正负样本"><a href="#1-正负样本" class="headerlink" title="1.正负样本"></a>1.正负样本</h4><p>作者对正负样本的指定做了修改，在原始fcos中，对于目标被的所有特征点均指定为正样本，以及该特征点到目标边界的距离满足所处的FPN层的约束；而在FCOS_imprv中则要求指定只有在目标中心区域的特征点才为正样本；目标中心区域的大小为：</p><script type="math/tex; mode=display">\left(c_{x}-r s, c_{y}-r s, c_{x}+r s, c_{y}+r s\right)</script><p>其中 $\left(c_{x}, c_{y}\right)$ 为目标中心，s为当前层的stride，r 为超参数，在论文中r设置为1.5。这个代码的实现可以看一下mmdetection中FCOS的实现：<a href="mmdet/models/dense_heads/fcos_head.py">fcos_head.py</a> 其中center_sample_radius参数默认设置为1.5.</p><h4 id="2-回归目标修改"><a href="#2-回归目标修改" class="headerlink" title="2.回归目标修改"></a>2.回归目标修改</h4><p>Fcos的回归目标直接是特征点到目标边界的距离，由于Head是共用的，所以在预测时为每个level预设一个可学习的scale因子，而fcos_imprv则加入stride，变得更适应FPN的尺寸，可学习的scale因子依然使用，具体形式如下：</p><script type="math/tex; mode=display">\begin{array}{l}l^{*}=\left(x-x_{0}^{(i)}\right) / s, \quad t^{*}=\left(y-y_{0}^{(i)}\right) / s \\r^{*}=\left(x_{1}^{(i)}-x\right) / s, \quad b^{*}=\left(y_{1}^{(i)}-y\right) / s\end{array}</script><p>这一部分的代码实现，可以看一下这个地方：<a href="https://github.com/open-mmlab/mmdetection/blob/master/mmdet/models/dense_heads/fcos_head.py#L147" target="_blank" rel="noopener">fcos_head.py#L147</a> 其中scale为可学习参数。</p><h4 id="3-centerness-修改"><a href="#3-centerness-修改" class="headerlink" title="3.centerness 修改"></a>3.centerness 修改</h4><p>在原始fcos中，centerness是和分类分支放在一起预测，而在fcos_imprv中，将该分支移至回归分支，以便更好的利用位置相关信息；</p><p><img src="/img/fcos_imprv-3999865.png" alt="fcos_imprv"></p><h4 id="4-回归损失函数"><a href="#4-回归损失函数" class="headerlink" title="4.回归损失函数"></a>4.回归损失函数</h4><p>在fcos方法中，训练损失函数如下：</p><script type="math/tex; mode=display">\begin{aligned}L\left(\left\{\boldsymbol{p}_{x, y}\right\},\left\{\boldsymbol{t}_{x, y}\right\}\right) &=\frac{1}{N_{\mathrm{pos}}} \sum_{x, y} L_{\mathrm{cls}}\left(\boldsymbol{p}_{x, y}, c_{x, y}^{*}\right) \\&+\frac{\lambda}{N_{\mathrm{pos}}} \sum_{x, y} \mathbb{1}_{\left\{c_{x, y}^{*}>0\right\}} L_{\mathrm{reg}}\left(\boldsymbol{t}_{x, y}, \boldsymbol{t}_{x, y}^{*}\right)\end{aligned}</script><p>第一项是分类分支的损失函数，使用的是的focal loss，第二项是回归分支的损失函数。在原始fcos中，使用的IOU loss，而在fcos_imprv中，使用的GIOU loss。这里顺便介绍一下IOU loss和GIOU loss的区别。</p><blockquote><ul><li>IOU loss，原始smooth l1 loss在计算目标检测的 bbox loss时，都是独立的求出4个点的 loss，然后相加得到最终的 bbox loss。这种做法的默认4个点是相互独立的，而目标检测评价的是IOU指标，与实际不符。IOU loss计算如下：</li></ul><script type="math/tex; mode=display">\text { IoU loss }=-\ln \operatorname{IoU}\left(bbox_\text {gt }, b b o x_{\text {pred }}\right)</script><ul><li><p>GIOU loss，iou loss计算的是预测框与gt框存在重叠情况下的损失函数，而对于不重叠情况下，损失函数不可导，无法优化两个框不相交的情况。此外，如果iou是确定的，其iou值是无法确定两个框是如何相交的。GIOU 计算如下：</p><script type="math/tex; mode=display">G I o U=I o U-\frac{|C \backslash(A \cup B)|}{|C|}</script><p>GIoU 的实现方式如上式，其中 C 为 A 和 B 的外接矩形。用 C 减去 A 和 B 的并集除以 C 得到一个数值，然后再用 A 和 B 的 IoU 减去这个数值即可得到 GIoU 的值。可以看出：1）GIoU 取值范围为 [-1, 1]，在两框重合时取最大值1，在两框无限远的时候取最小值-1；2）与 IoU 只关注重叠区域不同，GIoU不仅关注重叠区域，还关注其他的非重合区域，能更好的反映两者的重合度。</p><p>GIOU loss计算如下：</p><script type="math/tex; mode=display">\mathcal{L}_{G I o U}= 1-GIOU</script></li></ul></blockquote><h4 id="5-分数计算"><a href="#5-分数计算" class="headerlink" title="5.分数计算"></a>5.分数计算</h4><p>最终分数的计算，fcos采用分类分数以及center-ness之积，fcos_imprv则采用分类分数以及center-ness之积的平方根：</p><script type="math/tex; mode=display">\boldsymbol{s}_{x, y}=\sqrt{\boldsymbol{p}_{x, y} \times o_{x, y}}</script><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="/img/fcos_exp.png" alt="fcos_exp"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; Zhi Tian,Adelaide&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/aim-uofa/AdelaiDet&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/aim-uofa/AdelaiDet&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体框架：&lt;/strong&gt; 这篇论文是fcos原作者重新修改后发表在PAMI上面的一篇文章，fcos_imprv和fcos整体思想是一致的，只是在原作上做了部分修改，是网络性能得到进一步提升，在backbone为ResNet-101-FPN上面的性能从41.0提升到43.2，提升明显。本文主要是记录一下fcos_imprv相比原方法的一些改进和提升方法。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://blog.nicehuster.cn/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>DETR 基于transformer的目标检测方法</title>
    <link href="https://blog.nicehuster.cn/2020/10/27/detr/"/>
    <id>https://blog.nicehuster.cn/2020/10/27/detr/</id>
    <published>2020-10-27T11:13:39.000Z</published>
    <updated>2020-10-28T08:49:02.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/pdf/2005.12872.pdf" target="_blank" rel="noopener">End to End Object Detection with Transformers</a><br><strong>代码链接：</strong> <a href="https://github.com/facebookresearch/detr" target="_blank" rel="noopener">https://github.com/facebookresearch/detr</a><br><strong>整体信息：</strong> 这是FAIR最近新出了一篇用transformer做检测的文章。相比于以往所有的检测方法不同的是，没有使用先验知识比如anchor设计，以及后处理步骤比如nms等操作。而是使用transformer预测集合的形式，直接输出目标位置及类别信息。在object detection上DETR准确率和运行时间上和Faster RCNN相当；将模型generalize到panoptic segmentation任务上，DETR表现甚至还超过了其他的baseline.</p><a id="more"></a><h3 id="1-概览"><a href="#1-概览" class="headerlink" title="1.概览"></a>1.概览</h3><p><img src="/img/detr.png" alt="detr"></p><p>上图展示的是DETR的整体结构图。如上图所示，可以看出两个关键的部分：</p><blockquote><ul><li>使用transformer 的encoder-decoder结构生成N个 box predictions；其中N为事先设计的、远大于图像中object数的一个整数；</li><li>设计了bipartite matching loss，基于预测的box和gt box的二分图匹配计算loss，从而使得预测的box更接近gt box；</li></ul></blockquote><p>这篇文章的撰写风格比较诡异，不是按照正常的网络结构的先后顺序来介绍各个部分。而是先介绍了bipartite matching loss，再介绍transformer的encoder和decoder。为了便于理解，我这里还是按照网络结构的先后顺序来介绍各个模块。</p><h3 id="2-Transformer"><a href="#2-Transformer" class="headerlink" title="2.Transformer"></a>2.Transformer</h3><p><img src="/img/detr-transformer.png" alt="detr-transformer"></p><p>上图展示的是transformer的结构，其中包括encoder、decoder以及FFN三部分。</p><h4 id="2-1-Transformer-Encoder"><a href="#2-1-Transformer-Encoder" class="headerlink" title="2.1 Transformer Encoder"></a>2.1 Transformer Encoder</h4><p>如上图左侧所示，在DETR中，首先输入图片 $x_{\mathrm{img}} \in \mathbb{R}^{3 \times H_{0} \times W_{0}}$ 经过CNN backbone处理后，输出feature map$f \in \mathbb{R}^{C \times H \times W}$ ，一般 $C=2048,H, W=\frac{H_{0}}{32}, \frac{W_{0}}{32}$ 。然后将backbone输出的feature map和position encoding相加，输入Transformer Encoder中处理，得到用于输入到Transformer Decoder的image embedding。由于Transformer的输入为序列化数据，因此会对backbone输出的feature map做进一步处理转化为序列化数据，具体处理包括如下：</p><blockquote><ul><li><strong>维度压缩</strong>：使用1x1卷积将feature map 维度从C压缩为d,生成新的feature map $z_{0} \in \mathbb{R}^{d \times H \times W}$ ；</li><li><strong>转化为序列化数据：</strong>将空间的维度（高和宽）压缩为一个维度，即把上一步得到的$d \times H \times W$维的feature map通过reshape成$d \times H W$维的feature map；</li><li><strong>加上positoin encoding:</strong> 由于transformer模型是permutation-invariant（转置不变性，也可以理解为和位置无关），而显然图像中目标是具有空间信息的，与原图的位置有关，所以需要加上position encoding反映位置信息。具体生成的方法后续补充讲解。</li></ul></blockquote><h4 id="2-2-Transformer-Decoder"><a href="#2-2-Transformer-Decoder" class="headerlink" title="2.2 Transformer Decoder"></a>2.2 Transformer Decoder</h4><p>这部分的话，有两个输入，一个是Transformer Encoder的输出，另一个是object queries。这里讲一下object queries，object queries有N个，N是一个事先设定的、比远远大于image中object个数的一个整数），输入Transformer Decoder后分别得到N个 output embedding，然后经过FFN处理之后，输出N个box的位置和类别信息。object queries具体是什么，论文阐述的很模糊。</p><h4 id="2-3-FFN"><a href="#2-3-FFN" class="headerlink" title="2.3 FFN"></a>2.3 FFN</h4><p>这个是网络结构最后的输出部分，在DETR中，其实有两种FFN：一种预测bounding box的中心位置、高和宽，第二种预测class标签。下图是更细节的DETR Transformer结构，大家感兴趣可以仔细看一下。</p><p><img src="/img/detr-ffn.png" alt="detr-ffn"></p><h3 id="3-Loss设计"><a href="#3-Loss设计" class="headerlink" title="3. Loss设计"></a>3. Loss设计</h3><p>前面讲到transformer经过FFN之后会预测输出N个prediction boxes,其中，N是事先设计好的一个远大于image objects的正整数。通过得到这N个prediction boxes和image objects的最优二分图匹配，通过计算配对的box pair的loss来对模型进行优化。</p><h4 id="3-1-最优二分图匹配"><a href="#3-1-最优二分图匹配" class="headerlink" title="3.1 最优二分图匹配"></a>3.1 最优二分图匹配</h4><p>假设对于一张图来说，image objects的个数为m，由于N是事先设计好的一个远大于m的正整数，所以 N&gt;&gt;m，即生成的prediction boxes的数量会远远大于image objects 数量。这样怎么做匹配？</p><p>为了解决这个问题，作者人为构造了一个新的物体类别 $\varnothing$ (表示没有物体)并加入image objects中，上面所说到的多出来的N-m个个prediction embedding就会和类别 $\varnothing$ 配对。这样就可以将prediction boxes和image objects的配对看作两个等容量的集合的二分图匹配了。</p><p>设计好匹配的cost，就可以使用<a href="https://www.cxyxiaowu.com/874.html" target="_blank" rel="noopener">匈牙利算法</a>快速地找到使cost最小的二分图匹配方案了。cost计算如下：</p><script type="math/tex; mode=display">\hat{\sigma}=\underset{\sigma \in \widetilde{S}_{N}}{\arg \min } \sum_{i}^{N} \mathcal{L}_{\operatorname{match}}\left(y_{i}, \hat{y}_{\sigma(i)}\right)</script><p>对应单个prediction box和image object匹配cost $\mathcal{L}_{\operatorname{match}}\left(y_{i}, \hat{y}_{\sigma(i)}\right)$计算如下：</p><script type="math/tex; mode=display">-\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \hat{p}_{\sigma(i)}\left(c_{i}\right)+\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \mathcal{L}_{\mathrm{box}}\left(b_{i}, \hat{b}_{\sigma(i)}\right)</script><p>其中，</p><p>$c_i$ 第i个image object的class标签，$\sigma(i)$ 表示与第i个image object匹配的prediction box的index;</p><p>$\mathbb{1}_{c_{i} \neq \varnothing}$ 是一个函数，当 $c_{i} \neq \phi$ 时为1，否则为0；</p><p>$\hat{p}_{\sigma(i)}\left(c_{i}\right)$ 表示Transformer预测第$\sigma(i)$ 个prediction box为类别$c_i$ 的概率；</p><p>$b_{i}, \hat{b}_{\sigma(i)}$ 分别为第i个image object和第$\sigma(i)$ 个prediction box的位置向量；</p><p>$\mathcal{L}_{\mathrm{box}}\left(b_{i}, \hat{b}_{\sigma(i)}\right)$ 计算的是ground truth box和prediction box之间的距离；具体计算方式论文有提及：</p><script type="math/tex; mode=display">\lambda_{\text {iou }} \mathcal{L}_{\text {iou }}\left(b_{i}, \hat{b}_{\sigma(i)}\right)+\lambda_{\mathrm{L} 1}\left\|b_{i}-\hat{b}_{\sigma(i)}\right\|_{1}</script><p>计算box的距离实验的IOU loss以及L1 loss。</p><p>这样，我们就完全定义好了每对prediction box和 image object 配对时的cost。再利用匈牙利算法即可得到二分图最优匹配。</p><h4 id="3-2-计算set-prediction-loss"><a href="#3-2-计算set-prediction-loss" class="headerlink" title="3.2 计算set prediction loss"></a>3.2 计算set prediction loss</h4><p>上面我们得到了prediction boxes和image objects之间的最优匹配。这里我们基于这个最优匹配，来计算set prediction loss，即评价Transformer生成这些prediction boxes的效果好坏。 set prediction loss的计算如下：</p><script type="math/tex; mode=display">\mathcal{L}_{\text {Hungarian }}(y, \hat{y})=\sum^{N}\left[-\log \hat{p}_{\hat{\sigma}(i)}\left(c_{i}\right)+\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \mathcal{L}_{\text {box }}\left(b_{i}, \hat{b}_{\hat{\sigma}}(i)\right)\right]</script><p>其中，$\hat{\sigma}$ 为最优匹配。将第 i个image object匹配到第$\hat{\sigma}(i)$ 个prediction boxes。这里值得注意的是，和上面计算cost不太一样的地方是这里计算分类概率使用的是log对数的形式；将匹配到类别 $\varnothing$ 的概率考虑进去了，而前面cost的计算中则直接将其置为0了；</p><h3 id="4-实验"><a href="#4-实验" class="headerlink" title="4.实验"></a>4.实验</h3><p>论文中实验细节较多，而且实验结果也比较solid，这里不细讲，有兴趣的可以直接看原文，这里贴一张在coco上实验结果对比：</p><p><img src="/img/detr-exp.png" alt="detr-exp"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2005.12872.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;End to End Object Detection with Transformers&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt; &lt;a href=&quot;https://github.com/facebookresearch/detr&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/facebookresearch/detr&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt; 这是FAIR最近新出了一篇用transformer做检测的文章。相比于以往所有的检测方法不同的是，没有使用先验知识比如anchor设计，以及后处理步骤比如nms等操作。而是使用transformer预测集合的形式，直接输出目标位置及类别信息。在object detection上DETR准确率和运行时间上和Faster RCNN相当；将模型generalize到panoptic segmentation任务上，DETR表现甚至还超过了其他的baseline.&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://blog.nicehuster.cn/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>抠图神器U^2-Net</title>
    <link href="https://blog.nicehuster.cn/2020/09/14/U-2-Net/"/>
    <id>https://blog.nicehuster.cn/2020/09/14/U-2-Net/</id>
    <published>2020-09-14T11:13:39.000Z</published>
    <updated>2020-09-18T07:17:55.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> Going Deeper with Nested U-Structure for Salient Object Detection<br><strong>代码链接：</strong><a href="https://github.com/NathanUA/U-2-Net" target="_blank" rel="noopener">https://github.com/NathanUA/U-2-Net</a><br><strong>整体信息：</strong> 这是发表在PR2020上的一篇关于显著性检测的文章，作者是秦雪彬。从标题上可以看到本文的一个idea是设计了一个deeper的U型结构的网络解决显著性目标检测问题。作者认为，目前显著性目标检测有两种主流思路，一为多层次深层特征集成multi-level deep feature integration，一为多尺度特征提取Multi-scale feature extraction。多层次深层特征集成方法主要集中在开发更好的多层次特征聚合策略上。而多尺度特征提取这一类方法旨在设计更新的模块，从主干网获取的特征中同时提取局部和全局信息。而几乎所有上述方法，都是为了更好地利用现有的图像分类的Backbones生成的特征映射。而作者另辟蹊径，提出了一种新颖而简单的结构，它直接逐级提取多尺度特征，用于显著目标检测，而不是利用这些主干的特征来开发和添加更复杂的模块和策略。下图是该方法与其他方法的一个比较：</p><a id="more"></a><p><img src="/img/image-20200918105536819.png" alt="image-20200918105536819"></p><h3 id="显著性检测"><a href="#显著性检测" class="headerlink" title="显著性检测"></a>显著性检测</h3><p>在讲这篇文章之前，有必要先了解显著性检测这个任务。</p><blockquote><p><a href="https://zhuanlan.zhihu.com/p/115002897" target="_blank" rel="noopener">显著性检测</a></p></blockquote><p><strong>显著性检测</strong>，就是使用图像处理技术和计算机视觉算法来定位图片中最“显著”的区域。显著区域就是指图片中引人注目的区域或比较重要的区域，例如人眼在观看一幅图片时会首先关注的区域。例如下图，我们人眼一眼看过去首先注意到的不是草坪，而是躺在草坪上的内马尔，内马尔所在的区域就是显著性区域。这种自动定位图像或场景重要区域的过程称为<strong>显着性检测</strong>。</p><p><img src="/img/v2-422783ab5d5d66fd0dbf5653166ddbd8_720w.jpg" alt></p><h4 id="显著性检测和图像分割区别"><a href="#显著性检测和图像分割区别" class="headerlink" title="显著性检测和图像分割区别"></a>显著性检测和图像分割区别</h4><p>这个任务和图像分割十分类似，区别在于：</p><blockquote><ul><li>1.目标数量：显著性目标检测一般只检一个目标，一般目标检测不会限制数量；</li><li>目标类别： 显著性目标检测不关心目标类别，只关心显著性强的目标，一般目标检测会得到目标位置和类别；</li><li>问题建模不同：显著性目标检测，很早期的时候是通过对一些共性的特征建模，比如该目标一般在图像中心，一般是什么样的颜色分布等等。一般目标检测问题，则是希望特征能够更好的把目标表达出来，越细节越好，特征越丰富约好，因为要区分类别</li><li>groundtruth定义不同：两者互有交集，也有不同；前者，往往定义的是一些显著性较强的目标，比如行人，动物等等，而且前者是不输出类别标签信息；后者，gt标签定义是明确的，可以严格的输出对应类别标签；</li></ul></blockquote><p>其实，说白了，显著性检测等同于是一个二分类的语义分割模型，此外，显著的区域 不一定就是 目标，目标很可能不是显著的；</p><h3 id="方法设计"><a href="#方法设计" class="headerlink" title="方法设计"></a>方法设计</h3><h4 id="Residual-U-blocks"><a href="#Residual-U-blocks" class="headerlink" title="Residual U-blocks"></a>Residual U-blocks</h4><p>了解了显著性检测这个任务之后，来具体了解一下这篇文章的具体方法设计。作者设计一种Residual U-blocks，用于捕获 intra-stage 的 multi-scales 特征。</p><p><img src="/img/image-20200917201033485.png" alt="image-20200917201033485"></p><p>上图为普通卷积block，Res-like block，Inception-like block，Dense-like block和Residual U-blocks的对比图，明显可以看出Residual U-blocks是受了U-Net的启发。</p><p>Residual U-blocks有以下三部分组成：</p><blockquote><ul><li>一个输入卷积层，它将输入的feature map x (H × W × $C_{in}$)转换成中间feature map $F_1(x)$，$F_1(x)$通道数为$C_{out}$。这是一个用于局部特征提取的普通卷积层。</li><li>一个U-like的对称的encoder-decoder结构，高度为L，以中间feature map $F_1(x)$为输入，去学习提取和编码多尺度文本信息$U(F_1(x))$,U表示类U-Net结构。更大L会得到更深层的U-block（RSU），更多的池操作，更大的感受野和更丰富的局部和全局特征。配置此参数允许从具有任意空间分辨率的输入特征图中提取多尺度特征。从逐渐降采样特征映射中提取多尺度特征，并通过渐进上采样、合并和卷积等方法将其编码到高分辨率的特征图中。这一过程减少了大尺度直接上采样造成的细节损失。</li><li>一种残差连接，它通过求和来融合局部特征和多尺度特征：$F_1(x) + U(F_1(x))$。</li></ul></blockquote><p><img src="/img/image-20200917201132725.png" alt="image-20200917201132725"></p><p>RSU与Res block的主要设计区别在于RSU用U-Net结构代替了普通的单流卷积，用一个权重层(weight layer)形成的局部特征来代替原始特征。这种设计的变更使网络能够从多个尺度直接从每个残差块提取特征。更值得注意的是，U结构的计算开销很小，因为大多数操作都是在下采样的特征映射上进行的。</p><h4 id="U-2-Net的结构"><a href="#U-2-Net的结构" class="headerlink" title="U^2-Net的结构"></a>U^2-Net的结构</h4><p>U^2-Net的网络结构如下：</p><p><img src="/img/image-20200917201255253.png" alt="image-20200917201255253"></p><p>与U-Net的网络结构做一个对比：</p><p><img src="/img/1*TXfEPqTbFBPCbXYh2bstlA.png" alt="Learn How to Train U-Net On Your Dataset | by Sukriti Paul | Coinmonks |  Medium"></p><p>直观上可以发现，U^2-Net的每一个Block都是一个U-Net结构的模块，即上述Residual U-blocks。当然，你也可以继续Going Deeper, 每个Block里面的U-Net的子Block仍然可以是一个U-Net结构，命名为U^3-Net。</p><h3 id="损失函数设计"><a href="#损失函数设计" class="headerlink" title="损失函数设计"></a>损失函数设计</h3><p>类似于HED算法的deep supervision方式，作者设计了如下函数：</p><script type="math/tex; mode=display">\mathcal{L}=\sum_{m=1}^{M} w_{\text {side}}^{(m)} \ell_{\text {side}}^{(m)}+w_{\text {fuse}} \ell_{\text {fuse}}</script><p>其中，M=6, 为U2Net 的 Sup1, Sup2, …, Sup6 stage.$w_{\text {side}}^{(m)}$  $l_{\text {side}}^{(m)}$ 为对应的损失函数输出和权重；$w_{f u s e} \ell_{f u s e}$ 为融合的损失函数和权重;对于每一个$l$使用的都是标准的BCE Loss：</p><script type="math/tex; mode=display">\ell=-\sum_{(r, c)}^{(H, W)}\left[P_{G(r, c)} \log P_{S(r, c)}+\left(1-P_{G(r, c)}\right) \log \left(1-P_{S(r, c)}\right)\right]</script><h3 id="实验可视化"><a href="#实验可视化" class="headerlink" title="实验可视化"></a>实验可视化</h3><p>所提出的模型是使用DUTS-TR数据集进行训练，该数据集包含大约10000个样本图像，并使用标准数据增强技术进行扩充。研究人员在6个用于突出目标检测的基准数据集上评估了该模型：DUT-OMRON、DUTS-TE、HKU-IS、ECSSD、PASCAL-S和SOD。评价结果表明，在这6个基准点上，新模型与现有方法具有相当好的性能。</p><p><img src="/img/image-20200918110749081.png" alt="image-20200918110749081"></p><p>U^2-Net的实现是开源的，并提供了两种不同的预训练模型：U2Net(176.3M的较大模型，在GTX 1080Ti GPU上为30 FPS)，以及U2NetP(4.7M小模型，最高可达到40 FPS)。代码和预训练模型都可以在<a href="https://github.com/NathanUA/U-2-Net" target="_blank" rel="noopener">Github</a>。下面是我直接用作者开源的模型跑出来的结果，抠图效果很好，精细到发丝的那种。</p><p><img src="/img/image-20200917201523395.png" alt="image-20200917201523395"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>该论文的优势在于：</p><blockquote><ul><li>提出RSU模块，融合不同尺度感受野的特征，来捕捉不同尺度的上下文信息；</li><li>基于 RSU 模块的 池化(pooling) 操作，在不显著增加计算成本的前提下，增加了整个网络结构的深度(depth).</li></ul></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; Going Deeper with Nested U-Structure for Salient Object Detection&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/NathanUA/U-2-Net&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/NathanUA/U-2-Net&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt; 这是发表在PR2020上的一篇关于显著性检测的文章，作者是秦雪彬。从标题上可以看到本文的一个idea是设计了一个deeper的U型结构的网络解决显著性目标检测问题。作者认为，目前显著性目标检测有两种主流思路，一为多层次深层特征集成multi-level deep feature integration，一为多尺度特征提取Multi-scale feature extraction。多层次深层特征集成方法主要集中在开发更好的多层次特征聚合策略上。而多尺度特征提取这一类方法旨在设计更新的模块，从主干网获取的特征中同时提取局部和全局信息。而几乎所有上述方法，都是为了更好地利用现有的图像分类的Backbones生成的特征映射。而作者另辟蹊径，提出了一种新颖而简单的结构，它直接逐级提取多尺度特征，用于显著目标检测，而不是利用这些主干的特征来开发和添加更复杂的模块和策略。下图是该方法与其他方法的一个比较：&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="saliency detection" scheme="https://blog.nicehuster.cn/tags/saliency-detection/"/>
    
  </entry>
  
  <entry>
    <title>mmdetection详解指北 (四)</title>
    <link href="https://blog.nicehuster.cn/2020/09/08/mmdetection-4/"/>
    <id>https://blog.nicehuster.cn/2020/09/08/mmdetection-4/</id>
    <published>2020-09-08T11:16:39.000Z</published>
    <updated>2020-09-17T12:44:41.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h3><p>训练流程的包装过程大致如下:tools/train.py-&gt;apis/train.py-&gt;mmcv/runner.py-&gt;mmcv/hook.py(后面是分散的), 其中 runner 维护了数据信息,优化器, 日志系统, 训练 loop 中的各节点信息, 模型保存, 学习率等. 另外补充一点, 以上包装过程, 在 mmdet 中无处不在, 包括 mmcv 的代码也是对日常频繁使用的函数进行了统一封装。</p><a id="more"></a><h4 id="训练逻辑"><a href="#训练逻辑" class="headerlink" title="训练逻辑"></a>训练逻辑</h4><p><img src="/img/image-20200917203908378.png" alt="image-20200917203908378"></p><p>图见2, 注意它的四个层级. 代码上, 主要查看 apis/train.py, mmcv 中的runner 相关文件. 核心围绕 Runner,Hook 两个类. Runner 将模型, 批处理函数 batch_pro cessor, 优化器作为基本属性, 训练过程中与训练状态, 各节点相关的信息被记录在mode,_hooks,_epoch,_iter,_inner_iter,_max_epochs, _max_iters中，这些信息维护了训练过程中插入不同 hook 的操作方式. 理清训练流程只需看 Runner 的成员函数 run. 在 run 里会根据 mode 按配置中 workflow 的 epoch 循环调用 train 和 val 函数, 跑完所有的 epoch. 比如train:</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EpochBasedRunner</span>(<span class="title">BaseRunner</span>):</span></span><br><span class="line">    <span class="string">""</span><span class="string">"Epoch-based Runner.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This runner train models epoch by epoch.</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(<span class="keyword">self</span>, data_loader, **kwargs)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">self</span>.model.train()</span><br><span class="line">        <span class="keyword">self</span>.mode = <span class="string">'train'</span></span><br><span class="line">        <span class="keyword">self</span>.data_loader = data_loader</span><br><span class="line">        <span class="keyword">self</span>._max_iters = <span class="keyword">self</span>._max_epochs * len(<span class="keyword">self</span>.data_loader) <span class="comment"># 最大迭代次数</span></span><br><span class="line">        <span class="keyword">self</span>.call_hook(<span class="string">'before_train_epoch'</span>)<span class="comment"># 根据名字获取hook对象函数</span></span><br><span class="line">        time.sleep(<span class="number">2</span>)  <span class="comment"># Prevent possible deadlock during epoch transition</span></span><br><span class="line">        <span class="keyword">for</span> i, data_batch <span class="keyword">in</span> enumerate(<span class="keyword">self</span>.data_loader)<span class="symbol">:</span></span><br><span class="line">            <span class="keyword">self</span>._inner_iter = i <span class="comment"># 记录当前训练迭代次数</span></span><br><span class="line">            <span class="keyword">self</span>.call_hook(<span class="string">'before_train_iter'</span>) <span class="comment">#一个batch 前向操作开始</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">self</span>.batch_processor is <span class="symbol">None:</span></span><br><span class="line">                outputs = <span class="keyword">self</span>.model.train_step(data_batch, <span class="keyword">self</span>.optimizer,</span><br><span class="line">                                                **kwargs)</span><br><span class="line">            <span class="symbol">else:</span></span><br><span class="line">                outputs = <span class="keyword">self</span>.batch_processor(</span><br><span class="line">                    <span class="keyword">self</span>.model, data_batch, train_mode=True, **kwargs) </span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> isinstance(outputs, dict)<span class="symbol">:</span></span><br><span class="line">                raise TypeError(<span class="string">'"batch_processor()" or "model.train_step()"'</span></span><br><span class="line">                                <span class="string">' must return a dict'</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'log_vars'</span> <span class="keyword">in</span> <span class="symbol">outputs:</span></span><br><span class="line">                <span class="keyword">self</span>.log_buffer.update(outputs[<span class="string">'log_vars'</span>],</span><br><span class="line">                                       outputs[<span class="string">'num_samples'</span>])</span><br><span class="line">            <span class="keyword">self</span>.outputs = outputs</span><br><span class="line">            <span class="keyword">self</span>.call_hook(<span class="string">'after_train_iter'</span>)<span class="comment">#一个batch 前向操作结束</span></span><br><span class="line">            <span class="keyword">self</span>._iter += <span class="number">1</span> <span class="comment"># 方便resume，知道从哪次开始优化</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.call_hook(<span class="string">'after_train_epoch'</span>) <span class="comment">#一个epoch结束</span></span><br><span class="line">        <span class="keyword">self</span>._epoch += <span class="number">1</span> <span class="comment">#记录训练epoch状态，方便resume</span></span><br></pre></td></tr></table></figure><p>上面需要说明的是自定义 hook 类, 自定义 hook 类需继承 mmcv 的Hook 类, 其默认了 6+8+4 个成员函数, 也即2所示的 6 个层级节点, 外加 2*4 个区分 train 和 val 的节点记录函数, 以及 4 个边界检查函数. 从train.py 中容易看出, 在训练之前, 已经将需要的 hook 函数注册到 Runner的 self._hook 中了, 包括从配置文件解析的优化器, 学习率调整函数, 模型保存, 一个 batch 的时间记录等 (注册 hook 算子在 self._hook 中按优先级升序排列). 这里的 call_hook 函数定义如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BaseRunner</span><span class="params">(metaclass=ABCMeta)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call_hook</span><span class="params">(self, fn_name)</span>:</span></span><br><span class="line">        <span class="string">"""Call all hooks.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            fn_name (str): The function name in each hook to be called, such as</span></span><br><span class="line"><span class="string">                "before_train_epoch".</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> hook <span class="keyword">in</span> self._hooks:</span><br><span class="line">            getattr(hook, fn_name)(self)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>容易看出, 在训练的不同节点, 将从注册列表中调用实现了该节点函数的类成员函数. 比如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@HOOKS.register_module()</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OptimizerHook</span><span class="params">(Hook)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, grad_clip=None)</span>:</span></span><br><span class="line">        self.grad_clip = grad_clip</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">clip_grads</span><span class="params">(self, params)</span>:</span></span><br><span class="line">        params = list(</span><br><span class="line">            filter(<span class="keyword">lambda</span> p: p.requires_grad <span class="keyword">and</span> p.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>, params))</span><br><span class="line">        <span class="keyword">if</span> len(params) &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> clip_grad.clip_grad_norm_(params, **self.grad_clip)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">after_train_iter</span><span class="params">(self, runner)</span>:</span></span><br><span class="line">        runner.optimizer.zero_grad()</span><br><span class="line">        runner.outputs[<span class="string">'loss'</span>].backward()</span><br><span class="line">        <span class="keyword">if</span> self.grad_clip <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            grad_norm = self.clip_grads(runner.model.parameters())</span><br><span class="line">            <span class="keyword">if</span> grad_norm <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="comment"># Add grad norm to the logger</span></span><br><span class="line">                runner.log_buffer.update(&#123;<span class="string">'grad_norm'</span>: float(grad_norm)&#125;,</span><br><span class="line">                                         runner.outputs[<span class="string">'num_samples'</span>])</span><br><span class="line">        runner.optimizer.step()</span><br></pre></td></tr></table></figure><p>将在每个 train_iter 后实现反向传播和参数更新。学习率优化相对复杂一点, 其基类 LrUpdaterHook, 实现了 before_run,before_train_epoch, before_train_iter 三个 hook 函数, 意义自明. . 这里选一个余弦式变化, 稍作说明:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">annealing_cos</span><span class="params">(start, end, factor, weight=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Calculate annealing cos learning rate.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Cosine anneal from `weight * start + (1 - weight) * end` to `end` as</span></span><br><span class="line"><span class="string">    percentage goes from 0.0 to 1.0.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        start (float): The starting learning rate of the cosine annealing.</span></span><br><span class="line"><span class="string">        end (float): The ending learing rate of the cosine annealing.</span></span><br><span class="line"><span class="string">        factor (float): The coefficient of `pi` when calculating the current</span></span><br><span class="line"><span class="string">            percentage. Range from 0.0 to 1.0.</span></span><br><span class="line"><span class="string">        weight (float, optional): The combination factor of `start` and `end`</span></span><br><span class="line"><span class="string">            when calculating the actual starting learning rate. Default to 1.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    cos_out = cos(pi * factor) + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> end + <span class="number">0.5</span> * weight * (start - end) * cos_out</span><br><span class="line"></span><br><span class="line"><span class="meta">@HOOKS.register_module()</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CosineAnnealingLrUpdaterHook</span><span class="params">(LrUpdaterHook)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, min_lr=None, min_lr_ratio=None, **kwargs)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> (min_lr <span class="keyword">is</span> <span class="keyword">None</span>) ^ (min_lr_ratio <span class="keyword">is</span> <span class="keyword">None</span>)</span><br><span class="line">        self.min_lr = min_lr</span><br><span class="line">        self.min_lr_ratio = min_lr_ratio</span><br><span class="line">        super(CosineAnnealingLrUpdaterHook, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_lr</span><span class="params">(self, runner, base_lr)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.by_epoch:</span><br><span class="line">            progress = runner.epoch</span><br><span class="line">            max_progress = runner.max_epochs</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            progress = runner.iter</span><br><span class="line">            max_progress = runner.max_iters</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.min_lr_ratio <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            target_lr = base_lr * self.min_lr_ratio</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            target_lr = self.min_lr</span><br><span class="line">        <span class="keyword">return</span> annealing_cos(base_lr, target_lr, progress / max_progress)</span><br></pre></td></tr></table></figure><p>从 get_lr 可以看到, 学习率变换周期有两种,epoch-&gt;max_epoch, 或者更大的 iter-&gt;max_iter, 后者表明一个 epoch 内不同 batch 的学习率可以不同, 因为没有什么理论, 所有这两种方式都行. 其中 base_lr 为初始学习率,target_lr 为学习率衰减的上界, 而当前学习率即为返回值.</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;训练流程&quot;&gt;&lt;a href=&quot;#训练流程&quot; class=&quot;headerlink&quot; title=&quot;训练流程&quot;&gt;&lt;/a&gt;训练流程&lt;/h3&gt;&lt;p&gt;训练流程的包装过程大致如下:tools/train.py-&amp;gt;apis/train.py-&amp;gt;mmcv/runner.py-&amp;gt;mmcv/hook.py(后面是分散的), 其中 runner 维护了数据信息,优化器, 日志系统, 训练 loop 中的各节点信息, 模型保存, 学习率等. 另外补充一点, 以上包装过程, 在 mmdet 中无处不在, 包括 mmcv 的代码也是对日常频繁使用的函数进行了统一封装。&lt;/p&gt;
    
    </summary>
    
      <category term="mmdetection" scheme="https://blog.nicehuster.cn/categories/mmdetection/"/>
    
    
      <category term="mmdetection" scheme="https://blog.nicehuster.cn/tags/mmdetection/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：Delving Deeper into Anti-aliasing in ConvNets</title>
    <link href="https://blog.nicehuster.cn/2020/09/06/Adaptive-anti-Aliasing/"/>
    <id>https://blog.nicehuster.cn/2020/09/06/Adaptive-anti-Aliasing/</id>
    <published>2020-09-06T11:13:39.000Z</published>
    <updated>2020-09-16T09:01:33.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> Xueyan Zou,UC Davis,NVIDIA<br><strong>代码链接：</strong><a href="https://github.com/MaureenZOU/" target="_blank" rel="noopener">https://github.com/MaureenZOU/</a><br><strong>整体框架：</strong>这篇文章是前几天BMVC2020获得best paper award 的一篇文章，这篇文章提出了一个plugin  module ，用来提高CNN的鲁棒性。在图像分类、图像分割、目标检测等任务上都能带来1+个点的提升。对于cnn，众所周知存在一个比较明显的缺陷：即使图像移动几个pixel都可能导致分类识别任务结果发生改变，作者发现其中重要原因在于网络中被大量用来降低参数量的下采样层导致混叠(aliasing)问题，即高频信号在采样后退化为完全不同的部分现象。为此提出了这样一个content-aware anti-aliasing的模块，用于缓解下采样过程中带来的高频信息的退化问题。</p><h3 id="下采样的问题"><a href="#下采样的问题" class="headerlink" title="下采样的问题"></a>下采样的问题</h3><p>以一维信号的的降采样为例：</p><script type="math/tex; mode=display">\begin{array}{l}001100110011 \frac{\mathrm{k}=2, \text { stride }=2}{\operatorname{maxpool}} 010101 \\011001100110 \frac{\mathrm{k}=2, \text { stride }=2}{\operatorname{maxpool}} 111111\end{array}</script><a id="more"></a><p>对于k=2,stride=2的maxpool操作而言，对输入信号移动一位数字，经过maxpool之后的输出结果是截然不同的。而在CNN中大量使用降采样来降低参数量，这种aliasing问题更为明显，标准解决方案是在下采样之前应用低通滤波器（例如，高斯模糊）。但是，在整个内容上应用相同的过滤器可能不是最佳选择，因为特征的频率可能会在<strong>空间位置</strong>和<strong>特征通道</strong>之间发生变化。可以看下作者在论文中给出的实验结果：</p><p><img src="/img/image-20200916161719818.png" alt="image-20200916161719818" style="zoom:50%;"></p><p>上图(a)输入图片；(b)直接4x下采样；(c)应用经过调整以匹配噪声频率的单个高斯滤波器后的下采样结果;(d)应用多个空间自适应高斯滤波后的下采样结果（具有很强的背景模糊和边界弱化能力）</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p><img src="/img/image-20200916161851654.png" alt="image-20200916161851654" style="zoom:50%;"></p><p>作者针对空间和通道分别生成低通滤波器用于缓解aliasing问题。基于空间自适应的低通滤波器就是一个简单的分组卷积操作：</p><script type="math/tex; mode=display">Y_{i, j}=\sum_{p, q \in \Omega} w_{i, j}^{p, q} \cdot X_{i+p, j+q}</script><p>在论文中也有提及，为了避免权重为负数，作者取了一个softmax操作。</p><p>之后对上面生成的权重进行分组再和输入进行一次卷积操作，最终输出Y：</p><script type="math/tex; mode=display">Y_{i, j}^{g}=\sum_{p, q \in \Omega} w_{i, j, g}^{p, q} \cdot X_{i+p, j+q}^{c}</script><p>具体可以看下作者放出来的代码：</p><blockquote><p><a href="https://github.com/MaureenZOU/Adaptive-anti-Aliasing/blob/master/models_lpf/layers/pasa.py" target="_blank" rel="noopener">Adaptive-anti-Aliasing/models_lpf/layers/pasa.py</a></p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Downsample_PASA_group_softmax</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_channels, kernel_size, stride=<span class="number">1</span>, pad_type=<span class="string">'reflect'</span>, group=<span class="number">2</span>)</span>:</span></span><br><span class="line">        super(Downsample_PASA_group_softmax, self).__init__()</span><br><span class="line">        self.pad = get_pad_layer(pad_type)(kernel_size//<span class="number">2</span>)</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.kernel_size = kernel_size</span><br><span class="line">        self.group = group</span><br><span class="line"></span><br><span class="line">        self.conv = nn.Conv2d(in_channels, group*kernel_size*kernel_size, kernel_size=kernel_size, stride=<span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.bn = nn.BatchNorm2d(group*kernel_size*kernel_size)</span><br><span class="line">        self.softmax = nn.Softmax(dim=<span class="number">1</span>)</span><br><span class="line">        nn.init.kaiming_normal_(self.conv.weight, mode=<span class="string">'fan_out'</span>, nonlinearity=<span class="string">'relu'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        sigma = self.conv(self.pad(x))</span><br><span class="line">        sigma = self.bn(sigma)</span><br><span class="line">        sigma = self.softmax(sigma)</span><br><span class="line"></span><br><span class="line">        n,c,h,w = sigma.shape</span><br><span class="line"></span><br><span class="line">        sigma = sigma.reshape(n,<span class="number">1</span>,c,h*w)</span><br><span class="line"></span><br><span class="line">        n,c,h,w = x.shape</span><br><span class="line">        x = F.unfold(self.pad(x), kernel_size=self.kernel_size).reshape((n,c,self.kernel_size*self.kernel_size,h*w))</span><br><span class="line"></span><br><span class="line">        n,c1,p,q = x.shape</span><br><span class="line">        x = x.permute(<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>).reshape(self.group, c1//self.group, n, p, q).permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        n,c2,p,q = sigma.shape</span><br><span class="line">        sigma = sigma.permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>).reshape((p//(self.kernel_size*self.kernel_size), self.kernel_size*self.kernel_size,n,c2,q)).permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">        x = torch.sum(x*sigma, dim=<span class="number">3</span>).reshape(n,c1,h,w)</span><br><span class="line">        <span class="keyword">return</span> x[:,:,torch.arange(h)%self.stride==<span class="number">0</span>,:][:,:,:,torch.arange(w)%self.stride==<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>作者分别在分类，检测，分割任务上均有验证其有效性，均能提高1个点左右。具体实验结果可以看原论文，这里就不贴上来了。</p><h3 id="一致性指标"><a href="#一致性指标" class="headerlink" title="一致性指标"></a>一致性指标</h3><p>作者为了验证该方法的有效性，针对不同任务提出了一系列的一致性指标。比如针对分类任务的一致性指标计算如下：</p><script type="math/tex; mode=display">\text { Consistency }=\mathbb{E}_{X, h_{1}, w_{1}, h_{2}, w_{2}} \mathbb{I}\left\{F\left(X_{h_{1}, w_{1}}\right)=F\left(X_{h_{2}, w_{2}}\right)\right\}</script><p>X表示输入图像，$h_{1}, w_{1}, h_{2}, w_{2}$ 表示偏移量。$F(\cdot)$ 表示模型输出的top1的类别标签。具体代码实现可以看下面：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">output0 = model(input[:,:,off0[<span class="number">0</span>]:off0[<span class="number">0</span>]+args.size,off0[<span class="number">1</span>]:off0[<span class="number">1</span>]+args.size])</span><br><span class="line">output1 = model(input[:,:,off1[<span class="number">0</span>]:off1[<span class="number">0</span>]+args.size,off1[<span class="number">1</span>]:off1[<span class="number">1</span>]+args.size])</span><br><span class="line">cur_agree = agreement_correct(output0, output1, target).type(torch.FloatTensor).to(output0.device)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">agreement_correct</span><span class="params">(output0, output1, target)</span>:</span></span><br><span class="line">    pred0 = output0.argmax(dim=<span class="number">1</span>, keepdim=<span class="keyword">False</span>)</span><br><span class="line">    pred1 = output1.argmax(dim=<span class="number">1</span>, keepdim=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    agree = pred0.eq(pred1)</span><br><span class="line">    agree_target_pred0 = pred0.eq(target)</span><br><span class="line">    agree_target_pred1 = pred1.eq(target)</span><br><span class="line"></span><br><span class="line">    correct_or = (agree_target_pred0 + agree_target_pred1) &gt; <span class="number">0</span></span><br><span class="line">    agree = agree * correct_or</span><br><span class="line"></span><br><span class="line">    agree = <span class="number">100.</span>*(torch.sum(agree).float() / (torch.sum(correct_or).float() + <span class="number">1e-10</span>)).to(output0.device)</span><br><span class="line">    <span class="keyword">return</span> agree</span><br></pre></td></tr></table></figure><p><code>agreement_correct</code> 是统计一致性指标的函数，具体可看出来统计方法是分别对输入图像随机偏移<code>off0</code>和<code>off1</code> 然后统计在该偏移量下，两者输出一致且和gt一致的比例。对于检测分割任务，作者也提出了相应的一致性指标计算方法：mAISC和mAISC这两个指标。具体计算如下图。</p><p><img src="/img/%E4%BC%81%E4%B8%9A%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_16002437758073.png" alt="企业微信截图_16002437758073"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; Xueyan Zou,UC Davis,NVIDIA&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/MaureenZOU/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/MaureenZOU/&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体框架：&lt;/strong&gt;这篇文章是前几天BMVC2020获得best paper award 的一篇文章，这篇文章提出了一个plugin  module ，用来提高CNN的鲁棒性。在图像分类、图像分割、目标检测等任务上都能带来1+个点的提升。对于cnn，众所周知存在一个比较明显的缺陷：即使图像移动几个pixel都可能导致分类识别任务结果发生改变，作者发现其中重要原因在于网络中被大量用来降低参数量的下采样层导致混叠(aliasing)问题，即高频信号在采样后退化为完全不同的部分现象。为此提出了这样一个content-aware anti-aliasing的模块，用于缓解下采样过程中带来的高频信息的退化问题。&lt;/p&gt;
&lt;h3 id=&quot;下采样的问题&quot;&gt;&lt;a href=&quot;#下采样的问题&quot; class=&quot;headerlink&quot; title=&quot;下采样的问题&quot;&gt;&lt;/a&gt;下采样的问题&lt;/h3&gt;&lt;p&gt;以一维信号的的降采样为例：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\begin{array}{l}
001100110011 \frac{\mathrm{k}=2, \text { stride }=2}{\operatorname{maxpool}} 010101 \\
011001100110 \frac{\mathrm{k}=2, \text { stride }=2}{\operatorname{maxpool}} 111111
\end{array}&lt;/script&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="classification" scheme="https://blog.nicehuster.cn/tags/classification/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：Minimum Delay Object Detection from Video</title>
    <link href="https://blog.nicehuster.cn/2020/09/05/minidelay-detection/"/>
    <id>https://blog.nicehuster.cn/2020/09/05/minidelay-detection/</id>
    <published>2020-09-05T11:13:39.000Z</published>
    <updated>2020-09-15T06:28:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近逛知乎上了解到<strong>低延迟目标检测</strong>这个方向，这个方向解决的是视觉任务工程化过程中存在一个痛点问题，如何解决延迟和误报的情况。拿视频中的目标检测问题来讲，我们通常会使用一个单帧检测器来检测视频中每一帧存在目标情况，静态上看可能会存在一些误检漏检情况，动态上表现为检测到的目标一闪一闪的情况，导致视觉效果很不友好。而低延迟目标检测的任务就是通过贝叶斯概率建模的方式融合多帧信息来解决这样的问题。</p><p>总体来说，对于任何检测任务来说，延迟和误报存在如下图的关系：</p><p><img src="/img/v2-2c0d4736cf5fde58e2b58530d7643006_1440w.jpg" alt="img"></p><a id="more"></a><p>这个关系不难理解，不只是视觉问题，世间万物，更长的决策过程（delay）往往能带来更高的准确度，但是这个更长的决策过程也会带来更大的延迟。两者之间的平衡，对很多需要在线决策（online process）的系统来说非常重要。例如生物视觉，假定一个动物检测到掠食者就需要逃跑，如果追求低误报率，就要承担高延迟带来的风险，有可能检测到掠食者时为时已晚无法逃脱；如果追求低延迟，虽然相对安全，但是误报率高，有一点风吹草动就犹如惊弓之鸟。</p><h3 id="低延迟检测思路"><a href="#低延迟检测思路" class="headerlink" title="低延迟检测思路"></a>低延迟检测思路</h3><p>在视频物体检测中，如果使用上一帧的检测结果作为先验，将下一帧的检测结果输入贝叶斯框架，输出后验，那么总体来说，这个后验结果融合了两帧的信息，会比单帧更准。在这个思路下，理论上来说使用的帧数越多，检测越准。如以下单帧和多帧的对比：</p><p><img src="/img/v2-fcf08710643bcb172cb661c687f36fe5_b.webp" alt="img"></p><p>但是同时更多的帧数会造成更长的延迟（延迟 := 物体被检测到的时刻 - 物体出现的时刻）。如何在保证物体检测精度的情况下，尽量降低延迟呢？我们参照QD理论进行如下建模：</p><p>假设一个物体在时刻$t_s$出现在视频中，在 $t_e$离开视频，则这个物体的移动轨迹可以用时序上的一组检测框$b_{t_s,t_e}=(b_{t_s},b_{t_s+1},…,b_{t_e})$表示。这样的一组检测框检测框序列，在目标追踪（Data association / tracking）领域被一些人称作tracklet。简单说，我们的算法目标是以低延迟判断检测框序列内是否含有物体。因此，我们称这样一组检测框序列为一个candidate。在quickest change detection框架下， 可以用如下似然比检验判断$t$时刻物体是否出现在该candidate内：</p><script type="math/tex; mode=display">\begin{aligned}\Lambda_{t}\left(b_{1, t}\right) &=\max _{i} \frac{\mathrm{p}\left(\Gamma_{0, i}<t \mid D_{1, t}, b_{1, t}\right)}{\mathrm{p}\left(\Gamma_{0, i} \geq t \mid D_{1, t}, b_{1, t}\right)} \\&=\max _{i} \max _{t_{c} \geq 1} \frac{\mathrm{p}_{i}\left(D_{t_{c}, t} \mid b_{t_{c}, t}\right)}{\mathrm{p}_{0}\left(D_{t_{c}, t} \mid b_{t_{c}, t}\right)}\end{aligned}</script><p>其中$D_t$代表一个单帧检测器在$I_t$上的检测结果，$T_{0,i}$代表candidate中的内容从背景变为第类$i$物体（如行人）这一事件发生的时刻,$p_i(\bullet )=p(\bullet |l=l_i)$ 代表给定类别$i$的时候，$\bullet$ 事件发生的概率。由条件概率的独立性与,$p_i(\bullet |b_t)$类别$i$和检测框独$b_t$立，继而时序上各时刻检测结果的联合概率变成各时刻概率的连乘：</p><script type="math/tex; mode=display">\Lambda_{t}\left(b_{1, t}\right)=\max _{i} \max _{t_{c} \geq 1} \prod_{j=t_{c}}^{t} \frac{\mathrm{p}_{i}\left(D_{j} \mid b_{j}\right)}{\mathrm{p}_{0}\left(D_{j} \mid b_{j}\right)}</script><p>这里需要明确一下，上边公式中的条件概率并非简单的检测器输出的结果，具体如何计算$p$需要一套比较复杂的建模。由于这里只介绍低延迟检测的整体思路，关于$p$的建模待我有空时会附在文末，有兴趣的朋友可以直接去论文查阅。总之，我们可以对这个似然比取阈值，进行检测。阈值越高，结果越准，但是延迟越大，反之同理。由QD理论中递归算法（CuSum算法），我们可以对上述似然比取log，记为W。最终的检测流程可以参照如下框图。</p><p><img src="/img/v2-bd434876b196828b2db7d14ea5eb8c52_1440w.jpg" alt></p><p>整体流程为：</p><blockquote><ul><li>（1）将已有但似然比未超过阈值的candidate做tracking进入下一帧；</li><li>（2）在下一帧进行单帧检测，生成新的检测框，与前一帧tracking后的检测框合并到一起；</li><li>（3）对这些candidate进行似然比检验，W超出阈值则输出检测结果，W小于零则去除该检测框，W大于零小于阈值则回到（1），进入下一帧。</li></ul></blockquote><p>这样一个检测框架，可以与<strong>任何</strong>单帧检测器结合。</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><p>以上算法的具体实现过程,作者开源了一个简单的python实现，git地址在<a href="https://github.com/donglao/mindelay/blob/master/detection.py" target="_blank" rel="noopener">这里</a>。这里主要看下核心的代码：</p><p>detection.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">result = toolbox.initialize_result(num_cat) <span class="comment"># 第一步，外循环，根据类别初始化result</span></span><br><span class="line"></span><br><span class="line">result = association.update(result, result_det) <span class="comment">#使用当前帧信息更新历史轨迹，result保存的是历史帧的检测信息，result_det是当前帧的检测信息</span></span><br><span class="line"></span><br><span class="line">result = toolbox.combine_result(result, result_det, <span class="number">0.5</span>) <span class="comment">#对结合的轨迹信息和当前帧信息就行结合，输出最终需要alarm的检测结果；</span></span><br></pre></td></tr></table></figure><p>其中association.update的具体代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> mindelay.toolbox.IoU <span class="keyword">as</span> IoU</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_traj</span><span class="params">(old, det)</span>:</span></span><br><span class="line">    prior = <span class="number">0.5</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(old.shape[<span class="number">0</span>]):</span><br><span class="line">        a = old[i, <span class="number">0</span>:<span class="number">4</span>] * prior</span><br><span class="line">        b = prior</span><br><span class="line">        l = prior * <span class="number">0.5</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(det.shape[<span class="number">0</span>]):</span><br><span class="line">            weight = (IoU(old[i, <span class="number">0</span>:<span class="number">4</span>], det[j, <span class="number">0</span>:<span class="number">4</span>])&gt;<span class="number">0.5</span>)*IoU(old[i, <span class="number">0</span>:<span class="number">4</span>], det[j, <span class="number">0</span>:<span class="number">4</span>])</span><br><span class="line">        <span class="comment"># The bounding box update is here. I've tried different methods in the original Matlab code. </span></span><br><span class="line">        <span class="comment"># But here I just use the previous frame as the guidance of current frame and leave it as-is.</span></span><br><span class="line">        <span class="comment"># I will combine it with trackers (some work to be done!) and update in later version of the code. </span></span><br><span class="line">            a = a + weight * det[j, <span class="number">0</span>:<span class="number">4</span>]</span><br><span class="line">            b = b + weight</span><br><span class="line">            l = l + weight * det[j, <span class="number">4</span>]</span><br><span class="line">        old[i, <span class="number">0</span>:<span class="number">4</span>] = a / b</span><br><span class="line">        l = l / b</span><br><span class="line"></span><br><span class="line">        temp_lr = np.log(l + <span class="number">0.25</span>) - np.log((<span class="number">1</span> - l) + <span class="number">0.25</span>) + old[i, <span class="number">4</span>] - <span class="number">2.5</span>/b</span><br><span class="line">        <span class="comment"># the +0.25 is making the output smoother. If the output of the detector is not ideal you may want to tune it.</span></span><br><span class="line">        <span class="comment"># the 2.5/b is a prior. Instead of setting a fixed prior I am trying to make it adaptive. Feel free to play with it! </span></span><br><span class="line">    </span><br><span class="line">        old[i, <span class="number">4</span>] = max(temp_lr, <span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> old</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(result, result_det)</span>:</span></span><br><span class="line">    n = result.__len__()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        result[i] = update_traj(np.array(result[i]), np.array(result_det[i]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br></pre></td></tr></table></figure><p>其中，toolbox.combine_result的实现比较简单，对经过更新的轨迹信息和当前帧信息进行combine后经nms处理一下，代码如下：</p><figure class="highlight accesslog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def combine_result(result, result_det, thre_nms):</span><br><span class="line">    n = result.__len__()</span><br><span class="line">    output = np.empty((n,), dtype = np.object)</span><br><span class="line">    for i in range(n):</span><br><span class="line">        if result_det<span class="string">[i]</span>.shape<span class="string">[0]</span> &gt; <span class="number">0</span>:</span><br><span class="line">            temp_lr = np.log(result_det<span class="string">[i]</span><span class="string">[:,4]</span>+<span class="number">0</span>.<span class="number">25</span>) - np.log(<span class="number">1</span> - result_det<span class="string">[i]</span><span class="string">[:,4]</span>+<span class="number">0</span>.<span class="number">25</span>)</span><br><span class="line">            temp_lr<span class="string">[temp_lr &lt; 0]</span> = <span class="number">0</span></span><br><span class="line">            result_det<span class="string">[i]</span><span class="string">[:, 4]</span> = temp_lr</span><br><span class="line">        tmp = np.vstack((result<span class="string">[i]</span>, result_det<span class="string">[i]</span>))</span><br><span class="line">        keep = py_cpu_nms(tmp,thre_nms)</span><br><span class="line">        #print(keep,thre_nms,tmp<span class="string">[keep]</span>)</span><br><span class="line">        output<span class="string">[i]</span> = tmp<span class="string">[keep]</span></span><br><span class="line"></span><br><span class="line">    return output</span><br></pre></td></tr></table></figure><p>以上理论内容转自：<a href="https://zhuanlan.zhihu.com/p/212842916" target="_blank" rel="noopener">计算机视觉中低延迟检测的相关理论和应用</a>；</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近逛知乎上了解到&lt;strong&gt;低延迟目标检测&lt;/strong&gt;这个方向，这个方向解决的是视觉任务工程化过程中存在一个痛点问题，如何解决延迟和误报的情况。拿视频中的目标检测问题来讲，我们通常会使用一个单帧检测器来检测视频中每一帧存在目标情况，静态上看可能会存在一些误检漏检情况，动态上表现为检测到的目标一闪一闪的情况，导致视觉效果很不友好。而低延迟目标检测的任务就是通过贝叶斯概率建模的方式融合多帧信息来解决这样的问题。&lt;/p&gt;
&lt;p&gt;总体来说，对于任何检测任务来说，延迟和误报存在如下图的关系：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/v2-2c0d4736cf5fde58e2b58530d7643006_1440w.jpg&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://blog.nicehuster.cn/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>mmdetection详解指北 (三)</title>
    <link href="https://blog.nicehuster.cn/2020/09/04/mmdetection-3/"/>
    <id>https://blog.nicehuster.cn/2020/09/04/mmdetection-3/</id>
    <published>2020-09-04T11:15:39.000Z</published>
    <updated>2020-09-18T07:19:41.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>数据处理可能是炼丹师接触最为密集的了，因为通常情况，除了数据的离线处理，写个数据类，就可以炼丹了。但本节主要涉及数据的在线处理，更进一步应该是检测分割数据的 pytorch 处理方式。虽然 mmdet 将常用的数据都实现了，而且也实现了中间通用数据格式，但，这和模型，损失函数，性能评估的实现也相关，比如你想把官网的 centernet 完整的改成 mmdet风格，就能看到 (看起来没必要)。</p><a id="more"></a><h4 id="CustomDataset"><a href="#CustomDataset" class="headerlink" title="CustomDataset"></a>CustomDataset</h4><p>看看配置文件，数据相关的有 data dict，里面包含了 train,val,test 的路径信息，用于数据类初始化, 有 pipeline，将各个函数及对应参数以字典形式放到列表里，是对 pytorch 原装的 transforms+compose，在检测，分割相关数据上的一次封装，使得形式更加统一。</p><p>从 builder.py 中 build_dataset 函数能看到，构建数据有三种方式，ConcatDataset，RepeatDataset 和从注册器中提取。其中 dataset_wrappers.py中 ConcatDataset 和 RepeatDataset 意义自明，前者继承自 pytorch 原始的ConcatDataset，将多个数据集整合到一起，具体为把不同序列（ 可参考<a href="https://docs.python.org/zh-cn/3/library/collections.abc.html" target="_blank" rel="noopener">容器的抽象基类</a>) 的长度相加<strong>getitem</strong> 函数对应 index 替换一下。后者就是单个数据类 (序列) 的多次重复。就功能来说，前者提高数据丰富度，后者可解决数据太少使得 loading 时间长的问题 (见代码注释)。而被注册的数据类在 datasets 下一些熟知的数据名文件中。其中，基类为 custom.py 中的 CustomDataset，coco 继承自它，cityscapes 继承自 coco，xml_style 的XMLDataset 继承 CustomDataset，然后 wider_face，voc 均继承自 XMLDataset。因此这里先分析一下CustomDataset。</p><p>CustomDataset 记录数据路径等信息，解析标注文件，将每一张图的所有信息以字典作为数据结构存在 results 中，然后进入pipeline: 数据增强相关操作，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@DATASETS.register_module()</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(...)</span>:</span></span><br><span class="line">        ...</span><br><span class="line">        self.pipeline = Compose(pipeline)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pre_pipeline</span><span class="params">(self, results)</span>:</span></span><br><span class="line">        <span class="string">"""Prepare results dict for pipeline."""</span></span><br><span class="line">        results[<span class="string">'img_prefix'</span>] = self.img_prefix</span><br><span class="line">        results[<span class="string">'seg_prefix'</span>] = self.seg_prefix</span><br><span class="line">        results[<span class="string">'proposal_file'</span>] = self.proposal_file</span><br><span class="line">        results[<span class="string">'bbox_fields'</span>] = []</span><br><span class="line">        results[<span class="string">'mask_fields'</span>] = []</span><br><span class="line">        results[<span class="string">'seg_fields'</span>] = []</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">prepare_train_img</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="string">"""Get training data and annotations after pipeline.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            idx (int): Index of data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            dict: Training data and annotation after pipeline with new keys \</span></span><br><span class="line"><span class="string">                introduced by pipeline.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        img_info = self.data_infos[idx]</span><br><span class="line">        ann_info = self.get_ann_info(idx)</span><br><span class="line">        <span class="comment"># 基本信息，初始化字典</span></span><br><span class="line">        results = dict(img_info=img_info, ann_info=ann_info)</span><br><span class="line">        <span class="keyword">if</span> self.proposals <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            results[<span class="string">'proposals'</span>] = self.proposals[idx]</span><br><span class="line">        self.pre_pipeline(results)</span><br><span class="line">        <span class="keyword">return</span> self.pipeline(results) <span class="comment"># 数据增强等</span></span><br></pre></td></tr></table></figure><p>这里数据结构的选取需要注意一下，字典结构，在数据增强库 albu 中也是如此处理，因此可以快速替换为 albu 中的算法。另外每个数据类增加了各自的 evaluate 函数。evaluate 基础函数在 mmdet.core.evaluation 中，后做补充。</p><p>mmdet 的数<strong>据处理，字典结构，pipeline，evaluate</strong> 是三个关键部分。其他所有类的文件解析部分，数据筛选等，看看即可。因为我们知道，pytorch读取数据，是将序列转化为迭代器后进行 io 操作，所以在 dataset 下除了pipelines 外还有 loader 文件夹，里面实现了分组，分布式分组采样方法，以及调用了 mmcv 中的 collate 函数 (此处为 1.x 版本，2.0 版本将 loader移植到了 builder.py 中)，且 build_dataloader 封装的 DataLoader 最后在train_detector 中被调用，这部分将在后面补充，这里说说 pipelines。</p><h4 id="data-config"><a href="#data-config" class="headerlink" title="data_config"></a>data_config</h4><p>返回 maskrcnn 的配置文件 (1.x,2.0 看 base config)，可以看到训练和测试的不同之处：LoadAnnotations，MultiScaleFlipAug，DefaultFormatBundle 和 Collect。额外提示，虽然测试没有 LoadAnnotations，根据 CustomDataset 可知，它仍需标注文件，这和 inference 的 pipeline 不同，也即这里的 test 实为 evaluate。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 序列中的dict可以随意删减，增加，属于数据增强调参内容</span></span><br><span class="line">train_pipeline = [</span><br><span class="line">    dict(type=<span class="string">'LoadImageFromFile'</span>),</span><br><span class="line">    dict(type=<span class="string">'LoadAnnotations'</span>, with_bbox=<span class="keyword">True</span>),</span><br><span class="line">    dict(type=<span class="string">'Resize'</span>, img_scale=(<span class="number">1333</span>, <span class="number">800</span>), keep_ratio=<span class="keyword">True</span>),</span><br><span class="line">    dict(type=<span class="string">'RandomFlip'</span>, flip_ratio=<span class="number">0.5</span>),</span><br><span class="line">    dict(type=<span class="string">'Normalize'</span>, **img_norm_cfg),</span><br><span class="line">    dict(type=<span class="string">'Pad'</span>, size_divisor=<span class="number">32</span>),</span><br><span class="line">    dict(type=<span class="string">'DefaultFormatBundle'</span>),</span><br><span class="line">    dict(type=<span class="string">'Collect'</span>, keys=[<span class="string">'img'</span>, <span class="string">'gt_bboxes'</span>, <span class="string">'gt_labels'</span>]),</span><br><span class="line">]</span><br><span class="line">test_pipeline = [</span><br><span class="line">    dict(type=<span class="string">'LoadImageFromFile'</span>),</span><br><span class="line">    dict(</span><br><span class="line">        type=<span class="string">'MultiScaleFlipAug'</span>,</span><br><span class="line">        img_scale=(<span class="number">1333</span>, <span class="number">800</span>),</span><br><span class="line">        flip=<span class="keyword">False</span>,</span><br><span class="line">        transforms=[</span><br><span class="line">            dict(type=<span class="string">'Resize'</span>, keep_ratio=<span class="keyword">True</span>),</span><br><span class="line">            dict(type=<span class="string">'RandomFlip'</span>),</span><br><span class="line">            dict(type=<span class="string">'Normalize'</span>, **img_norm_cfg),</span><br><span class="line">            dict(type=<span class="string">'Pad'</span>, size_divisor=<span class="number">32</span>),</span><br><span class="line">            dict(type=<span class="string">'ImageToTensor'</span>, keys=[<span class="string">'img'</span>]),</span><br><span class="line">            dict(type=<span class="string">'Collect'</span>, keys=[<span class="string">'img'</span>]),</span><br><span class="line">        ])</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>最后这些所有操作被 Compose 串联起来，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@PIPELINES.register_module()</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Compose</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, transforms)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(transforms, collections.abc.Sequence) <span class="comment">#列表是序列结构</span></span><br><span class="line">        self.transforms = []</span><br><span class="line">        <span class="keyword">for</span> transform <span class="keyword">in</span> transforms:</span><br><span class="line">            <span class="keyword">if</span> isinstance(transform, dict):</span><br><span class="line">                transform = build_from_cfg(transform, PIPELINES)</span><br><span class="line">                self.transforms.append(transform)</span><br><span class="line">            <span class="keyword">elif</span> callable(transform):</span><br><span class="line">                self.transforms.append(transform)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> TypeError(<span class="string">'transform must be callable or a dict'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> self.transforms:</span><br><span class="line">            data = t(data)</span><br><span class="line">            <span class="keyword">if</span> data <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">        <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        format_string = self.__class__.__name__ + <span class="string">'('</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> self.transforms:</span><br><span class="line">            format_string += <span class="string">'\n'</span></span><br><span class="line">            format_string += <span class="string">f'    <span class="subst">&#123;t&#125;</span>'</span></span><br><span class="line">        format_string += <span class="string">'\n)'</span></span><br><span class="line">        <span class="keyword">return</span> format_string</span><br></pre></td></tr></table></figure><p>上面代码能看到，配置文件中 pipeline 中的字典传入 build_from_cfg 函数，逐一实现了各个增强类 (方法)。扩展的增强类均需实现 <strong>call</strong> 方法，这和 pytorch 原始方法是一致的。有了以上认识，重新梳理一下 pipelines 的逻辑，由三部分组成，load，transforms，和 format。load 相关的 LoadImageFromFile，LoadAnnotations都是字典 results 进去，字典 results 出来。具体代码看下便知，LoadImageFromFile 增加了’filename’，’img’，’img_shape’，’ori_shape’,’pad_shape’,’scale_factor’,’img_norm_cfg’ 字段。其中 img 是 numpy 格式。LoadAnnotations 从 results[’ann_info’] 中解析出 bboxs,masks,labels 等信息。注意 coco 格式的原始解析来自 pycocotools，包括其评估方法，这里关键是字典结构 (这个和模型损失函数，评估等相关，统一结构，使得代码统一)。transforms 中的类作用于字典的 values，也即数据增强。format 中的 DefaultFormatBundle 是将数据转成 mmcv 扩展的容器类格式 DataContainer。另外 Collect 会根据不同任务的不同配置，从 results 中选取只含 keys 的信息生成新的字典，具体看下该类帮助文档。</p><h4 id="DataContainer"><a href="#DataContainer" class="headerlink" title="DataContainer"></a>DataContainer</h4><p>那么 DataContainer 是什么呢？它是对 tensor 的封装，将 results 中的 tensor 转成 DataContainer 格式，实际上只是增加了几个 property 函数，cpu_only，stack，padding_value，pad_dims，其含义自明，以及 size，dim用来获取数据的维度，形状信息。考虑到序列数据在进入 DataLoader 时，需要以 batch 方式进入模型，那么通常的 collate_fn 会要求 tensor 数据的形状一致。但是这样不是很方便，于是有了 DataContainer。它可以做到载入 GPU 的数据可以保持统一 shape，并被 stack，也可以不 stack，也可以保持原样，或者在非 batch 维度上做 pad。当然这个也要对 default_collate进行改造，mmcv 在 parallel.collate 中实现了这个。</p><p>collate_fn 是 DataLoader 中将序列 dataset 组织成 batch 大小的函数，这里帖三个普通例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collate_fn_1</span><span class="params">(batch)</span> :</span></span><br><span class="line">    <span class="comment"># 这 是 默 认 的， 明 显batch中 包 含 相 同 形 状 的img\_tensor和 label</span></span><br><span class="line">    <span class="keyword">return</span> tuple ( zip(*batch))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">coco_collate_2</span><span class="params">(batch)</span> :</span></span><br><span class="line">    <span class="comment"># 传 入 的batch数 据 是 被albu增 强 后 的(字 典 结 构)</span></span><br><span class="line">    imgs = [s [ ’image’ ] <span class="keyword">for</span> s <span class="keyword">in</span> batch] </span><br><span class="line">    annots = [s [ ’bboxes’ ] <span class="keyword">for</span> s <span class="keyword">in</span> batch]</span><br><span class="line">    labels = [s [ ’category_id’ ] <span class="keyword">for</span> s <span class="keyword">in</span> batch]</span><br><span class="line">    <span class="comment"># 以 当 前batch中 图 片annot数 量 的 最 大 值 作 为 标 记 数 据 的 第 二 维 度 值， 空 出 的 就补−1</span></span><br><span class="line">    max_num_annots = max( len (annot) <span class="keyword">for</span> annot <span class="keyword">in</span> annots)</span><br><span class="line">    annot_padded = np. ones (( len (annots) , max_num_annots, <span class="number">5</span>))*−<span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> max_num_annots &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">for</span> idx , (annot , lab) <span class="keyword">in</span> enumerate( zip (annots , labels )) :</span><br><span class="line">            <span class="keyword">if</span> len (annot) &gt; <span class="number">0</span>:</span><br><span class="line">                annot_padded[idx,:len(annot),:<span class="number">4</span>] = annot</span><br><span class="line">                annot_padded[idx,:len(annot),<span class="number">2</span>] += annot_padded[idx,:len(annot),<span class="number">0</span>]</span><br><span class="line">                annot_padded[idx,:len(annot),<span class="number">3</span>] += annot_padded[idx,:len(annot),<span class="number">1</span>]</span><br><span class="line">                annot_padded[idx,:len(annot),:] /= <span class="number">640</span></span><br><span class="line">    annot_padded[idx,:len(annot),<span class="number">4</span>] = lab</span><br><span class="line">   <span class="keyword">return</span> torch.stack(imgs,<span class="number">0</span>), torch.FloatTensor(annot_padded)   </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detection_collate_3</span><span class="params">(batch)</span> :</span></span><br><span class="line">    targets = []</span><br><span class="line">    imgs = [ ]</span><br><span class="line">    <span class="keyword">for</span> _, sample <span class="keyword">in</span> enumerate(batch) :</span><br><span class="line">        <span class="keyword">for</span> _, img_anno <span class="keyword">in</span> enumerate(sample) :</span><br><span class="line">            <span class="keyword">if</span> torch.is_tensor(img_anno) :</span><br><span class="line">                imgs.append(img_anno)</span><br><span class="line">           <span class="keyword">elif</span> isinstance(img_anno, np.ndarray) :  </span><br><span class="line">                annos = torch.from_numpy(img_anno).float()</span><br><span class="line">                targets.append(annos)</span><br><span class="line">   <span class="keyword">return</span> torch.stack(imgs,<span class="number">0</span>), targets <span class="comment"># 做了stack，DataContainer可以不做 stack</span></span><br></pre></td></tr></table></figure><p>以上就是数据处理的相关内容。最后再用 DataLoader 封装拆成迭代器，其相关细节，sampler 等暂略。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">data_loader = DataLoader(</span><br><span class="line">        dataset,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        sampler=sampler,</span><br><span class="line">        num_workers=num_workers,</span><br><span class="line">        collate_fn=partial(collate, samples_per_gpu=samples_per_gpu),</span><br><span class="line">        pin_memory=<span class="keyword">False</span>,</span><br><span class="line">        worker_init_fn=init_fn,</span><br><span class="line">        **kwargs)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;数据处理&quot;&gt;&lt;a href=&quot;#数据处理&quot; class=&quot;headerlink&quot; title=&quot;数据处理&quot;&gt;&lt;/a&gt;数据处理&lt;/h3&gt;&lt;p&gt;数据处理可能是炼丹师接触最为密集的了，因为通常情况，除了数据的离线处理，写个数据类，就可以炼丹了。但本节主要涉及数据的在线处理，更进一步应该是检测分割数据的 pytorch 处理方式。虽然 mmdet 将常用的数据都实现了，而且也实现了中间通用数据格式，但，这和模型，损失函数，性能评估的实现也相关，比如你想把官网的 centernet 完整的改成 mmdet风格，就能看到 (看起来没必要)。&lt;/p&gt;
    
    </summary>
    
      <category term="mmdetection" scheme="https://blog.nicehuster.cn/categories/mmdetection/"/>
    
    
      <category term="mmdetection" scheme="https://blog.nicehuster.cn/tags/mmdetection/"/>
    
  </entry>
  
  <entry>
    <title>mmdetection详解指北 (二)</title>
    <link href="https://blog.nicehuster.cn/2020/09/03/mmdetection-2/"/>
    <id>https://blog.nicehuster.cn/2020/09/03/mmdetection-2/</id>
    <published>2020-09-03T11:14:39.000Z</published>
    <updated>2020-09-18T07:19:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>上一篇博客主要介绍到mmdetection这个检测框架的一些结构设计以及代码的总体逻辑。这篇就主要介绍一下在mmdetection被大量使用的配置和注册。</p><h3 id="配置类"><a href="#配置类" class="headerlink" title="配置类"></a>配置类</h3><p>配置方式支持 python/json/yaml, 从 mmcv 的 Config 解析, 其功能同 maskrcnn-benchmark 的 yacs 类似, 将字典的取值方式属性化. 这里帖部分代码，以供学习。</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Config</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""A facility for config and config files.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It supports common file formats as configs: python/json/yaml. The interface</span></span><br><span class="line"><span class="string">    is the same as a dict object and also allows access config values as</span></span><br><span class="line"><span class="string">    attributes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Example:</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg = Config(dict(a=1, b=dict(b1=[0, 1])))</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg.a</span></span><br><span class="line"><span class="string">        1</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg.b</span></span><br><span class="line"><span class="string">        &#123;'b1': [0, 1]&#125;</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg.b.b1</span></span><br><span class="line"><span class="string">        [0, 1]</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg = Config.fromfile('tests/data/config/a.py')</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg.filename</span></span><br><span class="line"><span class="string">        "/home/kchen/projects/mmcv/tests/data/config/a.py"</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg.item4</span></span><br><span class="line"><span class="string">        'test'</span></span><br><span class="line"><span class="string">        &gt;&gt;&gt; cfg</span></span><br><span class="line"><span class="string">        "Config [path: /home/kchen/projects/mmcv/tests/data/config/a.py]: "</span></span><br><span class="line"><span class="string">        "&#123;'item1': [1, 2], 'item2': &#123;'a': 0&#125;, 'item3': True, 'item4': 'test'&#125;"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fromfile</span><span class="params">(filename)</span>:</span></span><br><span class="line">        filename = osp.abspath(osp.expanduser(filename))</span><br><span class="line">        check_file_exist(filename)</span><br><span class="line">        <span class="keyword">if</span> filename.endswith(<span class="string">'.py'</span>):</span><br><span class="line">            module_name = osp.basename(filename)[:<span class="number">-3</span>]</span><br><span class="line">            <span class="keyword">if</span> <span class="string">'.'</span> <span class="keyword">in</span> module_name:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">'Dots are not allowed in config file path.'</span>)</span><br><span class="line">            config_dir = osp.dirname(filename)</span><br><span class="line">            sys.path.insert(<span class="number">0</span>, config_dir)</span><br><span class="line">            mod = import_module(module_name)</span><br><span class="line">            sys.path.pop(<span class="number">0</span>)</span><br><span class="line">            cfg_dict = &#123;</span><br><span class="line">                name: value</span><br><span class="line">                <span class="keyword">for</span> name, value <span class="keyword">in</span> mod.__dict__.items()</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> name.startswith(<span class="string">'__'</span>)</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">elif</span> filename.endswith((<span class="string">'.yml'</span>, <span class="string">'.yaml'</span>, <span class="string">'.json'</span>)):</span><br><span class="line">            <span class="keyword">import</span> mmcv</span><br><span class="line">            cfg_dict = mmcv.load(filename)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> IOError(<span class="string">'Only py/yml/yaml/json type are supported now!'</span>)</span><br><span class="line">        <span class="keyword">return</span> Config(cfg_dict, filename=filename)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">auto_argparser</span><span class="params">(description=None)</span>:</span></span><br><span class="line">        <span class="string">"""Generate argparser from config file automatically (experimental)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        partial_parser = ArgumentParser(description=description)</span><br><span class="line">        partial_parser.add_argument(<span class="string">'config'</span>, help=<span class="string">'config file path'</span>)</span><br><span class="line">        cfg_file = partial_parser.parse_known_args()[<span class="number">0</span>].config</span><br><span class="line">        cfg = Config.fromfile(cfg_file)</span><br><span class="line">        parser = ArgumentParser(description=description)</span><br><span class="line">        parser.add_argument(<span class="string">'config'</span>, help=<span class="string">'config file path'</span>)</span><br><span class="line">        add_args(parser, cfg)</span><br><span class="line">        <span class="keyword">return</span> parser, cfg</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, cfg_dict=None, filename=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> cfg_dict <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            cfg_dict = dict()</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> isinstance(cfg_dict, dict):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">'cfg_dict must be a dict, but got &#123;&#125;'</span>.format(</span><br><span class="line">                type(cfg_dict)))</span><br><span class="line"></span><br><span class="line">        super(Config, self).__setattr__(<span class="string">'_cfg_dict'</span>, ConfigDict(cfg_dict))</span><br><span class="line">        super(Config, self).__setattr__(<span class="string">'_filename'</span>, filename)</span><br><span class="line">        <span class="keyword">if</span> filename:</span><br><span class="line">            <span class="keyword">with</span> open(filename, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                super(Config, self).__setattr__(<span class="string">'_text'</span>, f.read())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            super(Config, self).__setattr__(<span class="string">'_text'</span>, <span class="string">''</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">filename</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._filename</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">text</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._text</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'Config (path: &#123;&#125;): &#123;&#125;'</span>.format(self.filename,</span><br><span class="line">                                              self._cfg_dict.__repr__())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self._cfg_dict)</span><br><span class="line">    <span class="comment"># 获取key值</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getattr__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> getattr(self._cfg_dict, name)</span><br><span class="line">    <span class="comment"># 序列化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._cfg_dict.__getitem__(name)</span><br><span class="line">    <span class="comment"># 序列化</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setattr__</span><span class="params">(self, name, value)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(value, dict):</span><br><span class="line">            value = ConfigDict(value)</span><br><span class="line">        self._cfg_dict.__setattr__(name, value)</span><br><span class="line">    <span class="comment"># 更新key值</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setitem__</span><span class="params">(self, name, value)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(value, dict):</span><br><span class="line">            value = ConfigDict(value)</span><br><span class="line">        self._cfg_dict.__setitem__(name, value)</span><br><span class="line">    <span class="comment"># 迭代器</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> iter(self._cfg_dict)</span><br></pre></td></tr></table></figure><p>主要考虑点是自己怎么实现类似的东西，核心点就是 python 的基本魔法函数的应用，可同时参考 yacs。</p><h4 id="注册器"><a href="#注册器" class="headerlink" title="注册器"></a>注册器</h4><p>把基本对象放到一个继承了字典的对象中，实现了对象的灵活管理。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Registry</span>:</span></span><br><span class="line">    <span class="string">"""A registry to map strings to classes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        name (str): Registry name.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name)</span>:</span></span><br><span class="line">        self._name = name</span><br><span class="line">        self._module_dict = dict()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self._module_dict)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__contains__</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.get(key) <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        format_str = self.__class__.__name__ + \</span><br><span class="line">                     <span class="string">f'(name=<span class="subst">&#123;self._name&#125;</span>, '</span> \</span><br><span class="line">                     <span class="string">f'items=<span class="subst">&#123;self._module_dict&#125;</span>)'</span></span><br><span class="line">        <span class="keyword">return</span> format_str</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">name</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._name</span><br><span class="line"></span><br><span class="line"><span class="meta">    @property</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">module_dict</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self._module_dict</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="string">"""Get the registry record.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            key (str): The class name in string format.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            class: The corresponding class.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> self._module_dict.get(key, <span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_register_module</span><span class="params">(self, module_class, module_name=None, force=False)</span>:</span></span><br><span class="line">        <span class="comment"># 校验当前注册的module_class是否是类对象</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> inspect.isclass(module_class):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">'module must be a class, '</span></span><br><span class="line">                            <span class="string">f'but got <span class="subst">&#123;type(module_class)&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> module_name <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            module_name = module_class.__name__</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> force <span class="keyword">and</span> module_name <span class="keyword">in</span> self._module_dict:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(<span class="string">f'<span class="subst">&#123;module_name&#125;</span> is already registered '</span></span><br><span class="line">                           <span class="string">f'in <span class="subst">&#123;self.name&#125;</span>'</span>)</span><br><span class="line">        self._module_dict[module_name] = module_class  <span class="comment"># 类 名 : 类</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deprecated_register_module</span><span class="params">(self, cls=None, force=False)</span>:</span></span><br><span class="line">        warnings.warn(</span><br><span class="line">            <span class="string">'The old API of register_module(module, force=False) '</span></span><br><span class="line">            <span class="string">'is deprecated and will be removed, please use the new API '</span></span><br><span class="line">            <span class="string">'register_module(name=None, force=False, module=None) instead.'</span>)</span><br><span class="line">        <span class="keyword">if</span> cls <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> partial(self.deprecated_register_module, force=force)</span><br><span class="line">        self._register_module(cls, force=force)</span><br><span class="line">        <span class="keyword">return</span> cls</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">register_module</span><span class="params">(self, name=None, force=False, module=None)</span>:</span></span><br><span class="line">        <span class="comment"># 作 为 类 name 的 装 饰 器</span></span><br><span class="line">        <span class="string">"""Register a module.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        A record will be added to `self._module_dict`, whose key is the class</span></span><br><span class="line"><span class="string">        name or the specified name, and value is the class itself.</span></span><br><span class="line"><span class="string">        It can be used as a decorator or a normal function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Example:</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt; backbones = Registry('backbone')</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt; @backbones.register_module()</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt; class ResNet:</span></span><br><span class="line"><span class="string">            &gt;&gt;&gt;     pass</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            name (str | None): The module name to be registered. If not</span></span><br><span class="line"><span class="string">                specified, the class name will be used.</span></span><br><span class="line"><span class="string">            force (bool, optional): Whether to override an existing class with</span></span><br><span class="line"><span class="string">                the same name. Default: False.</span></span><br><span class="line"><span class="string">            module (type): Module class to be registered.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(force, bool):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">f'force must be a boolean, but got <span class="subst">&#123;type(force)&#125;</span>'</span>)</span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> This is a walkaround to be compatible with the old api,</span></span><br><span class="line">        <span class="comment"># while it may introduce unexpected bugs.</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(name, type):</span><br><span class="line">            <span class="keyword">return</span> self.deprecated_register_module(name, force=force)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># use it as a normal method: x.register_module(module=SomeClass)</span></span><br><span class="line">        <span class="keyword">if</span> module <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            self._register_module(</span><br><span class="line">                module_class=module, module_name=name, force=force)</span><br><span class="line">            <span class="keyword">return</span> module</span><br><span class="line"></span><br><span class="line">        <span class="comment"># raise the error ahead of time</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> (name <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">or</span> isinstance(name, str)):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">f'name must be a str, but got <span class="subst">&#123;type(name)&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># use it as a decorator: @x.register_module()</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">_register</span><span class="params">(cls)</span>:</span></span><br><span class="line">            self._register_module(</span><br><span class="line">                module_class=cls, module_name=name, force=force)</span><br><span class="line">            <span class="keyword">return</span> cls</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> _register</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_from_cfg</span><span class="params">(cfg, registry, default_args=None)</span>:</span></span><br><span class="line">    <span class="string">"""Build a module from config dict.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        cfg (dict): Config dict. It should at least contain the key "type".</span></span><br><span class="line"><span class="string">        registry (:obj:`Registry`): The registry to search the type from.</span></span><br><span class="line"><span class="string">        default_args (dict, optional): Default initialization arguments.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        object: The constructed object.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(cfg, dict):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">f'cfg must be a dict, but got <span class="subst">&#123;type(cfg)&#125;</span>'</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'type'</span> <span class="keyword">not</span> <span class="keyword">in</span> cfg:</span><br><span class="line">        <span class="keyword">raise</span> KeyError(</span><br><span class="line">            <span class="string">f'the cfg dict must contain the key "type", but got <span class="subst">&#123;cfg&#125;</span>'</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> isinstance(registry, Registry):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">'registry must be an mmcv.Registry object, '</span></span><br><span class="line">                        <span class="string">f'but got <span class="subst">&#123;type(registry)&#125;</span>'</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> (isinstance(default_args, dict) <span class="keyword">or</span> default_args <span class="keyword">is</span> <span class="keyword">None</span>):</span><br><span class="line">        <span class="keyword">raise</span> TypeError(<span class="string">'default_args must be a dict or None, '</span></span><br><span class="line">                        <span class="string">f'but got <span class="subst">&#123;type(default_args)&#125;</span>'</span>)</span><br><span class="line"></span><br><span class="line">    args = cfg.copy()</span><br><span class="line">    obj_type = args.pop(<span class="string">'type'</span>)</span><br><span class="line">    <span class="keyword">if</span> is_str(obj_type):</span><br><span class="line">        <span class="comment"># 从 注 册 类 中 拿 出obj_type类</span></span><br><span class="line">        obj_cls = registry.get(obj_type)</span><br><span class="line">        <span class="keyword">if</span> obj_cls <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> KeyError(</span><br><span class="line">                <span class="string">f'<span class="subst">&#123;obj_type&#125;</span> is not in the <span class="subst">&#123;registry.name&#125;</span> registry'</span>)</span><br><span class="line">    <span class="keyword">elif</span> inspect.isclass(obj_type):</span><br><span class="line">        obj_cls = obj_type</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> TypeError(</span><br><span class="line">            <span class="string">f'type must be a str or valid type, but got <span class="subst">&#123;type(obj_type)&#125;</span>'</span>)</span><br><span class="line">    <span class="comment"># 增 加 一 些 新 的 参 数</span></span><br><span class="line">    <span class="keyword">if</span> default_args <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">for</span> name, value <span class="keyword">in</span> default_args.items():</span><br><span class="line">            args.setdefault(name, value)</span><br><span class="line">    <span class="keyword">return</span> obj_cls(**args)<span class="comment"># **args 是 将 字 典 解 析 成 位 置 参 数(k=v)。</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;上一篇博客主要介绍到mmdetection这个检测框架的一些结构设计以及代码的总体逻辑。这篇就主要介绍一下在mmdetection被大量使用的配置和注册。&lt;/p&gt;
&lt;h3 id=&quot;配置类&quot;&gt;&lt;a href=&quot;#配置类&quot; class=&quot;headerlink&quot; title=&quot;配置类&quot;&gt;&lt;/a&gt;配置类&lt;/h3&gt;&lt;p&gt;配置方式支持 python/json/yaml, 从 mmcv 的 Config 解析, 其功能同 maskrcnn-benchmark 的 yacs 类似, 将字典的取值方式属性化. 这里帖部分代码，以供学习。&lt;/p&gt;
    
    </summary>
    
      <category term="mmdetection" scheme="https://blog.nicehuster.cn/categories/mmdetection/"/>
    
    
      <category term="mmdetection" scheme="https://blog.nicehuster.cn/tags/mmdetection/"/>
    
  </entry>
  
  <entry>
    <title>mmdetection详解指北 (一)</title>
    <link href="https://blog.nicehuster.cn/2020/09/01/mmdetection-1/"/>
    <id>https://blog.nicehuster.cn/2020/09/01/mmdetection-1/</id>
    <published>2020-09-01T11:13:39.000Z</published>
    <updated>2020-09-18T07:19:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>平时做的都些检测相关的项目，因此对于各类检测框架使用较多，以及一些不知名的repo都有用过，平时接触的都是业务项目，很少去认真的看一个repo中算法的框架设计，最近项目处于交付阶段，主要是客户和平台开发人员对界面的问题。趁有空mmdetection的代码重新看了一遍，顺便做了一些笔记。</p><h3 id="组件设计"><a href="#组件设计" class="headerlink" title="组件设计"></a>组件设计</h3><blockquote><ul><li>BackBone: 特征提取骨架网络,ResNet,ResneXt,ssd_vgg, hrnet 等。</li><li>Neck: 连接骨架和头部. 多层级特征融合,FPN,BFP,PAFPN 等。</li><li>DenseHead: 处理特征图上的密集框部分, 主要分 AnchorHead。AnchorFreeHead 两大类，分别有 RPNHead, SSDHead,RetinaHead 和 FCOSHead 等。</li><li>RoIHead (BBoxHead/MaskHead): 在特征图上对 roi 做类别分类或位置回归等 (1.x)。</li><li>ROIHead:bbox 或 mask 的 roi_extractor+head(2.0, 合并了 extractor 和 head)</li><li>SingleStage: BackBone + Neck + DenseHead</li><li>TwoStage: BackBone + Neck + DenseHead + RoIHead(2.0)</li></ul></blockquote><a id="more"></a><h3 id="结构设计"><a href="#结构设计" class="headerlink" title="结构设计"></a>结构设计</h3><h4 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h4><blockquote><ul><li><p>configs 网络组件结构等配置信息 </p></li><li><p>tools: 训练和测试的最终包装和一些实用脚本 </p></li><li><p>mmdet:</p><blockquote><ul><li>apis: 分布式环境设定 (1.x,2.0 移植到 mmcv), 推断, 测试, 训练基础代码;</li><li>core: anchor 生成,bbox,mask 编解码, 变换, 标签锚定, 采样等, 模型评估, 加速, 优化器，后处理;</li><li>datasets:coco,voc 等数据类, 数据 pipelines 的统一格式, 数据增强，数据采样;</li><li>models: 模型组件 (backbone,head,loss,neck)，采用注册和组合构建的形式完成模型搭建</li><li>ops: 优化加速代码, 包括 nms,roialign,dcn,masked_conv，focal_loss 等</li></ul></blockquote></li></ul></blockquote><p><img src="/img/image-20200917202833058.png" alt="image-20200917202833058"></p><h4 id="总体逻辑"><a href="#总体逻辑" class="headerlink" title="总体逻辑"></a>总体逻辑</h4><p>从 tools/train.py 中能看到整体可分如下 4 个步骤:</p><blockquote><ul><li><p>1.mmcv.Config.fromfile 从配置文件解析配置信息, 并做适当更新, 包括环境搜集，预加载模型文件, 分布式设置，日志记录等;</p></li><li><p>2.mmdet.models 中的 build_detector 根据配置信息构造模型 ;</p><blockquote><ul><li>2.1 build 系列函数调用 build_from_cfg 函数, 按 type 关键字从注册表中获取相应的对象, 对象的具名参数在注册文件中赋值;</li><li>2.2 registr.py 放置了模型的组件注册器。其中注册器的 register_module 成员函数是一个装饰器功能函数，在具体的类对象 <em>A</em> 头上装饰 @X.register _module，并同时在 <em>A</em> 对象所在包的初始化文件中调用 <em>A</em>，即可将 <em>A</em> 保存到 registry.module_dict 中, 完成注册;_</li><li>2.3 目前包含 BACKBONES,NECKS,ROI_EXTRACTORS,SHARED_ HEADS,HEADS,LOSSES,DETECTORS 七个模型相关注册器，另外还有数据类，优化器等注册器;</li></ul></blockquote></li></ul><ul><li><p>3.build_dataset 根据配置信息获取数据类;</p><blockquote><ul><li>3.1 coco，cityscapes，voc，deepfasion，lvis，wider_face 等数据 (数据类扩展见后续例子)。</li></ul></blockquote></li><li><p>4.train_detector 模型训练流程:</p><blockquote><ul><li>4.1数据 loader 化, 模型分布式化，优化器选取</li><li>4.2 进入 runner 训练流程 (来自 mmcv 库，采用 hook 方式，整合了 pytorch 训练流程)</li><li>4.3 训练 pipelines 具体细节见后续展开。</li></ul></blockquote></li></ul></blockquote><p>后续说说配置文件，注册机制和训练逻辑。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;平时做的都些检测相关的项目，因此对于各类检测框架使用较多，以及一些不知名的repo都有用过，平时接触的都是业务项目，很少去认真的看一个repo中算法的框架设计，最近项目处于交付阶段，主要是客户和平台开发人员对界面的问题。趁有空mmdetection的代码重新看了一遍，顺便做了一些笔记。&lt;/p&gt;
&lt;h3 id=&quot;组件设计&quot;&gt;&lt;a href=&quot;#组件设计&quot; class=&quot;headerlink&quot; title=&quot;组件设计&quot;&gt;&lt;/a&gt;组件设计&lt;/h3&gt;&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;BackBone: 特征提取骨架网络,ResNet,ResneXt,ssd_vgg, hrnet 等。&lt;/li&gt;
&lt;li&gt;Neck: 连接骨架和头部. 多层级特征融合,FPN,BFP,PAFPN 等。&lt;/li&gt;
&lt;li&gt;DenseHead: 处理特征图上的密集框部分, 主要分 AnchorHead。AnchorFreeHead 两大类，分别有 RPNHead, SSDHead,RetinaHead 和 FCOSHead 等。&lt;/li&gt;
&lt;li&gt;RoIHead (BBoxHead/MaskHead): 在特征图上对 roi 做类别分类或位置回归等 (1.x)。&lt;/li&gt;
&lt;li&gt;ROIHead:bbox 或 mask 的 roi_extractor+head(2.0, 合并了 extractor 和 head)&lt;/li&gt;
&lt;li&gt;SingleStage: BackBone + Neck + DenseHead&lt;/li&gt;
&lt;li&gt;TwoStage: BackBone + Neck + DenseHead + RoIHead(2.0)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="mmdetection" scheme="https://blog.nicehuster.cn/categories/mmdetection/"/>
    
    
      <category term="mmdetection" scheme="https://blog.nicehuster.cn/tags/mmdetection/"/>
    
  </entry>
  
</feed>
