<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一起打怪升级呀</title>
  <icon>https://www.gravatar.com/avatar/2555127dc0de830d31ceeb98d8565ac8</icon>
  <subtitle>别整太大鸭力,多鸡立自己qaq</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://blog.nicehuster.cn/"/>
  <updated>2022-06-15T03:22:11.133Z</updated>
  <id>https://blog.nicehuster.cn/</id>
  
  <author>
    <name>nicehuster</name>
    <email>nicehuster@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>基于CLIP模型的zero-shot目标检测方法</title>
    <link href="https://blog.nicehuster.cn/2022/06/13/ViLD/"/>
    <id>https://blog.nicehuster.cn/2022/06/13/ViLD/</id>
    <published>2022-06-13T11:13:39.000Z</published>
    <updated>2022-06-15T03:22:11.133Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息</strong>：<a href="https://arxiv.org/pdf/2104.13921.pdf" target="_blank" rel="noopener">Open-vocabulary Object Detection via Vision and Language Knowledge Distillation</a><br><strong>代码链接</strong>：<a href="https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild" target="_blank" rel="noopener">https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild</a><br><strong>整体信息</strong>：这是google research 发表在ICLR2022上有关CLIP在下游任务-目标检测任务上的应用。使用CLIP模型实现zero-shot场景下的目标检测任务。比较有想象意义的是，通过一句话就可以检测出图像中需要的指定目标。在之前<a href="https://nicehuster.github.io/2022/06/03/CLIP/#more" target="_blank" rel="noopener">CLIP图文多模态对比预训练方法详解</a>中也有提及过这篇工作。</p><a id="more"></a><p><img src="https://camo.githubusercontent.com/238affd721b73ef2ff8f95114352188693623796bcc57b83f94747a7d2b6ff3a/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f636c6f75642d7470752d636865636b706f696e74732f646574656374696f6e2f70726f6a656374732f76696c642f6173736574732f6e65775f7465617365722e706e67" alt></p><p>上图展示的是ViLD的检测结果。其中只有toy类别是训练过程中见到的类别，但zero-shot detection还检测到其他的属性，比如toy种类和颜色等。</p><h3 id="zero-shot-detection"><a href="#zero-shot-detection" class="headerlink" title="zero-shot detection"></a>zero-shot detection</h3><p>顾名思义，这个任务就是，对于任意一个新类别，一张训练图像都不给的情况下，训练出来的检测器也能检测这个类别。zero-shot detection的setting通常是，将数据集依据类别划分为俩部分：base类别和novel类别，base类别用于训练，novel类别在训练过程中不可见。该任务的目标在于，在novel类别上获得较好性能的同时还需要保持base类别的性能；在这篇文章中使用的是LVIS数据集进行实验对比分析，将common和frequency俩类别作为base类，将rare类别作为novel类。</p><h3 id="常规方法"><a href="#常规方法" class="headerlink" title="常规方法"></a>常规方法</h3><p><img src="/img/vild-Vanilla.png" alt="vild-Vanilla"></p><p>如上图展示的是zero-shot detection with cropped regions，具体地，使用在二阶段检测方法比如Mask-RCNN获得proposal之后，对每个proposal都crop &amp; resize 然后输入到CLIP-image-encoder中获得image-embedding，与对应类别的text-embedding进行对比，获取类别信息。该方法的的缺点是比较慢，需要one-by-one地处理每个object proposal，而且CLIP-text-encoder没有充分利用base类别的文本信息。</p><h3 id="ViLD方法"><a href="#ViLD方法" class="headerlink" title="ViLD方法"></a>ViLD方法</h3><p><img src="/img/vild-pipeline.png" alt="vild-pipeline"></p><p>上图展示的是ViLD方法的pipeline。具体地，在ViLD中包含俩部分：<strong>ViLD-text</strong>用于学习文本embedding和<strong>ViLD-image</strong>用于学习图像embedding。在ViLD-text中，将base类别文本送入CLIP-text-encoder中获得text embedding，然后用于classify目标区域，在ViLD-image中会将对应的proposal送入CLIP-image-encoder中获得图像embedding，对经过roi align之后的region embedding 进行知识蒸馏；相比于ViLD-text，ViLD-image蒸馏了base+novel的信息，因为proposal网络输出的proposal可能会包含novel，而ViLD-text只使用了base类的文本信息；</p><p><img src="/img/vild-overview.png" alt="vild-overview"></p><p>上图展示的是ViLD的训练和推理流程。相比于mask-rcnn，修改地是rcnn的分类分支；具体地，在训练过程中，在获取分类监督信号上包括俩部分：用CLIP获得image embedding蒸馏region embedding，以及用CLIP获得text embedding监督region embedding；总的损失如下公式所示：</p><script type="math/tex; mode=display">\mathcal{L}_{\mathrm{ViLD}}=\mathcal{L}_{\text {ViLD-text }}+w \cdot \mathcal{L}_{\mathrm{ViLD}-\mathrm{image}}</script><p>在推理过程，只需要将region embedding和text embedding(base+novel)进行对比即可得到类别信息。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p><strong>数据集</strong>：实验使用的是LVIS v1.0（1203类别），其中frequent(f: 405个类别)和common(c: 461个类别)作为base类，其余rare(r: 337个类别)作为novel类。</p><p><strong>目标proposal</strong>：由于训练过程中只使用了base类训练，下表展示的是仅使用base类训练时的RPN召回率和使用base+novel时的RPN召回率，从上可以看出二者相差1-2个点。因此可以看出RPN是具备从base类上泛化到novel。</p><p><img src="/img/vild-recall.png" alt="vild-recall"></p><p><strong>Ablation</strong>：作者在paper中做了较为详尽的ablation study实验，这里只提及一些证明idea有效的关键实验分析。</p><p><img src="/img/vild-ensemble.png" alt="vild-ensemble"></p><p>上表格中，CLIP on cropped regions就是前面介绍的常规方法，该方法在APr上可以达到13.0，ViLD-text和ViLD-image表示分别使用单一监督信号。ViLD(w=0.5)表示同时使用ViLD-text和ViLD-image监督训练。ViLD-text相比CLIP on cropped regions在APr上下降了3个点，说明使用base类信息监督ViLD-text在novel上的泛化性有所下降。ViLD(w=0.5)相比于ViLD-text和ViLD-image都提升幅度明显。ViLD-ensemble(w=0.5)表示同时使用ViLD-text和ViLD-image监督训练同时，在base预测上，倾向于ViLD-text，在novel预测上使用vice versa投票决定。可以看出ViLD-ensemble(w=0.5)方式在base类别上提升明显。</p><p><strong>Transfer to other detection datasets</strong>：这个是证明在不同数据集之间的一个迁移有效性。只需要替换类别 text embedding，无需进行fine-tune。</p><p><img src="/img/vild-transfer.png" alt="vild-transfer"></p><p>在不进行任何fine-tune下，ViLD在COCO数据集上就可以取得36.6AP，与fine-tune条件下AP只相差不到3个点。</p><h3 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h3><p>作者也在离线交互式检测上也做过一些实验，输入文本信息，就可以检测出对应的目标。这个还挺有意思的，随意说一句话就能检测到图像的指定目标。</p><p><img src="/img/vild-interactive detection.png" alt="vild-interactive detection"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息&lt;/strong&gt;：&lt;a href=&quot;https://arxiv.org/pdf/2104.13921.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Open-vocabulary Object Detection via Vision and Language Knowledge Distillation&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接&lt;/strong&gt;：&lt;a href=&quot;https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息&lt;/strong&gt;：这是google research 发表在ICLR2022上有关CLIP在下游任务-目标检测任务上的应用。使用CLIP模型实现zero-shot场景下的目标检测任务。比较有想象意义的是，通过一句话就可以检测出图像中需要的指定目标。在之前&lt;a href=&quot;https://nicehuster.github.io/2022/06/03/CLIP/#more&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CLIP图文多模态对比预训练方法详解&lt;/a&gt;中也有提及过这篇工作。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="multi-model" scheme="https://blog.nicehuster.cn/tags/multi-model/"/>
    
  </entry>
  
  <entry>
    <title>DeCLIP一种数据高效的CLIP训练方法</title>
    <link href="https://blog.nicehuster.cn/2022/06/09/DeCLIP/"/>
    <id>https://blog.nicehuster.cn/2022/06/09/DeCLIP/</id>
    <published>2022-06-09T11:13:39.000Z</published>
    <updated>2022-06-13T03:56:00.801Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息</strong>：<a href="https://arxiv.org/pdf/2110.05208.pdf" target="_blank" rel="noopener">Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm</a><br><strong>代码链接</strong>：<a href="https://github.com/Sense-GVT/DeCLIP" target="_blank" rel="noopener">https://github.com/Sense-GVT/DeCLIP</a><br><strong>整体信息</strong>：这是商汤科技发表在ICLR2022上关于多模态预训练的工作，在前面的文章中介绍过CLIP，是一种基于对比文本-图像对的预训练方法，该方法需要在大量的图像-文本对数据集进行训练，在CLIP工作上就使用了4亿的图像-文本对数据，数百张卡进行预训练。为了提高训练效率，这篇工作提出了DeCLIP(Data Efficiency CLIP)方法，在较少数据下依旧可以取得不错的效果。</p><a id="more"></a><p><img src="/img/declip-sota.png" alt="declip-sota"></p><h3 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h3><p><img src="/img/clip-vs-declip.png" alt="clip-vs-declip"></p><p>上图，直观地，展示的是CLIP和DeCLIP方法的差异。CLIP是直接学习原始图片与对应的文本信息，使用俩个encoder分别编码图像信息和文本信息。图像encoder一般是resnet或者ViT，文本encoder一般使用transformer。之后将俩个embedding映射到相同的空间中，使用对比学习的思想进行训练。从方法上看，其实只使用了图像-文本对匹配的一种监督信号进行训练。假设batch size是N，共计N个图像-文本对$\left\{\left(x_{i}^{I}, x_{i}^{T}\right)\right\}$，损失函数InfoNCE如下：</p><script type="math/tex; mode=display">L_{I}=-\frac{1}{N} \sum_{i=1}^{N} \log \frac{\exp \left(\operatorname{sim}\left(\boldsymbol{z}_{i}^{I}, \boldsymbol{z}_{i}^{T}\right) / \tau\right)}{\sum_{j=1}^{N} \exp \left(\operatorname{sim}\left(\boldsymbol{z}_{i}^{I}, \boldsymbol{z}_{j}^{T}\right) / \tau\right)}</script><p>不同于CLIP，在DeCLIP方法中，使用了更多的自监督信号：1. 单模态的自监督学习；2. 跨模态的多视角监督学习；3. 最近邻监督学习；具体地，</p><ol><li><p><strong>单模态自监督学习</strong>（self-supervision within each modality, SS），包括使用<strong>SimSiam</strong>作为图像的自监督信号，和使用掩码语言模型<strong>MLM</strong>作为文本的自监督信号；</p><p><img src="/img/declip-ss.png" alt="declip-ss"></p><p>(a)<strong>图像自监督</strong>：同一张图片进过数据增强获得俩个view：$(x^{I}, \tilde{x}^{I})$,将经过数据增强后的结果经过相同的encoder得到俩个embedding向量$(z^{I}, \tilde{z}^{I})$，之后将其中一个embedding向量$x^{I}$再经过一个perd层得到向量$p^{I}$,训练时让$p^{I}$和$\tilde{x}^{I}$ 尽量接近；</p><p>(b)<strong>文本自监督</strong>：文本自监督使用的是MLM方法，即随机mask掉文本中15%的token，然后利用前后token预测被mask掉的token；</p></li><li><p><strong>跨模态多视角监督学习</strong>（Multi-View Supervision, MVS）：CLIP只使用的原始图像-文本对$\left(z^{I}, z^{T}\right)$，计算infoNCE损失，而DeCLIP中使用的是增强后的文本和图像计算infoNCE损失：$\left(z^{I}, z^{T}\right), \quad\left(\tilde{z}^{I}, z^{T}\right),\left(z^{I}, \tilde{z}^{T}\right), \quad\left(\tilde{z}^{I}, \tilde{z}^{T}\right)$ ，相比CLIP多了3个监督信息；</p></li><li><p><strong>最近邻监督学习</strong>（Nearest-Neighbor Supervision, NNS）：考虑到相同的图像可能会有类似的语言描述，因此选择语言描述相似的图文进行对比学习，通过维护一个先入先出的队列来模拟整个数据的分布，从队列中选择最相似的句子作为正样本$z^{T^{\prime}}$，之后使用InfoNCE计算最近邻损失：$\left(z^{I}, z^{T^{\prime}}\right),\left(\tilde{z}^{I}, z^{T^{\prime}}\right)$;</p><p><img src="/img/declip-nss.png" alt="declip-nss"></p></li></ol><p>在损失函数层面上，对以上三种不同监督的损失进行加权求和，得到最终的loss，具体地，如下所示：</p><script type="math/tex; mode=display">L_{D e C L I P}=(1-\alpha-\beta-\gamma) L_{C L I P}+\alpha\left(L_{I S S}+L_{T S S}\right)+\beta L_{M V S}+\gamma L_{N N S}</script><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p><img src="/img/declip-dataset.png" alt="declip-dataset"></p><p>在DeCLIP中，数据集包含俩部分：开源数据集29M和网络下载的数据集59M，总共88M训练数据，相比于CLIP使用的400M数据少很多。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><ol><li><p>Zero-shot准确率;</p><p><img src="/img/declip-zero-shot.png" alt="declip-zero-shot"></p><p>相比于CLIP，使用更少的训练数据，得到了更高的准确率；</p></li><li><p>下游任务表现；</p><p><img src="/img/declip-finetune.png" alt="declip-finetune"></p><p>在resnet和ViT俩种不同的encoder上，都证明了DeCLIP学习到的特征表示相比CLIP要强；</p></li><li><p>Ablation study</p><p><img src="/img/declip-ablation.png" alt="declip-ablation"></p><p>如上图证明了使用多种监督信息可有效的提升zero-shot准确率，而且相比于CLIP，DeCLIP的训练效率更高；</p></li></ol><h3 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h3><p>作者还在DeCLIP的基础上提出了<a href="https://arxiv.org/abs/2203.05796" target="_blank" rel="noopener">CLIP-benchmark</a>，其中包含了高质量的YFCC15M-V2数据集，而且复现了CLIP系列的相关方法(CLIP，DeCLIP，FILIP，DeCLIP，DeFILIP)。目前代码均已开源在<a href="https://github.com/Sense-GVT/DeCLIP" target="_blank" rel="noopener">这里</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息&lt;/strong&gt;：&lt;a href=&quot;https://arxiv.org/pdf/2110.05208.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接&lt;/strong&gt;：&lt;a href=&quot;https://github.com/Sense-GVT/DeCLIP&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/Sense-GVT/DeCLIP&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息&lt;/strong&gt;：这是商汤科技发表在ICLR2022上关于多模态预训练的工作，在前面的文章中介绍过CLIP，是一种基于对比文本-图像对的预训练方法，该方法需要在大量的图像-文本对数据集进行训练，在CLIP工作上就使用了4亿的图像-文本对数据，数百张卡进行预训练。为了提高训练效率，这篇工作提出了DeCLIP(Data Efficiency CLIP)方法，在较少数据下依旧可以取得不错的效果。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="multi-model" scheme="https://blog.nicehuster.cn/tags/multi-model/"/>
    
  </entry>
  
  <entry>
    <title>视觉-语言预训练(Vision-Language Pretraining)综述</title>
    <link href="https://blog.nicehuster.cn/2022/06/05/VLP/"/>
    <id>https://blog.nicehuster.cn/2022/06/05/VLP/</id>
    <published>2022-06-05T11:13:39.000Z</published>
    <updated>2022-06-07T08:33:47.376Z</updated>
    
    <content type="html"><![CDATA[<p>前几天看完CLIP论文后觉得视觉-语言预训练(Vision-Language Pretraining)这个方向还挺有意思，就顺便找了一篇关于VLP的综述文章：<a href="https://arxiv.org/abs/2202.09061" target="_blank" rel="noopener">VLP: A Survey on Vision-Language Pre-training</a>，这篇文章有详细地介绍了VLP领域的最新进展和总结，包括了图像-文本和视频-文本的预训练。对于完整的了解VLP这个领域有很大帮助。</p><a id="more"></a><p>整篇综述从以下5个方面对视觉-语言预训练进行了详细地阐述：</p><blockquote><ol><li>特征提取</li><li>模型架构</li><li>预训练目标</li><li>预训练数据集</li><li>下游任务</li></ol></blockquote><p>VLP主要通过对大规模数据的预训练来学习不同模态之间的语义对应关系。例如，在图文预训练中，我们希望模型将文本中的“狗”与图像中的“狗”联系起来。在视频文本预训练中，我们期望模型将文本中的对象/动作映射到视频中的对象/动作。为了实现这一目标，需要巧妙地设计VLP对象和模型体系结构，以允许模型挖掘不同模式之间的关联。</p><h3 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h3><p>特征提取部分分为视觉特征提取和文本特征提取。视觉特征提取包括图像特征提取和视频特征提取。</p><h4 id="图像特征提取"><a href="#图像特征提取" class="headerlink" title="图像特征提取"></a>图像特征提取</h4><blockquote><ol><li>OD-based Region Features (OD-RFs)：使用预训练的目标检测模型识别图像中目标区域，并提取每个区域的表示，来提取视觉特征，常用的是Faster RCNN；</li><li>CNN-based Grid Features (CNN-GFs)：直接用CNN对整张图提取视觉特征获得网格特征；</li><li>ViT-based Patch Features (ViT-PFs)：使用ViT将图片压平成序列，对于transformer类型的encoder的输入比较友好；</li></ol></blockquote><h4 id="视频特征提取"><a href="#视频特征提取" class="headerlink" title="视频特征提取"></a>视频特征提取</h4><p>一般把视频当做M帧组成的图像信息。VLP模型利用上述图像特征提取方法提取frame特征。俩个常用的是CNN-GFs和ViT-PFs。对于CNN-GFs，一般是使用在imagenet上预训练的resnet和预先训练好的SlowFast来提取每个视频帧的2d特征和3d特征。将这些特征串联便可得到视频的视觉特征，通过FC层被投影到与token embedding相同的低维空间；对于ViT-PFs，一段视频clips $V_{i} \in \mathbb{R}^{M \times H \times W \times C}$ 会被分割为 $M \times N$ 的无重叠的时空patchs，大小为$P \times P$, 在这里 $N=\frac{H W}{P^{2}}$.</p><h4 id="文本特征提取"><a href="#文本特征提取" class="headerlink" title="文本特征提取"></a>文本特征提取</h4><p>对于文本特征，一般使用Bert进行特征提取。VLP模型首先将输入的句子分割成一系列字词。然后再序列的开头和结尾处插入序列开始和结束标记。</p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>模型结构主要从俩个视角进行划分：</p><blockquote><ol><li>从多模态融合上，可以划分为：Single-stream和Dual-stream；</li><li>从整体结构设计上，可以划分为：Encoder-Only和Encoder-Decoder；</li></ol></blockquote><p><img src="/img/vlp-single-stream-vs-dual-stream.png" alt="image-20220607114126559"></p><h4 id="Single-stream-vs-Dual-stream"><a href="#Single-stream-vs-Dual-stream" class="headerlink" title="Single-stream vs Dual-stream"></a>Single-stream vs Dual-stream</h4><p>单流架构是指将文本和视觉特征连接在一起，然后馈送到单个Transformer块中。双流架构是指文本和视觉特征不是串联在一起而是独立地发送到两个不同的Transformer块中。单流架构一般来说更加parameter-efficient。双流架构一般采用上图虚线所表示的cross attention进行特征交互。</p><h4 id="Encoder-Only-vs-Encoder-Decoder"><a href="#Encoder-Only-vs-Encoder-Decoder" class="headerlink" title="Encoder-Only vs Encoder-Decoder"></a>Encoder-Only vs Encoder-Decoder</h4><p>许多VLP模型采用encoder-only的体系结构，其中跨模态表示被直接馈送到输出层以生成最终输出。相比之下，其他VLP模型主张使用encoder-decoder体系结构，其中跨模态表示首先被馈送到decoder，然后被馈送到输出层。</p><h3 id="预训练目标"><a href="#预训练目标" class="headerlink" title="预训练目标"></a>预训练目标</h3><p>在论文中对预训练目标归纳为了四类：Completion、Matching、Temporal和Particular。</p><h4 id="Completion"><a href="#Completion" class="headerlink" title="Completion"></a>Completion</h4><p>这类方法主要是通过对masked的元素进行重建来理解模态信息。包括：Masked Language Modeling、Prefix Language Modeling和 Masked Vision Modeling。</p><h5 id="Masked-Language-Modeling"><a href="#Masked-Language-Modeling" class="headerlink" title="Masked Language Modeling"></a>Masked Language Modeling</h5><p>掩蔽语言建模(MLM)应该是被广泛应用在Bert模型中的一种预训练方式，通过利用上下文的可见的词向量预测masked词向量。而在VLP任务重则是使用上下文可见的词向量和视觉向量表征去预测masked的视觉或者词向量。</p><h5 id="Prefix-Language-Modeling"><a href="#Prefix-Language-Modeling" class="headerlink" title="Prefix Language Modeling"></a>Prefix Language Modeling</h5><p>前缀语言建模(Prefix Language Model，Prefix LM)是屏蔽语言模型和语言建模的统一。前缀模型的提出是为了使模型具有实体生成能力，使得文本诱导的zero-shot具有无需fine-tuning的泛化性。</p><h5 id="Masked-Vision-Modeling"><a href="#Masked-Vision-Modeling" class="headerlink" title="Masked Vision Modeling"></a>Masked Vision Modeling</h5><p>与MLM类似，MVM对视觉(图像或视频)区域或块进行采样，并通常以15%的概率mask其视觉特征。在给定剩余视觉特征和所有文本特征的情况下，VLP模型需要重建mask的视觉特征。</p><h4 id="Matching"><a href="#Matching" class="headerlink" title="Matching"></a>Matching</h4><p>Matching是将视觉和语言统一到一个共享的隐层，生成通用的视觉语言表征。包括Vision-Language Matching， Vision-Language Contrastive Learning， Word-Region Alignment。</p><h5 id="Vision-Language-Matching"><a href="#Vision-Language-Matching" class="headerlink" title="Vision-Language Matching"></a>Vision-Language Matching</h5><p>视觉语言匹配(VLM)是最常见的预训练模型的目标，以实现视觉和语言的匹配。以双流VLP模型为例，在得到视觉表征和文本表征之后将其串联起来，作为俩种模式的融合表征，然后经过FC层和sigmoid函数，以预测0-1分数，0表示视觉-语言不匹配，1表示视觉和语言匹配。</p><h5 id="Vision-Language-Contrastive-Learning"><a href="#Vision-Language-Contrastive-Learning" class="headerlink" title="Vision-Language Contrastive Learning"></a>Vision-Language Contrastive Learning</h5><p>视觉语言对比学习(VLC)从N × N个可能的视觉语言对中预测出匹配的视觉语言对。请注意，在一批训练中有N ~ N个负视觉语言对。VLP模型计算 softmax-normalized 的视觉(图像或视频)到文本的相似性和文本到视觉的相似性，并利用视觉到文本和文本到视觉相似性的交叉熵损失来更新自己。相似度通常用点积来实现。最常见的方法就是CLIP。</p><h5 id="Word-Region-Alignment"><a href="#Word-Region-Alignment" class="headerlink" title="Word-Region Alignment"></a>Word-Region Alignment</h5><p>单词-区域对齐(WRA)是一种无监督的预训练目标，用于对齐视觉区域(视觉patch)和单词。VLP模型利用最优传输来学习视觉和语言之间的对齐。</p><h3 id="预训练数据集"><a href="#预训练数据集" class="headerlink" title="预训练数据集"></a>预训练数据集</h3><p><img src="/img/vlp-datasets.png" alt="image-20220607151527603"></p><h3 id="下游任务"><a href="#下游任务" class="headerlink" title="下游任务"></a>下游任务</h3><p>下游任务主要有分类、回归、检索、生成以及其他任务。</p><blockquote><p><strong>分类任务</strong>：Visual Question Answering (VQA)，Visual Reasoning and Compositional Question Answering (GQA)，Video-Language Inference (VLI)，Natural Language for Visual Reasoning (NLVR)，Visual Entailment (VE)，Visual Commonsense Reasoning (VCR)，Grounding Referring Expressions (GRE)，Category Recognition (CR).<br><strong>回归任务</strong>：Multi-modal Sentiment Analysis (MSA).<br><strong>检索任务</strong>：Vision-Language Retrieval (VLR).<br><strong>生成任务</strong>：Visual Captioning (VC)，Novel Object Captioning at Scale (NoCaps)，Visual Dialogue (VD).<br><strong>其他任务</strong>：Multi-modal Machine Translation (MMT)，Vision-Language Navigation (VLN)，Optical Character Recognition (OCR).</p></blockquote><h3 id="SOTA-VLP-模型"><a href="#SOTA-VLP-模型" class="headerlink" title="SOTA VLP 模型"></a>SOTA VLP 模型</h3><p><img src="/img/vlp-sota.png" alt="vlp-sota"></p><h3 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h3><p>第一篇关于VLP的调研综述。文章从特征提取、模型结构、预训练目标、预训练数据集和下游任务五个方面综述了其最新进展，并对具体的SOTA VLP模型进行了详细的总结。这能够帮助研究人员更好地了解VLP，并启发新的工作来推动这一领域的发展。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前几天看完CLIP论文后觉得视觉-语言预训练(Vision-Language Pretraining)这个方向还挺有意思，就顺便找了一篇关于VLP的综述文章：&lt;a href=&quot;https://arxiv.org/abs/2202.09061&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;VLP: A Survey on Vision-Language Pre-training&lt;/a&gt;，这篇文章有详细地介绍了VLP领域的最新进展和总结，包括了图像-文本和视频-文本的预训练。对于完整的了解VLP这个领域有很大帮助。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="multi-model" scheme="https://blog.nicehuster.cn/tags/multi-model/"/>
    
  </entry>
  
  <entry>
    <title>CLIP图文多模态对比预训练方法详解</title>
    <link href="https://blog.nicehuster.cn/2022/06/03/CLIP/"/>
    <id>https://blog.nicehuster.cn/2022/06/03/CLIP/</id>
    <published>2022-06-03T11:13:39.000Z</published>
    <updated>2022-06-07T08:17:32.448Z</updated>
    
    <content type="html"><![CDATA[<p>CLIP是OpenAI在2021年发表的一种用NLP来监督CV的方法。成功连接文本和图像。CLIP全称是， Contrastive Language–Image Pre-training，一种基于对比文本-图像对的预训练方法。在了解CLIP具体方法之前，可以先看一下该工作的在一些下游任务的应用。<br><a id="more"></a></p><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>1.风格迁移<a href="https://arxiv.org/abs/2103.17249" target="_blank" rel="noopener">styleCLIP</a></p><p><img src="https://github.com/orpatashnik/StyleCLIP/raw/main/img/teaser.png" alt></p><p>styleCLIP很神奇，CLIP可以理解各种抽象的妆容，比如发型，卸妆等等</p><p>2.文本生成图像<a href="https://arxiv.org/abs/2106.14843" target="_blank" rel="noopener">CLIPDraw</a></p><p><img src="https://kvfrans.com/content/images/2021/06/Screen-Shot-2021-06-10-at-8.47.23-PM.png" alt></p><p>这个也是使用CLIP指导模型的生成，甚至无需训练，就可以生成简笔画，颇具有抽象派画风。</p><p>3.开集目标检测<a href="https://arxiv.org/abs/2104.13921" target="_blank" rel="noopener">ViLD</a></p><p><img src="https://camo.githubusercontent.com/238affd721b73ef2ff8f95114352188693623796bcc57b83f94747a7d2b6ff3a/68747470733a2f2f73746f726167652e676f6f676c65617069732e636f6d2f636c6f75642d7470752d636865636b706f696e74732f646574656374696f6e2f70726f6a656374732f76696c642f6173736574732f6e65775f7465617365722e706e67" alt></p><p>传统目标检测方法可能只能告诉你以上均是玩具类别，但是基于CLIP还可以知道玩具颜色，玩具种类。</p><p>4.文本视频检索<a href="https://github.com/johanmodin/clifs" target="_blank" rel="noopener">Clifs</a></p><p><img src="https://github.com/johanmodin/clifs/raw/master/media/odwalla.jpg" alt></p><p>CLIP还可以用于视频检索，如上图所示，告知“A truck with the text “odwalla””，可以基于文本内容检索到视频中对应帧的位置。</p><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><p>现有的模型都是基于固定的预定义物体类别集合进行监督训练。比如coco 80类等。但是这种限制性的监督型号限制了模型本身的泛化性，当需要识别新物体类别时需要重新收集新的数据重新训练，因此作者想到使用NLP里面文本信息获取监督信号。</p><h3 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h3><p>CLIP是一种基于对比学习的多模态模型，与CV中的一些对比学习方法如moco和simclr不同的是，CLIP的训练数据是文本-图像对：一张图像和它对应的文本描述，这里希望通过对比学习，模型能够学习到文本-图像对的匹配关系。如下图所示，CLIP包括两个模型：<strong>Text Encoder</strong>和<strong>Image Encoder</strong>，其中Text Encoder用来提取文本的特征，可以采用NLP中常用的text transformer模型；而Image Encoder用来提取图像的特征，可以采用常用CNN模型或者vision transformer。</p><p><img src="https://openaiassets.blob.core.windows.net/$web/clip/draft/20210104b/overview-a.svg" alt></p><p>这里对提取的文本特征和图像特征进行对比学习。对于一个包含N个文本-图像对的训练batch，将N个文本特征和N个图像特征两两组合，CLIP模型会预测出 $N^2$ 个可能的文本-图像对的相似度，这里的相似度直接<strong>计算文本特征和图像特征的余弦相似性（cosine similarity）</strong>，即上图所示的矩阵。这里共有N个正样本，即真正属于一对的文本和图像（矩阵中的对角线元素），而剩余的个$N^2-N$个文本-图像对为负样本，那么CLIP的训练目标就是最大个N正样本的相似度，同时最小化$N^2-N$个负样本的相似度，对应的伪代码实现如下所示：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># image_encoder - ResNet or Vision Transformer</span></span><br><span class="line"><span class="comment"># text_encoder - CBOW or Text Transformer</span></span><br><span class="line"><span class="comment"># I[n, h, w, c] - minibatch of aligned images</span></span><br><span class="line"><span class="comment"># T[n, l] - minibatch of aligned texts</span></span><br><span class="line"><span class="comment"># W_i[d_i, d_e] - learned proj of image to embed</span></span><br><span class="line"><span class="comment"># W_t[d_t, d_e] - learned proj of text to embed</span></span><br><span class="line"><span class="comment"># t - learned temperature parameter</span></span><br><span class="line"><span class="comment"># extract feature representations of each modality</span></span><br><span class="line">I_f = image_encoder(I) <span class="comment">#[n, d_i]</span></span><br><span class="line">T_f = text_encoder(T) <span class="comment">#[n, d_t]</span></span><br><span class="line"><span class="comment"># joint multimodal embedding [n, d_e]</span></span><br><span class="line">I_e = l2_normalize(np.dot(I_f, W_i), axis=<span class="number">1</span>)</span><br><span class="line">T_e = l2_normalize(np.dot(T_f, W_t), axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># scaled pairwise cosine similarities [n, n]</span></span><br><span class="line">logits = np.dot(I_e, T_e.T) * np.exp(t)</span><br><span class="line"><span class="comment"># symmetric loss function</span></span><br><span class="line">labels = np.arange(n)</span><br><span class="line">loss_i = cross_entropy_loss(logits, labels, axis=<span class="number">0</span>)</span><br><span class="line">loss_t = cross_entropy_loss(logits, labels, axis=<span class="number">1</span>)</span><br><span class="line">loss = (loss_i + loss_t)/<span class="number">2</span></span><br></pre></td></tr></table></figure><p>为了训练CLIP，OpenAI从互联网收集了4亿文本-图像对，论文称之为WebImageText。论文中Text Encoder固定选择一个包含63M参数的text transformer模型，而Image Encoder采用了两种的不同的架构，一是常用的CNN架构ResNet，二是基于transformer的ViT，其中ResNet包含5个不同大小的模型：ResNet50<strong>，</strong>ResNet101<strong>，</strong>RN50x4<strong>，</strong>RN50x16和RNx64（后面三个模型是按照EfficientNet缩放规则对ResNet分别增大4x，16x和64x得到），而ViT选择3个不同大小的模型：ViT-B/32<strong>，</strong>ViT-B/16<strong>和</strong>ViT-L/14。所有的模型都训练32个epoch，采用AdamW优化器，而且训练过程采用了一个较大的batch size：32768。由于数据量较大，最大的ResNet模型RN50x64需要在592个V100卡上训练18天，而最大ViT模型ViT-L/14需要在256张V100卡上训练12天，可见要训练CLIP需要耗费多大的资源。对于ViT-L/14，还在336的分辨率下额外finetune了一个epoch来增强性能，论文发现这个模型效果最好，记为ViT-L/14@336，论文中进行对比实验的CLIP模型也采用这个。</p><p>值得思考的是，在CLIP中，作者没有采用预测式的目标函数优化模型，而是采样对比学习的方式进行优化，在这一块，原文有介绍，使用对比学习的方式训练模型，放宽了约束，模型也更容易收敛，相比于预测型监督，对比式监督可以提升4倍的训练效率。</p><h3 id="使用CLIP实现zero-shot分类"><a href="#使用CLIP实现zero-shot分类" class="headerlink" title="使用CLIP实现zero-shot分类"></a>使用CLIP实现zero-shot分类</h3><p>当CLIP预训练好之后，有俩个编码器Text Encoder和Image Encoder，基于这俩训练好的encoder可以直接实现zero-shot分类，即无需任何训练数据，就能在某个下游任务上实现分类。具体步骤：</p><p><img src="https://openaiassets.blob.core.windows.net/$web/clip/draft/20210104b/overview-b.svg" alt></p><blockquote><ol><li>根据分类任务的分类标签构建每个类别的描述文本：A photo of {label}, 然后将这些文本送入Text Encoder获得对应的文本特征，如果类别数目为N，那么得到N个文本特征。</li><li>将要预测的图像送入Image Encoder得到图像特征，然后与N个文本特征计算余弦相似度，将相似度大的作为图像预测类别。</li></ol></blockquote><p>在zero-shot分类任务上，CLIP在多个数据集上取得优于在imagenet上fine的res101，具体可以看下图的结果对比：</p><p><img src="/img/zero-shot-cls.png" alt></p><p>在zero-shot任务上，作者在linear probe基础上做了详尽的实验，首先作者通过和经过强监督学习的Resnet-50提取的特征对比，任务都是分类任务，因此作者基于Resnet-50和CLIP提取出的特征，只是训练了最后的分类器，分类结果如下图所示。可以发现仅仅通过无监督的对比学习预训练得到的特征，即便是和强监督模型特征相比也是不分伯仲的。同时可以发现，zero-shot CLIP在一些动作识别任务中，比如Kinetics 700，UCF 101中有着比较大的提高，作者认为这可能是因为目前的文本描述中有很多以动词，动作为中心的句子导致的。</p><p><img src="/img/zero-shot-clip-res50.png" alt="image-20220606165016300"></p><h3 id="prompt-engineering"><a href="#prompt-engineering" class="headerlink" title="prompt engineering"></a>prompt engineering</h3><p>考虑到以单词作为标签存在歧义情况，比如在Oxford-IIIT Pet dataset 数据集中<code>boxer</code>表示斗牛犬，而在其他数据集中则可能表示拳击运动；在ImageNet中，<code>crane</code>同时表示了起重机和鹤。这种词语的多义显然对是因为缺少对标签的上下文描述导致的。为了解决这种问题，作者在指示上下文中添加了一些提示标签类型的词语，比如<code>A photo of a &lt;LABEL&gt;, a type of pet.</code>。作者将这个方法称之为“prompt engineering”。在合适地选取了不同的指示上下文，并且将其打分进行ensemble之后。作者发现这些Tricks竟能在zero-shot实验上提高5个绝对百分位，如Fig 2.3所示。</p><p><img src="/img/prompt-eng.png" alt></p><h3 id="limitation"><a href="#limitation" class="headerlink" title="limitation"></a>limitation</h3><p>这部分是最容易被忽略，但个人认为这部分往往更引人深思，这里简单的总结一下我关注的几个CLIP的limitation：</p><ol><li>CLIP在zero-shot上的性能虽然总体上比supervised baseline res50要好，但是在很多任务上是比不过sota，因此CLIP的迁移学习有待挖掘；</li><li>CLIP在一下几种task上的性能不好：细粒度分类，计数等任务；</li><li>CLIP本质上还是在有限的类别中进行对比推理，无法像image caption那样完全地灵活地生成新的概念，不同于生成模型；</li><li>CLIP依旧没有解决深度学习poor data efficiency的问题；</li></ol><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>CLIP可以再预训练阶段学习到更多通用的视觉语义信息，并且给下游任务提供帮助。而且相比于以往的训练方法，打破了之前的固定种类标签的范式。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CLIP是OpenAI在2021年发表的一种用NLP来监督CV的方法。成功连接文本和图像。CLIP全称是， Contrastive Language–Image Pre-training，一种基于对比文本-图像对的预训练方法。在了解CLIP具体方法之前，可以先看一下该工作的在一些下游任务的应用。&lt;br&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="multi-model" scheme="https://blog.nicehuster.cn/tags/multi-model/"/>
    
  </entry>
  
  <entry>
    <title>Real-ESRGAN详解</title>
    <link href="https://blog.nicehuster.cn/2022/05/12/real-esrgan/"/>
    <id>https://blog.nicehuster.cn/2022/05/12/real-esrgan/</id>
    <published>2022-05-12T11:13:39.000Z</published>
    <updated>2022-06-06T08:58:10.581Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong><a href="https://arxiv.org/pdf/2107.10833.pdf" target="_blank" rel="noopener">Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data</a><br><strong>代码链接：</strong> <a href="https://github.com/xinntao/Real-ESRGAN" target="_blank" rel="noopener">https://github.com/xinntao/Real-ESRGAN</a><br><strong>整体信息：</strong> Real-ESRGAN目前超分算法中比较热门应用较广的算法，在了解该算法前，可以先了解一下该方法的一个发展历程，SRCNN-&gt;SRGAN-&gt;ESRGAN-&gt;Real-ESRGAN。</p><a id="more"></a>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/pdf/2107.10833.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt; &lt;a href=&quot;https://github.com/xinntao/Real-ESRGAN&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/xinntao/Real-ESRGAN&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt; Real-ESRGAN目前超分算法中比较热门应用较广的算法，在了解该算法前，可以先了解一下该方法的一个发展历程，SRCNN-&amp;gt;SRGAN-&amp;gt;ESRGAN-&amp;gt;Real-ESRGAN。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="multi-model" scheme="https://blog.nicehuster.cn/tags/multi-model/"/>
    
  </entry>
  
  <entry>
    <title>Mask Transfiner详解</title>
    <link href="https://blog.nicehuster.cn/2022/05/08/mask-transfiner/"/>
    <id>https://blog.nicehuster.cn/2022/05/08/mask-transfiner/</id>
    <published>2022-05-08T11:13:39.000Z</published>
    <updated>2022-05-09T10:11:58.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong><a href="https://arxiv.org/abs/2111.13673" target="_blank" rel="noopener">Mask Transfiner for High-Quality Instance Segmentation</a><br><strong>代码链接：</strong><a href="https://github.com/SysCV/transfiner" target="_blank" rel="noopener">https://github.com/SysCV/transfiner</a><br><strong>整体信息：</strong>这是ETH和港科大合作发表在CVPR2022上有关实例分割的论文，该论文中提出的Mask Transfiner通过引入 Incoherent Regions检测机制的方式，在不产生额外计算成本的情况下，有效地改善目标分割mask。在COCO，Cityscapes和BDD100K上均取得了明显的性能提升。</p><p><img src="/img/mask-transfiner.png" alt="img"></p><a id="more"></a><h3 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h3><p><img src="/img/mask-transfiner-motivation.png" alt="img"></p><p>现有的实例分割方法在高频区域的分割依旧比较粗糙，边缘像素点存在大量错分的情况，要获取精细化的mask，需要大分辨率的深度特征图来保留物体细节，与此同时，大尺寸会带来高计算量和内存消耗，因此如何在不显著提高计算成本的条件下，获取精细化mask极具挑战。</p><p>大致做法是，如上图所示，首先识别容易出错并需要优化的pixel区域（黄色区域）,在论文里面称Incoherent Regions，这些区域往往出现在物体边界或者高频区域。只需要对这些pixel区域进行优化就行，优化的具体措施下面详细介绍。当然作者也有分析，实际预测错误的所有pixel中，有43%是分布在Incoherent Regions，作者将这部分区域的pixel直接替换成gt，coco mask ap可以提升倒51.</p><p><img src="/img/mask-transfiner-property-incoherent-region.png" alt="img"></p><h4 id="Incoherent-Regions定义"><a href="#Incoherent-Regions定义" class="headerlink" title="Incoherent Regions定义"></a>Incoherent Regions定义</h4><p>实例分割的边缘错误很多是空间分辨率过低产生的，例如对物体标注mask的下采样、过小的RoI池化、和基于PCA/DCT的系数压缩等。这些分割方法把低分辨率特征作为输入，由于丢失了高分辨率图上的物体细节，使准确地分割物体细节非常困难。而Incoherent Regions可以很好的计算mask由于分辨率下降而导致信息丢失的区域。</p><p><img src="/img/mask-transfiner-incoherent-region.png" alt="img"></p><h3 id="Mask-Transfiner框架"><a href="#Mask-Transfiner框架" class="headerlink" title="Mask Transfiner框架"></a>Mask Transfiner框架</h3><p><img src="/img/mask-transfiner-pipeline.png" alt="img"></p><h4 id="1-Incoherent-Regions检测"><a href="#1-Incoherent-Regions检测" class="headerlink" title="1. Incoherent Regions检测"></a>1. Incoherent Regions检测</h4><p>Incoherent Regions的检测遵循由低到高的级联设计（cascaded design）。如图上图右侧，为了检测RoI金字塔上的不同层级上信息损失节点，Transfiner先将最低层的RoI特征（28x28）和初始的物体mask预测作为输入，采用一个简单的全卷积网络（四个3×3 卷积）预测四叉树的根节点。每个根结点会分解到临近更高RoI层对应的4个子节点，例如从RoI大小28x28延伸到56x56。对于高层的RoI特征，Transfiner对上一层损失区域检测的mask做上采样后与RoI特征拼接，并使用单个1×1卷积层预测更精细的信息损失节点，以保持检测模块的轻量化。</p><h4 id="2-四叉树构造"><a href="#2-四叉树构造" class="headerlink" title="2. 四叉树构造"></a>2. 四叉树构造</h4><p>四叉树的结构如上图的 Point Quadtree部分所示，来自低层级的RoI特征（如分辨率28×28）的信息损失节点在其相邻的更高层的RoI中（如分辨率56×56）中有四个对应子节点。为了减少计算量，我们只将预测为损失节点的像素点向上层进一步分解，并把树的最大深度设为3。更为具体地，我们把从最低层级（28x28）检测到的信息损失点作为根节点，从上到下递归扩展四个子象限点，构建了一个多层次的四叉树。在更高层的特征图上选取子象限点，是因为大尺度特征具有更高的分辨率和更多的物体局部细节。</p><h4 id="3-节点编码"><a href="#3-节点编码" class="headerlink" title="3. 节点编码"></a>3. 节点编码</h4><p>节点编码器（Node Encoder）使用四种不同的信息线索对每个四叉树节点进行编码：</p><ol><li>从 FPN 金字塔的相应位置和层级提取的细粒度深度特征。</li><li>初始检测器的粗略掩码预测提供高层的语义信息。</li><li>相对位置编码，补充节点在RoI中的距离相关性。</li><li>每个节点的周围临近点信息来补充局部细节。</li></ol><h4 id="4-编码和像素解码"><a href="#4-编码和像素解码" class="headerlink" title="4. 编码和像素解码"></a>4. 编码和像素解码</h4><p>四叉树节点经编码器编码后，为了建模点之间的关联，序列编码器(Sequence Encoder)中的多头注意力模块会对输入序列进行点之间的特征融合及更新。相较于MLP，Transformer可以执行序列上的全局跨尺度推理。序列编码器的每一层都由多头自注意力模块和全连接的前馈网络（FFN）组成。为了给输入序列补充足够的前景和背景信息，我们还将RoI金字塔中最低层大小为14x14的196个特征点输入。与标准Transformer解码器不同，Transfiner 的像素解码器(Pixel Decoder)是一个简单的两层 MLP，不具有多头注意力模块。它对树中每个节点的输出查询进行解码，以预测最终的实例标签。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>1.在coco数据集上的SOTA结果对比</p><p><img src="/img/mask-transfiner-coco-exp.png" alt="img"></p><ol><li>可视化分析比较</li></ol><p><img src="/img/mask-transfiner-com.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.13673&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Mask Transfiner for High-Quality Instance Segmentation&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/SysCV/transfiner&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/SysCV/transfiner&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这是ETH和港科大合作发表在CVPR2022上有关实例分割的论文，该论文中提出的Mask Transfiner通过引入 Incoherent Regions检测机制的方式，在不产生额外计算成本的情况下，有效地改善目标分割mask。在COCO，Cityscapes和BDD100K上均取得了明显的性能提升。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/mask-transfiner.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="instance segmentation" scheme="https://blog.nicehuster.cn/tags/instance-segmentation/"/>
    
  </entry>
  
  <entry>
    <title>增量目标检测方法 Faster ILOD</title>
    <link href="https://blog.nicehuster.cn/2022/03/24/Faster_ILOD/"/>
    <id>https://blog.nicehuster.cn/2022/03/24/Faster_ILOD/</id>
    <published>2022-03-24T11:13:39.000Z</published>
    <updated>2022-04-13T04:30:08.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/abs/2003.03901" target="_blank" rel="noopener">Faster ILOD: Incremental Learning for Object Detectors based on Faster RCNN</a><br><strong>代码链接：</strong><a href="https://github.com/CanPeng123/Faster-ILOD" target="_blank" rel="noopener">https://github.com/CanPeng123/Faster-ILOD</a><br><strong>整体信息：</strong>这是发表在PRL2020上的一篇文章关于增量目标检测的文章，作者是来自The University of Queensland，这篇文章基于Faster RCNN，使用multi-network 自适应蒸馏，设计了一种end2end的增量目标检测方法。</p><a id="more"></a><h2 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h2><p>增量目标检测包含S个增量steps，在每个增量step，训练数据只包含新类别Cn ，给定一个在旧类别 C0 上已训练好的目标检测模型，增量目标检测的任务是在新类别 Cn 数据上重新训练模型(一般是fine-tune)，同时维持模型在旧类别 C0 上的性能，在增量训练过程中，旧类别 C0 不可见。</p><p><img src="/img/faster_ilod_1.png" alt="img"></p><p>上图展示的是一个在VOC数据集上的增量目标检测的示例，模型首先在前15类训练，然后逐步增加每个类别。增量训练过程中，只提供当前新类别的标注，其他类别不可见。Normal Training是使用所有数据（旧数据和新数据）从头开始重新训练模型，该模型在测试集上的指标即为增量目标检测的指标上界。 Catastrophic forgetting是使用已训练好的旧类模型直接在新类数据上fine-tune的结果，可以看到在不断进行增量训练时，总体指标在逐渐下降，即出现了灾难性遗忘的问题( Catastrophic forgetting)。</p><h2 id="本文方法"><a href="#本文方法" class="headerlink" title="本文方法"></a>本文方法</h2><p><img src="/img/faster_ilod_method.png" alt="img"></p><p>作者使用Multi-network自适应蒸馏的方法来解决增量目标检测中出现的Catastrophic forgetting问题。具体做法如上图所示，上面是旧模型即teacher模型T，下面是新模型即student模型S，中间为蒸馏过程。</p><p>（1）<strong>特征蒸馏</strong>，所有feature map进行减均值归一化，然后使用L1 loss进行蒸馏。具体地，针对特征图上每个激活值大小进行比较，确定是否蒸馏，如下公式所示，</p><script type="math/tex; mode=display">\mathcal{L}_{F_{-} D i s t}=\frac{1}{\mathcal{M}} \sum \begin{cases}\left\|\tilde{f}_{t e}-\tilde{f}_{s t}\right\|_{1}, & \text { if } \tilde{f}_{t e}>\tilde{f}_{s t} \\ 0, & \text { otherwise }\end{cases}</script><p>（2）<strong>RPN蒸馏</strong>，同样地，使用T模型RPN的输出作为下界进行自适应蒸馏，使用的是L2 loss。</p><script type="math/tex; mode=display">\begin{aligned} &\mathcal{L}_{R P N_{-} D i s t}=\frac{1}{\mathcal{N}} \sum \begin{cases}\left\|q_{t e}-q_{s t}\right\|_{2}^{2}+\beta\left\|r_{t e}-r_{s t}\right\|_{2}^{2}, & \text { if } q_{t e}>q_{s t} \\ 0, & \text { otherwise }\end{cases} \\ &\text { where } \\ &\beta= \begin{cases}1, & \text { if } q_{t e}>\left(q_{s t}+\mathcal{T}\right) \\ 0, & \text { otherwise. }\end{cases} \end{aligned}</script><p>（3）<strong>RCNN蒸馏</strong>，具体做法和ILOD做法一致，在T模型中背景分数最小的128个ROI中随机选择64个proposal进行蒸馏，即只蒸馏背景信息，新类别不参与RCNN蒸馏，具体地，如下公式所示，</p><script type="math/tex; mode=display">\mathcal{L}_{R C N_{-} D i s t}=\frac{1}{\mathcal{K} \times C_{o}} \sum\left[\left\|\tilde{p}_{t e}-\tilde{p}_{s t}\right\|_{2}^{2}+\left\|t_{t e}-t_{s t}\right\|_{2}^{2}\right]</script><p>最后，总的loss为三者相加，</p><script type="math/tex; mode=display">\mathcal{L}_{\text {total }}=\mathcal{L}_{R C N N}+\lambda_{1} \mathcal{L}_{F_{-} \text {Dist }}+\lambda_{2} \mathcal{L}_{R P N_{-} \text {Dist }}+\lambda_{3} \mathcal{L}_{R C N_{-} \text {Dist }}</script><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><ul><li>数据集(指标)：PASCAL VOC(AP@0.5)，COCO(mAP)</li><li>settings：one-step 和 multi-step</li><li><p>对比方法，ILOD</p></li><li><p>对比方法，ILOD</p></li></ul><p><img src="/img/faster_ilod_exp1.png" alt="img"></p><p><img src="/img/faster_ilod_exp2.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2003.03901&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Faster ILOD: Incremental Learning for Object Detectors based on Faster RCNN&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/CanPeng123/Faster-ILOD&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/CanPeng123/Faster-ILOD&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这是发表在PRL2020上的一篇文章关于增量目标检测的文章，作者是来自The University of Queensland，这篇文章基于Faster RCNN，使用multi-network 自适应蒸馏，设计了一种end2end的增量目标检测方法。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="Continual learning" scheme="https://blog.nicehuster.cn/tags/Continual-learning/"/>
    
  </entry>
  
  <entry>
    <title>增量目标检测方法调研</title>
    <link href="https://blog.nicehuster.cn/2022/03/21/IOD/"/>
    <id>https://blog.nicehuster.cn/2022/03/21/IOD/</id>
    <published>2022-03-21T11:13:39.000Z</published>
    <updated>2022-04-12T11:36:16.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近参加了CVPR2022 有关增量学习的一个Workshop：<a href="https://sites.google.com/view/clvision2022/overview" target="_blank" rel="noopener">Workshop on Continual Learning in Computer Vision</a>，这个workshop有三个赛道：(1)Instance Classification Track，(2)Category Detection Track；(3)Instance Detection Track，三个赛道的任务介绍可以看官网。个人是做检测方向出身，因此关注了一下track2，并基于增量目标检测方向做了部分调研，并简单介绍其中部分算法。</p><a id="more"></a><h3 id="增量目标检测方法调研"><a href="#增量目标检测方法调研" class="headerlink" title="增量目标检测方法调研"></a>增量目标检测方法调研</h3><p><img src="/img/iod_survey.png" alt="img"></p><h3 id="部分方法介绍"><a href="#部分方法介绍" class="headerlink" title="部分方法介绍"></a>部分方法介绍</h3><p><img src="/img/IOD_1.PNG" alt="IOD (1)"></p><p><img src="/img/IOD_2.PNG" alt="IOD (2)"></p><p><img src="/img/IOD_3.PNG" alt="IOD (3)"></p><p><img src="/img/IOD_4.PNG" alt="IOD (4)"></p><p><img src="/img/IOD_5.PNG" alt="IOD (5)"></p><p><img src="/img/IOD_6.PNG" alt="IOD (6)"></p><p><img src="/img/IOD_7.PNG" alt="IOD (7)"></p><p><img src="/img/IOD_8.PNG" alt="IOD (8)"></p><p><img src="/img/IOD_9.PNG" alt="IOD (9)"></p><p><img src="/img/IOD_10.PNG" alt="IOD (10)"></p><p><img src="/img/IOD_11.PNG" alt="IOD (11)"></p><p><img src="/img/IOD_12.PNG" alt="IOD (12)"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近参加了CVPR2022 有关增量学习的一个Workshop：&lt;a href=&quot;https://sites.google.com/view/clvision2022/overview&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Workshop on Continual Learning in Computer Vision&lt;/a&gt;，这个workshop有三个赛道：(1)Instance Classification Track，(2)Category Detection Track；(3)Instance Detection Track，三个赛道的任务介绍可以看官网。个人是做检测方向出身，因此关注了一下track2，并基于增量目标检测方向做了部分调研，并简单介绍其中部分算法。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="Continual learning" scheme="https://blog.nicehuster.cn/tags/Continual-learning/"/>
    
  </entry>
  
  <entry>
    <title>Panoptic Segmentation详解</title>
    <link href="https://blog.nicehuster.cn/2022/01/07/PSeg/"/>
    <id>https://blog.nicehuster.cn/2022/01/07/PSeg/</id>
    <published>2022-01-07T11:13:39.000Z</published>
    <updated>2022-04-12T04:35:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近在知乎上频繁地刷到有关Mask2Former地帖子，这么多人吹捧地必是精品，就跟一波风看了一下Mask2Former，顺带地也了解一下Panoptic Segmentation这个任务。众所周知，图像分割主要有两个方向：</p><ul><li><strong>语义分割（semantic segmentation）</strong>，常用来识别天空、草地、道路等没有固定形状的不可数<strong>事物（stuff）</strong>。语义分割的标记方法通常是给每个像素加上标签。</li><li><strong>实例分割（instance segmentation）</strong>，人、动物或工具等可数的、独立的明显物体<strong>（things）</strong>。实例分割通常用包围盒或分割掩码标记目标。</li></ul><p><strong>全景分割（Panoptic Segmentation）</strong>其实就是把这两个方向结合起来，生成统一的、全局的分割图像，既识别事物，也识别物体。</p><p><img src="/img/ps.png" alt="img"></p><a id="more"></a><p><strong>标记方法</strong></p><p>全景分割的标记方法结合了语义分割和实例分割，给每个像素加上标签$\left(l_{i}, z_{i}\right)$,其中i表示第i个像素，l表示语义类别，z表示实例ID。语义类别由两部分组成，事物类别$L^{ST}$和$L^{TH}$分别为stuff和thing的简写）。当$l_{i} \in L^{S T}$，忽略$z_i$（事物类别）；</p><p><strong>评估标准</strong></p><p>首先是常规的IoU &gt; 0.5，然后结合TF、FN、FP搞出了一个PQ标准。（PQ是Panoptic Quality，即全景质量的简称。)</p><p><img src="/img/metric.png" alt="img"></p><p>PQ的具体公式为：</p><script type="math/tex; mode=display">\mathrm{PQ}=\frac{\sum_{(p, g) \in T P} \operatorname{IoU}(p, g)}{|T P|+\frac{1}{2}|F P|+\frac{1}{2}|F N|}</script><p>另外，PQ可以分解为<strong>分割质量（segmentation quality，SQ）</strong>和<strong>识别质量（recognition quality，RQ）</strong>的乘积，便于进一步评估分割和识别环节的表现。</p><script type="math/tex; mode=display">\mathrm{PQ}=\underbrace{\frac{\sum_{(p, g) \in T P} \operatorname{IoU}(p, g)}{|T P|}}_{\text {segmentation quality }(\mathrm{SQ})} \times \underbrace{\frac{|T P|}{|T P|+\frac{1}{2}|F P|+\frac{1}{2}|F N|}}_{\text {recognition quality }(\mathrm{RQ})}</script><p><strong>数据集</strong></p><p>全景分割数据集需要既有语义分割标注，也有实例分割标注。</p><ul><li>Cityscapes(19classes)：5000张街景图片，97%的图片有像素标注，共有19个类别，其中8个类别符合语义分割的特征；</li><li>ADE20k(150classes)：图像总量超过25000张，并经过公开标注。其中包括100种物体和59种事物。</li><li>Mapillary Vistas(65classes)：25000张分辨率不同的街景照片。其中98%的图片都经过了像素标注，涵盖28种事物与37种物体。</li><li>COCO：知名数据集COCO最近加入了全景分割标注。</li></ul><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h4 id="1-MaskFormer"><a href="#1-MaskFormer" class="headerlink" title="1.MaskFormer"></a>1.<strong>MaskFormer</strong></h4><p>Per-Pixel Classification is Not All You Need for Semantic Segmentation</p><p>篇文章提出了一个新的分割输出端范式，传统方法会将每个像素点预测成一种类别来完成分割任务，而本文则是会输出很多个二分类分割图，如下图所示：</p><p><img src="/img/mask_cls.png" alt="img"></p><p>上图左侧为传统方法，右侧为本文方法，不是只输出 K 个类别二值分类图，而是提前设定一个较大值，同时与分类图一同预测的还有一个预测类别，这个类别可以为空，即该二值分类图没有用。因此损失函数由两个组成，一个是预测分类图与真值图的损失，另一个是预测类别的交叉熵损失.</p><script type="math/tex; mode=display">\mathcal{L}_{\text {mask-cls }}\left(z, z^{\mathrm{gt}}\right)=\sum_{j=1}^{N}\left[-\log p_{\sigma(j)}\left(c_{j}^{\mathrm{gt}}\right)+\mathbb{1}_{c_{j}^{\mathrm{gt}} \neq \varnothing} \mathcal{L}_{\text {mask }}\left(m_{\sigma(j)}, m_{j}^{\mathrm{gt}}\right)\right]</script><p>匹配策略上依旧采用匈牙利匹配，匹配成本：</p><script type="math/tex; mode=display">-p_{i}\left(c_{j}^{\mathrm{gt}}\right)+\mathcal{L}_{\text {mask }}\left(m_{i}, m_{j}^{\mathrm{gt}}\right)</script><p>其中，$L_{mask}$为二类交叉熵损失。MaskFormer具体结构如下：</p><p><img src="/img/maskformer.png" alt="img"></p><h4 id="2-Mask2Former"><a href="#2-Mask2Former" class="headerlink" title="2.Mask2Former"></a>2.Mask2Former</h4><p>Masked-attention Mask Transformer for Universal Image Segmentation</p><p>一个model去建模所有的分割任务：语义分割，实例分割和以及全景分割。一个模型取得三个不同分割任务STOA.。具体而言本文方法沿用了上一篇的分割图分别产生方式，另外有两个主要创新点，包括每个transformer decoder层都会使用pixel-decoder的金字塔对应结构，以及 Mask Attention 形式。可参考下图：</p><p><img src="/img/mask2former.png" alt="img"></p><p>因此，作者基于上述的问题和最初的motivation(一个模型取得三个不同分割任务STOA)，提出了几个改进，节省训练的时间(MaskFormer训练需要300epoch)和cost的同时能够提升性能。</p><p><strong>第一个改进</strong>：Mask Attention加速收敛,相比于之前的cross attention，这个里面的attention affinity是一种稀疏的attention，其实就是将上一层预测的分割图使用阈值0.5转换成[0,1]mask图，将转换后的mask进一步转换成[-inf,0]，然后和原始att相加，过softmax得到att_mask，相当于不计算原始分割图中为0的区域att.</p><p>standard cross-attention:  $\mathbf{X}_{l}=\operatorname{softmax}\left(\mathbf{Q}_{l} \mathbf{K}_{l}^{\mathrm{T}}\right) \mathbf{V}_{l}+\mathbf{X}_{l-1}$</p><p> masked cross-attention: </p><script type="math/tex; mode=display">\mathbf{X}_{l}=\operatorname{softmax}\left(\mathcal{M}_{l-1}+\mathbf{Q}_{l} \mathbf{K}_{l}^{\mathrm{T}}\right) \mathbf{V}_{l}+\mathbf{X}_{l-1}</script><script type="math/tex; mode=display">\mathcal{M}_{l-1}(x, y)=\left\{\begin{array}{ll}0 & \text { if } \mathbf{M}_{l-1}(x, y)=1 \\-\infty & \text { otherwise }\end{array} .\right.</script><p>具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiScaleMaskedTransformerDecoder</span><span class="params">(nn.Module)</span>:</span> </span><br><span class="line">       ...</span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask_features, mask = None)</span>:</span></span><br><span class="line">       ...</span><br><span class="line">       outputs_class, outputs_mask, attn_mask = self.forward_prediction_heads(output, mask_features, attn_mask_target_size=size_list[<span class="number">0</span>])</span><br><span class="line">       <span class="comment">#print(outputs_class.shape,outputs_mask.shape,attn_mask.shape)</span></span><br><span class="line">       predictions_class.append(outputs_class)</span><br><span class="line">       predictions_mask.append(outputs_mask)</span><br><span class="line"></span><br><span class="line">       <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_layers):</span><br><span class="line">           level_index = i % self.num_feature_levels</span><br><span class="line">           attn_mask[torch.where(attn_mask.sum(<span class="number">-1</span>) == attn_mask.shape[<span class="number">-1</span>])] = <span class="keyword">False</span></span><br><span class="line">           <span class="comment"># attention: cross-attention first</span></span><br><span class="line">           output = self.transformer_cross_attention_layers[i](</span><br><span class="line">               output, src[level_index],</span><br><span class="line">               memory_mask=attn_mask,</span><br><span class="line">               memory_key_padding_mask=<span class="keyword">None</span>,  <span class="comment"># here we do not apply masking on padded region</span></span><br><span class="line">               pos=pos[level_index], query_pos=query_embed</span><br><span class="line">           )</span><br><span class="line"></span><br><span class="line">           output = self.transformer_self_attention_layers[i](</span><br><span class="line">               output, tgt_mask=<span class="keyword">None</span>,</span><br><span class="line">               tgt_key_padding_mask=<span class="keyword">None</span>,</span><br><span class="line">               query_pos=query_embed</span><br><span class="line">           )</span><br><span class="line"></span><br><span class="line">           <span class="comment"># FFN</span></span><br><span class="line">           output = self.transformer_ffn_layers[i](</span><br><span class="line">               output</span><br><span class="line">           )</span><br><span class="line"></span><br><span class="line">           outputs_class, outputs_mask, attn_mask = self.forward_prediction_heads(output, mask_features, attn_mask_target_size=size_list[(i + <span class="number">1</span>) % self.num_feature_levels])</span><br><span class="line">           predictions_class.append(outputs_class)</span><br><span class="line">           predictions_mask.append(outputs_mask)</span><br><span class="line">           ...</span><br><span class="line">           </span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">forward_prediction_heads</span><span class="params">(self, output, mask_features, attn_mask_target_size)</span>:</span></span><br><span class="line">       decoder_output = self.decoder_norm(output)</span><br><span class="line">       decoder_output = decoder_output.transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">       outputs_class = self.class_embed(decoder_output)</span><br><span class="line">       mask_embed = self.mask_embed(decoder_output)</span><br><span class="line">       outputs_mask = torch.einsum(<span class="string">"bqc,bchw-&gt;bqhw"</span>, mask_embed, mask_features)</span><br><span class="line"></span><br><span class="line">       <span class="comment"># <span class="doctag">NOTE:</span> prediction is of higher-resolution</span></span><br><span class="line">       <span class="comment"># [B, Q, H, W] -&gt; [B, Q, H*W] -&gt; [B, h, Q, H*W] -&gt; [B*h, Q, HW]</span></span><br><span class="line">       attn_mask = F.interpolate(outputs_mask, size=attn_mask_target_size, mode=<span class="string">"bilinear"</span>, align_corners=<span class="keyword">False</span>)</span><br><span class="line">       <span class="comment">#print(outputs_mask.shape,attn_mask.shape)</span></span><br><span class="line">       <span class="comment"># must use bool type</span></span><br><span class="line">       <span class="comment"># If a BoolTensor is provided, positions with ``True`` are not allowed to attend while ``False`` values will be unchanged.</span></span><br><span class="line">       attn_mask = (attn_mask.sigmoid().flatten(<span class="number">2</span>).unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, self.num_heads, <span class="number">1</span>, <span class="number">1</span>).flatten(<span class="number">0</span>, <span class="number">1</span>) &lt; <span class="number">0.5</span>).bool()</span><br><span class="line">       attn_mask = attn_mask.detach()</span><br><span class="line"></span><br><span class="line">       <span class="keyword">return</span> outputs_class, outputs_mask, attn_mask</span><br></pre></td></tr></table></figure><p><strong>第二个改进是多尺度特征改善小目标分割</strong>，对应于pixel-decoder，作者使用了类似于Deformable DETR decoder端的设置，在decoder端采用了multi scale的特征输入做attention。这个步骤对于提升small object的segmentation帮助很大。具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MSDeformAttnPixelDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_features</span><span class="params">(self, features)</span>:</span> <span class="comment">#[res3,res4,res5]</span></span><br><span class="line">        srcs = []</span><br><span class="line">        pos = []</span><br><span class="line">        <span class="comment"># Reverse feature maps into top-down order (from low to high resolution)</span></span><br><span class="line">        <span class="keyword">for</span> idx, f <span class="keyword">in</span> enumerate(self.transformer_in_features[::<span class="number">-1</span>]):</span><br><span class="line">            x = features[f].float()  <span class="comment"># deformable detr does not support half precision</span></span><br><span class="line">            srcs.append(self.input_proj[idx](x))</span><br><span class="line">            pos.append(self.pe_layer(x))</span><br><span class="line"></span><br><span class="line">        y, spatial_shapes, level_start_index = self.transformer(srcs, pos) <span class="comment">#MSDeformAttnTransformerEncoderOnly</span></span><br><span class="line"></span><br><span class="line">        bs = y.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        split_size_or_sections = [<span class="keyword">None</span>] * self.transformer_num_feature_levels</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.transformer_num_feature_levels):</span><br><span class="line">            <span class="keyword">if</span> i &lt; self.transformer_num_feature_levels - <span class="number">1</span>:</span><br><span class="line">                split_size_or_sections[i] = level_start_index[i + <span class="number">1</span>] - level_start_index[i]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                split_size_or_sections[i] = y.shape[<span class="number">1</span>] - level_start_index[i]</span><br><span class="line">        y = torch.split(y, split_size_or_sections, dim=<span class="number">1</span>) [x3,x4,x5]</span><br><span class="line">        </span><br><span class="line">        out = []</span><br><span class="line">        multi_scale_features = []</span><br><span class="line">        num_cur_levels = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, z <span class="keyword">in</span> enumerate(y):</span><br><span class="line">            zz = z.transpose(<span class="number">1</span>, <span class="number">2</span>).view(bs, <span class="number">-1</span>, spatial_shapes[i][<span class="number">0</span>], spatial_shapes[i][<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">            out.append(zz)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># append `out` with extra FPN levels</span></span><br><span class="line">        <span class="comment"># Reverse feature maps into top-down order (from low to high resolution)</span></span><br><span class="line">        <span class="keyword">for</span> idx, f <span class="keyword">in</span> enumerate(self.in_features[:self.num_fpn_levels][::<span class="number">-1</span>]):</span><br><span class="line"></span><br><span class="line">            x = features[f].float()</span><br><span class="line"></span><br><span class="line">            lateral_conv = self.lateral_convs[idx]</span><br><span class="line">            output_conv = self.output_convs[idx]</span><br><span class="line">            cur_fpn = lateral_conv(x)</span><br><span class="line">            <span class="comment"># Following FPN implementation, we use nearest upsampling here</span></span><br><span class="line">            y = cur_fpn + F.interpolate(out[<span class="number">-1</span>], size=cur_fpn.shape[<span class="number">-2</span>:], mode=<span class="string">"bilinear"</span>, align_corners=<span class="keyword">False</span>)</span><br><span class="line">            y = output_conv(y)</span><br><span class="line">            out.append(y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> o <span class="keyword">in</span> out:</span><br><span class="line">            <span class="keyword">if</span> num_cur_levels &lt; self.maskformer_num_feature_levels:</span><br><span class="line">                multi_scale_features.append(o)</span><br><span class="line">                num_cur_levels += <span class="number">1</span>      </span><br><span class="line">        <span class="keyword">return</span> self.mask_features(out[<span class="number">-1</span>]), out[<span class="number">0</span>], multi_scale_features</span><br></pre></td></tr></table></figure><p>此外，文章还额外提出了三个小改进：</p><ul><li>将 Self 和 Cross Attention 的顺序做一个变换，让 Cross 在前面，因为在图像特征没加入计算时自身做 Self 效率会较低；</li><li>文章将 Transformer 解码器的初始序列设置为可学版本；</li><li>整个模型将不使用 dropout 操作。</li><li>使用point-rend head 改善分割边界质量，同时降低显存；</li></ul><p>mask2former/modeling/meta_arch/mask_former_head.py </p><p><strong>backbone:res50</strong></p><p>800x800(short size=800) —&gt;res2[1, 256, 200, 200],res3[1, 512, 100, 100],res4[1, 1024, 50, 50],res5[1, 2048, 25, 25],</p><p><strong>pixel decoder:MSDeformAttnPixelDecoder</strong>(6 layers)</p><p> res3,res4,res5—&gt;MSDeformAttnTransformerEncoderOnly—&gt;x1[1, 256, 25, 25],x2[1, 256, 50, 50],x3[1, 256, 100, 100],</p><p>x1,x2,x3+FPN(res2) —-&gt;x1[1, 256, 25, 25],x2[1, 256, 50, 50],x3[1, 256, 100, 100],x4[1, 256, 200, 200]</p><p> res3,res4,res5—&gt;  <strong>mask_features</strong>:conv[x4],<strong>transformer_encoder_features</strong>:x4,<strong>multi_scale_features</strong>:[x1,x2,x3]</p><p><strong>Transformer decoder:MultiScaleMaskedTransformerDecoder</strong></p><p>input：multi_scale_features,mask_features </p><p>query_feat(100x256)，mask_features[1, 256, 200, 200]  —&gt;forward_prediction_heads—&gt;outputs_class[1, 100, 134],outputs_mask[1, 100, 200, 200],attn_mask[8, 100, 625]，attn_mask是有outputs_mask插值降采样得到</p><h4 id="3-PointRend"><a href="#3-PointRend" class="headerlink" title="3.PointRend"></a>3.PointRend</h4><p>文中使用了各种术语，比如rend、subdivision、ray-tracing，是想说明一个问题: 图像是对真实目标的一个离散化表达，真实目标的一些属性，如区域联通性、边缘连续性，在图像中同样存在。那么分割问题就可以看作预测一个真实目标在离散化后的图像中所占的区域，即：point-wise label prediction（点对点分类）。连续性体现在：图像中的像素可以通过插值得到与真实目标一一对应的坐标点。分割是在离散化的网格区域点对点的分类，但是有些点很难分类的准确，这些点大部分处在目标的边缘。pointrend方法的提出是对这些模糊分割的点，做更进一步的预测，即：精细分割。主要分成3步，如下图所示：1）候选点（或模糊分割点）选取；2）点特征提取；3）点分割预测。</p><p><img src="/img/pointRend.png" alt="img"></p><h5 id="1-候选点选取"><a href="#1-候选点选取" class="headerlink" title="1.候选点选取"></a><strong>1.候选点选取</strong></h5><p>候选点选取在训练和测试过程是不一样的，其中推断过程是通过迭代的方式从一个低分辨率的分割图像得到一个高分辨率的图像，这种方式不适合训练过程中的梯度反传，故采用另一种方式。</p><p><strong>Point select for training</strong></p><ul><li>随机生成kN个点，其中k&gt;1。</li><li>估计这kN个点的不确定程度，并根据不确定程度，筛选前βN个点，β取值范围为[0, 1]。其中不确定程度的估计方式与推断过程估计方式相同，使用低分辨率的分割置信度。</li><li>在剩余的点中，均匀采用得到（1-β）N个点。</li></ul><p><img src="/img/point_select_train.png" alt="img"></p><p>在PointRend中，采用了k=3和$\beta=0.75$的采样策略参数，采样得到$14^{2}$个点；从 coarse prediction 中插值的 GT 类别的概率和 0.5 之间的距离作为 point-wise 不确定性度量.</p><p><strong>Point select for inference</strong></p><p>推断过程是迭代进行的，具体过程如下图所示，首先通过上采样，将模型直接输出的低分辨率的分割图的长宽各扩大2倍得到高分辨的分割图，如从4x4的特征图上采样得到8x8的特征图；然后在高分辨分割图中，筛选N个分割模糊的点，即分割置信度（若置信度区间为[0,1]）在0.5左右的点，如在下图8x8的高分辨率分割图中，点集中在边缘附近。这N个点为最终筛选出来进行再次确认的点。以此类推，逐步迭代，得到最终目标分辨率的分割图。</p><p><img src="/img/subdivision.png" alt="img"></p><p>在PointRend中，对于预测类别 c 的矩形框，除非特别说明，均采用 adaptive subdivision 来通过 5 steps 将 7x7 coarse 预测精细化到 224x224. 每一次迭代，选择并更新 $N=28^2$个基于预测值和 0.5 之间的差异性距离得到最不确定的点.</p><h5 id="2-点特征提取"><a href="#2-点特征提取" class="headerlink" title="2.点特征提取"></a><strong>2.点特征提取</strong></h5><p>点特征提取包括fine-grained特征和coarse特征，其中coarse为K维，来自低分辨的分割mask，fine-grained特征来自原cnn backbone某个stage或者多个stage的组合特征。文中使用P2层的卷积特征作为fine-grained特征提取的特征图，具体流程如下图。</p><p><img src="/img/pointRend-ppl.png" alt="img"></p><h5 id="3-点分割预测"><a href="#3-点分割预测" class="headerlink" title="3. 点分割预测"></a>3. 点分割预测</h5><p>如上图所示紫色箭头，得到候选点的特征表达后，经过一组MLP，来得到最后的N个点的分割预测结果。其中，在实验中使用了4个全连接层，其中<strong>每个全连接层的输出，都联合coarse feature</strong>，作为下一个全连接层的输入。</p><h5 id="5-Results"><a href="#5-Results" class="headerlink" title="5. Results"></a>5. Results</h5><p><img src="/img/pointRend-res.png" alt="img"></p><p>其中左边一列图为mask rcnn的结果，右边一列图为PointRend的结果，从视觉效果看，PointRend对物体边缘描述更加精细化。</p><h4 id="4-Panoptic-SegFormer"><a href="#4-Panoptic-SegFormer" class="headerlink" title="4.Panoptic SegFormer"></a>4.Panoptic SegFormer</h4><p> Delving Deeper into Panoptic Segmentation with Transformers</p><p><img src="/img/PSFormer-ppl.png" alt="img"></p><p>该结构与DETR类似，不同之处在于：</p><p>（1）backbone使用多尺度特征(C3,C4,C5);</p><p>（2）针对decoder中query做了进一步精细划分,解耦location和Mask。针对Thing Query使用location Decoder捕获thing类别位置信息对Thing Query进行refine；此后使用refine后的Thing Query和Stuff Query作为Mask Decoder 输出mask结果。其中location Decoders使用bbox信息辅助监督，可以加速网络收敛；</p><p>（3）后处理部分，使用Mask-wise merge策略融合things和stuff获取最终的mask结果.</p><p>下面详细讲一下location decoder、mask decoder和mask-wise merge部分。</p><p><strong>Location Decoder</strong></p><p>给定N个初始化queries，训练阶段，在location decoder后面添加一个辅助MLP来预测位置和尺寸，location decoder的输出称为location-aware queries；推理阶段，去除辅助MLP。这一个辅助loss，可以帮助网络快速收敛，每个query关注区域指向性更明确。</p><p><strong>Mask Decoder</strong></p><p>mask decoder将location decoder的输出location-wise queries当作query，和MaskFormer预测mask和类别不同的是，Panoptic SegFormer预测mask需要先将attention map拆分成A3，A4，A5，然后都上采样到H/8xW/8的分辨率，concat在一起得到A_fuse，最后通过1x1卷积得到mask预测结果。</p><p><strong>Mask-wise merge</strong></p><p><img src="/img/mask-wise-merge.png" alt="img"></p><p>之前的分割去重，一般都是使用pixel-wise argmax策略，也就是重叠部分保留预测分数最大的类别。本文提出的mask-wise merge策略，对于重叠部分进行舍弃，上图是伪代码。</p><p>很喜欢作者在conclusion里面提到的一句话， Given the similarities and differences among the various segmentation tasks, “seek common ground while reserving differences” is a more reasonable guiding ideology.  完全的统一框架不见得是最好的选择，“求同存异”才是一个更合理的指导思想。</p><p>代码链接：<a href="https://github.com/zhiqi-li/Panoptic-SegFormer" target="_blank" rel="noopener">https://github.com/zhiqi-li/Panoptic-SegFormer</a></p><p>结果复现：</p><div class="table-container"><table><thead><tr><th>Method</th><th>PQ</th><th>SQ</th><th>RQ</th><th>N</th></tr></thead><tbody><tr><td>All(paper)</td><td>49.600</td><td>81.600</td><td>59.900</td><td>133</td></tr><tr><td>All(rep)</td><td>49.900</td><td>81.500</td><td>60.200</td><td>133</td></tr></tbody></table></div><h4 id="5-K-Net"><a href="#5-K-Net" class="headerlink" title="5. K-Net"></a>5. K-Net</h4><p>K-Net: Towards Unified Image Segmentation</p><p><img src="/img/knet-simple.png" alt="img"></p><p>K-Net的目标也是在于统一实例分割和语义分割。如上图所示，语义分割核心结构就是由一组kernel来负责语义mask的生成，让kernel数量与数据集类别数据保持一致，每个kernel负责一个固定类别masker的生成。受此启发，在实例分割中，可以通过同样方式引入一组卷积核来负责 mask 的生成，限定一个 kernel 只分割一个物体，每个kernel负责分割不同的物体，实例分割任务统一到一个框架内。</p><p><strong>Group-Aware Kernels</strong></p><p>理论上一组instance kernel就可以得到实例分割结果，但实验结果却相差甚远，与sem seg 相比，ins seg需要的kernel要求更高：</p><p>(1)在sem seg中，每个单独的sem kernel与类别(sem class)是绑定的，在每张图上都可以学习分割同一个类别，而ins seg不具备，而是通过Bipartite matching 来做的 target assignment,这导致每个kernel在每张图上学习的目标是根据当前的预测情况动态分配的。</p><p>(2)ins kernel需要区分appearence和scale变化的物体，需要具备更强的判别特性；</p><p>基于此，作者设计了<strong>Kernel Update Head</strong> 基于 mask 和特征图来将 kernel 动态化；如下图所示，Kernel Update Head 首先获得每个 kernel 对应 pixel group 的 feature，然后以某种方式动态地更新当前kernel。</p><p><img src="/img/kernel-update-head.png" alt="img"></p><p>此外，为了使得kernel可以modeling全局信息，作者还新增kernel interaction模块，最终得到的特征可用于class prediction, dynamic kernels 和mask predictions. </p><p><img src="/img/knet-ppl.png" alt="img"></p><p>为了得到更精细化的mask，可以通过叠加多个Kernel Update Head对mask和kernel进行迭代式refine.最终K-Net pipeline如上图所示，在论文中使用了3个Kernel Update Head和100个ins kernel.</p><p>注：在COCO-Panoptic上多尺度训练36epoch，训练一个K-Net，使用16张V1100需要两天半，在两台机器的情况下，训练时间有点长。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近在知乎上频繁地刷到有关Mask2Former地帖子，这么多人吹捧地必是精品，就跟一波风看了一下Mask2Former，顺带地也了解一下Panoptic Segmentation这个任务。众所周知，图像分割主要有两个方向：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;语义分割（semantic segmentation）&lt;/strong&gt;，常用来识别天空、草地、道路等没有固定形状的不可数&lt;strong&gt;事物（stuff）&lt;/strong&gt;。语义分割的标记方法通常是给每个像素加上标签。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实例分割（instance segmentation）&lt;/strong&gt;，人、动物或工具等可数的、独立的明显物体&lt;strong&gt;（things）&lt;/strong&gt;。实例分割通常用包围盒或分割掩码标记目标。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;全景分割（Panoptic Segmentation）&lt;/strong&gt;其实就是把这两个方向结合起来，生成统一的、全局的分割图像，既识别事物，也识别物体。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/ps.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="transformer" scheme="https://blog.nicehuster.cn/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>自监督学习--基于contrastive learning方法</title>
    <link href="https://blog.nicehuster.cn/2021/12/13/self-sup-contrastive-learning/"/>
    <id>https://blog.nicehuster.cn/2021/12/13/self-sup-contrastive-learning/</id>
    <published>2021-12-13T11:13:39.000Z</published>
    <updated>2022-04-23T13:38:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>基于contrastive learning的自监督学习方法从2020-2021年涌现了许多工作，这里不一一列举，但是会简单介绍一些高引用的方法：MoCo，SimCLR，MoCov2，BYOL，SwAV，SimSiam，MoCov3。</p><p><img src="/img/contrastive learning.svg" alt="contrastive learning"></p><a id="more"></a><h3 id="1-MoCo-CVPR2020"><a href="#1-MoCo-CVPR2020" class="headerlink" title="1. MoCo(CVPR2020)"></a><a href="https://arxiv.org/abs/1911.05722" target="_blank" rel="noopener">1. MoCo(CVPR2020)</a></h3><p><img src="/img/MoCo.svg" alt="MoCo"></p><h3 id="2-SimCLR-ICML2020"><a href="#2-SimCLR-ICML2020" class="headerlink" title="2. SimCLR(ICML2020)"></a><a href="https://arxiv.org/abs/2002.05709" target="_blank" rel="noopener">2. SimCLR(ICML2020)</a></h3><p><img src="/img/SimCLR.svg" alt="SimCLR"></p><h3 id="3-MoCov2"><a href="#3-MoCov2" class="headerlink" title="3. MoCov2"></a><a href="https://arxiv.org/abs/2003.04297" target="_blank" rel="noopener">3. MoCov2</a></h3><p><img src="/img/MoCov2.svg" alt="MoCov2"></p><h3 id="4-BYOL-NIPS2020"><a href="#4-BYOL-NIPS2020" class="headerlink" title="4. BYOL(NIPS2020)"></a><a href="https://arxiv.org/abs/2006.07733" target="_blank" rel="noopener">4. BYOL(NIPS2020)</a></h3><p>待更新。。。</p><h3 id="5-SwAV-NIPS2020"><a href="#5-SwAV-NIPS2020" class="headerlink" title="5. SwAV(NIPS2020)"></a><a href="https://arxiv.org/abs/2006.07733" target="_blank" rel="noopener">5. SwAV(NIPS2020)</a></h3><p>待更新。。。</p><h3 id="6-SimSiam-CVPR2021"><a href="#6-SimSiam-CVPR2021" class="headerlink" title="6. SimSiam(CVPR2021)"></a><a href="https://arxiv.org/abs/2011.10566" target="_blank" rel="noopener">6. SimSiam(CVPR2021)</a></h3><p>待更新。。。</p><h3 id="7-MoCov3-ICCV2021"><a href="#7-MoCov3-ICCV2021" class="headerlink" title="7.MoCov3(ICCV2021)"></a><a href="https://arxiv.org/abs/2104.02057" target="_blank" rel="noopener">7.MoCov3(ICCV2021)</a></h3><p>待更新。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;基于contrastive learning的自监督学习方法从2020-2021年涌现了许多工作，这里不一一列举，但是会简单介绍一些高引用的方法：MoCo，SimCLR，MoCov2，BYOL，SwAV，SimSiam，MoCov3。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/contrastive learning.svg&quot; alt=&quot;contrastive learning&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="self-supervised" scheme="https://blog.nicehuster.cn/tags/self-supervised/"/>
    
  </entry>
  
  <entry>
    <title>自监督学习--基于pretext-task方法</title>
    <link href="https://blog.nicehuster.cn/2021/12/10/self-sup-pretext-task/"/>
    <id>https://blog.nicehuster.cn/2021/12/10/self-sup-pretext-task/</id>
    <published>2021-12-10T11:13:39.000Z</published>
    <updated>2022-04-23T13:11:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>自从2019年MoCo横空出世，掀起了一股自监督学习的浪潮，随后SimCLR,MoCo,BYOL,SwAV等一系列优秀的工作被提出，2021年底，何凯明的MAE更是将自监督学习带到另外一个高度。自监督学习的背后一个强大的动机就是，打破目前神经网络训练对于标注数据的依赖，即使在没有标注数据的情况下，也可以高效的训练网络。自监督学习的核心在于合理构建有利于模型学习的任务，其大致可分为三类：</p><p><img src="/img/self-sup-categories.png" alt="img"></p><a id="more"></a><p>基于pretext task的自监督学习方法大概有四篇经典的工作：</p><h3 id="1-Relative-Location-CVPR2015"><a href="#1-Relative-Location-CVPR2015" class="headerlink" title="1. Relative Location(CVPR2015)"></a>1. <a href="https://arxiv.org/abs/1505.05192" target="_blank" rel="noopener">Relative Location(CVPR2015)</a></h3><p><img src="/img/self-sup-rl.png" alt="img"></p><h3 id="2-Colorization-ECCV2016"><a href="#2-Colorization-ECCV2016" class="headerlink" title="2. Colorization(ECCV2016)"></a>2. <a href="https://arxiv.org/abs/1603.08511" target="_blank" rel="noopener">Colorization(ECCV2016)</a></h3><p><img src="/img/self-sup-color.png" alt="img"></p><h3 id="3-Context-Encoders（CVPR2016）"><a href="#3-Context-Encoders（CVPR2016）" class="headerlink" title="3. Context Encoders（CVPR2016）"></a>3. <a href="https://arxiv.org/abs/1604.07379" target="_blank" rel="noopener">Context Encoders（CVPR2016）</a></h3><p><img src="/img/self-sup-ce.png" alt="img"></p><h3 id="4-Rotation-Prediction-ICLR2018"><a href="#4-Rotation-Prediction-ICLR2018" class="headerlink" title="4. Rotation Prediction(ICLR2018)"></a>4. <a href="https://arxiv.org/abs/1803.07728" target="_blank" rel="noopener">Rotation Prediction(ICLR2018)</a></h3><p><img src="/img/self-sup-rp.png" alt="img"></p><p>从上面4种方法可以看出，基于pretext task的自监督学习方法都具备俩个特点：</p><blockquote><ol><li>良好的任务定义，比如预测旋转角度，或者相对位置</li><li>合理的限制条件，避免模型出现无效解</li></ol></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;自从2019年MoCo横空出世，掀起了一股自监督学习的浪潮，随后SimCLR,MoCo,BYOL,SwAV等一系列优秀的工作被提出，2021年底，何凯明的MAE更是将自监督学习带到另外一个高度。自监督学习的背后一个强大的动机就是，打破目前神经网络训练对于标注数据的依赖，即使在没有标注数据的情况下，也可以高效的训练网络。自监督学习的核心在于合理构建有利于模型学习的任务，其大致可分为三类：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/self-sup-categories.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="self-supervised learning" scheme="https://blog.nicehuster.cn/tags/self-supervised-learning/"/>
    
  </entry>
  
  <entry>
    <title>Transformer 杂记</title>
    <link href="https://blog.nicehuster.cn/2021/12/05/transformer-soup/"/>
    <id>https://blog.nicehuster.cn/2021/12/05/transformer-soup/</id>
    <published>2021-12-05T11:13:39.000Z</published>
    <updated>2022-04-12T11:39:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>近期断断续续的看了一些transformer相关的paper，看的比较杂，有些是对应领域比较有代表性地工作。偷个懒就不详细介绍每篇Paper，简单地记录一下这些paper大致要解决地问题。</p><a id="more"></a><h4 id="1-MAE-Masked-Autoencoders-Are-Scalable-Vision-Learners"><a href="#1-MAE-Masked-Autoencoders-Are-Scalable-Vision-Learners" class="headerlink" title="1. MAE:Masked Autoencoders Are Scalable Vision Learners"></a>1. MAE:Masked Autoencoders Are Scalable Vision Learners</h4><p>自监督学习方法，核心思想是以一定比例随机 mask 掉图片中的一些图像块(patch)然后重建这些部分的像素值</p><h4 id="2-SegFormer-Simple-and-Efficient-Design-for-Semantic-Segmentation-with-Transformers"><a href="#2-SegFormer-Simple-and-Efficient-Design-for-Semantic-Segmentation-with-Transformers" class="headerlink" title="2.SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers"></a>2.SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</h4><p>设计多层次backbone MIT，丢弃PE，优化self-att加速推理，此外从SETR痛点出发设计轻量级MLP解码器</p><h4 id="3-Early-Convolutions-Help-Transformers-See-Better"><a href="#3-Early-Convolutions-Help-Transformers-See-Better" class="headerlink" title="3.Early Convolutions Help Transformers See Better"></a>3.Early Convolutions Help Transformers See Better</h4><p>Vit训练不稳定在于Patch Embedding时使用大卷积核以及大步长导致，进一步提出使用step-wise conv stem进行替换，以此改进vit训练稳定性问题</p><h4 id="4-Visformer-The-Vision-friendly-Transformer"><a href="#4-Visformer-The-Vision-friendly-Transformer" class="headerlink" title="4.Visformer: The Vision-friendly Transformer"></a>4.Visformer: The Vision-friendly Transformer</h4><p>提升transformer方法的性能下限，即使是小数据集依然可以得到很好的性能</p><h4 id="5-Conditional-Positional-Encodings-for-Vision-Transformers"><a href="#5-Conditional-Positional-Encodings-for-Vision-Transformers" class="headerlink" title="5.Conditional Positional Encodings for Vision Transformers"></a>5.Conditional Positional Encodings for Vision Transformers</h4><p>利用卷积+zero-padding来编码局部位置信息，从而丢弃现有的PE，解决输入大小变化时需要对PE进行插值和fine-tune的问题</p><h4 id="6-MetaFormer-is-Actually-What-You-Need-for-Vision"><a href="#6-MetaFormer-is-Actually-What-You-Need-for-Vision" class="headerlink" title="6.MetaFormer is Actually What You Need for Vision"></a>6.MetaFormer is Actually What You Need for Vision</h4><p>transformer优于cnn在于其结构，而不是attention，即使替换成pooling，也能达到不错的性能</p><h4 id="7-Per-Pixel-Classification-is-Not-All-You-Need-for-Semantic-Segmentation"><a href="#7-Per-Pixel-Classification-is-Not-All-You-Need-for-Semantic-Segmentation" class="headerlink" title="7.Per-Pixel Classification is Not All You Need for Semantic Segmentation"></a>7.Per-Pixel Classification is Not All You Need for Semantic Segmentation</h4><p>提出了一种新的分割范式，解耦分割和分类，统一语义分割和实例分割任务</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;近期断断续续的看了一些transformer相关的paper，看的比较杂，有些是对应领域比较有代表性地工作。偷个懒就不详细介绍每篇Paper，简单地记录一下这些paper大致要解决地问题。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="transformer" scheme="https://blog.nicehuster.cn/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>知识蒸馏[转载]</title>
    <link href="https://blog.nicehuster.cn/2021/11/09/knowledge-distillation/"/>
    <id>https://blog.nicehuster.cn/2021/11/09/knowledge-distillation/</id>
    <published>2021-11-09T11:13:39.000Z</published>
    <updated>2022-04-24T08:30:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>知识蒸馏是一种模型压缩方法，是一种基于“教师-学生网络思想”的训练方法，由于其简单，有效，在工业界被广泛应用。这一技术的理论来自于2015年Hinton发表的一篇神作: <a href="https://arxiv.org/pdf/1503.02531.pdf" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a>。Knowledge Distillation，简称KD，顾名思义，就是将已经训练好的模型包含的知识(”Knowledge”)，蒸馏(“Distill”)提取到另一个模型里面去。</p><a id="more"></a><p>以下内容均转载于：<a href="https://zhuanlan.zhihu.com/p/102038521" target="_blank" rel="noopener">【经典简读】知识蒸馏(Knowledge Distillation) 经典之作</a> ，侵权删。</p><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h2><h3 id="1-1-论文提出的背景"><a href="#1-1-论文提出的背景" class="headerlink" title="1.1. 论文提出的背景"></a>1.1. 论文提出的背景</h3><p>虽然在一般情况下，我们不会去区分训练和部署使用的模型，但是训练和部署之间存在着一定的不一致性:</p><ul><li>在训练过程中，我们需要使用复杂的模型，大量的计算资源，以便从非常大、高度冗余的数据集中提取出信息。在实验中，效果最好的模型往往规模很大，甚至由多个模型集成得到。而大模型不方便部署到服务中去，常见的瓶颈如下:</li></ul><ol><li>推断速度慢</li><li>对部署资源要求高(内存，显存等)</li></ol><ul><li>在部署时，我们对延迟以及计算资源都有着严格的限制。</li></ul><p>因此，模型压缩（在保证性能的前提下减少模型的参数量）成为了一个重要的问题。而”模型蒸馏“属于模型压缩的一种方法。</p><p><strong>插句题外话</strong>，我们可以从模型参数量和训练数据量之间的相对关系来理解underfitting和overfitting。AI领域的从业者可能对此已经习以为常，但是为了力求让小白也能读懂本文，还是引用我同事的解释（我印象很深）形象地说明一下:</p><blockquote><p>模型就像一个容器，训练数据中蕴含的知识就像是要装进容器里的水。当数据知识量(水量)超过模型所能建模的范围时(容器的容积)，加再多的数据也不能提升效果(水再多也装不进容器)，因为模型的表达空间有限(容器容积有限)，就会造成<strong>underfitting</strong>；而当模型的参数量大于已有知识所需要的表达空间时(容积大于水量，水装不满容器)，就会造成<strong>overfitting</strong>，即模型的variance会增大(想象一下摇晃半满的容器，里面水的形状是不稳定的)。</p></blockquote><h3 id="1-2-“思想歧路”"><a href="#1-2-“思想歧路”" class="headerlink" title="1.2. “思想歧路”"></a>1.2. “思想歧路”</h3><p>上面容器和水的比喻非常经典和贴切，但是会引起一个误解: 人们在直觉上会觉得，要保留相近的知识量，必须保留相近规模的模型。也就是说，一个模型的参数量基本决定了其所能捕获到的数据内蕴含的“知识”的量。</p><p>这样的想法是基本正确的，但是需要注意的是:</p><ol><li>模型的参数量和其所能捕获的“知识“量之间并非稳定的线性关系(下图中的1)，而是接近边际收益逐渐减少的一种增长曲线(下图中的2和3)</li><li>完全相同的模型架构和模型参数量，使用完全相同的训练数据，能捕获的“知识”量并不一定完全相同，另一个关键因素是训练的方法。合适的训练方法可以使得在模型参数总量比较小时，尽可能地获取到更多的“知识”(下图中的3与2曲线的对比).</li></ol><p><img src="https://pic2.zhimg.com/80/v2-f2fc2f02b87a38a9ff34a50664800045_720w.jpg" alt="img"></p><h2 id="2-知识蒸馏的理论依据"><a href="#2-知识蒸馏的理论依据" class="headerlink" title="2. 知识蒸馏的理论依据"></a>2. 知识蒸馏的理论依据</h2><h3 id="2-1-Teacher-Model和Student-Model"><a href="#2-1-Teacher-Model和Student-Model" class="headerlink" title="2.1. Teacher Model和Student Model"></a>2.1. Teacher Model和Student Model</h3><p>知识蒸馏使用的是Teacher—Student模型，其中teacher是“知识”的输出者，student是“知识”的接受者。知识蒸馏的过程分为2个阶段:</p><ol><li>原始模型训练: 训练”Teacher模型”, 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对”Teacher模型”不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值。</li><li>精简模型训练: 训练”Student模型”, 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值。</li></ol><p>在本论文中，作者将问题限定在<strong>分类问题</strong>下，或者其他本质上属于分类问题的问题，该类问题的共同点是模型最后会有一个softmax层，其输出值对应了相应类别的概率值。</p><h3 id="2-2-知识蒸馏的关键点"><a href="#2-2-知识蒸馏的关键点" class="headerlink" title="2.2. 知识蒸馏的关键点"></a>2.2. 知识蒸馏的关键点</h3><p>如果回归机器学习最最基础的理论，我们可以很清楚地意识到一点(而这一点往往在我们深入研究机器学习之后被忽略): <strong>机器学习最根本的目的</strong>在于训练出在某个问题上泛化能力强的模型。</p><ul><li><strong>泛化能力强</strong>: 在某问题的所有数据上都能很好地反应输入和输出之间的关系，无论是训练数据，还是测试数据，还是任何属于该问题的未知数据。</li></ul><p>而现实中，由于我们不可能收集到某问题的所有数据来作为训练数据，并且新数据总是在源源不断的产生，因此我们只能退而求其次，训练目标变成在已有的训练数据集上建模输入和输出之间的关系。由于训练数据集是对真实数据分布情况的采样，训练数据集上的最优解往往会多少偏离真正的最优解(这里的讨论不考虑模型容量)。</p><p>而在知识蒸馏时，由于我们已经有了一个泛化能力较强的Net-T，我们在利用Net-T来蒸馏训练Net-S时，可以直接让Net-S去学习Net-T的泛化能力。</p><p>一个很直白且高效的迁移泛化能力的方法就是：使用softmax层输出的类别的概率来作为“soft target”。</p><p><strong>【KD的训练过程和传统的训练过程的对比】</strong></p><ol><li>传统training过程(<strong>hard targets</strong>): 对ground truth求极大似然</li><li>KD的training过程(<strong>soft targets</strong>): 用large model的class probabilities作为soft targets</li></ol><p><img src="https://pic3.zhimg.com/80/v2-29a851c6fa9cc809e51ce738abbec2ce_720w.jpg" alt="img"></p><p>上图: Hard Target 下图: Soft Target</p><p><strong>KD的训练过程为什么更有效?</strong></p><p>softmax层的输出，除了正例之外，<strong>负标签也带有大量的信息</strong>，比如某些负标签对应的概率远远大于其他负标签。而在传统的训练过程(hard target)中，所有负标签都被统一对待。也就是说，<strong>KD的训练方式使得每个样本给Net-S带来的信息量大于传统的训练方式</strong>。</p><p>【<strong>举个例子】</strong></p><p>在手写体数字识别任务MNIST中，输出类别有10个。</p><p><img src="https://pic3.zhimg.com/80/v2-3d77281f38df62990c47d606dd581ee2_720w.jpg" alt="img"></p><p>假设某个输入的“2”更加形似”3”，softmax的输出值中”3”对应的概率为0.1，而其他负标签对应的值都很小，而另一个”2”更加形似”7”，”7”对应的概率为0.1。这两个”2”对应的hard target的值是相同的，但是它们的soft target却是不同的，由此我们可见soft target蕴含着比hard target多的信息。并且soft target分布的熵相对高时，其soft target蕴含的知识就更丰富。</p><p><img src="https://pic4.zhimg.com/80/v2-a9e90626c5ac6f64a7e04c89f6ce3013_720w.jpg" alt="img"></p><p>两个”2“的hard target相同而soft target不同</p><p>这就解释了为什么通过蒸馏的方法训练出的Net-S相比使用完全相同的模型结构和训练数据只使用hard target的训练方法得到的模型，拥有更好的泛化能力。</p><h3 id="2-3-softmax函数"><a href="#2-3-softmax函数" class="headerlink" title="2.3. softmax函数"></a>2.3. softmax函数</h3><p>先回顾一下原始的softmax函数:</p><script type="math/tex; mode=display">q_{i}=\frac{\exp \left(z_{i}\right)}{\sum_{j} \exp \left(z_{j}\right)}</script><p>但要是直接使用softmax层的输出值作为soft target, 这又会带来一个问题: 当softmax输出的概率分布熵相对较小时，负标签的值都很接近0，对损失函数的贡献非常小，小到可以忽略不计。因此<strong>“温度”</strong>这个变量就派上了用场。</p><p>下面的公式时加了温度这个变量之后的softmax函数:</p><script type="math/tex; mode=display">q_{i}=\frac{\exp \left(z_{i} / T\right)}{\sum_{j} \exp \left(z_{j} / T\right)}</script><ul><li>这里的T就是<strong>温度</strong>。</li><li>原来的softmax函数是T = 1的特例。 T越高，softmax的output probability distribution越趋于平滑，其分布的熵越大，负标签携带的信息会被相对地放大，模型训练将更加关注负标签。</li></ul><h2 id="3-知识蒸馏的具体方法"><a href="#3-知识蒸馏的具体方法" class="headerlink" title="3. 知识蒸馏的具体方法"></a>3. 知识蒸馏的具体方法</h2><h3 id="3-1-通用的知识蒸馏方法"><a href="#3-1-通用的知识蒸馏方法" class="headerlink" title="3.1. 通用的知识蒸馏方法"></a>3.1. 通用的知识蒸馏方法</h3><ul><li><strong>第一步</strong>是训练Net-T；<strong>第二步</strong>是在高温T下，蒸馏Net-T的知识到Net-S</li></ul><p><img src="https://pic2.zhimg.com/80/v2-d01f5142d06aa27bc5e207831b5131d9_720w.jpg" alt="img"></p><p>知识蒸馏示意图(来自<a href="https://nervanasystems.github.io/distiller/knowledge_distillation.html" target="_blank" rel="noopener">https://nervanasystems.github.io/distiller/knowledge_distillation.html</a>)</p><p>训练Net-T的过程很简单，下面详细讲讲第二步:高温蒸馏的过程。高温蒸馏过程的目标函数由distill loss(对应soft target)和student loss(对应hard target)加权得到。示意图如上。</p><script type="math/tex; mode=display">L=\alpha L_{s o f t}+\beta L_{h a r d}</script><ul><li><img src="https://www.zhihu.com/equation?tex=v_i" alt="[公式]">: Net-T的logits</li><li><img src="https://www.zhihu.com/equation?tex=z_i" alt="[公式]">: Net-S的logits</li><li><img src="https://www.zhihu.com/equation?tex=p%5ET_i" alt="[公式]">: Net-T的在温度=T下的softmax输出在第i类上的值</li><li><img src="https://www.zhihu.com/equation?tex=q%5ET_i" alt="[公式]">: Net-S的在温度=T下的softmax输出在第i类上的值</li><li><img src="https://www.zhihu.com/equation?tex=c_i" alt="[公式]">: 在第i类上的ground truth值, <img src="https://www.zhihu.com/equation?tex=c_i%5Cin%5C%7B0%2C1%5C%7D" alt="[公式]">, 正标签取1，负标签取0.</li><li><img src="https://www.zhihu.com/equation?tex=N" alt="[公式]">: 总标签数量</li><li>Net-T 和 Net-S同时输入 transfer set (这里可以直接复用训练Net-T用到的training set), 用Net-T产生的softmax distribution (with high temperature) 来作为soft target，Net-S在相同温度T条件下的softmax输出和soft target的cross entropy就是<strong>Loss函数的第一部分</strong> <img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D" alt="[公式]"></li></ul><p><img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D%3D-%5Csum_j%5EN+p%5ET_j%5Clog%28q%5ET_j%29" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=p%5ET_i%3D%5Cfrac%7B%5Cexp%28v_i%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28v_k%2FT%29%7D" alt="[公式]"> , <img src="https://www.zhihu.com/equation?tex=q%5ET_i%3D%5Cfrac%7B%5Cexp%28z_i%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%2FT%29%7D" alt="[公式]"></p><ul><li>Net-S在T=1的条件下的softmax输出和ground truth的cross entropy就是<strong>Loss函数的第二部分</strong> <img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D" alt="[公式]"> 。</li></ul><p><img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D%3D-%5Csum_j%5EN+c_j%5Clog%28q%5E1_j%29" alt="[公式]"> ，其中 <img src="https://www.zhihu.com/equation?tex=q%5E1_i%3D%5Cfrac%7B%5Cexp%28z_i%29%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%29%7D" alt="[公式]"></p><ul><li>第二部分Loss <img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D" alt="[公式]"> 的必要性其实很好理解: Net-T也有一定的错误率，使用ground truth可以有效降低错误被传播给Net-S的可能。打个比方，老师虽然学识远远超过学生，但是他仍然有出错的可能，而这时候如果学生在老师的教授之外，可以同时参考到标准答案，就可以有效地降低被老师偶尔的错误“带偏”的可能性。</li></ul><p><strong>【讨论】</strong></p><ul><li>实验发现第二部分所占比重比较小的时候，能产生最好的结果，这是一个经验的结论。一个可能的原因是，由于soft target产生的gradient与hard target产生的gradient之间有与 <img src="https://www.zhihu.com/equation?tex=T" alt="[公式]"> 相关的比值。原论文中只是一笔带过，我在下面补充了一些简单的推导。(ps. 下面推导可能有些错误，如果有读者能够正确推出来请私信我～)</li><li><strong>Soft Target:</strong><img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D" alt="[公式]"></li></ul><p><img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D%3D-%5Csum_j%5EN+p%5ET_j%5Clog%28q%5ET_j%29%3D-%5Csum_j%5EN+%5Cfrac%7Bz_j%2FT%5Ctimes%5Cexp%28v_j%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28v_k%2FT%29%7D%5Cleft%28%5Cfrac%7B1%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%2FT%29%7D-%5Cfrac%7B%5Cexp+%28z_j+%2F+T%29+%7D%7B%5Cleft%28++%5Csum_k%5EN+%5Cexp%28z_k%2F+T%29%5Cright%29+%5E+2%7D%5Cright%29" alt="[公式]"></p><p><img src="https://www.zhihu.com/equation?tex=%5Capprox+-%5Cfrac%7B1%7D%7BT%5Csum_k%5EN+%5Cexp%28v_k%2FT%29%7D%5Cleft%28%5Cfrac%7B%5Csum_j%5ENz_j%5Cexp%28v_j%2FT%29%7D%7B%5Csum_k%5EN+%5Cexp%28z_k%2FT%29%7D-%5Cfrac%7B%5Csum_j%5EN+z_j%5Cexp+%28z_j%2F+T%29%5Cexp%28v_j%2FT%29+%7D%7B%5Cleft%28++%5Csum_k%5EN+%5Cexp%28z_k+%2F+T%29%5Cright%29+%5E+2%7D+%5Cright%29" alt="[公式]"></p><ul><li><strong>Hard Target:</strong> <img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D" alt="[公式]"></li></ul><p><img src="https://www.zhihu.com/equation?tex=L_%7Bhard%7D%3D-%5Csum_j%5EN+c_j%5Clog%28q%5E1_j%29%3D-%5Cleft%28%5Cfrac%7B%5Csum_j%5EN+c_jz_j+%7D%7B+%5Csum_k%5EN+%5Cexp%28z_k+%29%7D-%5Cfrac%7B%5Csum_j%5EN+c_jz_j%5Cexp+%28z_j%29+%7D%7B%5Cleft%28++%5Csum_k%5EN+%5Cexp%28z_k%29%5Cright%29+%5E+2%7D+%5Cright%29" alt="[公式]"></p><ul><li>由于 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_i%7D" alt="[公式]">的magnitude大约是 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bhard%7D%7D%7B%5Cpartial+z_i%7D" alt="[公式]"> 的 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BT%5E2%7D" alt="[公式]"> ，因此在同时使用soft target和hard target的时候，需要在soft target之前乘上<img src="https://www.zhihu.com/equation?tex=T%5E%7B2%7D" alt="[公式]">的系数，这样才能保证soft target和hard target贡献的梯度量基本一致。</li></ul><p><strong>【注意】</strong> 在Net-S训练完毕后，做inference时其softmax的温度T要恢复到1.</p><h3 id="3-2-一种特殊情形-直接match-logits-不经过softmax"><a href="#3-2-一种特殊情形-直接match-logits-不经过softmax" class="headerlink" title="3.2. 一种特殊情形: 直接match logits(不经过softmax)"></a>3.2. 一种特殊情形: 直接match logits(不经过softmax)</h3><p>直接match logits指的是，直接使用softmax层的输入logits（而不是输出）作为soft targets，需要最小化的目标函数是Net-T和Net-S的logits之间的平方差。</p><p><strong>直接上结论: 直接match logits的做法是</strong> <img src="https://www.zhihu.com/equation?tex=T+%5Crightarrow+%5Cinfty" alt="[公式]"> <strong>的情况下的特殊情形。</strong></p><p>由单个case贡献的loss，推算出对应在Net-S每个logit <img src="https://www.zhihu.com/equation?tex=z_i" alt="[公式]"> 上的gradient:</p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_%7Bi%7D%7D%3D%5Cfrac%7B1%7D%7BT%7D%5Cleft%28q_%7Bi%7D-p_%7Bi%7D%5Cright%29%3D%5Cfrac%7B1%7D%7BT%7D%5Cleft%28%5Cfrac%7Be%5E%7Bz_%7Bi%7D+%2F+T%7D%7D%7B%5Csum_%7Bj%7D+e%5E%7Bz_%7Bj%7D+%2F+T%7D%7D-%5Cfrac%7Be%5E%7Bv_%7Bi%7D+%2F+T%7D%7D%7B%5Csum_%7Bj%7D+e%5E%7Bv_%7Bj%7D+%2F+T%7D%7D%5Cright%29" alt="[公式]"></p><p>当 <img src="https://www.zhihu.com/equation?tex=T+%5Crightarrow+%5Cinfty" alt="[公式]"> 时，我们使用 <img src="https://www.zhihu.com/equation?tex=1%2Bx%2FT" alt="[公式]"> 来近似 <img src="https://www.zhihu.com/equation?tex=e%5E%7Bx%2FT%7D" alt="[公式]"> ，于是得到</p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_%7Bi%7D%7D+%5Capprox+%5Cfrac%7B1%7D%7BT%7D%5Cleft%28%5Cfrac%7B1%2Bz_%7Bi%7D+%2F+T%7D%7BN%2B%5Csum_%7Bj%7D+z_%7Bj%7D+%2F+T%7D-%5Cfrac%7B1%2Bv_%7Bi%7D+%2F+T%7D%7BN%2B%5Csum_%7Bj%7D+v_%7Bj%7D+%2F+T%7D%5Cright%29" alt="[公式]"></p><p>如果再加上logits是零均值的假设</p><p><img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bj%7D+z_%7Bj%7D%3D%5Csum_%7Bj%7D+v_%7Bj%7D%3D0" alt="[公式]"></p><p>那么上面的公式可以简化成</p><p><img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L_%7Bsoft%7D%7D%7B%5Cpartial+z_%7Bi%7D%7D+%5Capprox+%5Cfrac%7B1%7D%7BN+T%5E%7B2%7D%7D%5Cleft%28z_%7Bi%7D-v_%7Bi%7D%5Cright%29" alt="[公式]"></p><p>也就是等价于minimise下面的损失函数</p><p><img src="https://www.zhihu.com/equation?tex=L_%7Bsoft%7D%27%3D1+%2F+2%5Cleft%28z_%7Bi%7D-v_%7Bi%7D%5Cright%29%5E%7B2%7D" alt="[公式]"></p><h2 id="4-关于”温度”的讨论"><a href="#4-关于”温度”的讨论" class="headerlink" title="4. 关于”温度”的讨论"></a>4. 关于”温度”的讨论</h2><p>【问题】 我们都知道“蒸馏”需要在高温下进行，那么这个“蒸馏”的温度代表了什么，又是如何选取合适的温度？</p><p><img src="https://pic2.zhimg.com/80/v2-a120cc4bbb70b96968210b995b2e39d1_720w.jpg" alt="img">随着温度T的增大，概率分布的熵逐渐增大</p><h3 id="4-1-温度的特点"><a href="#4-1-温度的特点" class="headerlink" title="4.1. 温度的特点"></a>4.1. 温度的特点</h3><p>在回答这个问题之前，先讨论一下<strong>温度T的特点</strong></p><ol><li>原始的softmax函数是 <img src="https://www.zhihu.com/equation?tex=T%3D1+" alt="[公式]"> 时的特例， <img src="https://www.zhihu.com/equation?tex=T%3C1" alt="[公式]"> 时，概率分布比原始更“陡峭”， <img src="https://www.zhihu.com/equation?tex=T%3E1" alt="[公式]"> 时，概率分布比原始更“平缓”。</li><li>温度越高，softmax上各个值的分布就越平均（思考极端情况: (i) <img src="https://www.zhihu.com/equation?tex=T%3D%5Cinfty" alt="[公式]"> , 此时softmax的值是平均分布的；(ii) <img src="https://www.zhihu.com/equation?tex=T%5Crightarrow0" alt="[公式]">，此时softmax的值就相当于 <img src="https://www.zhihu.com/equation?tex=argmax" alt="[公式]"> , 即最大的概率处的值趋近于1，而其他值趋近于0）</li><li>不管温度T怎么取值，Soft target都有忽略相对较小的 <img src="https://www.zhihu.com/equation?tex=p_i" alt="[公式]"> 携带的信息的倾向</li></ol><h3 id="4-2-温度代表了什么，如何选取合适的温度？"><a href="#4-2-温度代表了什么，如何选取合适的温度？" class="headerlink" title="4.2. 温度代表了什么，如何选取合适的温度？"></a><strong>4.2. 温度代表了什么，如何选取合适的温度？</strong></h3><p><strong>温度的高低改变的是Net-S训练过程中对负标签的关注程度</strong>: 温度较低时，对负标签的关注，尤其是那些显著低于平均值的负标签的关注较少；而温度较高时，负标签相关的值会相对增大，Net-S会相对多地关注到负标签。</p><p>实际上，负标签中包含一定的信息，尤其是那些值显著<strong>高于</strong>平均值的负标签。但由于Net-T的训练过程决定了负标签部分比较noisy，并且负标签的值越低，其信息就越不可靠。因此温度的选取比较empirical，本质上就是在下面两件事之中取舍:</p><ol><li>从有部分信息量的负标签中学习 —&gt; 温度要高一些</li><li>防止受负标签中噪声的影响 —&gt;温度要低一些</li></ol><p>总的来说，T的选择和Net-S的大小有关，Net-S参数量比较小的时候，相对比较低的温度就可以了（因为参数量小的模型不能capture all knowledge，所以可以适当忽略掉一些负标签的信息）</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;知识蒸馏是一种模型压缩方法，是一种基于“教师-学生网络思想”的训练方法，由于其简单，有效，在工业界被广泛应用。这一技术的理论来自于2015年Hinton发表的一篇神作: &lt;a href=&quot;https://arxiv.org/pdf/1503.02531.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Distilling the Knowledge in a Neural Network&lt;/a&gt;。Knowledge Distillation，简称KD，顾名思义，就是将已经训练好的模型包含的知识(”Knowledge”)，蒸馏(“Distill”)提取到另一个模型里面去。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="Knowledge Distillation" scheme="https://blog.nicehuster.cn/tags/Knowledge-Distillation/"/>
    
  </entry>
  
  <entry>
    <title>Swin-transformer</title>
    <link href="https://blog.nicehuster.cn/2021/08/12/swin/"/>
    <id>https://blog.nicehuster.cn/2021/08/12/swin/</id>
    <published>2021-08-12T11:13:39.000Z</published>
    <updated>2022-04-12T04:14:36.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/pdf/2103.14030.pdf" target="_blank" rel="noopener">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a><br><strong>代码链接：</strong><a href="https://github.com/microsoft/Swin-Transformer" target="_blank" rel="noopener">https://github.com/microsoft/Swin-Transformer</a><br><strong>整体信息：</strong> Swin Transformer 提出了一种针对视觉任务的通用的 Transformer 架构，Transformer 架构在 NLP 任务中已经算得上一种通用的架构，但是如果想迁移到视觉任务中有一个比较大的困难就是处理数据的尺寸不一样。作者分析表明，Transformer 从 NLP 迁移到 CV 上没有大放异彩主要有两点原因：(1)两个领域涉及的scale不同，NLP的scale是标准固定的，而CV的scale变化范围非常大。(2) CV比起NLP需要更大的分辨率，而且CV中使用Transformer的计算复杂度是图像尺度的平方，这会导致计算量过于庞大。为了解决这两个问题，Swin Transformer相比之前的ViT做了两个改进：1.引入CNN中常用的层次化构建方式构建层次化Transformer 2.引入locality思想，对无重合的window区域内进行self-attention计算。</p><p><img src="/img/swin.png" alt="swin"></p><a id="more"></a><p>相比于ViT，Swin Transfomer计算复杂度大幅度降低，具有输入图像大小线性计算复杂度。Swin Transformer随着深度加深，逐渐合并图像块来构建层次化Transformer，可以作为通用的视觉骨干网络，应用于图像分类、目标检测、语义分割等任务。</p><h3 id="1-整体结构"><a href="#1-整体结构" class="headerlink" title="1. 整体结构"></a>1. 整体结构</h3><p>我们先看下Swin Transformer的整体架构:</p><p><img src="/img/swin-arch.png" alt="swin"></p><p>整个模型采取层次化的设计，一共包含4个Stage，每个stage都会缩小输入特征图的分辨率，像CNN一样逐层扩大感受野。</p><ul><li>在输入开始的时候，做了一个<code>Patch Embedding</code>，将图片切成一个个图块，并嵌入到<code>Embedding</code>。</li><li>在每个Stage里，由<code>Patch Merging</code>和多个Block组成。</li><li>其中<code>Patch Merging</code>模块主要在每个Stage一开始降低图片分辨率。</li><li>而Block具体结构如右图所示，主要是<code>LayerNorm</code>，<code>MLP</code>，<code>Window Attention</code> 和 <code>Shifted Window Attention</code>组成 (为了方便讲解，我会省略掉一些参数)</li></ul><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SwinTransformer</span>(<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(...)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">super</span>().__init_<span class="number">_</span>()</span><br><span class="line">        ...</span><br><span class="line">        <span class="comment"># absolute position embedding</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">ape:</span></span><br><span class="line">            <span class="keyword">self</span>.absolute_pos_embed = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches, embed_dim))</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">self</span>.pos_drop = nn.Dropout(p=drop_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># build layers</span></span><br><span class="line">        <span class="keyword">self</span>.layers = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i_layer <span class="keyword">in</span> range(<span class="keyword">self</span>.num_layers)<span class="symbol">:</span></span><br><span class="line">            layer = BasicLayer(...)</span><br><span class="line">            <span class="keyword">self</span>.layers.append(layer)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">self</span>.norm = norm_layer(<span class="keyword">self</span>.num_features)</span><br><span class="line">        <span class="keyword">self</span>.avgpool = nn.AdaptiveAvgPool1d(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">self</span>.head = nn.Linear(<span class="keyword">self</span>.num_features, num_classes) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward_features</span><span class="params">(<span class="keyword">self</span>, x)</span></span><span class="symbol">:</span></span><br><span class="line">        x = <span class="keyword">self</span>.patch_embed(x)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">self</span>.<span class="symbol">ape:</span></span><br><span class="line">            x = x + <span class="keyword">self</span>.absolute_pos_embed</span><br><span class="line">        x = <span class="keyword">self</span>.pos_drop(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="keyword">self</span>.<span class="symbol">layers:</span></span><br><span class="line">            x = layer(x)</span><br><span class="line"></span><br><span class="line">        x = <span class="keyword">self</span>.norm(x)  <span class="comment"># B L C</span></span><br><span class="line">        x = <span class="keyword">self</span>.avgpool(x.transpose(<span class="number">1</span>, <span class="number">2</span>))  <span class="comment"># B C 1</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(<span class="keyword">self</span>, x)</span></span><span class="symbol">:</span></span><br><span class="line">        x = <span class="keyword">self</span>.forward_features(x)</span><br><span class="line">        x = <span class="keyword">self</span>.head(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>其中有几个地方处理方法与ViT不同：</p><blockquote><ul><li>ViT在输入会给embedding进行位置编码。而Swin-T这里则是作为一个<strong>可选项</strong>（<code>self.ape</code>），Swin-T是在计算Attention的时候做了一个<code>相对位置编码</code>;</li><li>ViT会单独加上一个可学习参数，作为分类的token。而Swin-T则是<strong>直接做平均</strong>，输出分类，有点类似CNN最后的全局平均池化层;</li></ul></blockquote><h3 id="2-Patch-Embedding"><a href="#2-Patch-Embedding" class="headerlink" title="2. Patch Embedding"></a>2. Patch Embedding</h3><p>在输入进Block前，我们需要将图片切成一个个patch，然后嵌入向量。具体做法是对原始图片裁成一个个 <code>window_size * window_size</code>的窗口大小，然后进行嵌入。这里可以通过二维卷积层，<strong>将stride，kernelsize设置为window_size大小</strong>。设定输出通道来确定嵌入向量的大小。最后将H,W维度展开，并移动到第一维度:</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">import</span> torch</span><br><span class="line"><span class="built_in">import</span> torch.nn as nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class PatchEmbed(nn.Module):</span><br><span class="line">    def __init__(self, <span class="attr">img_size=224,</span> <span class="attr">patch_size=4,</span> <span class="attr">in_chans=3,</span> <span class="attr">embed_dim=96,</span> <span class="attr">norm_layer=None):</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="attr">img_size</span> = to_2tuple(img_size) <span class="comment"># -&gt; (img_size, img_size)</span></span><br><span class="line">        <span class="attr">patch_size</span> = to_2tuple(patch_size) <span class="comment"># -&gt; (patch_size, patch_size)</span></span><br><span class="line">        <span class="attr">patches_resolution</span> = [img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>]]</span><br><span class="line">        self.<span class="attr">img_size</span> = img_size</span><br><span class="line">        self.<span class="attr">patch_size</span> = patch_size</span><br><span class="line">        self.<span class="attr">patches_resolution</span> = patches_resolution</span><br><span class="line">        self.<span class="attr">num_patches</span> = patches_resolution[<span class="number">0</span>] * patches_resolution[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">        self.<span class="attr">in_chans</span> = in_chans</span><br><span class="line">        self.<span class="attr">embed_dim</span> = embed_dim</span><br><span class="line"></span><br><span class="line">        self.<span class="attr">proj</span> = nn.Conv2d(in_chans, embed_dim, <span class="attr">kernel_size=patch_size,</span> <span class="attr">stride=patch_size)</span></span><br><span class="line">        <span class="keyword">if</span> norm_layer is not None:</span><br><span class="line">            self.<span class="attr">norm</span> = norm_layer(embed_dim)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.<span class="attr">norm</span> = None</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        <span class="comment"># 假设采取默认参数</span></span><br><span class="line">        <span class="attr">x</span> = self.proj(x) <span class="comment"># 出来的是(N, 96, 224/4, 224/4) </span></span><br><span class="line">        <span class="attr">x</span> = torch.flatten(x, <span class="number">2</span>) <span class="comment"># 把HW维展开，(N, 96, 56*56)</span></span><br><span class="line">        <span class="attr">x</span> = torch.transpose(x, <span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># 把通道维放到最后 (N, 56*56, 96)</span></span><br><span class="line">        <span class="keyword">if</span> self.norm is not None:</span><br><span class="line">            <span class="attr">x</span> = self.norm(x)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure><h3 id="3-Patch-Merging"><a href="#3-Patch-Merging" class="headerlink" title="3.Patch Merging"></a>3.Patch Merging</h3><p>该模块的作用是在每个Stage开始前做降采样，用于缩小分辨率，调整通道数 进而形成层次化的设计，同时也能节省一定运算量。在CNN中，则是在每个Stage开始前用<code>stride=2</code>的卷积/池化层来降低分辨率。每次降采样是两倍，因此<strong>在行方向和列方向上，间隔2选取元素</strong>。然后拼接在一起作为一整个张量，最后展开。<strong>此时通道维度会变成原先的4倍</strong>（因为H,W各缩小2倍），此时再通过一个<strong>全连接层再调整通道维度为原来的两倍</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PatchMerging</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_resolution, dim, norm_layer=nn.LayerNorm)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.input_resolution = input_resolution</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.reduction = nn.Linear(<span class="number">4</span> * dim, <span class="number">2</span> * dim, bias=<span class="keyword">False</span>)</span><br><span class="line">        self.norm = norm_layer(<span class="number">4</span> * dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        x: B, H*W, C</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        H, W = self.input_resolution</span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">"input feature has wrong size"</span></span><br><span class="line">        <span class="keyword">assert</span> H % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> W % <span class="number">2</span> == <span class="number">0</span>, <span class="string">f"x size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) are not even."</span></span><br><span class="line"></span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line"></span><br><span class="line">        x0 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x1 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x2 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x3 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># B H/2 W/2 C</span></span><br><span class="line">        x = torch.cat([x0, x1, x2, x3], <span class="number">-1</span>)  <span class="comment"># B H/2 W/2 4*C</span></span><br><span class="line">        x = x.view(B, <span class="number">-1</span>, <span class="number">4</span> * C)  <span class="comment"># B H/2*W/2 4*C</span></span><br><span class="line"></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.reduction(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>下面是一个示意图（输入张量N=1, H=W=8, C=1，不包含最后的全连接层调整）</p><p><img src="/img/patch_merging.png" alt="swin"></p><h3 id="3-Window-Attention"><a href="#3-Window-Attention" class="headerlink" title="3. Window Attention"></a>3. Window Attention</h3><p>这是这篇文章的关键。传统的Transformer都是<strong>基于全局来计算注意力的</strong>，因此计算复杂度十分高。而Swin Transformer则将<strong>注意力的计算限制在每个窗口内</strong>，进而减少了计算量。我们先简单看下公式：</p><script type="math/tex; mode=display">\operatorname{Attention}(Q, K, V)=\operatorname{SoftMax}\left(Q K^{T} / \sqrt{d}+B\right) V</script><p>主要区别是在原始计算Attention的公式中的Q,K时<strong>加入了相对位置编码</strong>。后续实验有证明相对位置编码的加入提升了模型性能。</p><h3 id="4-Shifted-Window-Attention"><a href="#4-Shifted-Window-Attention" class="headerlink" title="4. Shifted Window Attention"></a>4. Shifted Window Attention</h3><p>前面的Window Attention是在每个窗口下计算注意力的，为了更好的和其他window进行信息交互，Swin Transformer还引入了shifted window操作。</p><p><img src="/img/shifted.png" alt="swin"></p><p>左边是没有重叠的Window Attention，而右边则是将窗口进行移位的Shift Window Attention。可以看到移位后的窗口包含了原本相邻窗口的元素。但这也引入了一个新问题，即<strong>window的个数翻倍了</strong>，由原本四个窗口变成了9个窗口。在实际代码里，我们是<strong>通过对特征图移位，并给Attention设置mask来间接实现的</strong>。能在<strong>保持原有的window个数下</strong>，最后的计算结果等价。</p><p><img src="/img/partition.png" alt="swin"></p><h4 id="4-1-特征图移位操作"><a href="#4-1-特征图移位操作" class="headerlink" title="4.1 特征图移位操作"></a>4.1 特征图移位操作</h4><p>代码里对特征图移位是通过<code>torch.roll</code>来实现的，下面是示意图</p><p><img src="/img/shift_opt.png" alt="swin"></p><blockquote><p>第一位操作是针对行进行移位，第二位操作时针对列进行移位操作。如果需要<code>reverse cyclic shift</code>的话只需把参数<code>shifts</code>设置为对应的正数值。</p></blockquote><h4 id="4-2-attention-mask"><a href="#4-2-attention-mask" class="headerlink" title="4.2 attention mask"></a>4.2 attention mask</h4><p>我认为这是Swin Transformer的精华，通过设置合理的mask，让<code>Shifted Window Attention</code>在与<code>Window Attention</code>相同的窗口个数下，达到等价的计算结果。首先我们对Shift Window后的每个窗口都给上index，并且做一个<code>roll</code>操作（window_size=2, shift_size=1）</p><p><img src="/img/shift_index.jpg" alt="swin"></p><p>我们希望在计算Attention的时候，<strong>让具有相同index QK进行计算，而忽略不同index QK计算结果</strong>。最后正确的结果如下图所示.</p><p><strong>例1：</strong>比如右上角这个 window，如下图所示。它由4个 patch 组成，所以应该计算出的 attention map是4×4的。但是6和4是2个不同的 sub-window，我们又不想让它们的 attention 发生交叠。所以我们希望的 attention map 和attention  mask如下图所示。</p><p><img src="/img/att_mask.png" alt="swin"></p><p><strong>例2：</strong>比如右下角这个 window，对应的 attention map 和attention  mask是下面这个样子。</p><p><img src="/img/att_mask2.png" alt="swin"></p><h3 id="5-transformer-block-整体结构"><a href="#5-transformer-block-整体结构" class="headerlink" title="5. transformer block 整体结构"></a>5. transformer block 整体结构</h3><p><img src="/img/block.png" alt="swin"></p><p>两个连续的Block架构如上图所示，需要注意的是一个Stage包含的Block个数必须是偶数，因为需要交替包含一个含有<code>Window Attention</code>的Layer和含有<code>Shifted Window Attention</code>的Layer。我们看下Block的前向代码：</p><figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, x):</span><br><span class="line">    H, <span class="attr">W</span> = self.input_resolution</span><br><span class="line">    B, L, <span class="attr">C</span> = x.shape</span><br><span class="line">    <span class="keyword">assert</span> <span class="attr">L</span> == H * W, <span class="string">"input feature has wrong size"</span></span><br><span class="line"></span><br><span class="line">    <span class="attr">shortcut</span> = x</span><br><span class="line">    <span class="attr">x</span> = self.norm1(x)</span><br><span class="line">    <span class="attr">x</span> = x.view(B, H, W, C)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># cyclic shift</span></span><br><span class="line">    <span class="keyword">if</span> self.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="attr">shifted_x</span> = torch.roll(x, <span class="attr">shifts=(-self.shift_size,</span> -self.shift_size), <span class="attr">dims=(1,</span> <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="attr">shifted_x</span> = x</span><br><span class="line"></span><br><span class="line">    <span class="comment"># partition windows</span></span><br><span class="line">    <span class="attr">x_windows</span> = window_partition(shifted_x, self.window_size)  <span class="comment"># nW*B, window_size, window_size, C</span></span><br><span class="line">    <span class="attr">x_windows</span> = x_windows.view(-<span class="number">1</span>, self.window_size * self.window_size, C)  <span class="comment"># nW*B, window_size*window_size, C</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># W-MSA/SW-MSA</span></span><br><span class="line">    <span class="attr">attn_windows</span> = self.attn(x_windows, <span class="attr">mask=self.attn_mask)</span>  <span class="comment"># nW*B, window_size*window_size, C</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># merge windows</span></span><br><span class="line">    <span class="attr">attn_windows</span> = attn_windows.view(-<span class="number">1</span>, self.window_size, self.window_size, C)</span><br><span class="line">    <span class="attr">shifted_x</span> = window_reverse(attn_windows, self.window_size, H, W)  <span class="comment"># B H' W' C</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># reverse cyclic shift</span></span><br><span class="line">    <span class="keyword">if</span> self.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="attr">x</span> = torch.roll(shifted_x, <span class="attr">shifts=(self.shift_size,</span> self.shift_size), <span class="attr">dims=(1,</span> <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="attr">x</span> = shifted_x</span><br><span class="line">    <span class="attr">x</span> = x.view(B, H * W, C)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># FFN</span></span><br><span class="line">    <span class="attr">x</span> = shortcut + self.drop_path(x)</span><br><span class="line">    <span class="attr">x</span> = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line"></span><br><span class="line">    return x</span><br></pre></td></tr></table></figure><p>整体流程如下：</p><ul><li>先对特征图进行LayerNorm</li><li>通过<code>self.shift_size</code>决定是否需要对特征图进行shift</li><li>然后将特征图切成一个个窗口</li><li>计算Attention，通过<code>self.attn_mask</code>来区分<code>Window Attention</code>还是<code>Shift Window Attention</code></li><li>将各个窗口合并回来</li><li>如果之前有做shift操作，此时进行<code>reverse shift</code>，把之前的shift操作恢复</li><li>做dropout和残差连接</li><li>再通过一层LayerNorm+全连接层，以及dropout和残差连接</li></ul><h3 id="6-experiments"><a href="#6-experiments" class="headerlink" title="6. experiments"></a>6. experiments</h3><p><img src="/img/ink21.png" alt="swin"></p><p>在ImageNet22K数据集上，准确率能达到惊人的86.4%。另外在检测，分割等任务上表现也很优异，感兴趣的可以翻看论文最后的实验部分。</p><h3 id="7-conclusion"><a href="#7-conclusion" class="headerlink" title="7. conclusion"></a>7. conclusion</h3><p>这篇文章创新点很棒，引入window这一个概念，将CNN的局部性引入，还能控制模型整体计算量。在Shift Window Attention部分，用一个mask和移位操作，很巧妙的实现计算等价。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2103.14030.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Swin Transformer: Hierarchical Vision Transformer using Shifted Windows&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/microsoft/Swin-Transformer&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/microsoft/Swin-Transformer&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt; Swin Transformer 提出了一种针对视觉任务的通用的 Transformer 架构，Transformer 架构在 NLP 任务中已经算得上一种通用的架构，但是如果想迁移到视觉任务中有一个比较大的困难就是处理数据的尺寸不一样。作者分析表明，Transformer 从 NLP 迁移到 CV 上没有大放异彩主要有两点原因：(1)两个领域涉及的scale不同，NLP的scale是标准固定的，而CV的scale变化范围非常大。(2) CV比起NLP需要更大的分辨率，而且CV中使用Transformer的计算复杂度是图像尺度的平方，这会导致计算量过于庞大。为了解决这两个问题，Swin Transformer相比之前的ViT做了两个改进：1.引入CNN中常用的层次化构建方式构建层次化Transformer 2.引入locality思想，对无重合的window区域内进行self-attention计算。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/swin.png&quot; alt=&quot;swin&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="transformer" scheme="https://blog.nicehuster.cn/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>Transformer 在图像领域应用的开拓者VIT</title>
    <link href="https://blog.nicehuster.cn/2021/07/21/vit/"/>
    <id>https://blog.nicehuster.cn/2021/07/21/vit/</id>
    <published>2021-07-21T11:13:39.000Z</published>
    <updated>2022-04-12T04:11:38.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/pdf/2010.11929.pdf" target="_blank" rel="noopener">An image is worth 16x16 words: Transformers for image recognition at scale</a><br><strong>代码链接：</strong><a href="https://github.com/google-research/vision_transformer" target="_blank" rel="noopener">https://github.com/google-research/vision_transformer</a><br><strong>整体信息：</strong>ViT（vision transformer）是Google在2020年提出的直接将transformer应用在图像分类的模型，后面很多的工作都是基于ViT进行改进的。ViT的思路很简单：直接把图像分成固定大小的patchs，然后通过线性变换得到patch embedding，这就类比NLP的words和word embedding，由于transformer的输入就是a sequence of token embeddings，所以将图像的patch embeddings送入transformer后就能够进行特征提取从而分类了。</p><p><img src="/img/arch.png" alt="vit"></p><a id="more"></a><p>ViT模型原理如上图所示，其实ViT模型只是用了transformer的Encoder来提取特征（原始的transformer还有decoder部分，用于实现sequence to sequence，比如机器翻译）。下面将分别对各个部分做详细的介绍。</p><h3 id="1-Patch-Embedding"><a href="#1-Patch-Embedding" class="headerlink" title="1.Patch Embedding"></a>1.Patch Embedding</h3><p>对于ViT来说，首先要将原始patch形式的2-D图像转换成一系列1-D的patch embeddings，这就好似NLP中的word embedding。输入的2-D图像记为$\mathbf{x} \in \mathbb{R}^{H \times W \times C}$,其中$H$和$W$分别是图像的高和宽，而$C$为通道数，对于RGB图像为3。如果将图像分为大小为$P \times P$的patchs,可以通过reshape等操作得到一系列patchs：$\mathbf{x}_{p} \in \mathbb{R}^{N \times\left(P^{2} \cdot C\right)}$ ,总共可以得到的patch数是$N=H W / P^{2}$,这个就是序列的长度。注意这里直接将patch拉平为1-D，其特征大小为$P^{2} \cdot C$ .然后通过一个简单的线性变换将patchs映射成D大小的维度，这就是patch embeddings:$\mathbf{x}_{\mathbf{p}}^{\prime} \in \mathbb{R}^{N \times D}$, 在实现上等同于对于x进行一个$P \times P$ 且stride为$P$ 的卷积操作。下面是具体的实现代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PatchEmbed</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" Image to Patch Embedding</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_chans=<span class="number">3</span>, embed_dim=<span class="number">768</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        img_size = to_2tuple(img_size)</span><br><span class="line">        patch_size = to_2tuple(patch_size)</span><br><span class="line">        num_patches = (img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>]) * (img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>])</span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.num_patches = num_patches</span><br><span class="line"></span><br><span class="line">        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        <span class="comment"># FIXME look at relaxing size constraints</span></span><br><span class="line">        <span class="keyword">assert</span> H == self.img_size[<span class="number">0</span>] <span class="keyword">and</span> W == self.img_size[<span class="number">1</span>], \</span><br><span class="line">            <span class="string">f"Input image size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) doesn't match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>*<span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>)."</span></span><br><span class="line">        x = self.proj(x).flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="2-Position-Embedding"><a href="#2-Position-Embedding" class="headerlink" title="2. Position Embedding"></a>2. Position Embedding</h3><p>除了patch embeddings，模型还需要另外一个特殊的position embedding。transformer和CNN不同，需要position embedding来编码tokens的位置信息，这主要是因为self-attention是permutation-invariant，即打乱sequence里的tokens的顺序并不会改变结果。如果不给模型提供patch的位置信息，那么模型就需要通过patchs的语义来学习拼图，这就额外增加了学习成本。ViT论文中对比了几种不同的position embedding方案(如下），最后发现如果不提供positional embedding效果会差，但其它各种类型的positional embedding效果都接近，这主要是因为ViT的输入是相对较大的patchs而不是pixels，所以学习位置信息相对容易很多。</p><ul><li>无positional embedding</li><li>1-D positional embedding：把2-D的patchs看成1-D序列</li><li>2-D positional embedding：考虑patchs的2-D位置（x, y）</li><li>Relative positional embeddings：patchs的相对位置</li></ul><p>在ViT中默认采用学习（训练的）的1-D positional embedding，在输入transformer的encoder之前直接将patch embeddings和positional embedding相加:</p><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这里多1是为了后面要说的class token，embed_dim即patch embed_dim</span></span><br><span class="line"><span class="keyword">self</span>.pos_embed = nn.Parameter(torch.zeros(<span class="number">1</span>, num_patches + <span class="number">1</span>, embed_dim)) </span><br><span class="line"></span><br><span class="line"><span class="comment"># patch emded + pos_embed</span></span><br><span class="line">x = x + <span class="keyword">self</span>.pos_embed</span><br></pre></td></tr></table></figure><p>论文中也对学习到的positional embedding进行了可视化，发现相近的patchs的positional embedding比较相似，而且同行或同列的positional embedding也相近：</p><p><img src="/img/position_embedding.png" alt="vit"></p><p>这里额外要注意的一点，如果改变图像的输入大小，ViT不会改变patchs的大小，那patch的数量$N$也会发生变化，那么之前学习的pos_embed就维度对不上了，ViT采用的方案是通过插值来解决这个问题。但是这种情形一般会造成性能少许损失，可以通过finetune模型来解决。另外最新的论文<a href="https://arxiv.org/pdf/2102.10882.pdf" target="_blank" rel="noopener">CPVT</a>通过implicit Conditional Position encoding来解决这个问题（插入Conv来隐式编码位置信息，zero padding让Conv学习到绝对位置信息）。</p><h3 id="3-Class-Token"><a href="#3-Class-Token" class="headerlink" title="3. Class Token"></a>3. Class Token</h3><p>除了patch token，ViT借鉴BERT还增加了一个特殊的class token。后面会说，transformer的encoder输入是a sequence patch embeddings，输出也是同样长度的a sequence patch features，但图像分类最后需要获取image feature，简单的策略是采用pooling，比如求patch features的平均来获取image feature，但是ViT并没有采用类似的pooling策略，而是直接增加一个特殊的class token，其最后输出的特征加一个linear classifier就可以实现对图像的分类（ViT的pre-training时是接一个MLP head），所以输入ViT的sequence长度是$N+1$,class token对应的embedding在训练时随机初始化，然后通过训练得到，具体实现如下：</p><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机初始化</span></span><br><span class="line">self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Classifier head</span></span><br><span class="line">self.head = nn.Linear(self.num_features, num_classes) if num_classes &gt; 0 <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 具体forward过程</span></span><br><span class="line">B = x.shape[0]</span><br><span class="line">x = self.patch_embed(x)</span><br><span class="line">cls_tokens = self.cls_token.expand(B, -1, -1)  <span class="comment"># stole cls_tokens impl from Phil Wang, thanks</span></span><br><span class="line">x = torch.cat((cls_tokens, x), dim=1)</span><br><span class="line">x = x + self.pos_embed</span><br></pre></td></tr></table></figure><h3 id="4-Transformer-Encoder"><a href="#4-Transformer-Encoder" class="headerlink" title="4. Transformer Encoder"></a>4. Transformer Encoder</h3><p>transformer最核心的操作就是self-attention，其实attention机制很早就在NLP和CV领域应用了，比如带有attention机制的seq2seq模型，但是transformer完全摒弃RNN或LSTM结构，直接采用attention机制反而取得了更好的效果：attention is all you need！简单来说，attention就是根据当前查询对输入信息赋予不同的权重来聚合信息，从操作上看就是一种“加权平均”。attention中共有3个概念：query, key和value，其中key和value是成对的，对于一个给定的query向量$q \in \mathbb{R}^{d}$,通过计算内积来匹配k个key向量(维度也是d,$K \in \mathbb{R}^{k \times d}$),得到的内积通过softmax来归一化得到k个权重，那么对于query其attention的输出就是k个key向量对应的value向量（即矩阵$V \in \mathbb{R}^{k \times d}$),对于一系列的N个query(即矩阵$Q \in \mathbb{R}^{N \times d}$)，可以通过矩阵计算它们的attention输出：</p><script type="math/tex; mode=display">\operatorname{Attention}(Q, K, V)=\operatorname{Softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><p>这里的$\sqrt{d_{k}}$为缩放因子以避免点击带来的方差影响。上述的Attention机制称为<strong>Scaled dot product attention</strong>，其实attention机制的变种有很多，但基本原理是相似的。如果$Q,K,V$都是从一个包含$N$个向量的sequence($X \in \mathbb{R}^{N \times D}$)变换得到：</p><script type="math/tex; mode=display">Q=X W_{Q}, K=X W_{K}, V=X W_{V}</script><p>那么此时就变成了<strong>self-attention</strong>，这个时候就有N个(key,value)对,self-attention是transformer最核心部分，self-attention其实就是输入向量之间进行相互attention来学习到新特征。前面说过我们已经得到图像的patch sequence，那么送入self-attention就能到同样size的sequence输出，只不过特征改变了。更进一步，transformer采用的是<strong>multi-head self-attention (MSA）</strong>，所谓的MSA就是采用定义h个attention heads，即采用h个self-attention应用在输入sequence上，在操作上可以将sequence拆分成h个size为$N \times d$，这里$D=h d$,不同的heads得到的输出concat在一起然后通过线性变换得到最终的输出,size也是$N \times D$:</p><script type="math/tex; mode=display">\operatorname{MSA}(\mathbf{z})=\left[\mathrm{SA}_{1}(z) ; \mathrm{SA}_{2}(z) ; \cdots ; \mathrm{SA}_{k}(z)\right] \mathrm{U}_{m s a} \quad \mathrm{U}_{m s a} \in \mathbb{R}^{k \cdot D_{h} \times D}</script><p>MSA的计算量是和$N^{2}$成比例的，所以ViT的输入是patch embeddings，而不是pixel embeddings，这有计算量上的考虑。在实现上，MSA是可以并行计算各个head的，具体代码如下：</p><figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">Attention</span>(<span class="title">nn</span>.<span class="type">Module</span>):</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>, <span class="title">dim</span>, <span class="title">num_heads</span>=8, <span class="title">qkv_bias</span>=<span class="type">False</span>, <span class="title">qk_scale</span>=<span class="type">None</span>, <span class="title">attn_drop</span>=0., <span class="title">proj_drop</span>=0.):</span></span><br><span class="line"><span class="class">        super().__init__()</span></span><br><span class="line"><span class="class">        self.num_heads = num_heads</span></span><br><span class="line"><span class="class">        head_dim = dim // num_heads</span></span><br><span class="line"><span class="class">    </span></span><br><span class="line"><span class="class">        self.scale = qk_scale or head_dim ** -0.5</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        self.qkv = nn.<span class="type">Linear</span>(<span class="title">dim</span>, <span class="title">dim</span> * 3, <span class="title">bias</span>=<span class="title">qkv_bias</span>)</span></span><br><span class="line"><span class="class">        self.attn_drop = nn.<span class="type">Dropout</span>(<span class="title">attn_drop</span>)</span></span><br><span class="line"><span class="class">        self.proj = nn.<span class="type">Linear</span>(<span class="title">dim</span>, <span class="title">dim</span>)</span></span><br><span class="line"><span class="class">        # 这里包含了dropout</span></span><br><span class="line"><span class="class">        self.proj_drop = nn.<span class="type">Dropout</span>(<span class="title">proj_drop</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>, <span class="title">x</span>):</span></span><br><span class="line"><span class="class">        <span class="type">B</span>, <span class="type">N</span>, <span class="type">C</span> = x.shape</span></span><br><span class="line"><span class="class">        qkv = self.qkv(<span class="title">x</span>).reshape(<span class="type">B</span>, <span class="type">N</span>, 3, <span class="title">self</span>.<span class="title">num_heads</span>, <span class="type">C</span> // <span class="title">self</span>.<span class="title">num_heads</span>).permute(2, 0, 3, 1, 4)</span></span><br><span class="line"><span class="class">        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (<span class="title">cannot</span> <span class="title">use</span> <span class="title">tensor</span> <span class="title">as</span> <span class="title">tuple</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        attn = (<span class="title">q</span> @ <span class="title">k</span>.<span class="title">transpose</span>(-2, -1)) * self.scale</span></span><br><span class="line"><span class="class">        attn = attn.softmax(<span class="title">dim</span>=-1)</span></span><br><span class="line"><span class="class">        attn = self.attn_drop(<span class="title">attn</span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">        x = (<span class="title">attn</span> @ <span class="title">v</span>).transpose(1, 2).reshape(<span class="type">B</span>, <span class="type">N</span>, <span class="type">C</span>)</span></span><br><span class="line"><span class="class">        x = self.proj(<span class="title">x</span>)</span></span><br><span class="line"><span class="class">        x = self.proj_drop(<span class="title">x</span>)</span></span><br><span class="line"><span class="class">        return x</span></span><br></pre></td></tr></table></figure><p>在transformer中，MSA后跟一个FFN（Feed-forward network），这个FFN包含两个FC层，第一个FC层将特征从维度D变换成4D，后一个FC层将特征从维度4D变成D，中间的非线性激活函数采用GeLU，其实这就是一个MLP，具体实现如下：</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Mlp</span>(<span class="title">nn</span>.<span class="title">Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(<span class="keyword">self</span>, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=<span class="number">0</span>.)</span></span><span class="symbol">:</span></span><br><span class="line">        <span class="keyword">super</span>().__init_<span class="number">_</span>()</span><br><span class="line">        out_features = out_features <span class="keyword">or</span> in_features</span><br><span class="line">        hidden_features = hidden_features <span class="keyword">or</span> in_features</span><br><span class="line">        <span class="keyword">self</span>.fc1 = nn.Linear(in_features, hidden_features)</span><br><span class="line">        <span class="keyword">self</span>.act = act_layer()</span><br><span class="line">        <span class="keyword">self</span>.fc2 = nn.Linear(hidden_features, out_features)</span><br><span class="line">        <span class="keyword">self</span>.drop = nn.Dropout(drop)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(<span class="keyword">self</span>, x)</span></span><span class="symbol">:</span></span><br><span class="line">        x = <span class="keyword">self</span>.fc1(x)</span><br><span class="line">        x = <span class="keyword">self</span>.act(x)</span><br><span class="line">        x = <span class="keyword">self</span>.drop(x)</span><br><span class="line">        x = <span class="keyword">self</span>.fc2(x)</span><br><span class="line">        x = <span class="keyword">self</span>.drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>那么一个完成transformer encoder block就包含一个MSA后面接一个FFN，其实MSA和FFN均包含和ResNet一样的skip connection，另外MSA和FFN后面都包含layer norm层，具体实现如下：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class Block(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, dim, num_heads, <span class="attribute">mlp_ratio</span>=4., <span class="attribute">qkv_bias</span>=<span class="literal">False</span>, <span class="attribute">qk_scale</span>=None, <span class="attribute">drop</span>=0., <span class="attribute">attn_drop</span>=0.,</span><br><span class="line">                 <span class="attribute">drop_path</span>=0., <span class="attribute">act_layer</span>=nn.GELU, <span class="attribute">norm_layer</span>=nn.LayerNorm):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.norm1 = norm_layer(dim)</span><br><span class="line">        self.attn = Attention(</span><br><span class="line">            dim, <span class="attribute">num_heads</span>=num_heads, <span class="attribute">qkv_bias</span>=qkv_bias, <span class="attribute">qk_scale</span>=qk_scale, <span class="attribute">attn_drop</span>=attn_drop, <span class="attribute">proj_drop</span>=drop)</span><br><span class="line">        # NOTE: drop path <span class="keyword">for</span> stochastic depth, we shall see <span class="keyword">if</span> this is better than dropout here</span><br><span class="line">        self.drop_path = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; 0. <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.norm2 = norm_layer(dim)</span><br><span class="line">        mlp_hidden_dim = int(dim * mlp_ratio)</span><br><span class="line">        self.mlp = Mlp(<span class="attribute">in_features</span>=dim, <span class="attribute">hidden_features</span>=mlp_hidden_dim, <span class="attribute">act_layer</span>=act_layer, <span class="attribute">drop</span>=drop)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = x + self.drop_path(self.attn(self.norm1(x)))</span><br><span class="line">        x = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure><h3 id="5-ViT"><a href="#5-ViT" class="headerlink" title="5. ViT"></a>5. ViT</h3><p>对于ViT模型来说，就类似CNN那样，不断堆积transformer encoder blocks，最后提取class token对应的特征用于图像分类，论文中也给出了模型的公式表达，其中（1）就是提取图像的patch embeddings，然后和class token对应的embedding拼接在一起并加上positional embedding；（2）是MSA，而（3）是MLP，（2）和（3）共同组成了一个transformer encoder block，共有L层；（4)是对class token对应的输出做layer norm，然后就可以用来图像分类。</p><p><img src="/img/vit-procedu.png" alt="vit"></p><p>ViT模型的超参数主要包括以下，这些超参数直接影响模型参数以及计算量：</p><blockquote><ol><li>Layers：block的数量；</li><li>Hidden size D：隐含层特征，D在各个block是一直不变的；</li><li>MLP size：一般设置为4D大小；</li><li>Heads：MSA中的heads数量；</li><li>Patch size：模型输入的patch size，ViT中共有两个设置：14x14和16x16，这个只影响计算量；</li></ol></blockquote><p>类似BERT，ViT共定义了3中不同大小的模型：Base，Large和Huge，其对应的模型参数不同，如下所示。如ViT-L/16指的是采用Large结构，输入的patch size为16x16。类似BERT，ViT共定义了3中不同大小的模型：Base，Large和Huge，其对应的模型参数不同，如下所示。如ViT-L/16指的是采用Large结构，输入的patch size为16x16。</p><p><img src="/img/models.png" alt="vit"></p><h3 id="6-Experiments"><a href="#6-Experiments" class="headerlink" title="6. Experiments"></a>6. Experiments</h3><p>ViT并不像CNN那样具有inductive bias，论文中发现如果如果直接在ImageNet上训练，同level的ViT模型效果要差于ResNet，但是如果在比较大的数据集上petraining，然后再finetune，效果可以超越ResNet。比如ViT在Google私有的300M JFT数据集上pretrain后，在ImageNet上的最好Top-1 acc可达88.55%，这已经和ImageNet上的SOTA相当了（Noisy Student EfficientNet-L2效果为88.5%，Google最新的SOTA是Meta Pseudo Labels，效果可达90.2%）：</p><p><img src="/img/exp.png" alt="vit"></p><p>那么ViT至少需要多大的数据量才能和CNN旗鼓相当呢？这个论文也做了实验，结果如下图所示，从图上所示这个预训练所使用的数据量要达到100M时才能显示ViT的优势。transformer的一个特色是它的scalability：当模型和数据量提升时，性能持续提升。在大数据面前，ViT可能会发挥更大的优势。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/pdf/2010.11929.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;An image is worth 16x16 words: Transformers for image recognition at scale&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/google-research/vision_transformer&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/google-research/vision_transformer&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;ViT（vision transformer）是Google在2020年提出的直接将transformer应用在图像分类的模型，后面很多的工作都是基于ViT进行改进的。ViT的思路很简单：直接把图像分成固定大小的patchs，然后通过线性变换得到patch embedding，这就类比NLP的words和word embedding，由于transformer的输入就是a sequence of token embeddings，所以将图像的patch embeddings送入transformer后就能够进行特征提取从而分类了。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/arch.png&quot; alt=&quot;vit&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="transformer" scheme="https://blog.nicehuster.cn/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>unbiased teacher-半监督目标检测</title>
    <link href="https://blog.nicehuster.cn/2021/06/05/unbiased-teacher/"/>
    <id>https://blog.nicehuster.cn/2021/06/05/unbiased-teacher/</id>
    <published>2021-06-05T11:13:39.000Z</published>
    <updated>2022-04-24T07:41:56.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/abs/2102.09480" target="_blank" rel="noopener">Unbiased Teacher for Semi-Supervised Object Detection</a><br><strong>代码链接：</strong><a href="https://github.com/facebookresearch/unbiased-teacher" target="_blank" rel="noopener">https://github.com/facebookresearch/unbiased-teacher</a><br><strong>整体信息：</strong>这是facebook research在半监督目标检测上的工作。以往的半监督学习多聚焦与分类任务上，在检测任务上少有涉及。解决半监督目标检测的一个直接方法是应用半监督分类方法，但是由于目标检测的特性，图像分类方法并不适合，主要原因在于目标检测任务中的类别不平衡严重阻碍了伪标签的使用。</p><p><img src="/img/unbiased-teacher-ssod.png" alt="img"></p><a id="more"></a><p><img src="/img/unbiased-teacher-overfit.png" alt="img"></p><p><strong>目标检测中存在前景背景不平衡和前景之间类不平衡，这些不平衡导致半监督训练中产生有偏差的预测</strong>。伪标签作为图像分类半监督中最有效的方法之一，在目标检测中因为倾向置信度高的类别和区域导致产生偏差。<strong>将偏差伪标签加入训练会加重类不平衡问题和产生过拟合</strong>。如上图所示。</p><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p><img src="/img/unbiased-teacher-pipeline.png" alt="img"></p><p>如上图所示，Unbiased Teacher框架包含俩个部分，Burn-In 和 Teacher-Student Mutual Learning。在Burn-In 阶段，使用所有有标注数据训练目标检测器。在第二阶段，我们先将Burn-In训练好的检测器复制给两个模型（Teacher和Student）。第二阶段的目标是通过一种共同学习的机制来更新Teacher和Student模型，Teacher模型产生伪标签用于训练Student模型，然后student模型学到的知识再用于更新teacher模型，因此用于训练student模型的伪标签本身也在自我更新。</p><h4 id="1-Burn-In"><a href="#1-Burn-In" class="headerlink" title="1. Burn In"></a>1. Burn In</h4><p>该阶段使用所有标注数据来训练检测模型，使用$L_{sup}$ 优化模型参数：</p><script type="math/tex; mode=display">\mathcal{L}_{s u p}=\sum_{i} \mathcal{L}_{\text {cls }}^{r p n}\left(\boldsymbol{x}_{i}^{s}, \boldsymbol{y}_{i}^{s}\right)+\mathcal{L}_{r e g}^{r p n}\left(\boldsymbol{x}_{i}^{s}, \boldsymbol{y}_{i}^{s}\right)+\mathcal{L}_{\text {cls }}^{r o i}\left(\boldsymbol{x}_{i}^{s}, \boldsymbol{y}_{i}^{s}\right)+\mathcal{L}_{r e g}^{r o i}\left(\boldsymbol{x}_{i}^{s}, \boldsymbol{y}_{i}^{s}\right)</script><p>在作者的appendix有验证burn-in阶段的必要性，在早期阶段，burn-in可以使模型获得更准确的伪标签，而且可以加快模型收敛。</p><h4 id="2-Teacher-Student-Mutual-Learning"><a href="#2-Teacher-Student-Mutual-Learning" class="headerlink" title="2. Teacher-Student Mutual Learning"></a>2. Teacher-Student Mutual Learning</h4><p>teacher模型生成伪标签用于训练student模型，student模型的权重转移给teacher模型以更新teacher模型，在teacher和student的迭代训练中提高了检测准确率。随着检测准确率的提高，teacher产生更为准确和稳定的伪标签，这是性能改善的关键。我们也可以将teacher视为student在不同的时间步里进行temporal ensemble所得，这与我们的研究一致，即teacher的准确率高于student。与先前的研究相比，<strong>改善teacher的关键因素就是student模型的多样性</strong>，所以我们采用强增强图片作为student的输入，使用弱增强图片作为teacher的输入以提供可信赖的伪标签.</p><h5 id="Student-Learning-with-Pseudo-Labeling"><a href="#Student-Learning-with-Pseudo-Labeling" class="headerlink" title="Student Learning with Pseudo-Labeling"></a>Student Learning with Pseudo-Labeling</h5><p>我们首先设置一个置信度阈值 $\delta$，用于过滤低置信度的预测框，这些低置信度的预测框极有可能是错误的正样本。此外，含噪的伪标签会涌向teacher模型，所以我们将teacher和student分开训练。在获得伪标签后只有student的权重可以通过反向传播更新:</p><script type="math/tex; mode=display">\theta_{s} \leftarrow \theta_{s}+\gamma \frac{\partial\left(\mathcal{L}_{s u p}+\boldsymbol{\lambda}_{u} \mathcal{L}_{\text {unsup }}\right)}{\partial \theta_{s}}, \quad \mathcal{L}_{\text {unsup }}=\sum_{i} \mathcal{L}_{c l s}^{r p n}\left(\boldsymbol{x}_{i}^{u}, \hat{\boldsymbol{y}}_{i}^{u}\right)+\mathcal{L}_{c l s}^{r o i}\left(\boldsymbol{x}_{i}^{u}, \hat{\boldsymbol{y}}_{i}^{u}\right)</script><h5 id="Teacher-Refinement-via-Exponential-Moving-Average"><a href="#Teacher-Refinement-via-Exponential-Moving-Average" class="headerlink" title="Teacher Refinement via Exponential Moving Average"></a>Teacher Refinement via Exponential Moving Average</h5><p>为了获得稳定的伪标签，我们应用EMA来逐步更新teacher模型。</p><script type="math/tex; mode=display">\theta_{t} \leftarrow \alpha \theta_{t}+(1-\alpha) \theta_{s}</script><h4 id="3-Bias-in-pseudo-label"><a href="#3-Bias-in-pseudo-label" class="headerlink" title="3. Bias in pseudo-label"></a>3. Bias in pseudo-label</h4><p>理论上来说，基于伪标签的方法可以解决由于标签匮乏带来的问题，但是目标检测任务中的不平衡属性影响了该方法的有效性。目标检测中存在前景-背景不平衡和前景中类不平衡问题，如果在训练数据不充足的情况下使用标准CE，模型会倾向于预测主要类别，这会导致预测偏向数量较多的类别,生成类不平衡的伪标签(偏差伪标签)。在训练时使用偏差伪标签会使不平衡预测问题恶化。</p><p>为了解决这个问题，我们考虑一种简单但是高效的方法，在ROIhead的多类别分类loss中，我们使用Focal loss代替CE loss。Focal loss将给置信度较低的目标实例分配更多的loss权重，这样模型会看重hard目标而不是那些极有可能是主要类别的简单目标。</p><p>另一方面，采用EMA机制可以防止决策边界急剧的向少数类别倾斜，teacher模型的权重可表示为</p><script type="math/tex; mode=display">\theta_{t}^{i}=\hat{\theta}-\gamma \sum_{k=1}^{i-1}\left(1-\alpha^{-k+(i-1)}\right) \frac{\partial\left(\mathcal{L}_{s u p}+\boldsymbol{\lambda}_{u} \mathcal{L}_{u n s u p}\right)}{\partial \theta_{s}^{k}}</script><p>EMA-training使得Teacher模型有利于产生更稳定的伪标签，解决SS-OD中的类失衡问题。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>以FasterRCNN作为目标检测器，置信度阈值$\delta = 0.7$；<strong>弱增强</strong>：随机水平翻转flip；强增强：color jitter, grayscale,gaussian blur, cutout等等。</p><h4 id="COCO-standard"><a href="#COCO-standard" class="headerlink" title="COCO-standard"></a>COCO-standard</h4><p><img src="/img/unbiased-teacher-coco-standard.png" alt="img"></p><p>从上述表格可以看出，有标签数据越少，本文提出的方法改善越大。在作者看来，主要原因在于：（1）伪标签更准确；（2）EMA和Focal loss有效地解决伪标签类失衡问题；</p><h4 id="COCO-additional-and-VOC"><a href="#COCO-additional-and-VOC" class="headerlink" title="COCO-additional and VOC"></a>COCO-additional and VOC</h4><p>作者还有进一步验证在100%标注数据下，使用额外无标注数据能否进一步提升模型性能。</p><p><img src="/img/unbiased-teacher-coco-addition.png" alt="img"></p><h4 id="ablation-study"><a href="#ablation-study" class="headerlink" title="ablation study"></a>ablation study</h4><p><strong>EMA:</strong> teacher和student同步更新，目前半监督图像分类中最优的模型Fixmatch也是同步更新。</p><p><strong>Focal loss：</strong>有使用Focal loss的模型产生的伪标签分布与真是标签分布更相似，将KL散度从1.7915（无EMA无Focal loss）改善到0.2001（无EMA有Focal loss）</p><p><img src="/img/unbiased-teacher-ablation-study.png" alt="img"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这篇文章分析了直接将半监督分类方法应用与目标检测任务时存在的俩个问题：类失衡和过拟合，并提出了unbiased-teacher，简单高效的解决了上述俩个问题。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2102.09480&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Unbiased Teacher for Semi-Supervised Object Detection&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/facebookresearch/unbiased-teacher&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/facebookresearch/unbiased-teacher&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这是facebook research在半监督目标检测上的工作。以往的半监督学习多聚焦与分类任务上，在检测任务上少有涉及。解决半监督目标检测的一个直接方法是应用半监督分类方法，但是由于目标检测的特性，图像分类方法并不适合，主要原因在于目标检测任务中的类别不平衡严重阻碍了伪标签的使用。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/unbiased-teacher-ssod.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="semi-supervised learning" scheme="https://blog.nicehuster.cn/tags/semi-supervised-learning/"/>
    
  </entry>
  
  <entry>
    <title>fixmatch-基于数据增强实现半监督学习</title>
    <link href="https://blog.nicehuster.cn/2021/06/04/fixmatch/"/>
    <id>https://blog.nicehuster.cn/2021/06/04/fixmatch/</id>
    <published>2021-06-04T11:13:39.000Z</published>
    <updated>2022-04-24T07:42:12.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong><a href="https://arxiv.org/abs/2001.07685" target="_blank" rel="noopener">FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence</a><br><strong>代码链接：</strong><a href="https://github.com/google-research/fixmatch" target="_blank" rel="noopener">https://github.com/google-research/fixmatch</a><br><strong>整体信息：</strong>这在实际业务场景中，大量的标注数据对于模型性能的提升至关重要，但是获取标注数据是一个耗时耗力的过程，例如工业场景中由于机台型号的更换导致模型性能下降，花费大量时间对新数据进行重新标注大概率会导致模型上线时间delay，而半监督学习(Semi-Supervised Learning, SSL)探究了如何利用大量未标注数据和部分标注数据来提升模型性能。而本文，谷歌提出的fixmatch，是对现有SSL方法进行显著简化的算法。</p><a id="more"></a><p><img src="/img/fixmatch.jpg" alt="preview"></p><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>如上图所示，训练过程包括两个部分，有监督训练和无监督训练。有label的数据，执行有监督训练，和普通分类任务训练没有区别。没有label的数据，经过首先经过弱增强获取伪标签。然后利用该伪标签去监督强增强的输出值，只有大于一定阈值条件才执行伪标签的生成。无监督的训练过程包含两种思想在里面，即一致性正则化和伪标签训练。</p><h4 id="1-一致性正则"><a href="#1-一致性正则" class="headerlink" title="1. 一致性正则"></a>1. 一致性正则</h4><p>一致性正则化是当前半监督SOTA工作中一个重要的组件，其建立在一个基本假设：<strong>相同图片经过不同扰动（增强）经过网络会输出相同预测结果</strong>，因此对这二者进行loss计算便可以对网络进行监督训练，又被称为自监督训练。loss计算如下：</p><script type="math/tex; mode=display">\sum_{b=1}^{\mu B}\left\|p_{\mathrm{m}}\left(y \mid \alpha\left(u_{b}\right)\right)-p_{\mathrm{m}}\left(y \mid \alpha\left(u_{b}\right)\right)\right\|_{2}^{2}</script><p>其中，$\alpha$ 表示随机的弱增强操作。</p><h4 id="2-伪标签"><a href="#2-伪标签" class="headerlink" title="2. 伪标签"></a>2. 伪标签</h4><p>伪标签是利用模型本身为未标记数据获取人工标签的思想。通常是使用“hard”标签，也就是argmax获取的onehot标签，仅保留最大类概率超过阈值的标签。计算loss的时如下：</p><script type="math/tex; mode=display">\frac{1}{\mu B} \sum_{b=1}^{\mu B} \mathbb{1}\left(\max \left(q_{b}\right) \geq \tau\right) \mathrm{H}\left(\hat{q}_{b}, q_{b}\right)</script><p>其中，$\hat{q}_{b}=\arg \max \left(q_{b}\right)$ , $\tau$ 是阈值。</p><h4 id="3-为什么work？"><a href="#3-为什么work？" class="headerlink" title="3.为什么work？"></a>3.为什么work？</h4><p>无监督训练过程实际上是一个孪生网络，可以提取到图片的有用特征。弱增强不至于图像失真，再加上输出伪标签阈值的设置，极大程度上降低了引入错误标签噪声的可能性。而仅仅使用弱增强可能会导致训练过拟合，无法提取到本质的特征，所以使用强增强。强增强带来图片的严重失真，但是依然是保留足够可以辨认类别的特征。有监督和无监督混合训练，逐步提高模型的表达能力。</p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p><img src="/img/fixmatch-algo.png" alt="img"></p><blockquote><p>1.输入：有标签数据，无标签数据，另外需要设定一些超参，包括置信度阈值，无标签采样比例，loss权重。</p><p>2.对有标签数据进行监督训练，使Cross-Entropy loss；</p><p>3.遍历无标签数据，利用弱增强获取伪标签；</p><p>4.利用获取的伪标签对无标签数据进行训练，同样利用Cross-Entropy loss；</p><p>5.基于loss权重，对俩者loss进行融合；</p></blockquote><h3 id="loss设计"><a href="#loss设计" class="headerlink" title="loss设计"></a>loss设计</h3><p>loss包含俩部分：有标注数据的监督训练$L_s$和无标注数据的伪标签监督训练$L_u$。</p><script type="math/tex; mode=display">\ell_{s}=\frac{1}{B} \sum_{b=1}^{B} \mathrm{H}\left(p_{b}, p_{\mathrm{m}}\left(y \mid \alpha\left(x_{b}\right)\right)\right)</script><script type="math/tex; mode=display">\ell_{u}=\frac{1}{\mu B} \sum_{b=1}^{\mu B} \mathbb{1}\left(\max \left(q_{b}\right) \geq \tau\right) \mathrm{H}\left(\hat{q}_{b}, p_{\mathrm{m}}\left(y \mid \mathcal{A}\left(u_{b}\right)\right)\right)</script><p>其中，$\alpha(.)$ 表示弱增强，一般为flip翻转，shift平移；$\mathcal{A}$(.)为强增强，一般为颜色变换，对比度增强等等。</p><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>在fixmatch中包含俩种数据增强：weak aug和strong aug. weak aug为标准的flip-and-shift增强策略，50%的概率进行flip和12.5%的概率进行shift，包括水平和竖直方向。对于strong aug，论文主要应用RandAugment和CTAugment两种策略，都是为提高模型表现而提出的增强策略。</p><p><img src="/img/fixmatch-aug.png" alt="img"></p><p>对于RandAugment：(1)从这个列表里随机选出N个增强，例如N为2；(2)然后选择一个随机的幅度M，例如50%之类的；（3）将所选的增强应用于图像，每种增强都有50％的可能性被应用。</p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p>作者在CIFRAR，SVHN，STL数据集上做了详尽的实验，从实验结果来看，均优于以前的方法。</p><p><img src="/img/fixmatch-exp.png" alt="img"></p><p>在CIFAR-10和SVHN上选用的是Wide ResNet-28-2模型， CIFAR-100选用的是WRN-28-8，STL-10选用的是WRN-37-2。在每个类只有四张图片的情况下，fixmatch明显优于其他方法。</p><p>对于极端缺少标注的场景，仅仅使用每个类别使用1张图片，共10张标注的图片就可以达到78%的最大accuracy，当然这种做法和挑选的样本质量有关，作者也做了相关实验论证。不过也证明本文的方法的确work。</p><p><img src="/img/fixmatch-cifar.png" alt="img"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>fixmatch是SSL领域的一篇经典论文，做法简单有效，利用少量的标注图片就可以达到一个不错的效果，这对于获取标注困难的场景非常有意义。很值得在业务场景试一下。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt;&lt;a href=&quot;https://arxiv.org/abs/2001.07685&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/google-research/fixmatch&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/google-research/fixmatch&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这在实际业务场景中，大量的标注数据对于模型性能的提升至关重要，但是获取标注数据是一个耗时耗力的过程，例如工业场景中由于机台型号的更换导致模型性能下降，花费大量时间对新数据进行重新标注大概率会导致模型上线时间delay，而半监督学习(Semi-Supervised Learning, SSL)探究了如何利用大量未标注数据和部分标注数据来提升模型性能。而本文，谷歌提出的fixmatch，是对现有SSL方法进行显著简化的算法。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="semi-supervised learning" scheme="https://blog.nicehuster.cn/tags/semi-supervised-learning/"/>
    
  </entry>
  
  <entry>
    <title>目标检测上的自监督学习</title>
    <link href="https://blog.nicehuster.cn/2021/05/30/InsLoc/"/>
    <id>https://blog.nicehuster.cn/2021/05/30/InsLoc/</id>
    <published>2021-05-30T11:13:39.000Z</published>
    <updated>2022-04-24T04:06:08.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/abs/2102.08318" target="_blank" rel="noopener">Instance Localization for Self-supervised Detection Pretraining</a><br><strong>代码链接：</strong><a href="https://github.com/limbo0000/InstanceLoc" target="_blank" rel="noopener">https://github.com/limbo0000/InstanceLoc</a><br><strong>整体信息：</strong>这是被CVPR2021的文章，现有的自监督学习方法极大地提升了在图像分类任务上的指标上限，然而在目标检测任务上的迁移性能却与分类任务不一致。论文将这种迁移学习中存在的misalignment问题归咎为:(1)预训练网络结构和目标检测网络结构不一致；(2)现有的对比学习自监督方式是一个分类问题，没有对位置建模。</p><a id="more"></a><p><img src="/img/ins-loc.png" alt="img"></p><p> 方法很直观，将同一个Instance随机crop得到patch，随机粘贴到不同的背景中，使用对比学习进行自监督学习。其中对比学习框架上是按照MoCo来的，其中包括queue设计、动量更新等。</p><h4 id="Bounding-Box-Augmentation"><a href="#Bounding-Box-Augmentation" class="headerlink" title="Bounding-Box Augmentation"></a>Bounding-Box Augmentation</h4><p>在对比学习中，通常需要构建俩个不同view下的输入，在InsLoc这篇论文中，采用的是anchor的方式生成的。</p><p><img src="/img/ins-loc-box-aug.png" alt="img"></p><p>给定gt box，可以计算所有anchors和gt的iou，选取IOU&gt;0.5的anchor作为aug box构造输入对，进行对比学习。</p><script type="math/tex; mode=display">\begin{aligned}v_{q}^{\prime} &=\operatorname{RoIAlign}\left(f\left(I_{q}^{\prime}\right), b_{q}\right) \\v_{k_{+}}^{\prime} &=\operatorname{RoIAlign}\left(f\left(I_{k_{+}}^{\prime}\right), b_{k_{+}}\right)\end{aligned}</script><p>gt box和aug box经过roi align之后得到roi特征，再经由一个俩层的MLP层得到最终特征表示，用于后续的contrastive learning。</p><h4 id="Experients"><a href="#Experients" class="headerlink" title="Experients"></a>Experients</h4><p><img src="/img/ins-loc-ap-acc.png" alt="img"></p><p>从上图可以看出，相比于其他方法，insloc牺牲了很多分类上的acc，但都贡献在检测的ap上。</p><p><img src="/img/ins-loc-loc-eval.png" alt="img"></p><p>在论文中还提到如何评估自监督方法是否具有关注定位的能力。<strong>转化为patch块的位置预测。</strong> 如上图所示。下面是对应方法的实验结果。IncLoc对于定位能力是最强。</p><p><img src="/img/ins-loc-sl.png" alt="image-20220424103025560"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2102.08318&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Instance Localization for Self-supervised Detection Pretraining&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/limbo0000/InstanceLoc&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/limbo0000/InstanceLoc&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这是被CVPR2021的文章，现有的自监督学习方法极大地提升了在图像分类任务上的指标上限，然而在目标检测任务上的迁移性能却与分类任务不一致。论文将这种迁移学习中存在的misalignment问题归咎为:(1)预训练网络结构和目标检测网络结构不一致；(2)现有的对比学习自监督方式是一个分类问题，没有对位置建模。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="self-supervised learning" scheme="https://blog.nicehuster.cn/tags/self-supervised-learning/"/>
    
  </entry>
  
  <entry>
    <title>长尾目标检测-Seesaw Loss[转载]</title>
    <link href="https://blog.nicehuster.cn/2021/05/30/seesaw-loss/"/>
    <id>https://blog.nicehuster.cn/2021/05/30/seesaw-loss/</id>
    <published>2021-05-30T11:13:39.000Z</published>
    <updated>2022-04-22T09:06:44.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://arxiv.org/abs/2008.10032" target="_blank" rel="noopener">Seesaw Loss for Long-Tailed Instance Segmentation</a><br><strong>代码链接：</strong><a href="https://github.com/open-mmlab/mmdetection/tree/master/configs/seesaw_loss" target="_blank" rel="noopener">https://github.com/open-mmlab/mmdetection/tree/master/configs/seesaw_loss</a><br><strong>整体信息：</strong>这是 MMDet 团队参加 LVIS 2020 竞赛获得rank1时提出地损失函数，这篇文章指出了限制检测器在长尾分布数据上性能的一个关键原因：施加在尾部类别（tail class上的<strong>正负样本梯度的比例</strong>是不均衡的。因此，我们提出 <strong>Seesaw Loss</strong> <strong>来动态地抑制尾部类别上过量的负样本梯度，同时补充对误分类样本的惩罚</strong>。 Seesaw Loss 显著提升了尾部类别的分类准确率，进而为检测器在长尾数据集上的整体性能带来可观的增益。由于原作在<a href="https://zhuanlan.zhihu.com/p/339126633" target="_blank" rel="noopener">知乎</a>上已有讲解，我就不班门弄斧，直接搬运过来，侵权删。</p><a id="more"></a><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>在长尾分布的数据集中（例如：LVIS），大部分训练样本来自头部类别（head class），而只有少量样本来自尾部类别（tail class）。因此在训练过程中，来自头部类别的样本会对尾部类别施加过量的负样本梯度，淹没了来自尾部类别自身的正样本梯度。这种不平衡的学习过程导致分类器倾向于给予尾部类别很低的响应，以降低训练的loss。如下图所示，我们统计了在 LVIS v1.0 上训练Mask R-CNN过程中，施加在每个类别的分类器上正负样本累计梯度的分布。</p><p><img src="/img/seesaw-distribution.png" alt="img"></p><p>显然，头部类别获得的正负样本梯度比例接近1.0，而越是稀有的尾部类别，其获得的正负样本梯度的比例就越小。由此带来的结果就是分类的准确率随着样本数的减少而急剧下降，进而严重影响了检测器的性能。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>为了方便直观理解，我们可以把正负样本梯度不均衡的问题，类比于一个一边放有较重物体而另一边放有较轻物体的跷跷板（Seesaw），如下图所示。为了平衡这个跷跷板，一个简单可行的方案就是缩短重物一侧跷跷板的臂长，即减少重物的重量在平衡过程中的权重。回到正负样本梯度不均衡的问题，我们提出了 Seesaw Loss 来动态地减少由头部类别施加在尾部类别上过量的负样本梯度的权重，从而达到正负样本梯度相对平衡的效果。</p><p><img src="/img/seesaw-ce.png" alt="img"></p><p>Seesaw Loss的数学表达如下，</p><script type="math/tex; mode=display">\begin{aligned}&L_{\text {seesaw }}(\mathbf{z})=-\sum_{i=1}^{C} y_{i} \log \left(\widehat{\sigma}_{i}\right) \\&\text { with } \widehat{\sigma}_{i}=\frac{e^{z_{i}}}{\sum_{j \neq i}^{C} \mathcal{S}_{i j} e^{z_{j}}+e^{z_{i}}}\end{aligned}</script><p>$y_{i} \in\{0,1\}$ 是one-hot label, $\mathbf{z}=\left[z_{1}, z_{2}, \ldots, z_{C}\right]$ ,是每一类预测的 logit。此时，对于一个第 i类的样本，它施加在第 j类上的负样本梯度为</p><script type="math/tex; mode=display">\frac{\partial L_{\text {seesaw }}(\mathbf{z})}{\partial z_{j}}=\mathcal{S}_{i j} \frac{e^{z_{j}}}{e^{z_{i}}} \widehat{\sigma}_{i}</script><p>这里我们可以发现 $S_{ij}$就像一个平衡系数，通过调节 $S_{ij}$，我们可以达到放大或者缩小第 i类施加在第 j 类上的负样本梯度的效果。这样，我们就可以通过选择合适 $S_{ij}$来达到平衡正负样本梯度的目的。</p><p>在 Seesaw Loss 的设计中，我们考虑了两方面的因素，一方面我们需要考虑类别间样本分布的关系（class-wise），并据此减少头部类别对尾部类别的”惩罚” （负样本梯度）；另一方面，盲目减少对尾部类别的惩罚会增加错误分类的风险，因为部分误分类的样本受到的惩罚变小了，因此对于那些在训练过程中误分类的样本我们需要保证其受到足够的”惩罚”。据此， $S_{ij}$由两项相乘得到，</p><script type="math/tex; mode=display">\mathcal{S}_{i j}=\mathcal{M}_{i j} \cdot \mathcal{C}_{i j}</script><p>$M_{ij}$ <strong>（Mitigation Factor）</strong>用来缓解尾部类别上过量的负样本梯度, $C_{ij}$ (<strong>Compensation Factor）</strong>用来补充那些错误分类样本上的”惩罚”。</p><h4 id="1-Mitigation-Factor"><a href="#1-Mitigation-Factor" class="headerlink" title="1. Mitigation Factor"></a>1. Mitigation Factor</h4><p><img src="/img/seesaw-mitigation.png" alt="img"></p><p>既然正负样本梯度不平衡的问题来自于样本数量的不平衡，那么一种直接有效的办法就是根据不同类别之间样本数量的相对比例来进行调节。在训练过程中，Seesaw Loss在线地统计每一类的累计训练样本数量 $N_{ij}$ 并根据如下公式计算$M_{ij}$ ,</p><script type="math/tex; mode=display">\mathcal{M}_{i j}=\left\{\begin{array}{cc}1, & \text { if } N_{i} \leq N_{j} \\\left(\frac{N_{j}}{N_{i}}\right)^{p}, & \text { if } N_{i}>N_{j}\end{array}\right.</script><p>也就是说当第 i类比第j类出现地 就会自动根据两类之间不平衡的程度来减少第 i类对第j类施加的负样本梯度。此外，我们在线地累计样本数量，而非使用预先统计的数据集样本分布，这样的设计主要是因为一些高级的样本 sampling 方式会改变数据集的分布（例如：repeat factor sampler, class balanced sampler 等)。在这种情况下，预先统计的方式无法反映训练过程中数据的真实分布。</p><h4 id="2-Compensation-Factor"><a href="#2-Compensation-Factor" class="headerlink" title="2. Compensation Factor"></a>2. Compensation Factor</h4><p><img src="/img/seesaw-compensation.png" alt="img"></p><p>为了防止过度减少负样本梯度而带来的分类错误，Seesaw Loss会增加对那些错误分类样本的惩罚。具体来说，如果一个第i 类的样本错误分给了第 j类，Seesaw Loss会根据两类之间的分类置信度的相对比值来适当的增加对第 j类的惩罚。$C_{ij}$的计算如下，</p><script type="math/tex; mode=display">\mathcal{C}_{i j}=\left\{\begin{array}{cc}1, & \text { if } \sigma_{j} \leq \sigma_{i} \\\left(\frac{\sigma_{j}}{\sigma_{i}}\right)^{q}, & \text { if } \sigma_{j}>\sigma_{i}\end{array}\right.</script><h4 id="3-Normalized-Linear-Activation"><a href="#3-Normalized-Linear-Activation" class="headerlink" title="3. Normalized Linear Activation"></a>3. Normalized Linear Activation</h4><p>受到face recognition，few-shot learning等领域的启发，Seesaw Loss在预测分类logit的时候对weight和feature进行了归一化处理，即</p><script type="math/tex; mode=display">\begin{gathered}z=\tau \widetilde{\mathcal{W}}^{T} \tilde{x}+b \\\widetilde{\mathcal{W}}_{i}=\frac{\mathcal{W}_{i}}{\left\|\mathcal{W}_{i}\right\|_{2}}, i \in C, \tilde{x}=\frac{x}{\|x\|_{2}}\end{gathered}</script><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p><img src="/img/seesaw-exp.png" alt="img"></p><p>Seesaw相比于 EQL 和 BAGS 两种专门为 LVIS 数据设计的方法取得了显著的性能优势，在 end-to-end 训练的情况下在 test-dev 上取得高达30.0 AP的精度。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/2008.10032&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Seesaw Loss for Long-Tailed Instance Segmentation&lt;/a&gt;&lt;br&gt;&lt;strong&gt;代码链接：&lt;/strong&gt;&lt;a href=&quot;https://github.com/open-mmlab/mmdetection/tree/master/configs/seesaw_loss&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/open-mmlab/mmdetection/tree/master/configs/seesaw_loss&lt;/a&gt;&lt;br&gt;&lt;strong&gt;整体信息：&lt;/strong&gt;这是 MMDet 团队参加 LVIS 2020 竞赛获得rank1时提出地损失函数，这篇文章指出了限制检测器在长尾分布数据上性能的一个关键原因：施加在尾部类别（tail class上的&lt;strong&gt;正负样本梯度的比例&lt;/strong&gt;是不均衡的。因此，我们提出 &lt;strong&gt;Seesaw Loss&lt;/strong&gt; &lt;strong&gt;来动态地抑制尾部类别上过量的负样本梯度，同时补充对误分类样本的惩罚&lt;/strong&gt;。 Seesaw Loss 显著提升了尾部类别的分类准确率，进而为检测器在长尾数据集上的整体性能带来可观的增益。由于原作在&lt;a href=&quot;https://zhuanlan.zhihu.com/p/339126633&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;知乎&lt;/a&gt;上已有讲解，我就不班门弄斧，直接搬运过来，侵权删。&lt;/p&gt;
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="Long-tailed object detection" scheme="https://blog.nicehuster.cn/tags/Long-tailed-object-detection/"/>
    
  </entry>
  
  <entry>
    <title>VFNet高性能的密集目标检测器</title>
    <link href="https://blog.nicehuster.cn/2021/04/12/vfnet/"/>
    <id>https://blog.nicehuster.cn/2021/04/12/vfnet/</id>
    <published>2021-04-12T11:13:39.000Z</published>
    <updated>2022-04-12T04:03:06.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>论文信息：</strong> <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_VarifocalNet_An_IoU-Aware_Dense_Object_Detector_CVPR_2021_paper.pdf" target="_blank" rel="noopener">VarifocalNet  An IoU-aware Dense Object Detector</a><br><strong>代码链接：</strong><a href="https://github.com/hyz-xmaster/VarifocalNet" target="_blank" rel="noopener">https://github.com/hyz-xmaster/VarifocalNet</a></p><p><img src="/img/vfnet.png" alt="vfnet"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;论文信息：&lt;/strong&gt; &lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_VarifocalNet_An_IoU-Aware_Dense_Object_Detect
      
    
    </summary>
    
      <category term="paper reading" scheme="https://blog.nicehuster.cn/categories/paper-reading/"/>
    
    
      <category term="detection" scheme="https://blog.nicehuster.cn/tags/detection/"/>
    
  </entry>
  
</feed>
