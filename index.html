<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Wenlong Liu</title>
  
  <meta name="author" content="Wenlong Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Wenlong Liu</name>
              </p>
              <p> 
                I am currently working at <a href="https://idea.edu.cn/">The International Digital Economy Academy (IDEA)</a> as Computer Vision Engineer, advised by Prof. <a href="https://www.leizhang.org/"> Lei Zhang </a>. In 2019, I got my master's degree from Huazhong University of Science and Technology.
                I'm primarily interested in researching <strong>object detection and segmentation, multi-modal learning and vector graphic recognition</strong>. 
              </p>
              <p style="text-align:center">
                <a href="mailto:nicehuster@gmail.com">Email</a> &nbsp/&nbsp
                <!-- <a href="files/CV_YongmingRao.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=BTZ4ROsAAAAJ&hl"> Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="./files/Ren_Tianhe_CV.pdf"> Resume</a> &nbsp/&nbsp -->
                <a href="https://github.com/nicehuster"> Github </a>
                <!-- <a href="https://www.zhihu.com/people/shi-zhi-tou-xi-de-yang-guang">ZhiHu</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:50%;max-width:50%" alt="profile photo" src="images/avatar.jpg">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Research</heading></p>
              <p>
                See full list at <a href="https://scholar.google.com/citations?hl=zh-CN&user=BTZ4ROsAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar.</a> Representative papers are shown below.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/spv2.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2407.01928">
              <papertitle>SymPoint Revolutionized: Boosting Panoptic Symbol Spotting with Layer Feature Enhancement</papertitle>
              </a>
              <br>
              <strong> Wenlong Liu</strong>,
              <a href=""> Tianyu Yang</a>,
              <a href=""> Qizhi Yu</a>,
              <a href=""> Lei Zhang</a>,
              <br>
              Tech report, July. 2024
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/pdf/2407.01928">arXiv</a> /
                <a href="https://github.com/nicehuster/SymPointV2">code</a>
              </div>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/gd1.5_framework.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2405.10300">
              <papertitle>Grounding DINO 1.5: Advance the "Edge" of Open-Set Object Detection</papertitle>
              </a>
              <br>
              <a href=""> Tianhe Ren</a>,
              <a href=""> Qing Jiang</a>,
              <a href=""> Shilong Liu</a>,
              <a href=""> Zhaoyang Zeng</a>,
              <strong> Wenlong Liu</strong>,
              <a href=""> Han Gao</a>,
              <a href=""> Hongjie Huang</a>,
              <a href=""> Zhengyu Ma</a>,
              <a href=""> Xiaoke Jiang</a>,
              <a href=""> Yihao Chen</a>,
              <a href=""> Yuda Xiong</a>,
              <a href=""> Hao Zhang</a>,
              <a href=""> Feng Li</a>,
              <a href=""> Peijun Tang</a>,
              <a href=""> Kent Yu</a>,
              <a href=""> Lei Zhang#</a>
              <br>
              Tech report, May. 2024
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://deepdataspace.com/home">project page</a> /
                <a href="https://arxiv.org/abs/2405.10300">arXiv</a> /
                <a href="https://github.com/IDEA-Research/Grounding-DINO-1.5-API">code</a> /
                <a href="https://huggingface.co/spaces/Mountchicken/Grounding-DINO-1.5">online demo</a>
              </div>
              
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/spv1.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2401.10556">
              <papertitle>Symbol as Points: Panoptic Symbol Spotting via Point-based Representation</papertitle>
              </a>
              <br>
              <strong>Wenlong Liu*</strong>,
              <a href=""> Tianyu Yang</a>,
              <a href=""> Yuhan Wang</a>,
              <a href=""> Qizhi Yu</a>,
              <a href=""> Lei Zhang</a>,
              <br>
              Conference on International Conference on Learning Representations (<strong>ICLR</strong>), 2024
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/pdf/2401.10556">arXiv</a> /
                <a href="https://github.com/nicehuster/SymPoint">code</a>
              </div>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/tsf.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2211.00868">
              <papertitle>tSF: Transformer-based Semantic Filter for Few-Shot Learning</papertitle>
              </a>
              <br>
              <a href=""> Jinxiang Lai</a>,
              <a href=""> Siqian Yang</a>,
              <strong>Wenlong Liu</strong>,
              <a href=""> Yi Zeng</a>,
              <a href=""> Zhongyi Huang</a>,
              <br>
              <!-- <em>NeurIPS</em>, 2023 -->
              Conference on European Conference on Computer Vision (<strong>ECCV</strong>), 2022
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/pdf/2211.00868">arXiv</a> /
                <a href="">code</a>
              </div>
            </td>
          </tr>
          
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Selected Awards</heading>
            <p>
              <b>Double Winner in CVPR 2022 CLVision Challenge Track 2 and Track 3:</b>
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/pdf/2209.03603">nVFNet-RDC: Replay and Non-Local Distillation Collaboration for Continual Object Detection.</a>
              </div>
            </p>
          </td>
        </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Professional Services</heading>
            <p>
              <b>Conference Reviewer:</b>
              <li style="margin: 5px;" >
                European Conference on Computer Vision <b>(ECCV)</b>, 2022.
              </li>
              <li style="margin: 5px;" >
                IEEE Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b>, 2022.
              </li>
            </p>
          </td>
        </tr>
      
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Amazing template by <a href="https://jonbarron.info/">Jon Barron</a>. Big thanks!
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

</html>
